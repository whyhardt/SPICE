{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 - SPICE modules as hardcoded equations\n",
    "\n",
    "In this tutorial, you will learn how to add hardcoded equations as SPICE modules.\n",
    "\n",
    "In the previous tutorial, the equation for `x_reward_value_chosen` resembled the reward-prediction error.\n",
    "\n",
    "The reward-prediction error is fairly well understood in cognitive science - also thanks to neuroimaging studies, which underlie the assumption about the classic mathematical form.\n",
    "\n",
    "Therefore, the possible novel findings are quite limited here.\n",
    "\n",
    "The learning rate on the other hand is in many studies assumed to be a constant and is also embedded in the SPICE-discovered equation as a fixed parameter. \n",
    "It can of course model asymmetric learning rates for rewards and penalties but is not adapting dynamically.\n",
    "\n",
    "So what if we would like to find a dynamical equation which describes how the learning rate could possibly change with respect to rewards or the current value `x_reward_value_chosen`? \n",
    "\n",
    "The problem here is that we cannot just add another RNN-module in-line with `x_reward_value_chosen` which processes the same inputs.\n",
    "\n",
    "This would result in two modules which would not have unique solutions.\n",
    "\n",
    "Therefore, we have to disentangle here by adding knowledge about `x_reward_value_chosen` in the form of the classic reward-prediction error.\n",
    "\n",
    "That way we are ending up with an additional RNN-module in the form of a hard-coded equation, and we have to expand the memory state by one variable which is the learning rate `x_learning_rate_reward`. \n",
    "\n",
    "Additionally, we are going to set the initial values for all memory state variables.\n",
    "\n",
    "In the next section you will learn how to add hard-coded equations to the RNN.\n",
    "\n",
    "As usual, we first start with the precoded model and then show how to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data generation\n",
    "\n",
    "Again, we simulate another synthetic dataset from a Q-learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.resources.bandits import BanditsDrift, AgentQ, create_dataset\n",
    "\n",
    "# Set up the environment\n",
    "n_actions = 2\n",
    "sigma = 0.2\n",
    "\n",
    "environment = BanditsDrift(sigma=sigma, n_actions=n_actions)\n",
    "\n",
    "# Set up the agent\n",
    "agent = AgentQ(\n",
    "    n_actions=n_actions,\n",
    "    alpha_reward=0.3,\n",
    "    forget_rate=0.2,\n",
    ")\n",
    "\n",
    "# Create the dataset\n",
    "n_trials = 100\n",
    "n_sessions = 100\n",
    "\n",
    "dataset, _, _ = create_dataset(\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    n_trials=n_trials,\n",
    "    n_sessions=n_sessions,\n",
    ")\n",
    "\n",
    "# set all participant ids to 0 since this dataset was generated only by one parameterization\n",
    "dataset.xs[..., -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using the precoded model with hardcoded equation\n",
    "\n",
    "We first load the precoded model, which has the reward prediction error equation hardcoded as one of its modules. Additionally, it has an additional RNN module for the learning rate.\n",
    "\n",
    "*Note: Only SINDy-fitted modules are printed when calling `SpiceEstimator.print_spice_models()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceEstimator\n",
    "from spice.precoded import LearningRateRNN, LEARNING_RATE_RNN_CONFIG\n",
    "\n",
    "spice_estimator = SpiceEstimator(\n",
    "    rnn_class=LearningRateRNN,\n",
    "    spice_config=LEARNING_RATE_RNN_CONFIG,\n",
    "    learning_rate=1e-2,\n",
    "    epochs=1024,\n",
    ")\n",
    "\n",
    "spice_estimator.fit(dataset.xs, dataset.ys)\n",
    "\n",
    "spice_estimator.print_spice_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get analysis plot\n",
    "agents = {'groundtruth': agent, 'rnn': spice_estimator.rnn_agent, 'spice': spice_estimator.spice_agent}\n",
    "\n",
    "fig, axs = plot_session(agents, dataset.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing the RNN as a custom module\n",
    "\n",
    "Now, we are going to implement the RNN that discovers RW model with a forgetting mechanism. This RNN will update the values of both the chosen option, as in the previous tutorial, and has an additional module for the not chosen option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.resources.rnn import BaseRNN\n",
    "from spice.estimator import SpiceConfig\n",
    "\n",
    "CUSTOM_RNN_CONFIG = SpiceConfig(\n",
    "    rnn_modules=['x_learning_rate_reward', 'x_value_reward_not_chosen'],\n",
    "    \n",
    "    control_parameters=['c_action', 'c_reward', 'c_value_reward'],\n",
    "\n",
    "    library_setup={\n",
    "        'x_learning_rate_reward': ['c_reward', 'c_value_reward'],\n",
    "        'x_value_reward_not_chosen': [],\n",
    "    },\n",
    "\n",
    "    filter_setup={\n",
    "        'x_learning_rate_reward': ['c_action', 1, True],\n",
    "        'x_value_reward_not_chosen': ['c_action', 0, True],\n",
    "    },\n",
    ")\n",
    "\n",
    "class CustomLearningRateRNN(BaseRNN):\n",
    "    \n",
    "    init_values = {\n",
    "            'x_value_reward': 0.5,\n",
    "            'x_learning_rate_reward': 0,  # NOTE: 1st major change -> extended memory state with learning rate\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CustomLearningRateRNN, self).__init__(n_actions=n_actions)\n",
    "        \n",
    "        # set up the submodules\n",
    "        self.submodules_rnn['x_learning_rate_reward'] = self.setup_module(input_size=2)  # NOTE: 2nd major change -> Replaced the module 'x_value_reward_chosen' with the learning rate module 'x_learning_rate_reward'\n",
    "        self.submodules_rnn['x_value_reward_not_chosen'] = self.setup_module(input_size=0)\n",
    "\n",
    "        # set up hard-coded equations\n",
    "        # add here a RNN-module in the form of an hard-coded equation to compute the update for the chosen reward-based value\n",
    "        self.submodules_eq['x_value_reward_chosen'] = lambda value, inputs: value + inputs[..., 1] * (inputs[..., 0] - value)  # NOTE: 3rd major change -> Added a hard-coded equation for the replaced module 'x_value_reward_chosen', i.e. the reward-prediction-error\n",
    "\n",
    "        # NOTE: 4th major change -> added a learnable scaling factor (i.e. inverse noise temperature) for 'x_value_reward'\n",
    "        self.betas['x_value_reward'] = self.setup_constant()\n",
    "        \n",
    "    def forward(self, inputs, prev_state=None, batch_first=False):\n",
    "        \"\"\"Forward pass of the RNN\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): includes all necessary inputs (action, reward, participant id) to the RNN to let it compute the next action\n",
    "            prev_state (Tuple[torch.Tensor], optional): That's the previous memory state of the RNN containing the reward-based value. Defaults to None.\n",
    "            batch_first (bool, optional): Indicates whether the first dimension of inputs is batch (True) or timesteps (False). Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, we have to initialize all the inputs and outputs (i.e. logits)\n",
    "        inputs, ids, logits, timesteps = self.init_forward_pass(inputs, prev_state, batch_first)\n",
    "        actions, rewards, _, _ = inputs\n",
    "                \n",
    "        for timestep, action, reward in zip(timesteps, actions, rewards):\n",
    "            \n",
    "            # record the inputs for training SINDy later on\n",
    "            self.record_signal('c_action', action)\n",
    "            self.record_signal('c_reward', reward)\n",
    "            self.record_signal('c_value_reward', self.state['x_value_reward'])\n",
    "            self.record_signal('x_value_reward_not_chosen', self.state['x_value_reward'])\n",
    "            self.record_signal('x_learning_rate_reward', self.state['x_learning_rate_reward'])\n",
    "            \n",
    "            # Let's compute the learning rate dynamically\n",
    "            # Now we have to use a sigmoid activation function on the output learning rate to constrain it to a value range of (0, 1)\n",
    "            # this is necessary for two reasons:\n",
    "            #   1. Preventing exploding gradients\n",
    "            #   2. Remember the found equation for 'x_value_reward_chosen' from before: \n",
    "            #       The learning rate was scaled according to the magnitudes of the reward and the actual value \n",
    "            #       e.g. for the reward: alpha*beta -> alpha * beta = 0.3 * 3 = 0.9 and for the reward-based value: 1-alpha = 1 - 0.3 = 0.7\n",
    "            #       The hard-coded equation for the reward-prediction error does not permit this flexibility. \n",
    "            #       But we can circumvein this by applying the sigmoid activation to the learning rate to staying conform with the reward-prediction error\n",
    "            #       and later applying the inverse noise temperature (i.e. trainable parameter) to the updated value \n",
    "            learning_rate_reward = self.call_module(\n",
    "                key_module='x_learning_rate_reward',\n",
    "                key_state='x_learning_rate_reward',\n",
    "                action=action,\n",
    "                inputs=(reward, self.state['x_value_reward']),\n",
    "                activation_rnn=torch.nn.functional.sigmoid,\n",
    "            )\n",
    "            \n",
    "            # Let's perform the belief update for the reward-based value of the chosen option            \n",
    "            next_value_reward_chosen = self.call_module(\n",
    "                key_module='x_value_reward_chosen',\n",
    "                key_state='x_value_reward',\n",
    "                action=action,\n",
    "                inputs=(reward, learning_rate_reward),\n",
    "                )\n",
    "            \n",
    "            # Update of the not-chosen reward-based value\n",
    "            next_value_reward_not_chosen = self.call_module(\n",
    "                key_module='x_value_reward_not_chosen',\n",
    "                key_state='x_value_reward',\n",
    "                action=1-action,\n",
    "                inputs=None,\n",
    "                )\n",
    "            \n",
    "            # updating the memory state\n",
    "            self.state['x_learning_rate_reward'] = learning_rate_reward\n",
    "            self.state['x_value_reward'] = next_value_reward_chosen + next_value_reward_not_chosen\n",
    "            \n",
    "            # Now keep track of the logit in the output array\n",
    "            logits[timestep] = self.state['x_value_reward'] * self.betas['x_value_reward']()\n",
    "        \n",
    "        # post-process the forward pass; give here as inputs the logits, batch_first and all values from the memory state\n",
    "        logits = self.post_forward_pass(logits, batch_first)\n",
    "        \n",
    "        return logits, self.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceEstimator\n",
    "\n",
    "\n",
    "spice_estimator = SpiceEstimator(\n",
    "    rnn_class=CustomLearningRateRNN,\n",
    "    spice_config=CUSTOM_RNN_CONFIG,\n",
    "    learning_rate=1e-2,\n",
    "    epochs=1024,\n",
    ")\n",
    "\n",
    "spice_estimator.fit(dataset.xs, dataset.ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "\n",
    "# get analysis plot\n",
    "agents = {'groundtruth': agent, 'rnn': spice_estimator.rnn_agent, 'spice': spice_estimator.spice_agent}\n",
    "\n",
    "fig, axs = plot_session(agents, dataset.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are making use of the established theories about the reward-prediction error in human reinforcement learning, we were able to to shift our focus on a less understood area i.e. the learning rate. In this example, the synthetic participant does not exhibit real dynamics in the sense of $\\alpha_{t+1} = f(\\alpha_{t})$, but the network is in principle equipped to learn also such a mechanism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
