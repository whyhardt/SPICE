{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4 - Tackling Individual Differences with Participant Embeddings\n",
    "\n",
    "By talking about individual differences, cognitive scientists are referring to individual notions in exhibited behavior, e.g. different learning rates.\n",
    "\n",
    "As a simplified case, we are going to collect data from two participants with two individual learning rates. Then, we try to recover them once with the last RNN (learning rate with hardcoded equations for the reward prediction error, precoded) and a version which implements a participant-embedding layer. As usual, we first use the precoded model and then show how to implement it.\n",
    "\n",
    "<!-- \n",
    "But this thought can go even deeper if we not only consider different parameterizations for each participant but also different model architectures for each participant!\n",
    "\n",
    "Think of the following scenario: \n",
    "\n",
    "One participant exhibits only goal-directed behavior (i.e. making choices based only on the reward-based values) while another participant shows an additional positivity bias (i.e. increasing the learning rate for positive outcomes while the value is low and a decreasing the learning rate for negative outcomes while the value is high). Such a positivity bias can help to account for reward volatility, by preventing to drastic value updates in the case of an unexpected penalty ($reward = 0$). Additionally, it can also induce goal-directed exploration by making more drastic updates in the case of an unexpected reward.  -->\n",
    "\n",
    "<!-- But that's enough theory for now. Let's get our hands dirty again! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data generation\n",
    "\n",
    "First of all we have to generate a dataset with multiple participants. Let's start with two different ones.\n",
    "\n",
    "We are going generate half the dataset with participant #1 and the other half with participant #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.resources.bandits import BanditsDrift, AgentQ, create_dataset\n",
    "from spice.resources.rnn_utils import DatasetRNN\n",
    "\n",
    "# Set up the environment\n",
    "n_actions = 2\n",
    "sigma = 0.2\n",
    "\n",
    "environment = BanditsDrift(sigma=sigma, n_actions=n_actions)\n",
    "\n",
    "# Set up the agents\n",
    "# participant #1\n",
    "agent_1 = AgentQ(\n",
    "    n_actions=n_actions,\n",
    "    alpha_reward=0.8,  # Higher learning rate in participant #1\n",
    "    forget_rate=0.2, # Forgetting mechanism in participant #1\n",
    ")\n",
    "\n",
    "# participant #2\n",
    "agent_2 = AgentQ(\n",
    "    n_actions=n_actions,\n",
    "    alpha_reward=0.2,  # Lower learning rate in participant #2\n",
    ")\n",
    "\n",
    "# Create the dataset\n",
    "n_trials_per_participant = 100\n",
    "n_sessions_per_participant = 50\n",
    "\n",
    "dataset_1, _, _ = create_dataset(\n",
    "    agent=agent_1,\n",
    "    environment=environment,\n",
    "    n_trials=n_trials_per_participant,\n",
    "    n_sessions=n_sessions_per_participant,  # Generate half the dataset with participant #1\n",
    ")\n",
    "\n",
    "# change the participant id of all sessions to 0\n",
    "dataset_1.xs[..., -1] = 0\n",
    "\n",
    "dataset_2, _, _ = create_dataset(\n",
    "    agent=agent_2,\n",
    "    environment=environment,\n",
    "    n_trials=n_trials_per_participant,\n",
    "    n_sessions=n_sessions_per_participant,  # Generate the other half with participant #2\n",
    ")\n",
    "# Set the participant id in the second dataset to 1 in order to mark the different participants\n",
    "dataset_2.xs[..., -1] = 1\n",
    "\n",
    "# Combine the two datasets\n",
    "dataset_xs = torch.cat((dataset_1.xs, dataset_2.xs))\n",
    "dataset_ys = torch.cat((dataset_1.ys, dataset_2.ys))\n",
    "dataset = DatasetRNN(dataset_xs, dataset_ys)\n",
    "n_participants = len(dataset.xs[..., -1].unique())\n",
    "\n",
    "print(f'Shape of the new dataset: {dataset.xs.shape}')\n",
    "print(f'Number of participants: {n_participants}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how different these two participants behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "agents = {'groundtruth': agent_1, 'benchmark': agent_2}\n",
    "fig, axs = plot_session(agents, dataset.xs[0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the previous model without participant information\n",
    "\n",
    "Let's inspect first how our last RNN (precoded `LearningRateRNN`) would perform in such a situation to verify the need for tackling individual differences with a participant-embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceEstimator\n",
    "from spice.precoded import LearningRateRNN, LEARNING_RATE_RNN_CONFIG\n",
    "\n",
    "spice_estimator = SpiceEstimator(\n",
    "    rnn_class=LearningRateRNN,\n",
    "    spice_config=LEARNING_RATE_RNN_CONFIG,\n",
    "    learning_rate=1e-2,\n",
    "    epochs=1024,\n",
    ")\n",
    "\n",
    "spice_estimator.fit(dataset.xs, dataset.ys)\n",
    "\n",
    "for participant in range(2):\n",
    "    print(f\"\\nDiscovered SPICE model for participant {participant}:\")\n",
    "    spice_estimator.print_spice_model(participant_id=participant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the predictions for participant #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get analysis plot\n",
    "agents = {'groundtruth': agent_1, 'rnn': spice_estimator.rnn_agent, 'spice': spice_estimator.spice_agent}\n",
    "fig, axs = plot_session(agents, dataset_1.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for participant #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = {'groundtruth': agent_2, 'rnn': spice_estimator.rnn_agent, 'spice': spice_estimator.spice_agent}\n",
    "fig, axs = plot_session(agents, dataset_2.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit across both participants is  worse compared to the other use-cases, where we had only one participant to fit. \n",
    "\n",
    "Therefore, we can definitely see the need for modeling individual differences - already in this case. The need is even stronger for real datasets with dozens of participants.  \n",
    "\n",
    "Let's implement now a RNN which can be trained on the whole dataset but account for the differences in the individuals' behavior.\n",
    "\n",
    "For that we are going to add a participant-embedding layer. It works similar to e.g. word-embeddings which translate a one-hot encoded vector into a dense representation in a multidimensional space.\n",
    "\n",
    "The one-hot encoded vector corresponds to e.g. a dictionary marking the index of a certain word. After translating this word into the multidimensional space, researchers can do inference on word similarity or try to interpret the representation and extract characteristics from it.\n",
    "\n",
    "We are making use of the embedding layer by passing a unique participant id into it and retrieve a dense representation of this participant. Afterwards we can feed this representation to the RNN-modules or compute individual noise temperatures.\n",
    "\n",
    "That way the RNN-modules can learn *(theoretically)* any type of cognitive mechanism present in the data across all participants and the participant-embedding can give the individual notions to them by e.g. switching mechanisms on and off and parameterizing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Incorporating Participant Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the precoded model with support for participant embeddings, before implementing it ourselves.\n",
    "\n",
    "As you can imagine the individual differences can be quite subtle. Unlike big architectural differences, individual differences have to be fitted very cautiously, i.e. in the machine learning context with a very low learning rate.\n",
    "\n",
    "Low learning rates have a couple of disadvantages, i.e. (1) they take very long to converge, and (2) they can get stuck easily in local minima. \n",
    "\n",
    "These are the reasons why we should use a learning rate scheduler in this scenario. The implemented learning rate scheduler can be activated in the fitting method by setting `scheduler=True`. This way we are making use of a learning rate scheduler which adjusts the learning rate dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceEstimator\n",
    "from spice.precoded import ParticipantEmbeddingRNN, LEARNING_RATE_RNN_CONFIG\n",
    "\n",
    "# Get the number of participants from data\n",
    "participant_ids = dataset.xs[..., -1].unique()\n",
    "n_participants = len(participant_ids)\n",
    "\n",
    "spice_estimator_participant_embedding = SpiceEstimator(\n",
    "    rnn_class=ParticipantEmbeddingRNN,\n",
    "    spice_config=LEARNING_RATE_RNN_CONFIG,  # Same config as before\n",
    "    learning_rate=1e-2,\n",
    "    epochs=1024,\n",
    "    n_participants=n_participants,\n",
    ")\n",
    "\n",
    "spice_estimator_participant_embedding.fit(dataset.xs, dataset.ys)\n",
    "\n",
    "for participant in range(2):\n",
    "    print(f\"\\nDiscovered SPICE model for participant {participant}:\")\n",
    "    spice_estimator_participant_embedding.print_spice_model(participant_id=participant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "\n",
    "# get analysis plot\n",
    "agents = {'groundtruth': agent_1, 'rnn': spice_estimator_participant_embedding.rnn_agent, 'spice': spice_estimator_participant_embedding.spice_agent}\n",
    "fig, axs = plot_session(agents, dataset_1.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "\n",
    "# get analysis plot\n",
    "agents = {'groundtruth': agent_2, 'rnn': spice_estimator_participant_embedding.rnn_agent, 'spice': spice_estimator_participant_embedding.spice_agent}\n",
    "fig, axs = plot_session(agents, dataset_2.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the discovered SPICE models for each participant, we were not only able to recover the difference in learning rates but also the structural difference in terms of the forgetting mechanism.\n",
    "\n",
    "This ability makes SPICE exceptionally powerful because we can now discover individual differences not only in terms of parameters but also in terms of model architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing the RNN as a custom module\n",
    "\n",
    "Now, we are going to implement the RNN with participant embeddings. This will be the same as the learning rate RNN implemented in the previous tutorial, with an additional embedding layer for participant IDs.\n",
    "\n",
    "The structure of this RNN is shown in the following figure:\n",
    "\n",
    "![](../figures/spice_rnn_participant_embedding.png)\n",
    "\n",
    "As you can see we are going to add one module which won't be an RNN-module but instead a hard-coded equation. The learning rate on the other hand will be updated via an additional RNN-module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.resources.rnn import BaseRNN\n",
    "from spice.estimator import SpiceConfig\n",
    "\n",
    "\n",
    "# Same config as before\n",
    "CUSTOM_RNN_CONFIG = SpiceConfig(\n",
    "    rnn_modules=['x_learning_rate_reward', 'x_value_reward_not_chosen'],\n",
    "    \n",
    "    control_parameters=['c_action', 'c_reward', 'c_value_reward'],\n",
    "\n",
    "    library_setup={\n",
    "        'x_learning_rate_reward': ['c_reward', 'c_value_reward'],\n",
    "        'x_value_reward_not_chosen': [],\n",
    "    },\n",
    "\n",
    "    filter_setup={\n",
    "        'x_learning_rate_reward': ['c_action', 1, True],\n",
    "        'x_value_reward_not_chosen': ['c_action', 0, True],\n",
    "    },\n",
    ")\n",
    "\n",
    "class CustomRNN(BaseRNN):\n",
    "    \n",
    "    init_values = {\n",
    "            'x_value_reward': 0.5,\n",
    "            'x_learning_rate_reward': 0,\n",
    "        }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        # add an additional inputs to set the number of participants in your data\n",
    "        n_participants,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        # Add here the embedding size as an additional argument\n",
    "        super(CustomRNN, self).__init__(n_actions=n_actions, embedding_size=4)\n",
    "        \n",
    "        # Additional embedding layer for participant information\n",
    "        # NOTE: In realistic setups embedding_size=32 and additional dropout=0.5 (can be passed as an kwarg to setup_embedding) have proven to be useful\n",
    "        self.participant_embedding = self.setup_embedding(num_embeddings=n_participants, embedding_size=self.embedding_size, dropout=0.)\n",
    "        \n",
    "        # Add a scaling factor (inverse noise temperature) for each participant.\n",
    "        self.betas['x_value_reward'] = self.setup_constant(embedding_size=self.embedding_size)\n",
    "        \n",
    "        # and here we specify the general architecture\n",
    "        # add to the input_size the embedding_size as well because we are going to pass the participant-embedding to the RNN-modules\n",
    "        # set up the submodules\n",
    "        self.submodules_rnn['x_learning_rate_reward'] = self.setup_module(input_size=2+self.embedding_size)\n",
    "        self.submodules_rnn['x_value_reward_not_chosen'] = self.setup_module(input_size=0+self.embedding_size)\n",
    "        \n",
    "        # set up hard-coded equations\n",
    "        # add here a RNN-module in the form of an hard-coded equation to compute the update for the chosen reward-based value\n",
    "        self.submodules_eq['x_value_reward_chosen'] = lambda value, inputs: value + inputs[..., 1] * (inputs[..., 0] - value)\n",
    "        \n",
    "    def forward(self, inputs, prev_state=None, batch_first=False):\n",
    "        \"\"\"Forward pass of the RNN\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): includes all necessary inputs (action, reward, participant id) to the RNN to let it compute the next action\n",
    "            prev_state (Tuple[torch.Tensor], optional): That's the previous memory state of the RNN containing the reward-based value. Defaults to None.\n",
    "            batch_first (bool, optional): Indicates whether the first dimension of inputs is batch (True) or timesteps (False). Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, we have to initialize all the inputs and outputs (i.e. logits)\n",
    "        inputs, ids, logits, timesteps = self.init_forward_pass(inputs, prev_state, batch_first)\n",
    "        actions, rewards, _, _ = inputs\n",
    "        participant_id, _ = ids\n",
    "\n",
    "        # Here we compute now the participant embeddings for each entry in the batch\n",
    "        participant_embedding = self.participant_embedding(participant_id[:, 0].int())\n",
    "        \n",
    "        for timestep, action, reward in zip(timesteps, actions, rewards):\n",
    "            \n",
    "            # record the inputs for training SINDy later on\n",
    "            self.record_signal('c_action', action)\n",
    "            self.record_signal('c_reward', reward)\n",
    "            self.record_signal('c_value_reward', self.state['x_value_reward'])\n",
    "            self.record_signal('x_learning_rate_reward', self.state['x_learning_rate_reward'])\n",
    "            self.record_signal('x_value_reward_not_chosen', self.state['x_value_reward'])\n",
    "            \n",
    "            learning_rate_reward = self.call_module(\n",
    "                key_module='x_learning_rate_reward',\n",
    "                key_state='x_learning_rate_reward',\n",
    "                action=action,\n",
    "                inputs=(reward, self.state['x_value_reward']),\n",
    "                # add participant-embedding (for RNN-modules) and participant-index (later for SINDy-modules) \n",
    "                participant_embedding=participant_embedding,\n",
    "                participant_index=participant_id,\n",
    "                activation_rnn=torch.nn.functional.sigmoid,\n",
    "            )\n",
    "            \n",
    "            # Let's perform the belief update for the reward-based value of the chosen option            \n",
    "            next_value_reward_chosen = self.call_module(\n",
    "                key_module='x_value_reward_chosen',\n",
    "                key_state='x_value_reward',\n",
    "                action=action,\n",
    "                inputs=(reward, learning_rate_reward),\n",
    "                # add participant-embedding (for RNN-modules) and participant-index (later for SINDy-modules) \n",
    "                participant_embedding=participant_embedding,\n",
    "                participant_index=participant_id,\n",
    "                )\n",
    "            \n",
    "            # Update of the not-chosen reward-based value\n",
    "            next_value_reward_not_chosen = self.call_module(\n",
    "                key_module='x_value_reward_not_chosen',\n",
    "                key_state='x_value_reward',\n",
    "                action=1-action,\n",
    "                inputs=None,\n",
    "                # add participant-embedding (for RNN-modules) and participant-index (later for SINDy-modules) \n",
    "                participant_embedding=participant_embedding,\n",
    "                participant_index=participant_id,\n",
    "                )\n",
    "            \n",
    "            self.state['x_value_reward'] = next_value_reward_chosen + next_value_reward_not_chosen\n",
    "            self.state['x_learning_rate_reward'] = learning_rate_reward\n",
    "            \n",
    "            # Now keep track of the logit in the output array\n",
    "            logits[timestep] = self.state['x_value_reward'] * self.betas['x_value_reward'](participant_embedding)\n",
    "        \n",
    "        # post-process the forward pass; give here as inputs the logits, batch_first and all values from the memory state\n",
    "        logits = self.post_forward_pass(logits, batch_first)\n",
    "        \n",
    "        return logits, self.get_state()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceEstimator\n",
    "\n",
    "spice_estimator_custom = SpiceEstimator(\n",
    "    rnn_class=CustomRNN,\n",
    "    spice_config=CUSTOM_RNN_CONFIG,\n",
    "    learning_rate=1e-2,\n",
    "    epochs=1024,\n",
    "    n_participants=n_participants,\n",
    ")\n",
    "\n",
    "spice_estimator_custom.fit(dataset.xs, dataset.ys)\n",
    "\n",
    "for participant in range(2):\n",
    "    print(f\"\\nDiscovered SPICE model for participant {participant}:\")\n",
    "    spice_estimator_custom.print_spice_model(participant_id=participant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "\n",
    "# get analysis plot\n",
    "agents = {'groundtruth': agent_1, 'rnn': spice_estimator_custom.rnn_agent, 'spice': spice_estimator_custom.spice_agent}\n",
    "\n",
    "fig, axs = plot_session(agents, dataset_1.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "\n",
    "# get analysis plot\n",
    "agents = {'groundtruth': agent_2, 'rnn': spice_estimator_custom.rnn_agent, 'spice': spice_estimator_custom.spice_agent}\n",
    "\n",
    "fig, axs = plot_session(agents, dataset_2.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling empirical data\n",
    "\n",
    "When it comes to handling empirical data from experiments, observed behavior becomes much more complex and noisier at the same time, we are oftentimes handling only one session per participant, and the individual differences can be much stronger. All these circumstances make model fitting very difficult. \n",
    "\n",
    "In this context four measures proved to be very helpful:\n",
    "\n",
    "1. Regularization via dropout: You can add the kwarg `dropout=0.5` to `setup_embedding` to increase generalizability\n",
    "2. Regulization via l2-weight-decay: We trained here per default with `l2_weight_decay=1e-4`. We could increase this parameter in order to penalize the RNN for overfitting on noisy or too complex behavior. That way we are making sure that the fitted cognitive dynamics are recoverable by the default polynomial library.\n",
    "3. Use a learning rate scheduler: You can activate a cyclic learning rate scheduler with warm restarts by passing the kwarg `scheduler=True` to `SpiceEstimaror`. LR scheduler proved to be helpful when it comes to fitting very subtle individual differences by decreasing the learning rate gradually. In order to not get stuck in local minima, the warm restarts are increasing the lr back to its default value.\n",
    "4. When using a learning rate scheduler you have to make sure to increase the number of epochs. Until now we trained all models with 2^10=1024 epochs. For real data it basically cannot get enough. But 2^13=8192 epochs proved to be sufficient to get good results.\n",
    "\n",
    "Keep all these steps in mind when handling empirical data and you are good to go and apply SPICE to your very own project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
