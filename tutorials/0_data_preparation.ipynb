{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/whyhardt/SPICE/blob/main/tutorials/0_data_preparation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation in SPICE\n",
    "\n",
    "This tutorial covers how to prepare and manipulate datasets for use with SPICE. Weâ€™ll explore the core data structures and utilities that SPICE provides for handling experimental data. We'll cover:\n",
    "\n",
    "1. Preparing raw experimental data\n",
    "2. Converting experimental data to SPICE format\n",
    "3. Creating synthetic datasets\n",
    "4. DatasetRNN class\n",
    "5. Splitting data along time and session dimensions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this tutorial, make sure you have:\n",
    "- SPICE and required dependencies (pandas, numpy, etc.) installed\n",
    "- Basic understanding of reinforcement learning data\n",
    "- Basic understanding of Python and PyTorch data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below if you are using Google Colab\n",
    "\n",
    "#!pip uninstall -y numpy pandas\n",
    "#!pip install numpy==1.26.4 pandas==2.2.2\n",
    "#!pip install autospice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T07:37:51.432013Z",
     "start_time": "2025-08-20T07:37:42.967958Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spice.resources.bandits import create_dataset, BanditsDrift, get_update_dynamics\n",
    "from spice.resources.rnn_utils import DatasetRNN\n",
    "from spice.utils.plotting import plot_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing Raw Experimental Data\n",
    "\n",
    "SPICE expects data in a specific format for training and analysis. The basic requirements are:\n",
    "\n",
    "- Data should be in CSV format\n",
    "- Column names can be customized by setting `df_participant_id`, `df_block`, `df_experiment_id`, `df_choice` and `df_reward`.\n",
    "- Additional inputs can be given as a list of strings (`additional_inputs`) corresponding to column names\n",
    "- Required columns:\n",
    "  - `df_participant_id (default: 'session')`: Unique identifier for each experimental session/participant\n",
    "  - `df_choice (default: 'choice')`: The action taken by the participant (0-indexed)\n",
    "  - `df_reward (default: 'reward')`: The reward received for the action\n",
    "\n",
    "Let's look at an example of properly formatted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T07:37:54.336392Z",
     "start_time": "2025-08-20T07:37:54.327807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data format:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>choice</th>\n",
       "      <th>reward</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  choice  reward   rt\n",
       "0        1       0       1  0.5\n",
       "1        1       1       0  0.6\n",
       "2        1       0       1  0.4\n",
       "3        2       1       0  0.7\n",
       "4        2       0       1  0.5\n",
       "5        2       1       0  0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a sample dataset\n",
    "sample_data = {\n",
    "    'session': [1, 1, 1, 2, 2, 2],\n",
    "    'choice': [0, 1, 0, 1, 0, 1],\n",
    "    'reward': [1, 0, 1, 0, 1, 0],\n",
    "    'rt': [0.5, 0.6, 0.4, 0.7, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(\"Sample data format:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save it as a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T07:37:55.966378Z",
     "start_time": "2025-08-20T07:37:55.960233Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Converting Experimental Data to SPICE Format\n",
    "\n",
    "SPICE provides a utility function `convert_dataset()` to transform experimental data given in a CSV file into the `DatasetRNN` format. This function handles various aspects of data preprocessing and normalization, with high flexibility for customization.\n",
    "\n",
    "If your .csv file is in the right format and adheres to default values, all you need to do is to provide the filename:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.convert_dataset import convert_dataset\n",
    "\n",
    "dataset, experiment_list, df, dynamics = convert_dataset(file='sample_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is also highly customable, with additional parameters to specify how the data should be processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.convert_dataset import convert_dataset\n",
    "\n",
    "dataset, experiment_list, df, dynamics = convert_dataset(\n",
    "    file=\"sample_data.csv\",           # Path to CSV file\n",
    "    device=None,                          # (Optional) PyTorch device\n",
    "    sequence_length=None,                 # (Optional) Fixed sequence length\n",
    "    df_participant_id='session',          # (Optional) Column name for participant/session ID\n",
    "    df_block='block',                     # (Optional) Column name for block information\n",
    "    df_experiment_id='experiment',        # (Optional) Column name for experiment ID\n",
    "    df_choice='choice',                   # (Optional) Column name for choices\n",
    "    df_reward='reward',                   # (Optional) Column name for rewards\n",
    "    additional_inputs=['context']         # (Optional) Additional input columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Synthetic Datasets\n",
    "\n",
    "SPICE provides utilities to create synthetic datasets for testing and validation. Here's how to create a synthetic dataset using a simple bandit task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T07:40:56.835717Z",
     "start_time": "2025-08-20T07:40:56.798201Z"
    }
   },
   "outputs": [],
   "source": [
    "from spice.resources.bandits import AgentQ\n",
    "\n",
    "# Create a simple Q-learning agent\n",
    "agent = AgentQ(\n",
    "    beta_reward=1.0,\n",
    "    alpha_reward=0.5,\n",
    "    alpha_penalty=0.5\n",
    ")\n",
    "\n",
    "# Create environment\n",
    "environment = BanditsDrift(sigma=0.2)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_sessions = 2\n",
    "n_trials = 10\n",
    "\n",
    "dataset, experiments, _ = create_dataset(\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    n_trials=n_trials,\n",
    "    n_sessions=n_sessions,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "synthetic_data = []\n",
    "for i in range(len(dataset)):\n",
    "    experiment = dataset.xs[i].numpy()\n",
    "    session_data = pd.DataFrame({\n",
    "        'session': [i] * n_trials,\n",
    "        'choice': np.argmax(experiment[:, :2], axis=1),\n",
    "        'reward': np.max(experiment[:, 2:4], axis=1)\n",
    "    })\n",
    "    synthetic_data.append(session_data)\n",
    "\n",
    "synthetic_df = pd.concat(synthetic_data, ignore_index=True)\n",
    "print(\"Synthetic dataset:\")\n",
    "display(synthetic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T07:41:00.106197Z",
     "start_time": "2025-08-20T07:41:00.103361Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.remove('sample_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The DatasetRNN Class\n",
    "\n",
    "The `DatasetRNN` class is the fundamental data structure in SPICE for handling experimental data. It inherits from PyTorch's `Dataset` class and is specifically designed for working with sequential data. When you call `convert_dataset()` or `create_dataset()`, it returns an instance of `DatasetRNN`. To manually create a `DatasetRNN`, you can use the following constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T07:41:49.341421Z",
     "start_time": "2025-08-20T07:41:49.337282Z"
    }
   },
   "outputs": [],
   "source": [
    "from spice.resources.rnn_utils import DatasetRNN\n",
    "import torch\n",
    "\n",
    "n_sessions = 10  # Number of sessions\n",
    "n_timesteps = 100  # Number of timesteps per session\n",
    "n_features = 5  # Number of input features\n",
    "n_actions = 3  # Number of possible actions\n",
    "\n",
    "# Example of creating a DatasetRNN instance\n",
    "xs = torch.zeros((n_sessions, n_timesteps, n_features))  # Input features\n",
    "ys = torch.zeros((n_sessions, n_timesteps, n_actions))   # Target actions\n",
    "dataset = DatasetRNN(\n",
    "    xs=xs,\n",
    "    ys=ys,\n",
    "    normalize_features=(0, 1),  # (Optional) Normalize specific feature dimensions\n",
    "    sequence_length=50,         # (Optional) Set fixed sequence length\n",
    "    stride=1,                   # (Optional) Stride for sequence creation\n",
    "    device='cpu'               # (Optional) Specify torch device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Features of DatasetRNN\n",
    "\n",
    "1. **Feature Normalization**:\n",
    "   - Specify which feature dimensions to normalize using `normalize_features`\n",
    "   - Normalization is performed per feature to range [0, 1]\n",
    "\n",
    "2. **Sequence Processing**:\n",
    "   - Optional fixed sequence length with `sequence_length`\n",
    "   - Configurable stride for sequence creation\n",
    "   - Automatic handling of variable-length sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Splitting Datasets\n",
    "\n",
    "SPICE provides two main methods for splitting datasets for training and testing:\n",
    "\n",
    "### Splitting Along Time Dimension\n",
    "\n",
    "This method splits each session's data into training and testing sets based on time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T07:41:52.234364Z",
     "start_time": "2025-08-20T07:41:52.191375Z"
    }
   },
   "outputs": [],
   "source": [
    "from spice.resources.rnn_utils import split_data_along_timedim\n",
    "\n",
    "# Split dataset with 80% of timesteps for training\n",
    "train_dataset, test_dataset = split_data_along_timedim(\n",
    "    dataset=dataset,\n",
    "    split_ratio=0.8,\n",
    "    device=torch.device('cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach:\n",
    "- Preserves session structure\n",
    "- Splits each session at the same relative timepoint\n",
    "- Handles variable-length sessions appropriately\n",
    "- Maintains padding (-1) for shorter sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Along Session Dimension\n",
    "\n",
    "For splitting based on participants/sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T07:41:54.034494Z",
     "start_time": "2025-08-20T07:41:54.030957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming dataset.xs and dataset.ys contain your data\n",
    "n_sessions = dataset.xs.shape[0]\n",
    "split_idx = int(0.8 * n_sessions)\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = DatasetRNN(\n",
    "    xs=dataset.xs[:split_idx],\n",
    "    ys=dataset.ys[:split_idx],\n",
    "    device=dataset.device\n",
    ")\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = DatasetRNN(\n",
    "    xs=dataset.xs[split_idx:],\n",
    "    ys=dataset.ys[split_idx:],\n",
    "    device=dataset.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach:\n",
    "- Keeps entire sessions together\n",
    "- Useful for testing generalization across participants\n",
    "- Maintains all temporal information within sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Always check for missing values in your raw data\n",
    "   - Ensure consistent data types across sessions\n",
    "   - Normalize features when appropriate\n",
    "   - Use sequence_length parameter for long sequences\n",
    "\n",
    "2. **Splitting Strategy**:\n",
    "   - Use time-based splits when testing temporal generalization\n",
    "   - Use session-based splits when testing participant generalization\n",
    "   - Consider your research question when choosing split method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Learn how to [train a basic Rescorla-Wagner model](rescorla_wagner.html)\n",
    "- Understand how to [incorporate custom mechanisms such as forgetting](rescorla_wagner_forgetting.html)\n",
    "- Explore [working with hardcoded equations](hardcoded_equations.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
