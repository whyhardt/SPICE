{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Adding a Forgetting Mechanism\n",
    "\n",
    "In this tutorial, you will learn how to use SPICE for learning forgetting mechanisms for the not chosen actions.\n",
    "\n",
    "This module incorporates an additional RNN module which dynamically handles the reward-based values for the not-chosen action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data generation\n",
    "First, we simulate a synthetic dataset from a Q-learning agent performing the two-armed bandit task, as in the previous tutorial. This time we specify a forgetting rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.resources.bandits import BanditsDrift, AgentQ, create_dataset\n",
    "\n",
    "# Set up the environment\n",
    "n_actions = 2\n",
    "sigma = 0.2\n",
    "\n",
    "environment = BanditsDrift(sigma=sigma, n_actions=n_actions)\n",
    "\n",
    "# Set up the agent\n",
    "agent = AgentQ(\n",
    "    n_actions=n_actions,\n",
    "    alpha_reward=0.3,\n",
    "    forget_rate=0.2,\n",
    ")\n",
    "\n",
    "# Create the dataset\n",
    "n_trials = 100\n",
    "n_sessions = 100\n",
    "\n",
    "dataset, _, _ = create_dataset(\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    n_trials=n_trials,\n",
    "    n_sessions=n_sessions,\n",
    ")\n",
    "\n",
    "# set all participant ids to 0 since this dataset was generated only by one parameterization\n",
    "dataset.xs[..., -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the precoded Rescorla-Wagner RNN with Forgetting Mechanism\n",
    "\n",
    "First we will use the precoded RNN with the forgetting mechanism. This is basically the Rescorla-Wagner RNN with an additional mechanism for the not chosen action. Later, you will see how to implement it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceEstimator\n",
    "from spice.precoded import ForgettingRNN, FORGETTING_RNN_CONFIG\n",
    "\n",
    "spice_estimator = SpiceEstimator(\n",
    "    rnn_class=ForgettingRNN,\n",
    "    spice_config=FORGETTING_RNN_CONFIG,\n",
    "    learning_rate=1e-2,\n",
    "    epochs=1024,\n",
    ")\n",
    "\n",
    "spice_estimator.fit(dataset.xs, dataset.ys)\n",
    "\n",
    "spice_estimator.print_spice_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model behaves with respect to our synthetic participant.\n",
    "\n",
    "In the following plot you can compare the action probabilities P(action), the Q-Value, the reward-based as well as the choice-based values.\n",
    "(You can ignore for now the learning rate $\\alpha$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get analysis plot\n",
    "agents = {'groundtruth': agent, 'rnn': spice_estimator.rnn_agent, 'spice': spice_estimator.spice_agent}\n",
    "\n",
    "fig, axs = plot_session(agents, dataset.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing the RNN as a custom module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to implement the RNN that discovers RW model with a forgetting mechanism. This RNN will update the values of both the chosen option, as in the previous tutorial, and has an additional module for the not chosen option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.resources.rnn import BaseRNN\n",
    "from spice.estimator import SpiceConfig\n",
    "    \n",
    "\n",
    "FORGETTING_RNN_CONFIG = SpiceConfig(\n",
    "    # Add already here the new module and update the library and filter setup.\n",
    "    rnn_modules=['x_value_reward_chosen', 'x_value_reward_not_chosen'],\n",
    "    \n",
    "    control_parameters=['c_action', 'c_reward'],\n",
    "\n",
    "    # The new module which handles the not-chosen value, does not need any additional inputs except for the value\n",
    "    library_setup={\n",
    "        'x_value_reward_chosen': ['c_reward'],\n",
    "        'x_value_reward_not_chosen': [],\n",
    "    },\n",
    "\n",
    "    # Further, the new module should be applied only to the not-chosen values\n",
    "    filter_setup={\n",
    "        'x_value_reward_chosen': ['c_action', 1, True],\n",
    "        'x_value_reward_not_chosen': ['c_action', 0, True],\n",
    "    },\n",
    ")\n",
    "\n",
    "class ForgettingRNN(BaseRNN):\n",
    "    \n",
    "    init_values = {\n",
    "        'x_value_reward': 0.5,\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(ForgettingRNN, self).__init__(n_actions=n_actions)\n",
    "        \n",
    "        # set up the submodules\n",
    "        self.submodules_rnn['x_value_reward_chosen'] = self.setup_module(input_size=1)\n",
    "        self.submodules_rnn['x_value_reward_not_chosen'] = self.setup_module(input_size=0)\n",
    "        \n",
    "    def forward(self, inputs, prev_state=None, batch_first=False):\n",
    "        \"\"\"Forward pass of the RNN\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): includes all necessary inputs (action, reward, participant id) to the RNN to let it compute the next action\n",
    "            prev_state (Tuple[torch.Tensor], optional): That's the previous memory state of the RNN containing the reward-based value. Defaults to None.\n",
    "            batch_first (bool, optional): Indicates whether the first dimension of inputs is batch (True) or timesteps (False). Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, we have to initialize all the inputs and outputs (i.e. logits)\n",
    "        inputs, ids, logits, timesteps = self.init_forward_pass(inputs, prev_state, batch_first)\n",
    "        actions, rewards, _, _ = inputs\n",
    "                \n",
    "        for timestep, action, reward in zip(timesteps, actions, rewards):\n",
    "            \n",
    "            # record the inputs for training SINDy later on\n",
    "            self.record_signal('c_action', action)\n",
    "            self.record_signal('c_reward', reward)\n",
    "            self.record_signal('x_value_reward_chosen', self.state['x_value_reward'])\n",
    "            self.record_signal('x_value_reward_not_chosen', self.state['x_value_reward'])\n",
    "            \n",
    "            # Let's perform the belief update for the reward-based value of the chosen option\n",
    "            next_value_reward_chosen = self.call_module(\n",
    "                key_module='x_value_reward_chosen',\n",
    "                key_state='x_value_reward',\n",
    "                action=action,\n",
    "                inputs=reward,\n",
    "                activation_rnn=torch.nn.functional.leaky_relu\n",
    "                )\n",
    "\n",
    "            # Now a RNN-module updates the not-chosen reward-based value instead of keeping it the same\n",
    "            next_value_reward_not_chosen = self.call_module(\n",
    "                key_module='x_value_reward_not_chosen',\n",
    "                key_state='x_value_reward',\n",
    "                action=1-action,\n",
    "                inputs=None,\n",
    "                )\n",
    "            \n",
    "            # keep track of the updated value in the memory state\n",
    "            self.state['x_value_reward'] = next_value_reward_chosen + next_value_reward_not_chosen\n",
    "            \n",
    "            # Now keep track of this value in the output array\n",
    "            logits[timestep] = self.state['x_value_reward']\n",
    "        \n",
    "        # post-process the forward pass; give here as inputs the logits, batch_first and all values from the memory state\n",
    "        logits = self.post_forward_pass(logits, batch_first)\n",
    "        \n",
    "        return logits, self.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we implemented our RNN, we can train it to see how well it fits the behavior of our synthetic participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceEstimator\n",
    "\n",
    "\n",
    "custom_spice_estimator = SpiceEstimator(\n",
    "    rnn_class=ForgettingRNN,\n",
    "    spice_config=FORGETTING_RNN_CONFIG,\n",
    "    learning_rate=1e-2,\n",
    "    epochs=1024,\n",
    ")\n",
    "\n",
    "custom_spice_estimator.fit(dataset.xs, dataset.ys)\n",
    "\n",
    "custom_spice_estimator.print_spice_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model behaves with respect to our synthetic participant.\n",
    "\n",
    "In the following plot you can compare the action probabilities P(action), the Q-Value, the reward-based as well as the choice-based values.\n",
    "(You can ignore for now the learning rate $\\alpha$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing and fitting the SINDy-agent, we can now finally inspect whether all these steps resulted in well identified cognitive dynamics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "\n",
    "# get analysis plot\n",
    "agents = {'groundtruth': agent, 'rnn': custom_spice_estimator.rnn_agent, 'spice': custom_spice_estimator.spice_agent}\n",
    "\n",
    "fig, axs = plot_session(agents, dataset.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the discovered cognitive features. As you can see, there is a value update for the chosen action, based on the current value (`x_value_reward_chosen`) and the current reward (`c_reward`), same as the previous simple Rescorla-Wagner model.\n",
    "\n",
    "Additionally, there is now a second mechanism, `x_value_reward_not_chosen` which gets updated based on the value of the non-chosen action, with a weight of 0.78. This is very close to the original data, considering we set a forgetting rate of 0.2, which means the values will be retained with a weight of 0.78."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_spice_estimator.print_spice_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
