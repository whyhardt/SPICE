{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5 - Implementing Weinhardt et al. (2024)\n",
    "\n",
    "In this section we are going to set up the RNN as implemented by *Weinhardt et al (2024)* (https://openreview.net/forum?id=x2WDZrpgmB).\n",
    "\n",
    "This RNN includes RNN-modules for approximating goal-directed behavior (`x_value_reward`) as well non-goal-directed behavior (`x_value_choice`).\n",
    "\n",
    "Our synthetic participant has the parameters `beta_choice` and `alpha_choice`, which are handling the non-goal-directed behavior in the form of a choice-perseverance bias.\n",
    "\n",
    "This choice-perseverance bias makes the participant prefer to repeat previously chosen actions by increasing `x_value_choice` if chosen previously and decreasing if not chosen anymore.\n",
    "\n",
    "For that we are going to follow these steps:\n",
    "\n",
    "1. add the new memory-state value `x_value_choice` in `init_values`\n",
    "2. add new RNN-modules to process `x_value_choice` -> `x_value_choice_chosen` and `x_value_choice_chosen`\n",
    "3. add a sigmoid activation to the new RNN-modules and a scaling factor for the new value to prevent exploding values\n",
    "4. adapt the forward-method\n",
    "\n",
    "    4.1 handle the calls of the new RNN-modules\n",
    "\n",
    "    4.2 add the updated value `x_value_choice` to the logit\n",
    "\n",
    "5. adjust the SINDy-configuration accordingly\n",
    "\n",
    "We are going to keep here the participant-embedding even though we are not going to simulate different agents. We just have to make sure to set the participant-IDs in the dataset to `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data generation\n",
    "\n",
    "First of all we have to generate a dataset with multiple participants. Let's start with two different ones.\n",
    "\n",
    "We are going generate half the dataset with participant #1 and the other half with participant #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.resources.bandits import BanditsDrift, AgentQ, create_dataset\n",
    "from spice.resources.rnn_utils import DatasetRNN\n",
    "\n",
    "# Set up the environment\n",
    "n_actions = 2\n",
    "sigma = 0.2\n",
    "\n",
    "environment = BanditsDrift(sigma=sigma, n_actions=n_actions)\n",
    "\n",
    "# Set up the agent\n",
    "agent = AgentQ(\n",
    "    n_actions=n_actions,\n",
    "    alpha_reward=0.3,\n",
    "    forget_rate=0.2,\n",
    "    beta_choice=1.,\n",
    "    alpha_choice=1.,\n",
    ")\n",
    "\n",
    "# Create the dataset\n",
    "n_trials = 100\n",
    "n_sessions = 100\n",
    "\n",
    "dataset, _, _ = create_dataset(\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    n_trials=n_trials,\n",
    "    n_sessions=n_sessions,\n",
    ")\n",
    "\n",
    "# set all participant ids to 0 since this dataset was generated only by one parameterization\n",
    "dataset.xs[..., -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the precoded model\n",
    "\n",
    "First we setup and train the precoded SPICE model and inspect its behavior, before implementing it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceEstimator\n",
    "from spice.precoded import Weinhardt2024RNN, WEINHARDT_2024_CONFIG\n",
    "\n",
    "spice_estimator = SpiceEstimator(\n",
    "    rnn_class=Weinhardt2024RNN,\n",
    "    spice_config=WEINHARDT_2024_CONFIG,\n",
    "    learning_rate=1e-2,\n",
    "    epochs=1024,\n",
    "    n_participants=1,\n",
    ")\n",
    "\n",
    "spice_estimator.fit(dataset.xs, dataset.ys)\n",
    "\n",
    "spice_estimator.print_spice_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "\n",
    "# Let's see how well the dynamics were fitted\n",
    "agents = {'groundtruth': agent, 'rnn': spice_estimator.rnn_agent, 'spice': spice_estimator.spice_agent}\n",
    "fig, axs = plot_session(agents, dataset.xs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing the RNN from Scratch\n",
    "\n",
    "Below is the actual implementation of the RNN model by Weinhardt et al. (2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceConfig\n",
    "from spice.resources.rnn import BaseRNN\n",
    "\n",
    "custom_config = SpiceConfig(\n",
    "    rnn_modules=['x_learning_rate_reward', 'x_value_reward_not_chosen', 'x_value_choice_chosen', 'x_value_choice_not_chosen'],\n",
    "    control_parameters=['c_action', 'c_reward', 'c_value_reward'],\n",
    "    # The new module which handles the not-chosen value, does not need any additional inputs except for the value\n",
    "    library_setup = {\n",
    "        # 'x_value_reward_chosen': ['c_reward'] -> Remove this one from the library as we are not going to identify the dynamics of a hard-coded equation\n",
    "        'x_learning_rate_reward': ['c_reward', 'c_value_reward'],\n",
    "        'x_value_reward_not_chosen': [],\n",
    "        'x_value_choice_chosen': [],\n",
    "        'x_value_choice_not_chosen': [],\n",
    "    },\n",
    "\n",
    "    # Further, the new module should be applied only to the not-chosen values\n",
    "    filter_setup = {\n",
    "        # 'x_value_reward_chosen': ['c_action', 1, True], -> Remove this one as well\n",
    "        'x_learning_rate_reward': ['c_action', 1, True],\n",
    "        'x_value_reward_not_chosen': ['c_action', 0, True],\n",
    "        'x_value_choice_chosen': ['c_action', 1, True],\n",
    "        'x_value_choice_not_chosen': ['c_action', 0, True],\n",
    "    }    \n",
    ")\n",
    "\n",
    "class CustomRNN(BaseRNN):\n",
    "\n",
    "    init_values = {\n",
    "            'x_value_reward': 0.5,\n",
    "            'x_value_choice': 0.,\n",
    "            'x_learning_rate_reward': 0.,\n",
    "        }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        n_participants,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        super(CustomRNN, self).__init__(n_actions=n_actions, embedding_size=8)\n",
    "        \n",
    "        # set up the participant-embedding layer\n",
    "        self.participant_embedding = self.setup_embedding(num_embeddings=n_participants, embedding_size=self.embedding_size)\n",
    "        \n",
    "        # scaling factor (inverse noise temperature) for each participant for the values which are handled by an hard-coded equation\n",
    "        self.betas['x_value_reward'] = self.setup_constant(embedding_size=self.embedding_size)\n",
    "        self.betas['x_value_choice'] = self.setup_constant(embedding_size=self.embedding_size)\n",
    "        \n",
    "        # set up the submodules\n",
    "        self.submodules_rnn['x_learning_rate_reward'] = self.setup_module(input_size=2+self.embedding_size)\n",
    "        self.submodules_rnn['x_value_reward_not_chosen'] = self.setup_module(input_size=0+self.embedding_size)\n",
    "        self.submodules_rnn['x_value_choice_chosen'] = self.setup_module(input_size=0+self.embedding_size)\n",
    "        self.submodules_rnn['x_value_choice_not_chosen'] = self.setup_module(input_size=0+self.embedding_size)\n",
    "        \n",
    "        # set up hard-coded equations\n",
    "        self.submodules_eq['x_value_reward_chosen'] = lambda value, inputs: value + inputs[..., 1] * (inputs[..., 0] - value)\n",
    "        \n",
    "    def forward(self, inputs, prev_state=None, batch_first=False):\n",
    "        \"\"\"Forward pass of the RNN\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): includes all necessary inputs (action, reward, participant id) to the RNN to let it compute the next action\n",
    "            prev_state (Tuple[torch.Tensor], optional): That's the previous memory state of the RNN containing the reward-based value. Defaults to None.\n",
    "            batch_first (bool, optional): Indicates whether the first dimension of inputs is batch (True) or timesteps (False). Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, we have to initialize all the inputs and outputs (i.e. logits)\n",
    "        inputs, ids, logits, timesteps = self.init_forward_pass(inputs, prev_state, batch_first)\n",
    "        actions, rewards, _, _ = inputs\n",
    "        participant_id, _ = ids\n",
    "        \n",
    "        # Here we compute now the participant embeddings for each entry in the batch\n",
    "        participant_embedding = self.participant_embedding(participant_id[:, 0].int())\n",
    "        \n",
    "        for timestep, action, reward in zip(timesteps, actions, rewards):\n",
    "            \n",
    "            # record the inputs for training SINDy later on\n",
    "            self.record_signal('c_action', action)\n",
    "            self.record_signal('c_reward', reward)\n",
    "            self.record_signal('c_value_reward', self.state['x_value_reward'])\n",
    "            self.record_signal('x_learning_rate_reward', self.state['x_learning_rate_reward'])\n",
    "            self.record_signal('x_value_reward_not_chosen', self.state['x_value_reward'])\n",
    "            self.record_signal('x_value_choice_chosen', self.state['x_value_choice'])\n",
    "            self.record_signal('x_value_choice_not_chosen', self.state['x_value_choice'])\n",
    "            \n",
    "            # updates for x_value_reward\n",
    "            learning_rate_reward = self.call_module(\n",
    "                key_module='x_learning_rate_reward',\n",
    "                key_state='x_learning_rate_reward',\n",
    "                action=action,\n",
    "                inputs=(reward, self.state['x_value_reward']),\n",
    "                participant_embedding=participant_embedding,\n",
    "                participant_index=participant_id,\n",
    "                activation_rnn=torch.nn.functional.sigmoid,\n",
    "            )\n",
    "            \n",
    "            next_value_reward_chosen = self.call_module(\n",
    "                key_module='x_value_reward_chosen',\n",
    "                key_state='x_value_reward',\n",
    "                action=action,\n",
    "                inputs=(reward, learning_rate_reward),\n",
    "                participant_embedding=participant_embedding,\n",
    "                participant_index=participant_id,\n",
    "                )\n",
    "            \n",
    "            next_value_reward_not_chosen = self.call_module(\n",
    "                key_module='x_value_reward_not_chosen',\n",
    "                key_state='x_value_reward',\n",
    "                action=1-action,\n",
    "                inputs=None,\n",
    "                participant_embedding=participant_embedding,\n",
    "                participant_index=participant_id,\n",
    "                )\n",
    "            \n",
    "            # updates for x_value_choice\n",
    "            next_value_choice_chosen = self.call_module(\n",
    "                key_module='x_value_choice_chosen',\n",
    "                key_state='x_value_choice',\n",
    "                action=action,\n",
    "                inputs=None,\n",
    "                participant_embedding=participant_embedding,\n",
    "                participant_index=participant_id,\n",
    "                activation_rnn=torch.nn.functional.sigmoid,\n",
    "                )\n",
    "            \n",
    "            next_value_choice_not_chosen = self.call_module(\n",
    "                key_module='x_value_choice_not_chosen',\n",
    "                key_state='x_value_choice',\n",
    "                action=1-action,\n",
    "                inputs=None,\n",
    "                participant_embedding=participant_embedding,\n",
    "                participant_index=participant_id,\n",
    "                activation_rnn=torch.nn.functional.sigmoid,\n",
    "                )\n",
    "            \n",
    "            # updating the memory state\n",
    "            self.state['x_learning_rate_reward'] = learning_rate_reward\n",
    "            self.state['x_value_reward'] = next_value_reward_chosen + next_value_reward_not_chosen\n",
    "            self.state['x_value_choice'] = next_value_choice_chosen + next_value_choice_not_chosen\n",
    "            \n",
    "            # Now keep track of the logit in the output array\n",
    "            logits[timestep] = self.state['x_value_reward'] * self.betas['x_value_reward'](participant_embedding) + self.state['x_value_choice'] * self.betas['x_value_choice'](participant_embedding)\n",
    "            \n",
    "        # post-process the forward pass; give here as inputs the logits, batch_first and all values from the memory state\n",
    "        logits = self.post_forward_pass(logits, batch_first)\n",
    "        \n",
    "        return logits, self.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceEstimator\n",
    "\n",
    "spice_estimator = SpiceEstimator(\n",
    "    rnn_class=CustomRNN,\n",
    "    spice_config=custom_config,\n",
    "    learning_rate=1e-2,\n",
    "    epochs=1024,\n",
    "    n_participants=1,\n",
    ")\n",
    "\n",
    "spice_estimator.fit(dataset.xs, dataset.ys)\n",
    "\n",
    "print(\"Discovered SPICE model:\")\n",
    "spice_estimator.print_spice_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's see how well the dynamics were fitted\n",
    "agents = {'groundtruth': agent, 'rnn': spice_estimator.rnn_agent, 'spice': spice_estimator.spice_agent}\n",
    "fig, axs = plot_session(agents, dataset.xs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
