{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Discovering the Rescorla-Wagner Model\n",
    "\n",
    "This tutorial teaches how to use SPICE to discover the Rescorla-Wagner model directly from data.\n",
    "\n",
    "First, we will train the precoded RNN already setup for the Rescorla-Wagner case. This will allow us to see the training process, its inputs and outputs and performance plots. Then, you will learn how to implement this model as a custom RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data generation\n",
    "First, we simulate a synthetic dataset from a Q-learning agent performing the two-armed bandit task.\n",
    "\n",
    "In such bandit tasks the participant has to choose between several options across many trials and receives a reward $r$ each time after selecting one of them.\n",
    "\n",
    "This reward is based on a reward probability $p(r)$.\n",
    "\n",
    "In some experiments the reward probabilities of the different options are fixed and in others they have a more dynamic nature.\n",
    "\n",
    "In our case, the reward probabilities are going to change trial-by-trial randomly based on a drift rate $\\sigma$ according to\n",
    "\n",
    "$p(r;t+1) \\leftarrow p(r;t) + d$ with $d \\sim \\mathcal{N}(0, \\sigma)$,\n",
    "\n",
    "where $d$ is the current drift.\n",
    "\n",
    "Let's set up the environment first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.resources.bandits import BanditsDrift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 2\n",
    "sigma = 0.2\n",
    "\n",
    "environment = BanditsDrift(sigma=sigma, n_actions=n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the reward probabilities of the arms change across trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_trials = 100\n",
    "reward_probabilities = np.zeros((n_trials, n_actions))\n",
    "\n",
    "for index_trials in range(n_trials):\n",
    "    reward_probabilities[index_trials] = environment.reward_probs\n",
    "    environment.step(choice=0)\n",
    "\n",
    "for index_action in range(n_actions):\n",
    "    plt.plot(reward_probabilities[..., index_action], label=f'Option {index_action+1}')\n",
    "plt.legend()\n",
    "plt.xlabel(r'Trial $t$')\n",
    "plt.ylabel(r'$p(r)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! After setting up the environment, we can now go on to set up our participant!\n",
    "\n",
    "The agent's behavior is defined by its parameters. These parameters are set once in the beginning but you can also draw new parameters from a distribution for each new session (i.e. performing $t$ trials).\n",
    "\n",
    "Let's begin with the simpler case first and keep the parameters fixed for all trials.\n",
    "\n",
    "We are going to set up a simple Rescorla-Wagner model which has only a learning rate $\\alpha$ and an inverse noise temperature $\\beta_{reward}$ and generate a dataset with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.resources.bandits import AgentQ, create_dataset\n",
    "\n",
    "agent = AgentQ(\n",
    "    n_actions=n_actions,\n",
    "    alpha_reward=0.3,\n",
    ")\n",
    "\n",
    "dataset, _, _ = create_dataset(agent=agent, environment=environment, n_trials=100, n_sessions=100)\n",
    "\n",
    "# set all participant ids to 0 since this dataset was generated only by one parameterization\n",
    "dataset.xs[..., -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the agent perform now the task and track how the agent's internal believes change across trials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "\n",
    "fig, axs = plot_session(agents = {'groundtruth': agent}, experiment=dataset.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green and red ticks at the bottom mark whenever option 1 was chosen and rewarded/not rewarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at the experiment results either in dataset (which is used for training the RNN) or in the experiments (which is a list of performed sessions; more human-readable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Inputs (choice item 1 selected; choice item 2 selected; reward 1; reward 2; session id):')\n",
    "print(dataset.xs)\n",
    "\n",
    "print('Targets (next choice):')\n",
    "print(dataset.ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we can proceed to setup our RNN and train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the precoded Rescorla-Wagner RNN\n",
    "\n",
    "First we will use the precoded RNN for discovering models like the Rescorla-Wagner model directly from data. We use SpiceEstimator to fit this RNN to the data we simulated above. The RNN modules and SPICE configuration are predetermined for discovering the Rescorla-Wagner model.\n",
    "\n",
    "If `spice_participant_id` is not specified (set to `None`) then the SPICE model is fit to all participants. However, the fitting procedure would take a while. Instead, we set `spice_participant_id=0` to extract the SPICE model for participant 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceEstimator\n",
    "from spice.precoded import RescorlaWagnerRNN, RESCOLA_WAGNER_CONFIG\n",
    "\n",
    "spice_estimator = SpiceEstimator(\n",
    "    rnn_class=RescorlaWagnerRNN,\n",
    "    spice_config=RESCOLA_WAGNER_CONFIG,\n",
    "    learning_rate=1e-2,\n",
    "    epochs=1024,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "spice_estimator.fit(dataset.xs, dataset.ys)\n",
    "\n",
    "spice_estimator.print_spice_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the resulting equation in one of the last output lines.\n",
    "\n",
    "It should be similar to `(x_value_reward_chosen)[k+1] = 0.156 1 + 0.682 x_value_reward_chosen[k] + 0.895 c_reward[k]`.\n",
    "\n",
    "It's maybe not reminding you directly of the classic Rescorla-Wagner model \n",
    "\n",
    "`(x_value_reward_chosen)[k+1] = ...`\n",
    "\n",
    "`... = (x_value_reward_chosen)[k] + alpha_reward (c_reward[k] - (x_value_reward_chosen)[k])`\n",
    "\n",
    "`... = (1 - alpha_reward) (x_value_reward_chosen)[k] + alpha_reward c_reward[k]`,\n",
    "\n",
    "which is implemented by the synthetic participant, but let's break the identified equation down.\n",
    "\n",
    "1. The constant `0.156 1` is applied equally to both arms without considering the any reward or current value. Therefore, it could also be left out (there's actually a way for doing that in the method `create_dataset_sindy` which we could utilize; later more about that).\n",
    "\n",
    "2. The term `0.682 x_value_reward_chosen[k]` is actually pretty close to the classic model with `alpha_reward = 0.3`\n",
    "\n",
    "3. The term `0.895 c_reward[k]` can be a bit irritating but makes total sense when you consider that the classic model has the scaling factor $\\beta$ which is the inverse noise temperature with `beta_reward = 3`. Therefore, considering this scaling factor we get the parameter `0.895 c_reward[k] / beta_reward = 0.3 c_reward[k]`.\n",
    "\n",
    "By interpreting the identified equation, we can see that the pipeline was able to fit the exact mechanism as implemented in the synthetic participant!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model behaves with respect to our synthetic participant.\n",
    "\n",
    "In the following plot you can compare the action probabilities P(action), the Q-Value, the reward-based as well as the choice-based values.\n",
    "(You can ignore for now the learning rate $\\alpha$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "\n",
    "# get analysis plot\n",
    "agents = {'groundtruth': agent, 'rnn': spice_estimator.rnn_agent, 'spice': spice_estimator.spice_agent}\n",
    "\n",
    "fig, axs = plot_session(agents, dataset.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing the RNN as a custom module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to implement the precoded RNN from scratch.\n",
    "\n",
    "This RNN will inherit from the `BaseRNN`-class which itself inherits from `pytorch.nn.Module`. This is the base class for neural networks in the `PyTorch` framework.\n",
    "\n",
    "Therefore the RNN has to implement a `forward`-method which is used for prediction. Further, it needs submodules to perform computations. These submodules are stored in the dictionary ` submodules_rnn` with the key `x_ModuleName`. The start of the key `x_` means that we are talking here about a memory state variable of the RNN.\n",
    "\n",
    "Here, we are going to implement the simplest version of such a RNN. This RNN will update only the value of the chosen option based on the reward and leaves the values of the not chosen options untouched.\n",
    "\n",
    "The structure of this RNN is shown in the following figure:\n",
    "\n",
    "![](../figures/spice_rnn_rescorla_wagner.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.resources.rnn import BaseRNN\n",
    "\n",
    "class CustomRNN(BaseRNN):\n",
    "    \n",
    "    # set up a dictionary with initial values for each state in memory\n",
    "    init_values = {\n",
    "        'x_value_reward': 0.5,\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        **kwargs,\n",
    "    ):   \n",
    "        super(CustomRNN, self).__init__(n_actions=n_actions)\n",
    "        \n",
    "        # set up the submodules\n",
    "        self.submodules_rnn['x_value_reward_chosen'] = self.setup_module(input_size=1)\n",
    "        \n",
    "    def forward(self, inputs, prev_state=None, batch_first=False):\n",
    "        \"\"\"Forward pass of the RNN\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): includes all necessary inputs (action, reward, participant id) to the RNN to let it compute the next action\n",
    "            prev_state (Tuple[torch.Tensor], optional): That's the previous memory state of the RNN containing the reward-based value. Defaults to None.\n",
    "            batch_first (bool, optional): Indicates whether the first dimension of inputs is batch (True) or timesteps (False). Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, we have to initialize all the inputs and outputs (i.e. logits)\n",
    "        inputs, ids, logits, timesteps = self.init_forward_pass(inputs, prev_state, batch_first)\n",
    "        actions, rewards, _, _ = inputs\n",
    "        \n",
    "        for timestep, action, reward in zip(timesteps, actions, rewards):\n",
    "            \n",
    "            # record the inputs for training SINDy later on\n",
    "            self.record_signal('c_action', action)\n",
    "            self.record_signal('c_reward', reward)\n",
    "            self.record_signal('x_value_reward_chosen', self.state['x_value_reward'])\n",
    "            \n",
    "            # Let's perform the belief update for the reward-based value of the chosen option\n",
    "            # since all values are given to the rnn-module (independent of each other), the chosen value is selected by setting the action to the chosen one\n",
    "            # if we would like to perform a similar update by calling a rnn-module for the non-chosen action, we would set the parameter to action=1-action.\n",
    "            next_value_reward_chosen = self.call_module(\n",
    "                key_module='x_value_reward_chosen',\n",
    "                key_state='x_value_reward',\n",
    "                action=action,\n",
    "                inputs=reward,\n",
    "                )\n",
    "\n",
    "            # and keep the value of the not-chosen option unchanged\n",
    "            next_value_reward_not_chosen = self.state['x_value_reward'] * (1-action)\n",
    "            \n",
    "            self.state['x_value_reward'] = next_value_reward_chosen + next_value_reward_not_chosen  # memory state = (0.8, 0.3) <- next_value = (0.8, 0) + (0, 0.3)\n",
    "            \n",
    "            # Now keep track of this value in the output array\n",
    "            logits[timestep] = self.state['x_value_reward']\n",
    "        \n",
    "        # post-process the forward pass; give here as inputs the logits, batch_first and all values from the memory state\n",
    "        # self.state['x_value_reward'] = value_reward\n",
    "        logits = self.post_forward_pass(logits, batch_first)\n",
    "        \n",
    "        return logits, self.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we implemented our RNN, we can train it to see how well it fits the behavior of our synthetic participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceConfig\n",
    "\n",
    "custom_config = SpiceConfig(\n",
    "    # A list of all names of the RNN-modules which are computing the RNN's memory state\n",
    "    rnn_modules=['x_value_reward_chosen'],\n",
    "\n",
    "    # A list of all the control signals which are used as inputs to any of the RNN-modules\n",
    "    control_parameters=['c_action', 'c_reward'],\n",
    "\n",
    "    # Setup of the SINDy library\n",
    "    # Determines which terms are allowed as control inputs to each SINDy model in a dictionary.\n",
    "    # The key is the SINDy-model name (same as RNN-module), value is a list of allowed control inputs from the list of control signals \n",
    "    library_setup={\n",
    "        'x_value_reward_chosen': ['c_reward'],\n",
    "    },\n",
    "\n",
    "    # Setup of the filtering condition\n",
    "    # Determines the filtering condition on which samples are selected as training samples for each SINDy-model.\n",
    "    # Example:\n",
    "    # Since each RNN-module processes all values at once (but independet from each other), we have to filter for the updates of interest.\n",
    "    # In the case of the reward-based value of the chosen option this means to use only the chosen items and not the non-chosen ones. \n",
    "    # Therefore, we can set a filter condition to get rid of all value updates for non-chosen options.  \n",
    "    # The filter dictionary has the following structure:\n",
    "    # key -> the SINDy model name\n",
    "    # value -> triplet of values:\n",
    "    #   1. str: feature name to be used as a filter\n",
    "    #   2. numeric: the numeric filter condition\n",
    "    #   3. bool: remove feature from control inputs if not needed as input to the module\n",
    "    # Multiple conditions can also be given as a list of triplets, e.g. [['c_action', 1, True], ['c_reward', 0, False]]\n",
    "    filter_setup={\n",
    "        'x_value_reward_chosen': ['c_action', 1, True],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceEstimator\n",
    "\n",
    "\n",
    "custom_spice_estimator = SpiceEstimator(\n",
    "    rnn_class=CustomRNN,\n",
    "    spice_config=custom_config,\n",
    "    learning_rate=1e-2,\n",
    "    epochs=1024,\n",
    ")\n",
    "\n",
    "custom_spice_estimator.fit(dataset.xs, dataset.ys)\n",
    "\n",
    "custom_spice_estimator.print_spice_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model behaves with respect to our synthetic participant.\n",
    "\n",
    "In the following plot you can compare the action probabilities P(action), the Q-Value, the reward-based as well as the choice-based values.\n",
    "(You can ignore for now the learning rate $\\alpha$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing and fitting the SINDy-agent, we can now finally inspect whether all these steps resulted in well identified cognitive dynamics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "\n",
    "# get analysis plot\n",
    "agents = {'groundtruth': agent, 'rnn': custom_spice_estimator.rnn_agent, 'spice': custom_spice_estimator.spice_agent}\n",
    "\n",
    "fig, axs = plot_session(agents, dataset.xs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading models\n",
    "\n",
    "Below is how you can save and load SPICE models. You can specify `path_rnn`, `path_spice` or both to save or load either or both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model to file\n",
    "spice_estimator.save_spice(path_rnn='rnn_model.pkl', path_spice='spice_model.pkl')\n",
    "\n",
    "# Load saved model\n",
    "loaded_spice = SpiceEstimator(\n",
    "    rnn_class=CustomRNN,\n",
    "    spice_config=custom_config,\n",
    ")\n",
    "\n",
    "loaded_spice.load_spice(path_rnn='rnn_model.pkl', path_spice='spice_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
