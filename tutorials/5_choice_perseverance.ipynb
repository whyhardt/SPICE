{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a Choice Perseverance Mechanism\n",
    "\n",
    "In this tutorial, you will learn how to extend the model by adding a choice perseverance mechanism that captures non-goal-directed behavior.\n",
    "In real-world decision-making, humans don't always choose based purely on expected rewards. Sometimes we tend to repeat previous choices simply because we made them before, independent of their actual value. The choice perseverance mechanism models this by:\n",
    "\n",
    "Tracking which actions were chosen previously\n",
    "Gradually increasing the preference for recently chosen actions\n",
    "Gradually decreasing the preference for non-chosen actions\n",
    "Allowing for dynamic adjustment of perseverance strength\n",
    "\n",
    "This module incorporates an additional RNN module which dynamically handles choice-based preferences separately from reward-based learning.\n",
    "Prerequisites\n",
    "Before starting this tutorial, make sure you have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting this tutorial, make sure you have:\n",
    "- Completed the [Basic Rescorla-Wagner Tutorial](2_rescorla_wagner.html)\n",
    "- SPICE installed with all dependencies\n",
    "- Understanding of basic reinforcement learning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data generation\n",
    "\n",
    "First of all we have to generate a dataset with multiple participants. Let's start with two different ones.\n",
    "\n",
    "We are going generate half the dataset with participant #1 and the other half with participant #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below and execute the cell if you are using Google Colab\n",
    "\n",
    "#!pip uninstall -y numpy pandas\n",
    "#!pip install numpy==1.26.4 pandas==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below and execute the cell if you are using Google Colab\n",
    "\n",
    "#!pip install autospice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T08:55:54.705843Z",
     "start_time": "2025-08-20T08:55:53.691790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7200b40e0070>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T08:55:56.441546Z",
     "start_time": "2025-08-20T08:55:54.712073Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/spice/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 269.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from spice.resources.bandits import BanditsDrift, AgentQ, create_dataset\n",
    "from spice.resources.rnn_utils import DatasetRNN\n",
    "\n",
    "# Set up the environment\n",
    "n_actions = 2\n",
    "sigma = 0.2\n",
    "\n",
    "environment = BanditsDrift(sigma=sigma, n_actions=n_actions)\n",
    "\n",
    "# Set up the agent\n",
    "agent = AgentQ(\n",
    "    n_actions=n_actions,\n",
    "    alpha_reward=0.3,\n",
    "    forget_rate=0.2,\n",
    "    beta_choice=1.,\n",
    "    alpha_choice=1.,\n",
    ")\n",
    "\n",
    "# Create the dataset\n",
    "n_trials = 100\n",
    "n_sessions = 100\n",
    "\n",
    "dataset, _, _ = create_dataset(\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    n_trials=n_trials,\n",
    "    n_sessions=n_sessions,\n",
    ")\n",
    "\n",
    "# set all participant ids to 0 since this dataset was generated only by one parameterization\n",
    "dataset.xs[..., -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the precoded model\n",
    "\n",
    "First we setup and train the precoded SPICE model and inspect its behavior, before implementing it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T09:01:13.544085Z",
     "start_time": "2025-08-20T08:58:43.643097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the RNN...\n",
      "Epoch 1024/1024 --- L(Train): 0.5087311; Time: 0.17s; Convergence: 1.56e-03\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'c_value_reward'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspice\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprecoded\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChoiceRNN, CHOICE_CONFIG\n\u001b[32m      4\u001b[39m spice_estimator = SpiceEstimator(\n\u001b[32m      5\u001b[39m     rnn_class=ChoiceRNN,\n\u001b[32m      6\u001b[39m     spice_config=CHOICE_CONFIG,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     n_participants=\u001b[32m1\u001b[39m,\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mspice_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m spice_estimator.print_spice_model()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spice/lib/python3.11/site-packages/spice/estimator.py:243\u001b[39m, in \u001b[36mSpiceEstimator.fit\u001b[39m\u001b[34m(self, data, targets, data_test, target_test)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28mself\u001b[39m.spice_features = {}\n\u001b[32m    241\u001b[39m spice_modules = {rnn_module: {} \u001b[38;5;28;01mfor\u001b[39;00m rnn_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rnn_modules}\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28mself\u001b[39m.spice_agent = \u001b[43mfit_spice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrnn_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrol_signals\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontrol_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent_rnn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspice_library_polynomial_degree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_setup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspice_library_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_setup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspice_filter_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspice_optimizer_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspice_optim_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspice_optim_regularization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials_off_policy\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_trials_off_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_sessions_off_policy\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_sessions_off_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials_same_action_off_policy\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_trials_same_action_off_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_test_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_test_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_optuna\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muse_optuna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptuna_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptuna_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptuna_n_trials\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptuna_n_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# self.spice_features = self.spice_agent.get_spice_features()\u001b[39;00m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spice/lib/python3.11/site-packages/spice/resources/sindy_training.py:164\u001b[39m, in \u001b[36mfit_spice\u001b[39m\u001b[34m(rnn_modules, control_signals, agent_rnn, data, polynomial_degree, library_setup, filter_setup, optimizer_type, optimizer_threshold, optimizer_alpha, participant_id, shuffle, dataprocessing, n_trials_off_policy, n_sessions_off_policy, n_trials_same_action_off_policy, train_test_ratio, deterministic, get_loss, verbose, use_optuna, filter_bad_participants, pruning, optuna_threshold, optuna_n_trials)\u001b[39m\n\u001b[32m    161\u001b[39m     data_pid = DatasetRNN(*data[mask_participant_id])\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# just fit the SINDy modules with the given parameters \u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m sindy_modules_id = \u001b[43mfit_sindy_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparticipant_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent_rnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_pid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrnn_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrnn_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrol_signals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrol_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43msindy_library_setup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_setup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43msindy_filter_setup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_setup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43msindy_dataprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_sessions_off_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_sessions_off_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials_off_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials_off_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials_same_action_off_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials_same_action_off_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcatch_convergence_warning\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m spice_modules_id = {rnn_module: {} \u001b[38;5;28;01mfor\u001b[39;00m rnn_module \u001b[38;5;129;01min\u001b[39;00m rnn_modules}\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rnn_module \u001b[38;5;129;01min\u001b[39;00m rnn_modules:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spice/lib/python3.11/site-packages/spice/resources/fit_sindy.py:174\u001b[39m, in \u001b[36mfit_sindy_pipeline\u001b[39m\u001b[34m(participant_id, agent, data, rnn_modules, control_signals, sindy_library_setup, sindy_filter_setup, sindy_dataprocessing, optimizer_type, optimizer_alpha, optimizer_threshold, polynomial_degree, shuffle, n_sessions_off_policy, n_trials_off_policy, n_trials_same_action_off_policy, catch_convergence_warning, verbose)\u001b[39m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mOne of the arguments data or n_sessions_off_policy (> 0) must be given. If n_sessions_off_policy > 0 the SINDy modules will be fitted on the off-policy data regardless of data. If n_sessions_off_policy = 0 then data will be used to fit the SINDy modules.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# extract all necessary data from the RNN (memory state) and align with the control inputs (action, reward)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m rnn_variables, control_parameters, _, _ = \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrnn_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrnn_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrol_signals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrol_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[43msindy_dataprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# fit one SINDy-model per RNN-module\u001b[39;00m\n\u001b[32m    184\u001b[39m sindy_modules = fit_sindy(\n\u001b[32m    185\u001b[39m     variables=rnn_variables,\n\u001b[32m    186\u001b[39m     control=control_parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    196\u001b[39m     verbose=verbose,\n\u001b[32m    197\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spice/lib/python3.11/site-packages/spice/resources/sindy_utils.py:102\u001b[39m, in \u001b[36mcreate_dataset\u001b[39m\u001b[34m(agent, data, rnn_modules, control_signals, dataprocessing, shuffle, groupby)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# sort the data of one session into the corresponding signals\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys_x+keys_c:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_recording\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m) > \u001b[32m0\u001b[39m:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m index_action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(agent._n_actions):\n\u001b[32m    104\u001b[39m       \u001b[38;5;66;03m# get all recorded values for the current session of one specific key \u001b[39;00m\n\u001b[32m    105\u001b[39m       recording = agent._model.get_recording(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spice/lib/python3.11/site-packages/spice/resources/rnn.py:213\u001b[39m, in \u001b[36mBaseRNN.get_recording\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_recording\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecording\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'c_value_reward'"
     ]
    }
   ],
   "source": [
    "from spice.estimator import SpiceEstimator\n",
    "from spice.precoded import ChoiceRNN, CHOICE_CONFIG\n",
    "\n",
    "spice_estimator = SpiceEstimator(\n",
    "    rnn_class=ChoiceRNN,\n",
    "    spice_config=CHOICE_CONFIG,\n",
    "    learning_rate=1e-2,\n",
    "    epochs=1024,\n",
    "    n_participants=1,\n",
    ")\n",
    "\n",
    "spice_estimator.fit(dataset.xs, dataset.ys)\n",
    "\n",
    "spice_estimator.print_spice_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T09:01:14.118083Z",
     "start_time": "2025-08-20T09:01:13.567026Z"
    }
   },
   "outputs": [],
   "source": [
    "from spice.utils.plotting import plot_session\n",
    "\n",
    "# Let's see how well the dynamics were fitted\n",
    "agents = {'groundtruth': agent, 'rnn': spice_estimator.rnn_agent, 'spice': spice_estimator.spice_agent}\n",
    "fig, axs = plot_session(agents, dataset.xs[0], signals_to_plot=['x_value_reward', 'x_value_choice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing the RNN from Scratch\n",
    "\n",
    "Below is the actual implementation of the RNN model. This RNN includes an additional group of reward-agnostic RNN-modules. These modules update choice-based values of the chosen and non-chosen options.\n",
    "\n",
    "The structure of this RNN is shown in the following figure:\n",
    "\n",
    "![](../figures/spice_rnn_choice.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T09:01:14.133979Z",
     "start_time": "2025-08-20T09:01:14.123789Z"
    }
   },
   "outputs": [],
   "source": [
    "from spice.estimator import SpiceConfig\n",
    "from spice.resources.rnn import BaseRNN\n",
    "\n",
    "custom_config = SpiceConfig(\n",
    "    rnn_modules=['x_value_reward_chosen', 'x_value_reward_not_chosen', 'x_value_choice_chosen', 'x_value_choice_not_chosen'],\n",
    "    control_parameters=['c_action', 'c_reward'],\n",
    "    # The new module which handles the not-chosen value, does not need any additional inputs except for the value\n",
    "    library_setup = {\n",
    "        'x_value_reward_chosen': ['c_reward'],\n",
    "        'x_value_reward_not_chosen': [],\n",
    "        'x_value_choice_chosen': [],\n",
    "        'x_value_choice_not_chosen': [],\n",
    "    },\n",
    "\n",
    "    # Further, the new module should be applied only to the not-chosen values\n",
    "    filter_setup = {\n",
    "        'x_value_reward_chosen': ['c_action', 1, True],\n",
    "        'x_value_reward_not_chosen': ['c_action', 0, True],\n",
    "        'x_value_choice_chosen': ['c_action', 1, True],\n",
    "        'x_value_choice_not_chosen': ['c_action', 0, True],\n",
    "    }    \n",
    ")\n",
    "\n",
    "class CustomRNN(BaseRNN):\n",
    "\n",
    "    init_values = {\n",
    "            'x_value_reward': 0.5,\n",
    "            'x_value_choice': 0.,\n",
    "        }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        n_participants,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        super(CustomRNN, self).__init__(n_actions=n_actions, embedding_size=8)\n",
    "        \n",
    "        # set up the participant-embedding layer\n",
    "        self.participant_embedding = self.setup_embedding(num_embeddings=n_participants, embedding_size=self.embedding_size)\n",
    "        \n",
    "        # scaling factor (inverse noise temperature) for each participant for the values which are handled by an hard-coded equation\n",
    "        self.betas['x_value_reward'] = self.setup_constant(embedding_size=self.embedding_size)\n",
    "        self.betas['x_value_choice'] = self.setup_constant(embedding_size=self.embedding_size)\n",
    "        \n",
    "        # set up the submodules\n",
    "        self.submodules_rnn['x_value_reward_chosen'] = self.setup_module(input_size=1+self.embedding_size)\n",
    "        self.submodules_rnn['x_value_reward_not_chosen'] = self.setup_module(input_size=0+self.embedding_size)\n",
    "        self.submodules_rnn['x_value_choice_chosen'] = self.setup_module(input_size=0+self.embedding_size)\n",
    "        self.submodules_rnn['x_value_choice_not_chosen'] = self.setup_module(input_size=0+self.embedding_size)\n",
    "        \n",
    "    def forward(self, inputs, prev_state=None, batch_first=False):\n",
    "        \"\"\"Forward pass of the RNN\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): includes all necessary inputs (action, reward, participant id) to the RNN to let it compute the next action\n",
    "            prev_state (Tuple[torch.Tensor], optional): That's the previous memory state of the RNN containing the reward-based value. Defaults to None.\n",
    "            batch_first (bool, optional): Indicates whether the first dimension of inputs is batch (True) or timesteps (False). Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, we have to initialize all the inputs and outputs (i.e. logits)\n",
    "        inputs, ids, logits, timesteps = self.init_forward_pass(inputs, prev_state, batch_first)\n",
    "        actions, rewards, _, _ = inputs\n",
    "        participant_id, _ = ids\n",
    "        \n",
    "        # Here we compute now the participant embeddings for each entry in the batch\n",
    "        participant_embedding = self.participant_embedding(participant_id[:, 0].int())\n",
    "        \n",
    "        for timestep, action, reward in zip(timesteps, actions, rewards):\n",
    "            \n",
    "            # record the inputs for training SINDy later on\n",
    "            self.record_signal('c_action', action)\n",
    "            self.record_signal('c_reward', reward)\n",
    "            self.record_signal('x_value_reward_chosen', self.state['x_value_reward'])\n",
    "            self.record_signal('x_value_reward_not_chosen', self.state['x_value_reward'])\n",
    "            self.record_signal('x_value_choice_chosen', self.state['x_value_choice'])\n",
    "            self.record_signal('x_value_choice_not_chosen', self.state['x_value_choice'])\n",
    "            \n",
    "            # updates for x_value_reward\n",
    "            next_value_reward_chosen = self.call_module(\n",
    "                key_module='x_value_reward_chosen',\n",
    "                key_state='x_value_reward',\n",
    "                action=action,\n",
    "                inputs=(reward),\n",
    "                participant_embedding=participant_embedding,\n",
    "                participant_index=participant_id,\n",
    "                activation_rnn=torch.nn.functional.sigmoid,\n",
    "            )\n",
    "            \n",
    "            next_value_reward_not_chosen = self.call_module(\n",
    "                key_module='x_value_reward_not_chosen',\n",
    "                key_state='x_value_reward',\n",
    "                action=1-action,\n",
    "                inputs=None,\n",
    "                participant_embedding=participant_embedding,\n",
    "                participant_index=participant_id,\n",
    "                )\n",
    "            \n",
    "            # updates for x_value_choice\n",
    "            next_value_choice_chosen = self.call_module(\n",
    "                key_module='x_value_choice_chosen',\n",
    "                key_state='x_value_choice',\n",
    "                action=action,\n",
    "                inputs=None,\n",
    "                participant_embedding=participant_embedding,\n",
    "                participant_index=participant_id,\n",
    "                activation_rnn=torch.nn.functional.sigmoid,\n",
    "                )\n",
    "            \n",
    "            next_value_choice_not_chosen = self.call_module(\n",
    "                key_module='x_value_choice_not_chosen',\n",
    "                key_state='x_value_choice',\n",
    "                action=1-action,\n",
    "                inputs=None,\n",
    "                participant_embedding=participant_embedding,\n",
    "                participant_index=participant_id,\n",
    "                activation_rnn=torch.nn.functional.sigmoid,\n",
    "                )\n",
    "            \n",
    "            # updating the memory state\n",
    "            self.state['x_value_reward'] = next_value_reward_chosen + next_value_reward_not_chosen\n",
    "            self.state['x_value_choice'] = next_value_choice_chosen + next_value_choice_not_chosen\n",
    "            \n",
    "            # Now keep track of the logit in the output array\n",
    "            logits[timestep] = self.state['x_value_reward'] * self.betas['x_value_reward'](participant_embedding) + self.state['x_value_choice'] * self.betas['x_value_choice'](participant_embedding)\n",
    "            \n",
    "        # post-process the forward pass; give here as inputs the logits, batch_first and all values from the memory state\n",
    "        logits = self.post_forward_pass(logits, batch_first)\n",
    "        \n",
    "        return logits, self.get_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
