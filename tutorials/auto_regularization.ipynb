{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9efc9c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library setup is valid. All keys and features appear in the provided list of features.\n",
      "Library setup is valid. All keys and features appear in the provided list of features.\n",
      "Library setup is valid. All keys and features appear in the provided list of features.\n",
      "Library setup is valid. All keys and features appear in the provided list of features.\n",
      "Library setup is valid. All keys and features appear in the provided list of features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e932a58790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Disable cudnn\n",
    "from torch.backends import cudnn\n",
    "cudnn.enabled = False\n",
    "\n",
    "from spice import pipeline_rnn_autoreg\n",
    "from spice.resources.old_rnn import RLRNN_dezfouli2019, RLRNN_eckstein2022 # Get predefined RNN architectures\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "np.random.seed(186)\n",
    "torch.manual_seed(186)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d8378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set most important arguments:\n",
    "dataset = 'eckstein2022'  # 'eckstein2022' or 'dezfouli2019'\n",
    "epochs = 8192\n",
    "metaopt_type = 'imaml'\n",
    "\n",
    "# AWD\n",
    "lambda_awd = 1e-1\n",
    "\n",
    "# iMAML\n",
    "initial_reg_param = 1e-4\n",
    "outer_lr = 1e-1\n",
    "\n",
    "# Fixed model path name\n",
    "if metaopt_type == 'awd':\n",
    "    path_model = f'params/{dataset}/AWD_{dataset}_ep{epochs}_lawd-{lambda_awd:.0e}_rnn.pkl'\n",
    "elif metaopt_type == 'imaml':\n",
    "    path_model = f'params/{dataset}/iMAML_{dataset}_ep{epochs}_metalr-{outer_lr:.0e}_in-{initial_reg_param:.0e}_rnn.pkl'\n",
    "else:\n",
    "    raise ValueError('metaopt_type must be either \"awd\" or \"imaml\"')\n",
    "\n",
    "# SPICE config\n",
    "path_data = f'../data/{dataset}/{dataset}.csv'\n",
    "additional_inputs = None\n",
    "\n",
    "if dataset == 'eckstein2022':\n",
    "    train_test_ratio = 0.8\n",
    "    class_rnn = RLRNN_eckstein2022\n",
    "elif dataset == 'dezfouli2019':\n",
    "    train_test_ratio = [3, 6, 9]\n",
    "    class_rnn = RLRNN_dezfouli2019\n",
    "else:\n",
    "    raise ValueError('Dataset must be either \"eckstein2022\" or \"dezfouli2019\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b4aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: False\n",
      "Setup of the RNN model complete.\n",
      "Training the RNN...\n",
      "Using Implicit MAML (iMAML) for meta-optimization.\n",
      "Epoch 1/8192 --- L(Train): 0.6725; L(Val): 0.5252; Reg Param: 0.0001; Time: 2.59\n",
      "Epoch 2/8192 --- L(Train): 0.5263; L(Val): 0.5161; Reg Param: 0.0001; Time: 1.11\n",
      "Epoch 3/8192 --- L(Train): 0.5259; L(Val): 0.5083; Reg Param: 0.0001; Time: 1.42\n",
      "Epoch 4/8192 --- L(Train): 0.5154; L(Val): 0.5016; Reg Param: 0.0001; Time: 1.12\n",
      "Epoch 5/8192 --- L(Train): 0.5090; L(Val): 0.4958; Reg Param: 0.0001; Time: 1.04\n",
      "Epoch 6/8192 --- L(Train): 0.5094; L(Val): 0.4906; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 7/8192 --- L(Train): 0.5007; L(Val): 0.4858; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 8/8192 --- L(Train): 0.4907; L(Val): 0.4815; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 9/8192 --- L(Train): 0.4883; L(Val): 0.4776; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 10/8192 --- L(Train): 0.4788; L(Val): 0.4743; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 11/8192 --- L(Train): 0.4700; L(Val): 0.4715; Reg Param: 0.0001; Time: 1.06\n",
      "Epoch 12/8192 --- L(Train): 0.4789; L(Val): 0.4692; Reg Param: 0.0001; Time: 0.97\n",
      "Epoch 13/8192 --- L(Train): 0.4773; L(Val): 0.4673; Reg Param: 0.0001; Time: 1.05\n",
      "Epoch 14/8192 --- L(Train): 0.4605; L(Val): 0.4657; Reg Param: 0.0001; Time: 0.96\n",
      "Epoch 15/8192 --- L(Train): 0.4785; L(Val): 0.4645; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 16/8192 --- L(Train): 0.4679; L(Val): 0.4634; Reg Param: 0.0001; Time: 1.15\n",
      "Epoch 17/8192 --- L(Train): 0.4684; L(Val): 0.4623; Reg Param: 0.0001; Time: 0.98\n",
      "Epoch 18/8192 --- L(Train): 0.4579; L(Val): 0.4614; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 19/8192 --- L(Train): 0.4719; L(Val): 0.4605; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 20/8192 --- L(Train): 0.4594; L(Val): 0.4597; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 21/8192 --- L(Train): 0.4582; L(Val): 0.4590; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 22/8192 --- L(Train): 0.4493; L(Val): 0.4583; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 23/8192 --- L(Train): 0.4597; L(Val): 0.4577; Reg Param: 0.0001; Time: 0.96\n",
      "Epoch 24/8192 --- L(Train): 0.4688; L(Val): 0.4571; Reg Param: 0.0001; Time: 1.07\n",
      "Epoch 25/8192 --- L(Train): 0.4636; L(Val): 0.4565; Reg Param: 0.0001; Time: 1.06\n",
      "Epoch 26/8192 --- L(Train): 0.4628; L(Val): 0.4560; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 27/8192 --- L(Train): 0.4610; L(Val): 0.4554; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 28/8192 --- L(Train): 0.4643; L(Val): 0.4549; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 29/8192 --- L(Train): 0.4643; L(Val): 0.4543; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 30/8192 --- L(Train): 0.4682; L(Val): 0.4538; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 31/8192 --- L(Train): 0.4551; L(Val): 0.4533; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 32/8192 --- L(Train): 0.4487; L(Val): 0.4528; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 33/8192 --- L(Train): 0.4493; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 34/8192 --- L(Train): 0.4607; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 35/8192 --- L(Train): 0.4542; L(Val): 0.4513; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 36/8192 --- L(Train): 0.4494; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 37/8192 --- L(Train): 0.4430; L(Val): 0.4504; Reg Param: 0.0001; Time: 1.16\n",
      "Epoch 38/8192 --- L(Train): 0.4586; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.96\n",
      "Epoch 39/8192 --- L(Train): 0.4537; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 40/8192 --- L(Train): 0.4409; L(Val): 0.4488; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 41/8192 --- L(Train): 0.4477; L(Val): 0.4483; Reg Param: 0.0001; Time: 1.01\n",
      "Epoch 42/8192 --- L(Train): 0.4525; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 43/8192 --- L(Train): 0.4583; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 44/8192 --- L(Train): 0.4548; L(Val): 0.4469; Reg Param: 0.0001; Time: 0.98\n",
      "Epoch 45/8192 --- L(Train): 0.4464; L(Val): 0.4465; Reg Param: 0.0001; Time: 1.06\n",
      "Epoch 46/8192 --- L(Train): 0.4444; L(Val): 0.4461; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 47/8192 --- L(Train): 0.4429; L(Val): 0.4456; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 48/8192 --- L(Train): 0.4385; L(Val): 0.4452; Reg Param: 0.0001; Time: 0.96\n",
      "Epoch 49/8192 --- L(Train): 0.4456; L(Val): 0.4448; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 50/8192 --- L(Train): 0.4370; L(Val): 0.4448; Reg Param: 0.0001; Time: 9.31\n",
      "Epoch 51/8192 --- L(Train): 0.4482; L(Val): 0.4440; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 52/8192 --- L(Train): 0.4433; L(Val): 0.4437; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 53/8192 --- L(Train): 0.4383; L(Val): 0.4433; Reg Param: 0.0001; Time: 0.71\n",
      "Epoch 54/8192 --- L(Train): 0.4377; L(Val): 0.4429; Reg Param: 0.0001; Time: 0.71\n",
      "Epoch 55/8192 --- L(Train): 0.4373; L(Val): 0.4426; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 56/8192 --- L(Train): 0.4346; L(Val): 0.4422; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 57/8192 --- L(Train): 0.4361; L(Val): 0.4418; Reg Param: 0.0001; Time: 1.00\n",
      "Epoch 58/8192 --- L(Train): 0.4337; L(Val): 0.4414; Reg Param: 0.0001; Time: 1.01\n",
      "Epoch 59/8192 --- L(Train): 0.4355; L(Val): 0.4410; Reg Param: 0.0001; Time: 1.17\n",
      "Epoch 60/8192 --- L(Train): 0.4289; L(Val): 0.4405; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 61/8192 --- L(Train): 0.4355; L(Val): 0.4401; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 62/8192 --- L(Train): 0.4258; L(Val): 0.4397; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 63/8192 --- L(Train): 0.4435; L(Val): 0.4393; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 64/8192 --- L(Train): 0.4385; L(Val): 0.4389; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 65/8192 --- L(Train): 0.4235; L(Val): 0.4385; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 66/8192 --- L(Train): 0.4310; L(Val): 0.4382; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 67/8192 --- L(Train): 0.4159; L(Val): 0.4378; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 68/8192 --- L(Train): 0.4351; L(Val): 0.4374; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 69/8192 --- L(Train): 0.4308; L(Val): 0.4370; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 70/8192 --- L(Train): 0.4327; L(Val): 0.4367; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 71/8192 --- L(Train): 0.4345; L(Val): 0.4364; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 72/8192 --- L(Train): 0.4254; L(Val): 0.4361; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 73/8192 --- L(Train): 0.4288; L(Val): 0.4358; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 74/8192 --- L(Train): 0.4318; L(Val): 0.4354; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 75/8192 --- L(Train): 0.4322; L(Val): 0.4351; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 76/8192 --- L(Train): 0.4338; L(Val): 0.4347; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 77/8192 --- L(Train): 0.4289; L(Val): 0.4343; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 78/8192 --- L(Train): 0.4293; L(Val): 0.4339; Reg Param: 0.0001; Time: 0.96\n",
      "Epoch 79/8192 --- L(Train): 0.4243; L(Val): 0.4335; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 80/8192 --- L(Train): 0.4306; L(Val): 0.4331; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 81/8192 --- L(Train): 0.4237; L(Val): 0.4328; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 82/8192 --- L(Train): 0.4309; L(Val): 0.4325; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 83/8192 --- L(Train): 0.4260; L(Val): 0.4322; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 84/8192 --- L(Train): 0.4175; L(Val): 0.4319; Reg Param: 0.0001; Time: 0.95\n",
      "Epoch 85/8192 --- L(Train): 0.4245; L(Val): 0.4316; Reg Param: 0.0001; Time: 1.06\n",
      "Epoch 86/8192 --- L(Train): 0.4292; L(Val): 0.4314; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 87/8192 --- L(Train): 0.4252; L(Val): 0.4311; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 88/8192 --- L(Train): 0.4247; L(Val): 0.4309; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 89/8192 --- L(Train): 0.4249; L(Val): 0.4307; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 90/8192 --- L(Train): 0.4229; L(Val): 0.4305; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 91/8192 --- L(Train): 0.4198; L(Val): 0.4303; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 92/8192 --- L(Train): 0.4276; L(Val): 0.4302; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 93/8192 --- L(Train): 0.4151; L(Val): 0.4301; Reg Param: 0.0001; Time: 1.03\n",
      "Epoch 94/8192 --- L(Train): 0.4233; L(Val): 0.4299; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 95/8192 --- L(Train): 0.4159; L(Val): 0.4296; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 96/8192 --- L(Train): 0.4221; L(Val): 0.4293; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 97/8192 --- L(Train): 0.4163; L(Val): 0.4290; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 98/8192 --- L(Train): 0.4158; L(Val): 0.4288; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 99/8192 --- L(Train): 0.4301; L(Val): 0.4286; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 100/8192 --- L(Train): 0.4146; L(Val): 0.4286; Reg Param: 0.0001; Time: 9.45\n",
      "Epoch 101/8192 --- L(Train): 0.4178; L(Val): 0.4283; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 102/8192 --- L(Train): 0.4198; L(Val): 0.4283; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 103/8192 --- L(Train): 0.4077; L(Val): 0.4282; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 104/8192 --- L(Train): 0.4300; L(Val): 0.4281; Reg Param: 0.0001; Time: 1.01\n",
      "Epoch 105/8192 --- L(Train): 0.4205; L(Val): 0.4280; Reg Param: 0.0001; Time: 1.06\n",
      "Epoch 106/8192 --- L(Train): 0.4142; L(Val): 0.4279; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 107/8192 --- L(Train): 0.4191; L(Val): 0.4277; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 108/8192 --- L(Train): 0.4103; L(Val): 0.4276; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 109/8192 --- L(Train): 0.4123; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 110/8192 --- L(Train): 0.4244; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 111/8192 --- L(Train): 0.4133; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 112/8192 --- L(Train): 0.4225; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 113/8192 --- L(Train): 0.4201; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 114/8192 --- L(Train): 0.4213; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 115/8192 --- L(Train): 0.4073; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 116/8192 --- L(Train): 0.4192; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 117/8192 --- L(Train): 0.4127; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 118/8192 --- L(Train): 0.4147; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 119/8192 --- L(Train): 0.4153; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 120/8192 --- L(Train): 0.4161; L(Val): 0.4262; Reg Param: 0.0001; Time: 0.96\n",
      "Epoch 121/8192 --- L(Train): 0.4070; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 122/8192 --- L(Train): 0.4066; L(Val): 0.4260; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 123/8192 --- L(Train): 0.4172; L(Val): 0.4260; Reg Param: 0.0001; Time: 1.17\n",
      "Epoch 124/8192 --- L(Train): 0.4095; L(Val): 0.4259; Reg Param: 0.0001; Time: 1.05\n",
      "Epoch 125/8192 --- L(Train): 0.4244; L(Val): 0.4259; Reg Param: 0.0001; Time: 1.05\n",
      "Epoch 126/8192 --- L(Train): 0.4171; L(Val): 0.4259; Reg Param: 0.0001; Time: 1.04\n",
      "Epoch 127/8192 --- L(Train): 0.4128; L(Val): 0.4259; Reg Param: 0.0001; Time: 1.25\n",
      "Epoch 128/8192 --- L(Train): 0.4056; L(Val): 0.4259; Reg Param: 0.0001; Time: 1.17\n",
      "Epoch 129/8192 --- L(Train): 0.4336; L(Val): 0.4259; Reg Param: 0.0001; Time: 1.17\n",
      "Epoch 130/8192 --- L(Train): 0.4147; L(Val): 0.4259; Reg Param: 0.0001; Time: 1.13\n",
      "Epoch 131/8192 --- L(Train): 0.4090; L(Val): 0.4259; Reg Param: 0.0001; Time: 1.12\n",
      "Epoch 132/8192 --- L(Train): 0.4133; L(Val): 0.4258; Reg Param: 0.0001; Time: 1.13\n",
      "Epoch 133/8192 --- L(Train): 0.4070; L(Val): 0.4257; Reg Param: 0.0001; Time: 1.03\n",
      "Epoch 134/8192 --- L(Train): 0.4078; L(Val): 0.4256; Reg Param: 0.0001; Time: 1.03\n",
      "Epoch 135/8192 --- L(Train): 0.4171; L(Val): 0.4255; Reg Param: 0.0001; Time: 1.01\n",
      "Epoch 136/8192 --- L(Train): 0.4233; L(Val): 0.4255; Reg Param: 0.0001; Time: 1.12\n",
      "Epoch 137/8192 --- L(Train): 0.4144; L(Val): 0.4255; Reg Param: 0.0001; Time: 1.21\n",
      "Epoch 138/8192 --- L(Train): 0.4095; L(Val): 0.4255; Reg Param: 0.0001; Time: 1.06\n",
      "Epoch 139/8192 --- L(Train): 0.4167; L(Val): 0.4256; Reg Param: 0.0001; Time: 1.06\n",
      "Epoch 140/8192 --- L(Train): 0.3974; L(Val): 0.4257; Reg Param: 0.0001; Time: 1.09\n",
      "Epoch 141/8192 --- L(Train): 0.4099; L(Val): 0.4258; Reg Param: 0.0001; Time: 1.00\n",
      "Epoch 142/8192 --- L(Train): 0.4086; L(Val): 0.4259; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 143/8192 --- L(Train): 0.3997; L(Val): 0.4260; Reg Param: 0.0001; Time: 1.15\n",
      "Epoch 144/8192 --- L(Train): 0.4000; L(Val): 0.4261; Reg Param: 0.0001; Time: 1.00\n",
      "Epoch 145/8192 --- L(Train): 0.4156; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.98\n",
      "Epoch 146/8192 --- L(Train): 0.4163; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 147/8192 --- L(Train): 0.4190; L(Val): 0.4260; Reg Param: 0.0001; Time: 1.02\n",
      "Epoch 148/8192 --- L(Train): 0.4145; L(Val): 0.4260; Reg Param: 0.0001; Time: 1.14\n",
      "Epoch 149/8192 --- L(Train): 0.4130; L(Val): 0.4260; Reg Param: 0.0001; Time: 1.17\n",
      "Epoch 150/8192 --- L(Train): 0.4153; L(Val): 0.4260; Reg Param: 0.0001; Time: 11.96\n",
      "Epoch 151/8192 --- L(Train): 0.4126; L(Val): 0.4260; Reg Param: 0.0001; Time: 1.29\n",
      "Epoch 152/8192 --- L(Train): 0.4188; L(Val): 0.4260; Reg Param: 0.0001; Time: 1.12\n",
      "Epoch 153/8192 --- L(Train): 0.4121; L(Val): 0.4261; Reg Param: 0.0001; Time: 1.13\n",
      "Epoch 154/8192 --- L(Train): 0.4132; L(Val): 0.4263; Reg Param: 0.0001; Time: 1.37\n",
      "Epoch 155/8192 --- L(Train): 0.4150; L(Val): 0.4264; Reg Param: 0.0001; Time: 1.13\n",
      "Epoch 156/8192 --- L(Train): 0.4129; L(Val): 0.4265; Reg Param: 0.0001; Time: 1.23\n",
      "Epoch 157/8192 --- L(Train): 0.4164; L(Val): 0.4265; Reg Param: 0.0001; Time: 1.06\n",
      "Epoch 158/8192 --- L(Train): 0.4141; L(Val): 0.4264; Reg Param: 0.0001; Time: 1.36\n",
      "Epoch 159/8192 --- L(Train): 0.4097; L(Val): 0.4262; Reg Param: 0.0001; Time: 1.30\n",
      "Epoch 160/8192 --- L(Train): 0.4040; L(Val): 0.4261; Reg Param: 0.0001; Time: 1.13\n",
      "Epoch 161/8192 --- L(Train): 0.4152; L(Val): 0.4261; Reg Param: 0.0001; Time: 1.32\n",
      "Epoch 162/8192 --- L(Train): 0.4164; L(Val): 0.4261; Reg Param: 0.0001; Time: 2.52\n",
      "Epoch 163/8192 --- L(Train): 0.4119; L(Val): 0.4261; Reg Param: 0.0001; Time: 1.57\n",
      "Epoch 164/8192 --- L(Train): 0.4129; L(Val): 0.4261; Reg Param: 0.0001; Time: 1.41\n",
      "Epoch 165/8192 --- L(Train): 0.4104; L(Val): 0.4261; Reg Param: 0.0001; Time: 2.43\n",
      "Epoch 166/8192 --- L(Train): 0.4090; L(Val): 0.4261; Reg Param: 0.0001; Time: 1.10\n",
      "Epoch 167/8192 --- L(Train): 0.4064; L(Val): 0.4260; Reg Param: 0.0001; Time: 1.13\n",
      "Epoch 168/8192 --- L(Train): 0.4087; L(Val): 0.4259; Reg Param: 0.0001; Time: 1.02\n",
      "Epoch 169/8192 --- L(Train): 0.4069; L(Val): 0.4259; Reg Param: 0.0001; Time: 1.12\n",
      "Epoch 170/8192 --- L(Train): 0.4150; L(Val): 0.4259; Reg Param: 0.0001; Time: 1.03\n",
      "Epoch 171/8192 --- L(Train): 0.4189; L(Val): 0.4260; Reg Param: 0.0001; Time: 1.00\n",
      "Epoch 172/8192 --- L(Train): 0.3969; L(Val): 0.4261; Reg Param: 0.0001; Time: 1.06\n",
      "Epoch 173/8192 --- L(Train): 0.4227; L(Val): 0.4263; Reg Param: 0.0001; Time: 1.00\n",
      "Epoch 174/8192 --- L(Train): 0.4004; L(Val): 0.4265; Reg Param: 0.0001; Time: 1.03\n",
      "Epoch 175/8192 --- L(Train): 0.4076; L(Val): 0.4263; Reg Param: 0.0001; Time: 1.26\n",
      "Epoch 176/8192 --- L(Train): 0.4056; L(Val): 0.4260; Reg Param: 0.0001; Time: 1.00\n",
      "Epoch 177/8192 --- L(Train): 0.4008; L(Val): 0.4258; Reg Param: 0.0001; Time: 1.18\n",
      "Epoch 178/8192 --- L(Train): 0.4144; L(Val): 0.4256; Reg Param: 0.0001; Time: 1.27\n",
      "Epoch 179/8192 --- L(Train): 0.4180; L(Val): 0.4256; Reg Param: 0.0001; Time: 1.59\n",
      "Epoch 180/8192 --- L(Train): 0.4071; L(Val): 0.4256; Reg Param: 0.0001; Time: 1.18\n",
      "Epoch 181/8192 --- L(Train): 0.4097; L(Val): 0.4257; Reg Param: 0.0001; Time: 1.30\n",
      "Epoch 182/8192 --- L(Train): 0.4022; L(Val): 0.4258; Reg Param: 0.0001; Time: 1.36\n",
      "Epoch 183/8192 --- L(Train): 0.4163; L(Val): 0.4260; Reg Param: 0.0001; Time: 1.33\n",
      "Epoch 184/8192 --- L(Train): 0.4193; L(Val): 0.4262; Reg Param: 0.0001; Time: 1.08\n",
      "Epoch 185/8192 --- L(Train): 0.4053; L(Val): 0.4263; Reg Param: 0.0001; Time: 1.12\n",
      "Epoch 186/8192 --- L(Train): 0.4128; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.97\n",
      "Epoch 187/8192 --- L(Train): 0.4037; L(Val): 0.4262; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 188/8192 --- L(Train): 0.4007; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 189/8192 --- L(Train): 0.4222; L(Val): 0.4260; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 190/8192 --- L(Train): 0.4079; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.95\n",
      "Epoch 191/8192 --- L(Train): 0.3984; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.95\n",
      "Epoch 192/8192 --- L(Train): 0.4093; L(Val): 0.4262; Reg Param: 0.0001; Time: 0.95\n",
      "Epoch 193/8192 --- L(Train): 0.4095; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 194/8192 --- L(Train): 0.4055; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.97\n",
      "Epoch 195/8192 --- L(Train): 0.3990; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 196/8192 --- L(Train): 0.4152; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 197/8192 --- L(Train): 0.4078; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 198/8192 --- L(Train): 0.4115; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 199/8192 --- L(Train): 0.4152; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 200/8192 --- L(Train): 0.4143; L(Val): 0.4263; Reg Param: 0.0001; Time: 10.25\n",
      "Epoch 201/8192 --- L(Train): 0.4014; L(Val): 0.4263; Reg Param: 0.0001; Time: 1.14\n",
      "Epoch 202/8192 --- L(Train): 0.3982; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 203/8192 --- L(Train): 0.4196; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 204/8192 --- L(Train): 0.4040; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 205/8192 --- L(Train): 0.3952; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 206/8192 --- L(Train): 0.4141; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 207/8192 --- L(Train): 0.3872; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.97\n",
      "Epoch 208/8192 --- L(Train): 0.4003; L(Val): 0.4265; Reg Param: 0.0001; Time: 1.10\n",
      "Epoch 209/8192 --- L(Train): 0.4106; L(Val): 0.4264; Reg Param: 0.0001; Time: 1.06\n",
      "Epoch 210/8192 --- L(Train): 0.4105; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 211/8192 --- L(Train): 0.4095; L(Val): 0.4265; Reg Param: 0.0001; Time: 1.83\n",
      "Epoch 212/8192 --- L(Train): 0.4097; L(Val): 0.4266; Reg Param: 0.0001; Time: 1.49\n",
      "Epoch 213/8192 --- L(Train): 0.4183; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 214/8192 --- L(Train): 0.4067; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 215/8192 --- L(Train): 0.4172; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.98\n",
      "Epoch 216/8192 --- L(Train): 0.4083; L(Val): 0.4269; Reg Param: 0.0001; Time: 1.05\n",
      "Epoch 217/8192 --- L(Train): 0.4082; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.97\n",
      "Epoch 218/8192 --- L(Train): 0.4120; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 219/8192 --- L(Train): 0.4090; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.95\n",
      "Epoch 220/8192 --- L(Train): 0.4105; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 221/8192 --- L(Train): 0.3998; L(Val): 0.4272; Reg Param: 0.0001; Time: 1.01\n",
      "Epoch 222/8192 --- L(Train): 0.4195; L(Val): 0.4273; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 223/8192 --- L(Train): 0.4126; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 224/8192 --- L(Train): 0.4185; L(Val): 0.4273; Reg Param: 0.0001; Time: 1.12\n",
      "Epoch 225/8192 --- L(Train): 0.3908; L(Val): 0.4272; Reg Param: 0.0001; Time: 1.20\n",
      "Epoch 226/8192 --- L(Train): 0.4015; L(Val): 0.4271; Reg Param: 0.0001; Time: 1.10\n",
      "Epoch 227/8192 --- L(Train): 0.4037; L(Val): 0.4271; Reg Param: 0.0001; Time: 1.08\n",
      "Epoch 228/8192 --- L(Train): 0.4118; L(Val): 0.4271; Reg Param: 0.0001; Time: 1.00\n",
      "Epoch 229/8192 --- L(Train): 0.3980; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.97\n",
      "Epoch 230/8192 --- L(Train): 0.3992; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.98\n",
      "Epoch 231/8192 --- L(Train): 0.4093; L(Val): 0.4273; Reg Param: 0.0001; Time: 1.04\n",
      "Epoch 232/8192 --- L(Train): 0.4024; L(Val): 0.4275; Reg Param: 0.0001; Time: 1.18\n",
      "Epoch 233/8192 --- L(Train): 0.3951; L(Val): 0.4275; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 234/8192 --- L(Train): 0.3989; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 235/8192 --- L(Train): 0.3976; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.97\n",
      "Epoch 236/8192 --- L(Train): 0.4017; L(Val): 0.4271; Reg Param: 0.0001; Time: 1.03\n",
      "Epoch 237/8192 --- L(Train): 0.4069; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 238/8192 --- L(Train): 0.4032; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.98\n",
      "Epoch 239/8192 --- L(Train): 0.4063; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 240/8192 --- L(Train): 0.4142; L(Val): 0.4273; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 241/8192 --- L(Train): 0.4075; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 242/8192 --- L(Train): 0.4106; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 243/8192 --- L(Train): 0.4063; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 244/8192 --- L(Train): 0.3939; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 245/8192 --- L(Train): 0.4064; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 246/8192 --- L(Train): 0.4112; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 247/8192 --- L(Train): 0.4080; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 248/8192 --- L(Train): 0.4037; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 249/8192 --- L(Train): 0.4131; L(Val): 0.4270; Reg Param: 0.0001; Time: 1.09\n",
      "Epoch 250/8192 --- L(Train): 0.4138; L(Val): 0.4270; Reg Param: 0.0001; Time: 12.29\n",
      "Epoch 251/8192 --- L(Train): 0.4002; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.96\n",
      "Epoch 252/8192 --- L(Train): 0.4035; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 253/8192 --- L(Train): 0.4043; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 254/8192 --- L(Train): 0.4036; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 255/8192 --- L(Train): 0.4027; L(Val): 0.4271; Reg Param: 0.0001; Time: 1.10\n",
      "Epoch 256/8192 --- L(Train): 0.4033; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 257/8192 --- L(Train): 0.3944; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.95\n",
      "Epoch 258/8192 --- L(Train): 0.3986; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 259/8192 --- L(Train): 0.3958; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 260/8192 --- L(Train): 0.4043; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 261/8192 --- L(Train): 0.4055; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.97\n",
      "Epoch 262/8192 --- L(Train): 0.4016; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 263/8192 --- L(Train): 0.3918; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 264/8192 --- L(Train): 0.4095; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 265/8192 --- L(Train): 0.3927; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 266/8192 --- L(Train): 0.3932; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 267/8192 --- L(Train): 0.3962; L(Val): 0.4266; Reg Param: 0.0001; Time: 1.06\n",
      "Epoch 268/8192 --- L(Train): 0.4078; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 269/8192 --- L(Train): 0.4073; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 270/8192 --- L(Train): 0.4065; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 271/8192 --- L(Train): 0.4070; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 272/8192 --- L(Train): 0.4131; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 273/8192 --- L(Train): 0.3945; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 274/8192 --- L(Train): 0.4156; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 275/8192 --- L(Train): 0.4010; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 276/8192 --- L(Train): 0.4136; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 277/8192 --- L(Train): 0.4029; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 278/8192 --- L(Train): 0.4051; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 279/8192 --- L(Train): 0.4137; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.96\n",
      "Epoch 280/8192 --- L(Train): 0.4040; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 281/8192 --- L(Train): 0.4068; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 282/8192 --- L(Train): 0.3889; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 283/8192 --- L(Train): 0.4229; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 284/8192 --- L(Train): 0.3933; L(Val): 0.4266; Reg Param: 0.0001; Time: 1.30\n",
      "Epoch 285/8192 --- L(Train): 0.3973; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 286/8192 --- L(Train): 0.4162; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 287/8192 --- L(Train): 0.4038; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 288/8192 --- L(Train): 0.3972; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 289/8192 --- L(Train): 0.4036; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 290/8192 --- L(Train): 0.4061; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 291/8192 --- L(Train): 0.3952; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 292/8192 --- L(Train): 0.4148; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 293/8192 --- L(Train): 0.4016; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.97\n",
      "Epoch 294/8192 --- L(Train): 0.3933; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 295/8192 --- L(Train): 0.4014; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 296/8192 --- L(Train): 0.4099; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 297/8192 --- L(Train): 0.4078; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 298/8192 --- L(Train): 0.3998; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 299/8192 --- L(Train): 0.3843; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 300/8192 --- L(Train): 0.3949; L(Val): 0.4264; Reg Param: 0.0001; Time: 9.47\n",
      "Epoch 301/8192 --- L(Train): 0.4176; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 302/8192 --- L(Train): 0.3964; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 303/8192 --- L(Train): 0.4046; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 304/8192 --- L(Train): 0.4071; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 305/8192 --- L(Train): 0.3861; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 306/8192 --- L(Train): 0.4021; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 307/8192 --- L(Train): 0.4081; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 308/8192 --- L(Train): 0.3927; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 309/8192 --- L(Train): 0.4068; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 310/8192 --- L(Train): 0.4022; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 311/8192 --- L(Train): 0.4082; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 312/8192 --- L(Train): 0.4016; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 313/8192 --- L(Train): 0.4152; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 314/8192 --- L(Train): 0.3990; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 315/8192 --- L(Train): 0.4028; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 316/8192 --- L(Train): 0.3944; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 317/8192 --- L(Train): 0.4107; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 318/8192 --- L(Train): 0.4052; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.98\n",
      "Epoch 319/8192 --- L(Train): 0.4001; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.98\n",
      "Epoch 320/8192 --- L(Train): 0.4039; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 321/8192 --- L(Train): 0.4168; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 322/8192 --- L(Train): 0.3967; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 323/8192 --- L(Train): 0.4039; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 324/8192 --- L(Train): 0.4054; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 325/8192 --- L(Train): 0.4023; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 326/8192 --- L(Train): 0.4123; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 327/8192 --- L(Train): 0.4052; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 328/8192 --- L(Train): 0.4023; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 329/8192 --- L(Train): 0.3991; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 330/8192 --- L(Train): 0.3924; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 331/8192 --- L(Train): 0.3905; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 332/8192 --- L(Train): 0.4038; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 333/8192 --- L(Train): 0.3962; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 334/8192 --- L(Train): 0.3986; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 335/8192 --- L(Train): 0.3988; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 336/8192 --- L(Train): 0.4071; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 337/8192 --- L(Train): 0.3958; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 338/8192 --- L(Train): 0.4017; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 339/8192 --- L(Train): 0.3962; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 340/8192 --- L(Train): 0.4097; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 341/8192 --- L(Train): 0.4009; L(Val): 0.4273; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 342/8192 --- L(Train): 0.4023; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 343/8192 --- L(Train): 0.4001; L(Val): 0.4273; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 344/8192 --- L(Train): 0.4062; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 345/8192 --- L(Train): 0.3961; L(Val): 0.4270; Reg Param: 0.0001; Time: 1.00\n",
      "Epoch 346/8192 --- L(Train): 0.4009; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 347/8192 --- L(Train): 0.3975; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 348/8192 --- L(Train): 0.4048; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 349/8192 --- L(Train): 0.4090; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 350/8192 --- L(Train): 0.4088; L(Val): 0.4269; Reg Param: 0.0001; Time: 9.04\n",
      "Epoch 351/8192 --- L(Train): 0.4170; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 352/8192 --- L(Train): 0.4018; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 353/8192 --- L(Train): 0.4041; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 354/8192 --- L(Train): 0.3991; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 355/8192 --- L(Train): 0.3940; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 356/8192 --- L(Train): 0.3952; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 357/8192 --- L(Train): 0.3904; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 358/8192 --- L(Train): 0.3951; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 359/8192 --- L(Train): 0.3993; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 360/8192 --- L(Train): 0.3893; L(Val): 0.4271; Reg Param: 0.0001; Time: 1.00\n",
      "Epoch 361/8192 --- L(Train): 0.4039; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 362/8192 --- L(Train): 0.3993; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 363/8192 --- L(Train): 0.4084; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 364/8192 --- L(Train): 0.4087; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 365/8192 --- L(Train): 0.4006; L(Val): 0.4273; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 366/8192 --- L(Train): 0.3975; L(Val): 0.4273; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 367/8192 --- L(Train): 0.3844; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 368/8192 --- L(Train): 0.4075; L(Val): 0.4275; Reg Param: 0.0001; Time: 0.97\n",
      "Epoch 369/8192 --- L(Train): 0.3967; L(Val): 0.4276; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 370/8192 --- L(Train): 0.4093; L(Val): 0.4275; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 371/8192 --- L(Train): 0.3992; L(Val): 0.4275; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 372/8192 --- L(Train): 0.4034; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 373/8192 --- L(Train): 0.3953; L(Val): 0.4273; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 374/8192 --- L(Train): 0.4001; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 375/8192 --- L(Train): 0.4015; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 376/8192 --- L(Train): 0.4020; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.96\n",
      "Epoch 377/8192 --- L(Train): 0.3950; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 378/8192 --- L(Train): 0.4049; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 379/8192 --- L(Train): 0.3980; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 380/8192 --- L(Train): 0.3940; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 381/8192 --- L(Train): 0.3952; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 382/8192 --- L(Train): 0.4038; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 383/8192 --- L(Train): 0.3841; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 384/8192 --- L(Train): 0.4011; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 385/8192 --- L(Train): 0.3956; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 386/8192 --- L(Train): 0.4025; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 387/8192 --- L(Train): 0.3967; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 388/8192 --- L(Train): 0.3902; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 389/8192 --- L(Train): 0.3966; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 390/8192 --- L(Train): 0.4012; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 391/8192 --- L(Train): 0.4030; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 392/8192 --- L(Train): 0.4078; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 393/8192 --- L(Train): 0.3971; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 394/8192 --- L(Train): 0.3937; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 395/8192 --- L(Train): 0.3800; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 396/8192 --- L(Train): 0.3878; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 397/8192 --- L(Train): 0.3947; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 398/8192 --- L(Train): 0.4002; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 399/8192 --- L(Train): 0.3846; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 400/8192 --- L(Train): 0.3893; L(Val): 0.4271; Reg Param: 0.0001; Time: 9.10\n",
      "Epoch 401/8192 --- L(Train): 0.3921; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 402/8192 --- L(Train): 0.3948; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 403/8192 --- L(Train): 0.3992; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 404/8192 --- L(Train): 0.3927; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 405/8192 --- L(Train): 0.3999; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 406/8192 --- L(Train): 0.3984; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 407/8192 --- L(Train): 0.3869; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 408/8192 --- L(Train): 0.3891; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 409/8192 --- L(Train): 0.3982; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 410/8192 --- L(Train): 0.3921; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 411/8192 --- L(Train): 0.3990; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 412/8192 --- L(Train): 0.4009; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 413/8192 --- L(Train): 0.3864; L(Val): 0.4272; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 414/8192 --- L(Train): 0.3970; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 415/8192 --- L(Train): 0.3919; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 416/8192 --- L(Train): 0.3932; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 417/8192 --- L(Train): 0.4002; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.88\n",
      "Epoch 418/8192 --- L(Train): 0.4058; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 419/8192 --- L(Train): 0.3969; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 420/8192 --- L(Train): 0.4050; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.95\n",
      "Epoch 421/8192 --- L(Train): 0.3921; L(Val): 0.4266; Reg Param: 0.0001; Time: 1.00\n",
      "Epoch 422/8192 --- L(Train): 0.4019; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 423/8192 --- L(Train): 0.3845; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 424/8192 --- L(Train): 0.4010; L(Val): 0.4264; Reg Param: 0.0001; Time: 1.11\n",
      "Epoch 425/8192 --- L(Train): 0.3948; L(Val): 0.4264; Reg Param: 0.0001; Time: 1.01\n",
      "Epoch 426/8192 --- L(Train): 0.3954; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 427/8192 --- L(Train): 0.3927; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 428/8192 --- L(Train): 0.4015; L(Val): 0.4260; Reg Param: 0.0001; Time: 0.86\n",
      "Epoch 429/8192 --- L(Train): 0.4038; L(Val): 0.4259; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 430/8192 --- L(Train): 0.3897; L(Val): 0.4258; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 431/8192 --- L(Train): 0.3979; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 432/8192 --- L(Train): 0.3981; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 433/8192 --- L(Train): 0.4011; L(Val): 0.4260; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 434/8192 --- L(Train): 0.3994; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 435/8192 --- L(Train): 0.3909; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 436/8192 --- L(Train): 0.3937; L(Val): 0.4262; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 437/8192 --- L(Train): 0.3835; L(Val): 0.4259; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 438/8192 --- L(Train): 0.3985; L(Val): 0.4258; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 439/8192 --- L(Train): 0.4018; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 440/8192 --- L(Train): 0.3986; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.96\n",
      "Epoch 441/8192 --- L(Train): 0.4010; L(Val): 0.4258; Reg Param: 0.0001; Time: 0.97\n",
      "Epoch 442/8192 --- L(Train): 0.3965; L(Val): 0.4258; Reg Param: 0.0001; Time: 1.03\n",
      "Epoch 443/8192 --- L(Train): 0.3891; L(Val): 0.4258; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 444/8192 --- L(Train): 0.3993; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 445/8192 --- L(Train): 0.3774; L(Val): 0.4254; Reg Param: 0.0001; Time: 0.94\n",
      "Epoch 446/8192 --- L(Train): 0.3933; L(Val): 0.4254; Reg Param: 0.0001; Time: 1.00\n",
      "Epoch 447/8192 --- L(Train): 0.3877; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.89\n",
      "Epoch 448/8192 --- L(Train): 0.3974; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.99\n",
      "Epoch 449/8192 --- L(Train): 0.3892; L(Val): 0.4258; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 450/8192 --- L(Train): 0.3891; L(Val): 0.4258; Reg Param: 0.0001; Time: 10.23\n",
      "Epoch 451/8192 --- L(Train): 0.3963; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.91\n",
      "Epoch 452/8192 --- L(Train): 0.4011; L(Val): 0.4253; Reg Param: 0.0001; Time: 1.02\n",
      "Epoch 453/8192 --- L(Train): 0.3904; L(Val): 0.4251; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 454/8192 --- L(Train): 0.4046; L(Val): 0.4251; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 455/8192 --- L(Train): 0.4055; L(Val): 0.4251; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 456/8192 --- L(Train): 0.3960; L(Val): 0.4253; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 457/8192 --- L(Train): 0.4015; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 458/8192 --- L(Train): 0.4005; L(Val): 0.4259; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 459/8192 --- L(Train): 0.3920; L(Val): 0.4262; Reg Param: 0.0001; Time: 0.93\n",
      "Epoch 460/8192 --- L(Train): 0.3986; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 461/8192 --- L(Train): 0.3974; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 462/8192 --- L(Train): 0.4036; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.75\n",
      "Epoch 463/8192 --- L(Train): 0.3989; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 464/8192 --- L(Train): 0.3979; L(Val): 0.4262; Reg Param: 0.0001; Time: 0.75\n",
      "Epoch 465/8192 --- L(Train): 0.3926; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.75\n",
      "Epoch 466/8192 --- L(Train): 0.4020; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.75\n",
      "Epoch 467/8192 --- L(Train): 0.3835; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 468/8192 --- L(Train): 0.3962; L(Val): 0.4262; Reg Param: 0.0001; Time: 0.75\n",
      "Epoch 469/8192 --- L(Train): 0.3966; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 470/8192 --- L(Train): 0.3853; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 471/8192 --- L(Train): 0.3924; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.75\n",
      "Epoch 472/8192 --- L(Train): 0.3802; L(Val): 0.4271; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 473/8192 --- L(Train): 0.3933; L(Val): 0.4273; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 474/8192 --- L(Train): 0.3899; L(Val): 0.4275; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 475/8192 --- L(Train): 0.3867; L(Val): 0.4274; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 476/8192 --- L(Train): 0.3992; L(Val): 0.4270; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 477/8192 --- L(Train): 0.4033; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 478/8192 --- L(Train): 0.3949; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 479/8192 --- L(Train): 0.3947; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 480/8192 --- L(Train): 0.3868; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 481/8192 --- L(Train): 0.3944; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 482/8192 --- L(Train): 0.3919; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 483/8192 --- L(Train): 0.3980; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 484/8192 --- L(Train): 0.3957; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 485/8192 --- L(Train): 0.3836; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 486/8192 --- L(Train): 0.3899; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 487/8192 --- L(Train): 0.3852; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 488/8192 --- L(Train): 0.3985; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 489/8192 --- L(Train): 0.3879; L(Val): 0.4260; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 490/8192 --- L(Train): 0.3862; L(Val): 0.4258; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 491/8192 --- L(Train): 0.3987; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 492/8192 --- L(Train): 0.4009; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 493/8192 --- L(Train): 0.3871; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 494/8192 --- L(Train): 0.4013; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 495/8192 --- L(Train): 0.3939; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 496/8192 --- L(Train): 0.3982; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 497/8192 --- L(Train): 0.3895; L(Val): 0.4259; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 498/8192 --- L(Train): 0.3860; L(Val): 0.4260; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 499/8192 --- L(Train): 0.3946; L(Val): 0.4260; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 500/8192 --- L(Train): 0.3975; L(Val): 0.4260; Reg Param: 0.0001; Time: 8.09\n",
      "Epoch 501/8192 --- L(Train): 0.3925; L(Val): 0.4258; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 502/8192 --- L(Train): 0.3978; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 503/8192 --- L(Train): 0.3933; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.97\n",
      "Epoch 504/8192 --- L(Train): 0.3983; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 505/8192 --- L(Train): 0.3915; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.92\n",
      "Epoch 506/8192 --- L(Train): 0.3816; L(Val): 0.4258; Reg Param: 0.0001; Time: 0.87\n",
      "Epoch 507/8192 --- L(Train): 0.3904; L(Val): 0.4259; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 508/8192 --- L(Train): 0.4046; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.82\n",
      "Epoch 509/8192 --- L(Train): 0.3905; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 510/8192 --- L(Train): 0.3971; L(Val): 0.4262; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 511/8192 --- L(Train): 0.3802; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 512/8192 --- L(Train): 0.3998; L(Val): 0.4259; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 513/8192 --- L(Train): 0.3929; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 514/8192 --- L(Train): 0.3995; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 515/8192 --- L(Train): 0.3851; L(Val): 0.4254; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 516/8192 --- L(Train): 0.3872; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 517/8192 --- L(Train): 0.3889; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 518/8192 --- L(Train): 0.3867; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.84\n",
      "Epoch 519/8192 --- L(Train): 0.3917; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 520/8192 --- L(Train): 0.3958; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.75\n",
      "Epoch 521/8192 --- L(Train): 0.3909; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 522/8192 --- L(Train): 0.3944; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 523/8192 --- L(Train): 0.3930; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 524/8192 --- L(Train): 0.3963; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 525/8192 --- L(Train): 0.3777; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 526/8192 --- L(Train): 0.3871; L(Val): 0.4254; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 527/8192 --- L(Train): 0.3944; L(Val): 0.4253; Reg Param: 0.0001; Time: 0.90\n",
      "Epoch 528/8192 --- L(Train): 0.3855; L(Val): 0.4253; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 529/8192 --- L(Train): 0.4044; L(Val): 0.4253; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 530/8192 --- L(Train): 0.3865; L(Val): 0.4254; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 531/8192 --- L(Train): 0.3937; L(Val): 0.4254; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 532/8192 --- L(Train): 0.3967; L(Val): 0.4254; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 533/8192 --- L(Train): 0.3833; L(Val): 0.4254; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 534/8192 --- L(Train): 0.3723; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 535/8192 --- L(Train): 0.3965; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 536/8192 --- L(Train): 0.3884; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 537/8192 --- L(Train): 0.3950; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.85\n",
      "Epoch 538/8192 --- L(Train): 0.3888; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 539/8192 --- L(Train): 0.3902; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 540/8192 --- L(Train): 0.3944; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 541/8192 --- L(Train): 0.3850; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 542/8192 --- L(Train): 0.3921; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 543/8192 --- L(Train): 0.3918; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 544/8192 --- L(Train): 0.3950; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 545/8192 --- L(Train): 0.3947; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.83\n",
      "Epoch 546/8192 --- L(Train): 0.3797; L(Val): 0.4260; Reg Param: 0.0001; Time: 0.77\n",
      "Epoch 547/8192 --- L(Train): 0.3922; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.79\n",
      "Epoch 548/8192 --- L(Train): 0.3906; L(Val): 0.4260; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 549/8192 --- L(Train): 0.3865; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 550/8192 --- L(Train): 0.3921; L(Val): 0.4261; Reg Param: 0.0001; Time: 8.41\n",
      "Epoch 551/8192 --- L(Train): 0.3771; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 552/8192 --- L(Train): 0.3928; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 553/8192 --- L(Train): 0.3856; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 554/8192 --- L(Train): 0.3855; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 555/8192 --- L(Train): 0.3949; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 556/8192 --- L(Train): 0.3893; L(Val): 0.4261; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 557/8192 --- L(Train): 0.3926; L(Val): 0.4262; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 558/8192 --- L(Train): 0.3896; L(Val): 0.4264; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 559/8192 --- L(Train): 0.3917; L(Val): 0.4265; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 560/8192 --- L(Train): 0.3966; L(Val): 0.4266; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 561/8192 --- L(Train): 0.3838; L(Val): 0.4268; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 562/8192 --- L(Train): 0.3837; L(Val): 0.4269; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 563/8192 --- L(Train): 0.3833; L(Val): 0.4267; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 564/8192 --- L(Train): 0.3886; L(Val): 0.4263; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 565/8192 --- L(Train): 0.3889; L(Val): 0.4259; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 566/8192 --- L(Train): 0.3967; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 567/8192 --- L(Train): 0.3853; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 568/8192 --- L(Train): 0.3898; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 569/8192 --- L(Train): 0.3878; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 570/8192 --- L(Train): 0.3875; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 571/8192 --- L(Train): 0.4042; L(Val): 0.4257; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 572/8192 --- L(Train): 0.3857; L(Val): 0.4259; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 573/8192 --- L(Train): 0.3981; L(Val): 0.4260; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 574/8192 --- L(Train): 0.4067; L(Val): 0.4260; Reg Param: 0.0001; Time: 0.73\n",
      "Epoch 575/8192 --- L(Train): 0.3842; L(Val): 0.4258; Reg Param: 0.0001; Time: 0.81\n",
      "Epoch 576/8192 --- L(Train): 0.3938; L(Val): 0.4256; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 577/8192 --- L(Train): 0.3806; L(Val): 0.4254; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 578/8192 --- L(Train): 0.3856; L(Val): 0.4252; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 579/8192 --- L(Train): 0.3852; L(Val): 0.4252; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 580/8192 --- L(Train): 0.3978; L(Val): 0.4252; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 581/8192 --- L(Train): 0.3917; L(Val): 0.4252; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 582/8192 --- L(Train): 0.3795; L(Val): 0.4254; Reg Param: 0.0001; Time: 0.73\n",
      "Epoch 583/8192 --- L(Train): 0.3884; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 584/8192 --- L(Train): 0.3708; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 585/8192 --- L(Train): 0.3859; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 586/8192 --- L(Train): 0.3934; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 587/8192 --- L(Train): 0.3802; L(Val): 0.4254; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 588/8192 --- L(Train): 0.3855; L(Val): 0.4254; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 589/8192 --- L(Train): 0.3870; L(Val): 0.4254; Reg Param: 0.0001; Time: 0.74\n",
      "Epoch 590/8192 --- L(Train): 0.3935; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.75\n",
      "Epoch 591/8192 --- L(Train): 0.3818; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 592/8192 --- L(Train): 0.3836; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 593/8192 --- L(Train): 0.3779; L(Val): 0.4255; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 594/8192 --- L(Train): 0.3979; L(Val): 0.4253; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 595/8192 --- L(Train): 0.3894; L(Val): 0.4253; Reg Param: 0.0001; Time: 0.76\n",
      "Epoch 596/8192 --- L(Train): 0.3974; L(Val): 0.4253; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 597/8192 --- L(Train): 0.3854; L(Val): 0.4252; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 598/8192 --- L(Train): 0.3996; L(Val): 0.4251; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 599/8192 --- L(Train): 0.3873; L(Val): 0.4251; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 600/8192 --- L(Train): 0.3863; L(Val): 0.4251; Reg Param: 0.0008; Time: 7.59\n",
      "Epoch 601/8192 --- L(Train): 0.3700; L(Val): 0.4250; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 602/8192 --- L(Train): 0.3913; L(Val): 0.4250; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 603/8192 --- L(Train): 0.3825; L(Val): 0.4251; Reg Param: 0.0008; Time: 0.71\n",
      "Epoch 604/8192 --- L(Train): 0.3890; L(Val): 0.4252; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 605/8192 --- L(Train): 0.3789; L(Val): 0.4254; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 606/8192 --- L(Train): 0.3866; L(Val): 0.4256; Reg Param: 0.0008; Time: 0.69\n",
      "Epoch 607/8192 --- L(Train): 0.3751; L(Val): 0.4256; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 608/8192 --- L(Train): 0.3835; L(Val): 0.4256; Reg Param: 0.0008; Time: 0.77\n",
      "Epoch 609/8192 --- L(Train): 0.3812; L(Val): 0.4254; Reg Param: 0.0008; Time: 0.68\n",
      "Epoch 610/8192 --- L(Train): 0.3920; L(Val): 0.4253; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 611/8192 --- L(Train): 0.3784; L(Val): 0.4253; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 612/8192 --- L(Train): 0.3802; L(Val): 0.4256; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 613/8192 --- L(Train): 0.3832; L(Val): 0.4257; Reg Param: 0.0008; Time: 0.71\n",
      "Epoch 614/8192 --- L(Train): 0.3848; L(Val): 0.4254; Reg Param: 0.0008; Time: 0.71\n",
      "Epoch 615/8192 --- L(Train): 0.3834; L(Val): 0.4253; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 616/8192 --- L(Train): 0.3814; L(Val): 0.4253; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 617/8192 --- L(Train): 0.3802; L(Val): 0.4252; Reg Param: 0.0008; Time: 0.71\n",
      "Epoch 618/8192 --- L(Train): 0.3920; L(Val): 0.4251; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 619/8192 --- L(Train): 0.3702; L(Val): 0.4252; Reg Param: 0.0008; Time: 0.71\n",
      "Epoch 620/8192 --- L(Train): 0.3881; L(Val): 0.4253; Reg Param: 0.0008; Time: 0.71\n",
      "Epoch 621/8192 --- L(Train): 0.3809; L(Val): 0.4252; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 622/8192 --- L(Train): 0.3936; L(Val): 0.4252; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 623/8192 --- L(Train): 0.3821; L(Val): 0.4252; Reg Param: 0.0008; Time: 0.71\n",
      "Epoch 624/8192 --- L(Train): 0.3911; L(Val): 0.4252; Reg Param: 0.0008; Time: 0.84\n",
      "Epoch 625/8192 --- L(Train): 0.3779; L(Val): 0.4253; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 626/8192 --- L(Train): 0.3779; L(Val): 0.4253; Reg Param: 0.0008; Time: 0.69\n",
      "Epoch 627/8192 --- L(Train): 0.3906; L(Val): 0.4255; Reg Param: 0.0008; Time: 0.71\n",
      "Epoch 628/8192 --- L(Train): 0.3882; L(Val): 0.4256; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 629/8192 --- L(Train): 0.3949; L(Val): 0.4257; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 630/8192 --- L(Train): 0.3884; L(Val): 0.4258; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 631/8192 --- L(Train): 0.3928; L(Val): 0.4259; Reg Param: 0.0008; Time: 0.69\n",
      "Epoch 632/8192 --- L(Train): 0.3853; L(Val): 0.4260; Reg Param: 0.0008; Time: 0.69\n",
      "Epoch 633/8192 --- L(Train): 0.3779; L(Val): 0.4261; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 634/8192 --- L(Train): 0.3888; L(Val): 0.4262; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 635/8192 --- L(Train): 0.3781; L(Val): 0.4261; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 636/8192 --- L(Train): 0.3954; L(Val): 0.4260; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 637/8192 --- L(Train): 0.3877; L(Val): 0.4257; Reg Param: 0.0008; Time: 0.69\n",
      "Epoch 638/8192 --- L(Train): 0.3816; L(Val): 0.4255; Reg Param: 0.0008; Time: 0.69\n",
      "Epoch 639/8192 --- L(Train): 0.3910; L(Val): 0.4254; Reg Param: 0.0008; Time: 0.72\n",
      "Epoch 640/8192 --- L(Train): 0.3727; L(Val): 0.4254; Reg Param: 0.0008; Time: 0.69\n",
      "Epoch 641/8192 --- L(Train): 0.3853; L(Val): 0.4254; Reg Param: 0.0008; Time: 0.69\n",
      "Epoch 642/8192 --- L(Train): 0.3881; L(Val): 0.4257; Reg Param: 0.0008; Time: 0.71\n",
      "Epoch 643/8192 --- L(Train): 0.3839; L(Val): 0.4257; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 644/8192 --- L(Train): 0.3967; L(Val): 0.4255; Reg Param: 0.0008; Time: 0.69\n",
      "Epoch 645/8192 --- L(Train): 0.3829; L(Val): 0.4255; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 646/8192 --- L(Train): 0.3908; L(Val): 0.4255; Reg Param: 0.0008; Time: 0.70\n",
      "Epoch 647/8192 --- L(Train): 0.3884; L(Val): 0.4254; Reg Param: 0.0008; Time: 0.71\n",
      "Epoch 648/8192 --- L(Train): 0.3767; L(Val): 0.4252; Reg Param: 0.0008; Time: 0.71\n",
      "Epoch 649/8192 --- L(Train): 0.3769; L(Val): 0.4254; Reg Param: 0.0008; Time: 0.74\n",
      "Epoch 650/8192 --- L(Train): 0.3998; L(Val): 0.4254; Reg Param: 0.0017; Time: 7.65\n",
      "Epoch 651/8192 --- L(Train): 0.3932; L(Val): 0.4257; Reg Param: 0.0017; Time: 0.70\n",
      "Epoch 652/8192 --- L(Train): 0.3802; L(Val): 0.4255; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 653/8192 --- L(Train): 0.3835; L(Val): 0.4256; Reg Param: 0.0017; Time: 0.70\n",
      "Epoch 654/8192 --- L(Train): 0.3795; L(Val): 0.4260; Reg Param: 0.0017; Time: 0.70\n",
      "Epoch 655/8192 --- L(Train): 0.3767; L(Val): 0.4261; Reg Param: 0.0017; Time: 0.70\n",
      "Epoch 656/8192 --- L(Train): 0.3830; L(Val): 0.4261; Reg Param: 0.0017; Time: 0.77\n",
      "Epoch 657/8192 --- L(Train): 0.3954; L(Val): 0.4263; Reg Param: 0.0017; Time: 0.77\n",
      "Epoch 658/8192 --- L(Train): 0.3886; L(Val): 0.4267; Reg Param: 0.0017; Time: 0.70\n",
      "Epoch 659/8192 --- L(Train): 0.3858; L(Val): 0.4269; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 660/8192 --- L(Train): 0.3873; L(Val): 0.4268; Reg Param: 0.0017; Time: 0.70\n",
      "Epoch 661/8192 --- L(Train): 0.3828; L(Val): 0.4264; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 662/8192 --- L(Train): 0.3908; L(Val): 0.4264; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 663/8192 --- L(Train): 0.3836; L(Val): 0.4264; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 664/8192 --- L(Train): 0.3850; L(Val): 0.4264; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 665/8192 --- L(Train): 0.3724; L(Val): 0.4265; Reg Param: 0.0017; Time: 0.70\n",
      "Epoch 666/8192 --- L(Train): 0.3988; L(Val): 0.4267; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 667/8192 --- L(Train): 0.3862; L(Val): 0.4267; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 668/8192 --- L(Train): 0.3806; L(Val): 0.4265; Reg Param: 0.0017; Time: 0.67\n",
      "Epoch 669/8192 --- L(Train): 0.3942; L(Val): 0.4263; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 670/8192 --- L(Train): 0.3883; L(Val): 0.4261; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 671/8192 --- L(Train): 0.3874; L(Val): 0.4258; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 672/8192 --- L(Train): 0.3994; L(Val): 0.4257; Reg Param: 0.0017; Time: 0.70\n",
      "Epoch 673/8192 --- L(Train): 0.3750; L(Val): 0.4255; Reg Param: 0.0017; Time: 0.82\n",
      "Epoch 674/8192 --- L(Train): 0.3845; L(Val): 0.4255; Reg Param: 0.0017; Time: 0.71\n",
      "Epoch 675/8192 --- L(Train): 0.3895; L(Val): 0.4255; Reg Param: 0.0017; Time: 0.80\n",
      "Epoch 676/8192 --- L(Train): 0.3906; L(Val): 0.4257; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 677/8192 --- L(Train): 0.3836; L(Val): 0.4260; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 678/8192 --- L(Train): 0.4000; L(Val): 0.4259; Reg Param: 0.0017; Time: 0.67\n",
      "Epoch 679/8192 --- L(Train): 0.3901; L(Val): 0.4259; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 680/8192 --- L(Train): 0.3908; L(Val): 0.4259; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 681/8192 --- L(Train): 0.3849; L(Val): 0.4257; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 682/8192 --- L(Train): 0.3778; L(Val): 0.4255; Reg Param: 0.0017; Time: 0.67\n",
      "Epoch 683/8192 --- L(Train): 0.3840; L(Val): 0.4252; Reg Param: 0.0017; Time: 0.70\n",
      "Epoch 684/8192 --- L(Train): 0.3799; L(Val): 0.4250; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 685/8192 --- L(Train): 0.4009; L(Val): 0.4249; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 686/8192 --- L(Train): 0.3899; L(Val): 0.4249; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 687/8192 --- L(Train): 0.3860; L(Val): 0.4249; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 688/8192 --- L(Train): 0.3875; L(Val): 0.4250; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 689/8192 --- L(Train): 0.3722; L(Val): 0.4251; Reg Param: 0.0017; Time: 0.70\n",
      "Epoch 690/8192 --- L(Train): 0.3851; L(Val): 0.4252; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 691/8192 --- L(Train): 0.3818; L(Val): 0.4254; Reg Param: 0.0017; Time: 0.79\n",
      "Epoch 692/8192 --- L(Train): 0.3828; L(Val): 0.4255; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 693/8192 --- L(Train): 0.3859; L(Val): 0.4257; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 694/8192 --- L(Train): 0.3789; L(Val): 0.4258; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 695/8192 --- L(Train): 0.3797; L(Val): 0.4260; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 696/8192 --- L(Train): 0.3812; L(Val): 0.4262; Reg Param: 0.0017; Time: 0.68\n",
      "Epoch 697/8192 --- L(Train): 0.3863; L(Val): 0.4263; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 698/8192 --- L(Train): 0.3943; L(Val): 0.4265; Reg Param: 0.0017; Time: 0.67\n",
      "Epoch 699/8192 --- L(Train): 0.3808; L(Val): 0.4267; Reg Param: 0.0017; Time: 0.69\n",
      "Epoch 700/8192 --- L(Train): 0.3791; L(Val): 0.4267; Reg Param: 0.0031; Time: 7.50\n",
      "Epoch 701/8192 --- L(Train): 0.3821; L(Val): 0.4267; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 702/8192 --- L(Train): 0.3892; L(Val): 0.4264; Reg Param: 0.0031; Time: 0.69\n",
      "Epoch 703/8192 --- L(Train): 0.3719; L(Val): 0.4262; Reg Param: 0.0031; Time: 0.72\n",
      "Epoch 704/8192 --- L(Train): 0.3806; L(Val): 0.4261; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 705/8192 --- L(Train): 0.3771; L(Val): 0.4260; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 706/8192 --- L(Train): 0.3918; L(Val): 0.4260; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 707/8192 --- L(Train): 0.3882; L(Val): 0.4260; Reg Param: 0.0031; Time: 0.78\n",
      "Epoch 708/8192 --- L(Train): 0.3803; L(Val): 0.4259; Reg Param: 0.0031; Time: 0.75\n",
      "Epoch 709/8192 --- L(Train): 0.3804; L(Val): 0.4259; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 710/8192 --- L(Train): 0.3920; L(Val): 0.4259; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 711/8192 --- L(Train): 0.3930; L(Val): 0.4259; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 712/8192 --- L(Train): 0.3865; L(Val): 0.4259; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 713/8192 --- L(Train): 0.3733; L(Val): 0.4260; Reg Param: 0.0031; Time: 0.72\n",
      "Epoch 714/8192 --- L(Train): 0.3728; L(Val): 0.4262; Reg Param: 0.0031; Time: 0.79\n",
      "Epoch 715/8192 --- L(Train): 0.3865; L(Val): 0.4263; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 716/8192 --- L(Train): 0.3840; L(Val): 0.4265; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 717/8192 --- L(Train): 0.3762; L(Val): 0.4267; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 718/8192 --- L(Train): 0.3820; L(Val): 0.4269; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 719/8192 --- L(Train): 0.3749; L(Val): 0.4270; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 720/8192 --- L(Train): 0.3895; L(Val): 0.4272; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 721/8192 --- L(Train): 0.3869; L(Val): 0.4273; Reg Param: 0.0031; Time: 0.72\n",
      "Epoch 722/8192 --- L(Train): 0.3716; L(Val): 0.4275; Reg Param: 0.0031; Time: 0.76\n",
      "Epoch 723/8192 --- L(Train): 0.3829; L(Val): 0.4276; Reg Param: 0.0031; Time: 0.74\n",
      "Epoch 724/8192 --- L(Train): 0.3828; L(Val): 0.4275; Reg Param: 0.0031; Time: 0.78\n",
      "Epoch 725/8192 --- L(Train): 0.3762; L(Val): 0.4272; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 726/8192 --- L(Train): 0.3885; L(Val): 0.4271; Reg Param: 0.0031; Time: 0.72\n",
      "Epoch 727/8192 --- L(Train): 0.3711; L(Val): 0.4272; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 728/8192 --- L(Train): 0.3779; L(Val): 0.4271; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 729/8192 --- L(Train): 0.3912; L(Val): 0.4270; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 730/8192 --- L(Train): 0.3834; L(Val): 0.4271; Reg Param: 0.0031; Time: 0.88\n",
      "Epoch 731/8192 --- L(Train): 0.3791; L(Val): 0.4271; Reg Param: 0.0031; Time: 0.86\n",
      "Epoch 732/8192 --- L(Train): 0.3949; L(Val): 0.4270; Reg Param: 0.0031; Time: 0.74\n",
      "Epoch 733/8192 --- L(Train): 0.3892; L(Val): 0.4269; Reg Param: 0.0031; Time: 0.76\n",
      "Epoch 734/8192 --- L(Train): 0.3920; L(Val): 0.4268; Reg Param: 0.0031; Time: 0.80\n",
      "Epoch 735/8192 --- L(Train): 0.3780; L(Val): 0.4268; Reg Param: 0.0031; Time: 0.72\n",
      "Epoch 736/8192 --- L(Train): 0.3819; L(Val): 0.4268; Reg Param: 0.0031; Time: 0.72\n",
      "Epoch 737/8192 --- L(Train): 0.3859; L(Val): 0.4267; Reg Param: 0.0031; Time: 0.74\n",
      "Epoch 738/8192 --- L(Train): 0.3805; L(Val): 0.4267; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 739/8192 --- L(Train): 0.3901; L(Val): 0.4267; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 740/8192 --- L(Train): 0.3757; L(Val): 0.4268; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 741/8192 --- L(Train): 0.3798; L(Val): 0.4267; Reg Param: 0.0031; Time: 0.72\n",
      "Epoch 742/8192 --- L(Train): 0.3798; L(Val): 0.4266; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 743/8192 --- L(Train): 0.3808; L(Val): 0.4265; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 744/8192 --- L(Train): 0.3841; L(Val): 0.4265; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 745/8192 --- L(Train): 0.3737; L(Val): 0.4264; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 746/8192 --- L(Train): 0.3984; L(Val): 0.4264; Reg Param: 0.0031; Time: 0.70\n",
      "Epoch 747/8192 --- L(Train): 0.3924; L(Val): 0.4264; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 748/8192 --- L(Train): 0.3902; L(Val): 0.4267; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 749/8192 --- L(Train): 0.3878; L(Val): 0.4269; Reg Param: 0.0031; Time: 0.71\n",
      "Epoch 750/8192 --- L(Train): 0.3813; L(Val): 0.4269; Reg Param: 0.0051; Time: 7.62\n",
      "Epoch 751/8192 --- L(Train): 0.3780; L(Val): 0.4273; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 752/8192 --- L(Train): 0.3722; L(Val): 0.4275; Reg Param: 0.0051; Time: 0.73\n",
      "Epoch 753/8192 --- L(Train): 0.3847; L(Val): 0.4276; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 754/8192 --- L(Train): 0.3818; L(Val): 0.4275; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 755/8192 --- L(Train): 0.3843; L(Val): 0.4274; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 756/8192 --- L(Train): 0.3869; L(Val): 0.4275; Reg Param: 0.0051; Time: 0.76\n",
      "Epoch 757/8192 --- L(Train): 0.3769; L(Val): 0.4275; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 758/8192 --- L(Train): 0.3950; L(Val): 0.4276; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 759/8192 --- L(Train): 0.3822; L(Val): 0.4277; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 760/8192 --- L(Train): 0.3875; L(Val): 0.4277; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 761/8192 --- L(Train): 0.3821; L(Val): 0.4277; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 762/8192 --- L(Train): 0.3934; L(Val): 0.4277; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 763/8192 --- L(Train): 0.3824; L(Val): 0.4278; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 764/8192 --- L(Train): 0.3881; L(Val): 0.4279; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 765/8192 --- L(Train): 0.3728; L(Val): 0.4279; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 766/8192 --- L(Train): 0.3827; L(Val): 0.4280; Reg Param: 0.0051; Time: 0.71\n",
      "Epoch 767/8192 --- L(Train): 0.3811; L(Val): 0.4281; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 768/8192 --- L(Train): 0.3824; L(Val): 0.4280; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 769/8192 --- L(Train): 0.3757; L(Val): 0.4278; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 770/8192 --- L(Train): 0.3816; L(Val): 0.4275; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 771/8192 --- L(Train): 0.3883; L(Val): 0.4273; Reg Param: 0.0051; Time: 0.85\n",
      "Epoch 772/8192 --- L(Train): 0.3864; L(Val): 0.4272; Reg Param: 0.0051; Time: 0.73\n",
      "Epoch 773/8192 --- L(Train): 0.3932; L(Val): 0.4270; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 774/8192 --- L(Train): 0.3890; L(Val): 0.4269; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 775/8192 --- L(Train): 0.3837; L(Val): 0.4270; Reg Param: 0.0051; Time: 0.71\n",
      "Epoch 776/8192 --- L(Train): 0.3831; L(Val): 0.4271; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 777/8192 --- L(Train): 0.3890; L(Val): 0.4272; Reg Param: 0.0051; Time: 0.77\n",
      "Epoch 778/8192 --- L(Train): 0.3861; L(Val): 0.4274; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 779/8192 --- L(Train): 0.3996; L(Val): 0.4274; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 780/8192 --- L(Train): 0.3860; L(Val): 0.4274; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 781/8192 --- L(Train): 0.3832; L(Val): 0.4276; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 782/8192 --- L(Train): 0.3735; L(Val): 0.4277; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 783/8192 --- L(Train): 0.3835; L(Val): 0.4277; Reg Param: 0.0051; Time: 0.68\n",
      "Epoch 784/8192 --- L(Train): 0.3853; L(Val): 0.4278; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 785/8192 --- L(Train): 0.3815; L(Val): 0.4278; Reg Param: 0.0051; Time: 0.72\n",
      "Epoch 786/8192 --- L(Train): 0.3797; L(Val): 0.4279; Reg Param: 0.0051; Time: 0.71\n",
      "Epoch 787/8192 --- L(Train): 0.3762; L(Val): 0.4281; Reg Param: 0.0051; Time: 0.72\n",
      "Epoch 788/8192 --- L(Train): 0.3774; L(Val): 0.4282; Reg Param: 0.0051; Time: 0.75\n",
      "Epoch 789/8192 --- L(Train): 0.3838; L(Val): 0.4284; Reg Param: 0.0051; Time: 0.74\n",
      "Epoch 790/8192 --- L(Train): 0.3768; L(Val): 0.4285; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 791/8192 --- L(Train): 0.3745; L(Val): 0.4285; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 792/8192 --- L(Train): 0.3833; L(Val): 0.4286; Reg Param: 0.0051; Time: 0.71\n",
      "Epoch 793/8192 --- L(Train): 0.3842; L(Val): 0.4286; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 794/8192 --- L(Train): 0.3780; L(Val): 0.4287; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 795/8192 --- L(Train): 0.3754; L(Val): 0.4289; Reg Param: 0.0051; Time: 0.70\n",
      "Epoch 796/8192 --- L(Train): 0.3854; L(Val): 0.4290; Reg Param: 0.0051; Time: 0.69\n",
      "Epoch 797/8192 --- L(Train): 0.3851; L(Val): 0.4291; Reg Param: 0.0051; Time: 0.71\n",
      "Epoch 798/8192 --- L(Train): 0.3798; L(Val): 0.4291; Reg Param: 0.0051; Time: 0.86\n",
      "Epoch 799/8192 --- L(Train): 0.3809; L(Val): 0.4290; Reg Param: 0.0051; Time: 0.77\n",
      "Epoch 800/8192 --- L(Train): 0.3869; L(Val): 0.4290; Reg Param: 0.0077; Time: 7.48\n",
      "Epoch 801/8192 --- L(Train): 0.3743; L(Val): 0.4285; Reg Param: 0.0077; Time: 0.73\n",
      "Epoch 802/8192 --- L(Train): 0.3848; L(Val): 0.4280; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 803/8192 --- L(Train): 0.3900; L(Val): 0.4278; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 804/8192 --- L(Train): 0.3755; L(Val): 0.4278; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 805/8192 --- L(Train): 0.3796; L(Val): 0.4280; Reg Param: 0.0077; Time: 0.69\n",
      "Epoch 806/8192 --- L(Train): 0.3841; L(Val): 0.4283; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 807/8192 --- L(Train): 0.3937; L(Val): 0.4286; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 808/8192 --- L(Train): 0.3721; L(Val): 0.4287; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 809/8192 --- L(Train): 0.3823; L(Val): 0.4288; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 810/8192 --- L(Train): 0.3980; L(Val): 0.4287; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 811/8192 --- L(Train): 0.3822; L(Val): 0.4287; Reg Param: 0.0077; Time: 0.72\n",
      "Epoch 812/8192 --- L(Train): 0.3710; L(Val): 0.4288; Reg Param: 0.0077; Time: 0.72\n",
      "Epoch 813/8192 --- L(Train): 0.3784; L(Val): 0.4286; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 814/8192 --- L(Train): 0.3760; L(Val): 0.4286; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 815/8192 --- L(Train): 0.3776; L(Val): 0.4288; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 816/8192 --- L(Train): 0.3899; L(Val): 0.4289; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 817/8192 --- L(Train): 0.3766; L(Val): 0.4288; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 818/8192 --- L(Train): 0.3801; L(Val): 0.4288; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 819/8192 --- L(Train): 0.3868; L(Val): 0.4290; Reg Param: 0.0077; Time: 0.73\n",
      "Epoch 820/8192 --- L(Train): 0.3855; L(Val): 0.4291; Reg Param: 0.0077; Time: 0.81\n",
      "Epoch 821/8192 --- L(Train): 0.3743; L(Val): 0.4290; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 822/8192 --- L(Train): 0.3800; L(Val): 0.4287; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 823/8192 --- L(Train): 0.3857; L(Val): 0.4286; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 824/8192 --- L(Train): 0.3876; L(Val): 0.4284; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 825/8192 --- L(Train): 0.3752; L(Val): 0.4285; Reg Param: 0.0077; Time: 0.80\n",
      "Epoch 826/8192 --- L(Train): 0.3798; L(Val): 0.4287; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 827/8192 --- L(Train): 0.3737; L(Val): 0.4290; Reg Param: 0.0077; Time: 0.72\n",
      "Epoch 828/8192 --- L(Train): 0.3836; L(Val): 0.4290; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 829/8192 --- L(Train): 0.3873; L(Val): 0.4288; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 830/8192 --- L(Train): 0.3875; L(Val): 0.4287; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 831/8192 --- L(Train): 0.3820; L(Val): 0.4285; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 832/8192 --- L(Train): 0.3871; L(Val): 0.4282; Reg Param: 0.0077; Time: 0.69\n",
      "Epoch 833/8192 --- L(Train): 0.3740; L(Val): 0.4281; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 834/8192 --- L(Train): 0.3790; L(Val): 0.4278; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 835/8192 --- L(Train): 0.3863; L(Val): 0.4276; Reg Param: 0.0077; Time: 0.72\n",
      "Epoch 836/8192 --- L(Train): 0.3843; L(Val): 0.4272; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 837/8192 --- L(Train): 0.3881; L(Val): 0.4272; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 838/8192 --- L(Train): 0.3879; L(Val): 0.4276; Reg Param: 0.0077; Time: 0.72\n",
      "Epoch 839/8192 --- L(Train): 0.3829; L(Val): 0.4286; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 840/8192 --- L(Train): 0.3843; L(Val): 0.4297; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 841/8192 --- L(Train): 0.3808; L(Val): 0.4302; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 842/8192 --- L(Train): 0.3855; L(Val): 0.4300; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 843/8192 --- L(Train): 0.3838; L(Val): 0.4300; Reg Param: 0.0077; Time: 0.69\n",
      "Epoch 844/8192 --- L(Train): 0.3809; L(Val): 0.4298; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 845/8192 --- L(Train): 0.3883; L(Val): 0.4295; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 846/8192 --- L(Train): 0.3790; L(Val): 0.4295; Reg Param: 0.0077; Time: 0.71\n",
      "Epoch 847/8192 --- L(Train): 0.3735; L(Val): 0.4293; Reg Param: 0.0077; Time: 0.70\n",
      "Epoch 848/8192 --- L(Train): 0.3743; L(Val): 0.4293; Reg Param: 0.0077; Time: 0.82\n",
      "Epoch 849/8192 --- L(Train): 0.3766; L(Val): 0.4291; Reg Param: 0.0077; Time: 0.78\n",
      "Epoch 850/8192 --- L(Train): 0.3882; L(Val): 0.4291; Reg Param: 0.0107; Time: 7.40\n",
      "Epoch 851/8192 --- L(Train): 0.3836; L(Val): 0.4287; Reg Param: 0.0107; Time: 0.70\n",
      "Epoch 852/8192 --- L(Train): 0.3887; L(Val): 0.4281; Reg Param: 0.0107; Time: 0.70\n",
      "Epoch 853/8192 --- L(Train): 0.3810; L(Val): 0.4275; Reg Param: 0.0107; Time: 0.71\n",
      "Epoch 854/8192 --- L(Train): 0.3892; L(Val): 0.4279; Reg Param: 0.0107; Time: 0.70\n",
      "Epoch 855/8192 --- L(Train): 0.3913; L(Val): 0.4291; Reg Param: 0.0107; Time: 0.70\n",
      "Epoch 856/8192 --- L(Train): 0.3876; L(Val): 0.4298; Reg Param: 0.0107; Time: 0.68\n",
      "Epoch 857/8192 --- L(Train): 0.3831; L(Val): 0.4297; Reg Param: 0.0107; Time: 0.68\n",
      "Epoch 858/8192 --- L(Train): 0.3912; L(Val): 0.4292; Reg Param: 0.0107; Time: 0.68\n",
      "Epoch 859/8192 --- L(Train): 0.3839; L(Val): 0.4289; Reg Param: 0.0107; Time: 0.68\n",
      "Epoch 860/8192 --- L(Train): 0.3875; L(Val): 0.4292; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 861/8192 --- L(Train): 0.3837; L(Val): 0.4298; Reg Param: 0.0107; Time: 0.70\n",
      "Epoch 862/8192 --- L(Train): 0.3825; L(Val): 0.4307; Reg Param: 0.0107; Time: 0.76\n",
      "Epoch 863/8192 --- L(Train): 0.3836; L(Val): 0.4306; Reg Param: 0.0107; Time: 0.67\n",
      "Epoch 864/8192 --- L(Train): 0.3835; L(Val): 0.4287; Reg Param: 0.0107; Time: 0.70\n",
      "Epoch 865/8192 --- L(Train): 0.3832; L(Val): 0.4281; Reg Param: 0.0107; Time: 0.68\n",
      "Epoch 866/8192 --- L(Train): 0.3837; L(Val): 0.4273; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 867/8192 --- L(Train): 0.3838; L(Val): 0.4269; Reg Param: 0.0107; Time: 0.68\n",
      "Epoch 868/8192 --- L(Train): 0.3837; L(Val): 0.4270; Reg Param: 0.0107; Time: 0.75\n",
      "Epoch 869/8192 --- L(Train): 0.3860; L(Val): 0.4271; Reg Param: 0.0107; Time: 0.86\n",
      "Epoch 870/8192 --- L(Train): 0.3683; L(Val): 0.4273; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 871/8192 --- L(Train): 0.3838; L(Val): 0.4273; Reg Param: 0.0107; Time: 0.68\n",
      "Epoch 872/8192 --- L(Train): 0.3782; L(Val): 0.4273; Reg Param: 0.0107; Time: 0.76\n",
      "Epoch 873/8192 --- L(Train): 0.3894; L(Val): 0.4273; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 874/8192 --- L(Train): 0.3889; L(Val): 0.4275; Reg Param: 0.0107; Time: 0.68\n",
      "Epoch 875/8192 --- L(Train): 0.3892; L(Val): 0.4278; Reg Param: 0.0107; Time: 0.67\n",
      "Epoch 876/8192 --- L(Train): 0.3811; L(Val): 0.4284; Reg Param: 0.0107; Time: 0.67\n",
      "Epoch 877/8192 --- L(Train): 0.3829; L(Val): 0.4291; Reg Param: 0.0107; Time: 0.68\n",
      "Epoch 878/8192 --- L(Train): 0.3809; L(Val): 0.4301; Reg Param: 0.0107; Time: 0.67\n",
      "Epoch 879/8192 --- L(Train): 0.3842; L(Val): 0.4305; Reg Param: 0.0107; Time: 0.68\n",
      "Epoch 880/8192 --- L(Train): 0.3824; L(Val): 0.4310; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 881/8192 --- L(Train): 0.3913; L(Val): 0.4314; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 882/8192 --- L(Train): 0.3825; L(Val): 0.4322; Reg Param: 0.0107; Time: 0.75\n",
      "Epoch 883/8192 --- L(Train): 0.3885; L(Val): 0.4329; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 884/8192 --- L(Train): 0.3751; L(Val): 0.4327; Reg Param: 0.0107; Time: 0.70\n",
      "Epoch 885/8192 --- L(Train): 0.3792; L(Val): 0.4338; Reg Param: 0.0107; Time: 0.70\n",
      "Epoch 886/8192 --- L(Train): 0.3867; L(Val): 0.4323; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 887/8192 --- L(Train): 0.3755; L(Val): 0.4328; Reg Param: 0.0107; Time: 0.67\n",
      "Epoch 888/8192 --- L(Train): 0.3807; L(Val): 0.4311; Reg Param: 0.0107; Time: 0.68\n",
      "Epoch 889/8192 --- L(Train): 0.3849; L(Val): 0.4320; Reg Param: 0.0107; Time: 0.70\n",
      "Epoch 890/8192 --- L(Train): 0.3851; L(Val): 0.4311; Reg Param: 0.0107; Time: 0.68\n",
      "Epoch 891/8192 --- L(Train): 0.3766; L(Val): 0.4334; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 892/8192 --- L(Train): 0.3790; L(Val): 0.4328; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 893/8192 --- L(Train): 0.3809; L(Val): 0.4311; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 894/8192 --- L(Train): 0.3858; L(Val): 0.4311; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 895/8192 --- L(Train): 0.3817; L(Val): 0.4307; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 896/8192 --- L(Train): 0.3967; L(Val): 0.4313; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 897/8192 --- L(Train): 0.3944; L(Val): 0.4305; Reg Param: 0.0107; Time: 0.69\n",
      "Epoch 898/8192 --- L(Train): 0.3850; L(Val): 0.4304; Reg Param: 0.0107; Time: 0.80\n",
      "Epoch 899/8192 --- L(Train): 0.3847; L(Val): 0.4297; Reg Param: 0.0107; Time: 0.72\n",
      "Epoch 900/8192 --- L(Train): 0.3905; L(Val): 0.4297; Reg Param: 0.0163; Time: 7.60\n",
      "Epoch 901/8192 --- L(Train): 0.3760; L(Val): 0.4298; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 902/8192 --- L(Train): 0.3807; L(Val): 0.4294; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 903/8192 --- L(Train): 0.3863; L(Val): 0.4295; Reg Param: 0.0163; Time: 0.77\n",
      "Epoch 904/8192 --- L(Train): 0.3691; L(Val): 0.4310; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 905/8192 --- L(Train): 0.3851; L(Val): 0.4306; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 906/8192 --- L(Train): 0.3947; L(Val): 0.4311; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 907/8192 --- L(Train): 0.3821; L(Val): 0.4320; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 908/8192 --- L(Train): 0.3802; L(Val): 0.4323; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 909/8192 --- L(Train): 0.3906; L(Val): 0.4330; Reg Param: 0.0163; Time: 0.71\n",
      "Epoch 910/8192 --- L(Train): 0.3886; L(Val): 0.4314; Reg Param: 0.0163; Time: 0.72\n",
      "Epoch 911/8192 --- L(Train): 0.3828; L(Val): 0.4309; Reg Param: 0.0163; Time: 0.72\n",
      "Epoch 912/8192 --- L(Train): 0.3845; L(Val): 0.4309; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 913/8192 --- L(Train): 0.3688; L(Val): 0.4311; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 914/8192 --- L(Train): 0.3743; L(Val): 0.4316; Reg Param: 0.0163; Time: 0.68\n",
      "Epoch 915/8192 --- L(Train): 0.3905; L(Val): 0.4321; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 916/8192 --- L(Train): 0.3794; L(Val): 0.4322; Reg Param: 0.0163; Time: 0.71\n",
      "Epoch 917/8192 --- L(Train): 0.3832; L(Val): 0.4325; Reg Param: 0.0163; Time: 0.71\n",
      "Epoch 918/8192 --- L(Train): 0.3794; L(Val): 0.4326; Reg Param: 0.0163; Time: 0.72\n",
      "Epoch 919/8192 --- L(Train): 0.3734; L(Val): 0.4330; Reg Param: 0.0163; Time: 0.82\n",
      "Epoch 920/8192 --- L(Train): 0.3902; L(Val): 0.4318; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 921/8192 --- L(Train): 0.3831; L(Val): 0.4325; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 922/8192 --- L(Train): 0.3752; L(Val): 0.4326; Reg Param: 0.0163; Time: 0.71\n",
      "Epoch 923/8192 --- L(Train): 0.3799; L(Val): 0.4322; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 924/8192 --- L(Train): 0.3901; L(Val): 0.4330; Reg Param: 0.0163; Time: 0.68\n",
      "Epoch 925/8192 --- L(Train): 0.3726; L(Val): 0.4332; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 926/8192 --- L(Train): 0.3848; L(Val): 0.4354; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 927/8192 --- L(Train): 0.3859; L(Val): 0.4351; Reg Param: 0.0163; Time: 0.68\n",
      "Epoch 928/8192 --- L(Train): 0.3796; L(Val): 0.4349; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 929/8192 --- L(Train): 0.3834; L(Val): 0.4331; Reg Param: 0.0163; Time: 0.77\n",
      "Epoch 930/8192 --- L(Train): 0.3868; L(Val): 0.4332; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 931/8192 --- L(Train): 0.3733; L(Val): 0.4334; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 932/8192 --- L(Train): 0.3789; L(Val): 0.4352; Reg Param: 0.0163; Time: 0.72\n",
      "Epoch 933/8192 --- L(Train): 0.3867; L(Val): 0.4352; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 934/8192 --- L(Train): 0.3875; L(Val): 0.4350; Reg Param: 0.0163; Time: 0.71\n",
      "Epoch 935/8192 --- L(Train): 0.3868; L(Val): 0.4339; Reg Param: 0.0163; Time: 0.71\n",
      "Epoch 936/8192 --- L(Train): 0.3841; L(Val): 0.4332; Reg Param: 0.0163; Time: 0.72\n",
      "Epoch 937/8192 --- L(Train): 0.3855; L(Val): 0.4321; Reg Param: 0.0163; Time: 0.72\n",
      "Epoch 938/8192 --- L(Train): 0.3944; L(Val): 0.4302; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 939/8192 --- L(Train): 0.3855; L(Val): 0.4306; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 940/8192 --- L(Train): 0.3794; L(Val): 0.4312; Reg Param: 0.0163; Time: 0.72\n",
      "Epoch 941/8192 --- L(Train): 0.3896; L(Val): 0.4330; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 942/8192 --- L(Train): 0.3823; L(Val): 0.4338; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 943/8192 --- L(Train): 0.3869; L(Val): 0.4334; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 944/8192 --- L(Train): 0.3752; L(Val): 0.4323; Reg Param: 0.0163; Time: 0.69\n",
      "Epoch 945/8192 --- L(Train): 0.3910; L(Val): 0.4320; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 946/8192 --- L(Train): 0.3715; L(Val): 0.4310; Reg Param: 0.0163; Time: 0.70\n",
      "Epoch 947/8192 --- L(Train): 0.3943; L(Val): 0.4304; Reg Param: 0.0163; Time: 0.84\n",
      "Epoch 948/8192 --- L(Train): 0.3832; L(Val): 0.4302; Reg Param: 0.0163; Time: 0.77\n",
      "Epoch 949/8192 --- L(Train): 0.3897; L(Val): 0.4317; Reg Param: 0.0163; Time: 0.72\n",
      "Epoch 950/8192 --- L(Train): 0.3892; L(Val): 0.4317; Reg Param: 0.0216; Time: 7.36\n",
      "Epoch 951/8192 --- L(Train): 0.3747; L(Val): 0.4311; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 952/8192 --- L(Train): 0.3882; L(Val): 0.4317; Reg Param: 0.0216; Time: 0.68\n",
      "Epoch 953/8192 --- L(Train): 0.3833; L(Val): 0.4324; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 954/8192 --- L(Train): 0.3776; L(Val): 0.4328; Reg Param: 0.0216; Time: 0.71\n",
      "Epoch 955/8192 --- L(Train): 0.3752; L(Val): 0.4324; Reg Param: 0.0216; Time: 0.71\n",
      "Epoch 956/8192 --- L(Train): 0.3834; L(Val): 0.4319; Reg Param: 0.0216; Time: 0.68\n",
      "Epoch 957/8192 --- L(Train): 0.3776; L(Val): 0.4325; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 958/8192 --- L(Train): 0.3881; L(Val): 0.4313; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 959/8192 --- L(Train): 0.3769; L(Val): 0.4324; Reg Param: 0.0216; Time: 0.71\n",
      "Epoch 960/8192 --- L(Train): 0.3801; L(Val): 0.4325; Reg Param: 0.0216; Time: 0.71\n",
      "Epoch 961/8192 --- L(Train): 0.3772; L(Val): 0.4324; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 962/8192 --- L(Train): 0.3790; L(Val): 0.4338; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 963/8192 --- L(Train): 0.3832; L(Val): 0.4327; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 964/8192 --- L(Train): 0.3788; L(Val): 0.4330; Reg Param: 0.0216; Time: 0.68\n",
      "Epoch 965/8192 --- L(Train): 0.3763; L(Val): 0.4310; Reg Param: 0.0216; Time: 0.68\n",
      "Epoch 966/8192 --- L(Train): 0.3736; L(Val): 0.4298; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 967/8192 --- L(Train): 0.3741; L(Val): 0.4292; Reg Param: 0.0216; Time: 0.81\n",
      "Epoch 968/8192 --- L(Train): 0.3813; L(Val): 0.4297; Reg Param: 0.0216; Time: 0.76\n",
      "Epoch 969/8192 --- L(Train): 0.3886; L(Val): 0.4286; Reg Param: 0.0216; Time: 0.71\n",
      "Epoch 970/8192 --- L(Train): 0.3823; L(Val): 0.4298; Reg Param: 0.0216; Time: 0.71\n",
      "Epoch 971/8192 --- L(Train): 0.3827; L(Val): 0.4313; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 972/8192 --- L(Train): 0.3954; L(Val): 0.4313; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 973/8192 --- L(Train): 0.3691; L(Val): 0.4310; Reg Param: 0.0216; Time: 0.79\n",
      "Epoch 974/8192 --- L(Train): 0.3852; L(Val): 0.4316; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 975/8192 --- L(Train): 0.3850; L(Val): 0.4318; Reg Param: 0.0216; Time: 0.80\n",
      "Epoch 976/8192 --- L(Train): 0.3885; L(Val): 0.4314; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 977/8192 --- L(Train): 0.3792; L(Val): 0.4315; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 978/8192 --- L(Train): 0.3778; L(Val): 0.4322; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 979/8192 --- L(Train): 0.3832; L(Val): 0.4325; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 980/8192 --- L(Train): 0.3815; L(Val): 0.4320; Reg Param: 0.0216; Time: 0.71\n",
      "Epoch 981/8192 --- L(Train): 0.3791; L(Val): 0.4317; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 982/8192 --- L(Train): 0.3739; L(Val): 0.4322; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 983/8192 --- L(Train): 0.3821; L(Val): 0.4320; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 984/8192 --- L(Train): 0.3891; L(Val): 0.4306; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 985/8192 --- L(Train): 0.3822; L(Val): 0.4299; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 986/8192 --- L(Train): 0.3884; L(Val): 0.4298; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 987/8192 --- L(Train): 0.3782; L(Val): 0.4314; Reg Param: 0.0216; Time: 0.68\n",
      "Epoch 988/8192 --- L(Train): 0.3813; L(Val): 0.4326; Reg Param: 0.0216; Time: 0.68\n",
      "Epoch 989/8192 --- L(Train): 0.3794; L(Val): 0.4333; Reg Param: 0.0216; Time: 0.68\n",
      "Epoch 990/8192 --- L(Train): 0.3794; L(Val): 0.4337; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 991/8192 --- L(Train): 0.3789; L(Val): 0.4320; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 992/8192 --- L(Train): 0.3827; L(Val): 0.4310; Reg Param: 0.0216; Time: 0.78\n",
      "Epoch 993/8192 --- L(Train): 0.3912; L(Val): 0.4313; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 994/8192 --- L(Train): 0.3751; L(Val): 0.4318; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 995/8192 --- L(Train): 0.3806; L(Val): 0.4317; Reg Param: 0.0216; Time: 0.69\n",
      "Epoch 996/8192 --- L(Train): 0.3724; L(Val): 0.4328; Reg Param: 0.0216; Time: 0.71\n",
      "Epoch 997/8192 --- L(Train): 0.3840; L(Val): 0.4347; Reg Param: 0.0216; Time: 0.73\n",
      "Epoch 998/8192 --- L(Train): 0.3941; L(Val): 0.4337; Reg Param: 0.0216; Time: 0.70\n",
      "Epoch 999/8192 --- L(Train): 0.3802; L(Val): 0.4322; Reg Param: 0.0216; Time: 0.68\n",
      "Epoch 1000/8192 --- L(Train): 0.3806; L(Val): 0.4322; Reg Param: 0.0268; Time: 7.33\n",
      "Epoch 1001/8192 --- L(Train): 0.3752; L(Val): 0.4306; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1002/8192 --- L(Train): 0.3948; L(Val): 0.4294; Reg Param: 0.0268; Time: 0.78\n",
      "Epoch 1003/8192 --- L(Train): 0.3824; L(Val): 0.4300; Reg Param: 0.0268; Time: 0.72\n",
      "Epoch 1004/8192 --- L(Train): 0.3708; L(Val): 0.4312; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1005/8192 --- L(Train): 0.3706; L(Val): 0.4329; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1006/8192 --- L(Train): 0.3930; L(Val): 0.4327; Reg Param: 0.0268; Time: 0.71\n",
      "Epoch 1007/8192 --- L(Train): 0.3771; L(Val): 0.4335; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1008/8192 --- L(Train): 0.3910; L(Val): 0.4329; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1009/8192 --- L(Train): 0.3856; L(Val): 0.4331; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1010/8192 --- L(Train): 0.3847; L(Val): 0.4316; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1011/8192 --- L(Train): 0.3872; L(Val): 0.4312; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1012/8192 --- L(Train): 0.3828; L(Val): 0.4325; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1013/8192 --- L(Train): 0.3894; L(Val): 0.4316; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1014/8192 --- L(Train): 0.3836; L(Val): 0.4317; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1015/8192 --- L(Train): 0.3855; L(Val): 0.4326; Reg Param: 0.0268; Time: 0.79\n",
      "Epoch 1016/8192 --- L(Train): 0.3898; L(Val): 0.4311; Reg Param: 0.0268; Time: 0.74\n",
      "Epoch 1017/8192 --- L(Train): 0.3949; L(Val): 0.4290; Reg Param: 0.0268; Time: 0.73\n",
      "Epoch 1018/8192 --- L(Train): 0.3976; L(Val): 0.4276; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1019/8192 --- L(Train): 0.3852; L(Val): 0.4311; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1020/8192 --- L(Train): 0.3902; L(Val): 0.4319; Reg Param: 0.0268; Time: 0.77\n",
      "Epoch 1021/8192 --- L(Train): 0.3926; L(Val): 0.4317; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1022/8192 --- L(Train): 0.3915; L(Val): 0.4330; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1023/8192 --- L(Train): 0.3935; L(Val): 0.4336; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1024/8192 --- L(Train): 0.3963; L(Val): 0.4316; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1025/8192 --- L(Train): 0.3903; L(Val): 0.4309; Reg Param: 0.0268; Time: 0.71\n",
      "Epoch 1026/8192 --- L(Train): 0.3804; L(Val): 0.4294; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1027/8192 --- L(Train): 0.3807; L(Val): 0.4294; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1028/8192 --- L(Train): 0.3827; L(Val): 0.4288; Reg Param: 0.0268; Time: 0.68\n",
      "Epoch 1029/8192 --- L(Train): 0.3821; L(Val): 0.4284; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1030/8192 --- L(Train): 0.3850; L(Val): 0.4282; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1031/8192 --- L(Train): 0.3811; L(Val): 0.4300; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1032/8192 --- L(Train): 0.3823; L(Val): 0.4309; Reg Param: 0.0268; Time: 0.71\n",
      "Epoch 1033/8192 --- L(Train): 0.3871; L(Val): 0.4307; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1034/8192 --- L(Train): 0.3818; L(Val): 0.4298; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1035/8192 --- L(Train): 0.3795; L(Val): 0.4290; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1036/8192 --- L(Train): 0.3828; L(Val): 0.4285; Reg Param: 0.0268; Time: 0.71\n",
      "Epoch 1037/8192 --- L(Train): 0.3832; L(Val): 0.4295; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1038/8192 --- L(Train): 0.3756; L(Val): 0.4314; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1039/8192 --- L(Train): 0.3753; L(Val): 0.4322; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1040/8192 --- L(Train): 0.3917; L(Val): 0.4334; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1041/8192 --- L(Train): 0.3895; L(Val): 0.4336; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1042/8192 --- L(Train): 0.3930; L(Val): 0.4327; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1043/8192 --- L(Train): 0.3871; L(Val): 0.4316; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1044/8192 --- L(Train): 0.3845; L(Val): 0.4314; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1045/8192 --- L(Train): 0.3915; L(Val): 0.4308; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1046/8192 --- L(Train): 0.3699; L(Val): 0.4309; Reg Param: 0.0268; Time: 0.70\n",
      "Epoch 1047/8192 --- L(Train): 0.3746; L(Val): 0.4319; Reg Param: 0.0268; Time: 0.83\n",
      "Epoch 1048/8192 --- L(Train): 0.3688; L(Val): 0.4324; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1049/8192 --- L(Train): 0.3928; L(Val): 0.4322; Reg Param: 0.0268; Time: 0.69\n",
      "Epoch 1050/8192 --- L(Train): 0.3837; L(Val): 0.4322; Reg Param: 0.0316; Time: 7.47\n",
      "Epoch 1051/8192 --- L(Train): 0.3861; L(Val): 0.4322; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1052/8192 --- L(Train): 0.3849; L(Val): 0.4323; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1053/8192 --- L(Train): 0.3817; L(Val): 0.4324; Reg Param: 0.0316; Time: 0.71\n",
      "Epoch 1054/8192 --- L(Train): 0.3867; L(Val): 0.4319; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1055/8192 --- L(Train): 0.3886; L(Val): 0.4311; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1056/8192 --- L(Train): 0.3788; L(Val): 0.4301; Reg Param: 0.0316; Time: 0.71\n",
      "Epoch 1057/8192 --- L(Train): 0.3876; L(Val): 0.4296; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1058/8192 --- L(Train): 0.3804; L(Val): 0.4302; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1059/8192 --- L(Train): 0.3823; L(Val): 0.4316; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1060/8192 --- L(Train): 0.3760; L(Val): 0.4322; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1061/8192 --- L(Train): 0.3925; L(Val): 0.4338; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1062/8192 --- L(Train): 0.3887; L(Val): 0.4355; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1063/8192 --- L(Train): 0.3802; L(Val): 0.4363; Reg Param: 0.0316; Time: 0.68\n",
      "Epoch 1064/8192 --- L(Train): 0.3857; L(Val): 0.4352; Reg Param: 0.0316; Time: 0.68\n",
      "Epoch 1065/8192 --- L(Train): 0.3866; L(Val): 0.4334; Reg Param: 0.0316; Time: 0.68\n",
      "Epoch 1066/8192 --- L(Train): 0.3864; L(Val): 0.4339; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1067/8192 --- L(Train): 0.3805; L(Val): 0.4335; Reg Param: 0.0316; Time: 0.84\n",
      "Epoch 1068/8192 --- L(Train): 0.3775; L(Val): 0.4322; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1069/8192 --- L(Train): 0.3757; L(Val): 0.4317; Reg Param: 0.0316; Time: 0.68\n",
      "Epoch 1070/8192 --- L(Train): 0.3844; L(Val): 0.4319; Reg Param: 0.0316; Time: 0.71\n",
      "Epoch 1071/8192 --- L(Train): 0.3875; L(Val): 0.4321; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1072/8192 --- L(Train): 0.3864; L(Val): 0.4319; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1073/8192 --- L(Train): 0.3801; L(Val): 0.4310; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1074/8192 --- L(Train): 0.3790; L(Val): 0.4314; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1075/8192 --- L(Train): 0.3913; L(Val): 0.4325; Reg Param: 0.0316; Time: 0.71\n",
      "Epoch 1076/8192 --- L(Train): 0.3801; L(Val): 0.4335; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1077/8192 --- L(Train): 0.3703; L(Val): 0.4345; Reg Param: 0.0316; Time: 0.71\n",
      "Epoch 1078/8192 --- L(Train): 0.3819; L(Val): 0.4351; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1079/8192 --- L(Train): 0.3762; L(Val): 0.4358; Reg Param: 0.0316; Time: 0.71\n",
      "Epoch 1080/8192 --- L(Train): 0.3803; L(Val): 0.4351; Reg Param: 0.0316; Time: 0.68\n",
      "Epoch 1081/8192 --- L(Train): 0.3798; L(Val): 0.4339; Reg Param: 0.0316; Time: 0.68\n",
      "Epoch 1082/8192 --- L(Train): 0.3707; L(Val): 0.4332; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1083/8192 --- L(Train): 0.3792; L(Val): 0.4338; Reg Param: 0.0316; Time: 0.67\n",
      "Epoch 1084/8192 --- L(Train): 0.3828; L(Val): 0.4340; Reg Param: 0.0316; Time: 0.68\n",
      "Epoch 1085/8192 --- L(Train): 0.3846; L(Val): 0.4346; Reg Param: 0.0316; Time: 0.67\n",
      "Epoch 1086/8192 --- L(Train): 0.3825; L(Val): 0.4356; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1087/8192 --- L(Train): 0.3782; L(Val): 0.4342; Reg Param: 0.0316; Time: 0.67\n",
      "Epoch 1088/8192 --- L(Train): 0.3874; L(Val): 0.4349; Reg Param: 0.0316; Time: 0.75\n",
      "Epoch 1089/8192 --- L(Train): 0.3799; L(Val): 0.4368; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1090/8192 --- L(Train): 0.3820; L(Val): 0.4387; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1091/8192 --- L(Train): 0.3846; L(Val): 0.4381; Reg Param: 0.0316; Time: 0.72\n",
      "Epoch 1092/8192 --- L(Train): 0.3754; L(Val): 0.4368; Reg Param: 0.0316; Time: 0.76\n",
      "Epoch 1093/8192 --- L(Train): 0.3864; L(Val): 0.4359; Reg Param: 0.0316; Time: 0.72\n",
      "Epoch 1094/8192 --- L(Train): 0.3758; L(Val): 0.4371; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1095/8192 --- L(Train): 0.3842; L(Val): 0.4364; Reg Param: 0.0316; Time: 0.69\n",
      "Epoch 1096/8192 --- L(Train): 0.3838; L(Val): 0.4350; Reg Param: 0.0316; Time: 0.78\n",
      "Epoch 1097/8192 --- L(Train): 0.3798; L(Val): 0.4340; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1098/8192 --- L(Train): 0.3784; L(Val): 0.4333; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1099/8192 --- L(Train): 0.3766; L(Val): 0.4338; Reg Param: 0.0316; Time: 0.70\n",
      "Epoch 1100/8192 --- L(Train): 0.3775; L(Val): 0.4338; Reg Param: 0.0360; Time: 7.37\n",
      "Epoch 1101/8192 --- L(Train): 0.3669; L(Val): 0.4311; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1102/8192 --- L(Train): 0.3707; L(Val): 0.4323; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1103/8192 --- L(Train): 0.3828; L(Val): 0.4343; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1104/8192 --- L(Train): 0.3849; L(Val): 0.4330; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1105/8192 --- L(Train): 0.3732; L(Val): 0.4330; Reg Param: 0.0360; Time: 0.68\n",
      "Epoch 1106/8192 --- L(Train): 0.3893; L(Val): 0.4334; Reg Param: 0.0360; Time: 0.68\n",
      "Epoch 1107/8192 --- L(Train): 0.3714; L(Val): 0.4382; Reg Param: 0.0360; Time: 0.68\n",
      "Epoch 1108/8192 --- L(Train): 0.3816; L(Val): 0.4338; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1109/8192 --- L(Train): 0.3695; L(Val): 0.4366; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1110/8192 --- L(Train): 0.3836; L(Val): 0.4355; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1111/8192 --- L(Train): 0.3857; L(Val): 0.4406; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1112/8192 --- L(Train): 0.3782; L(Val): 0.4338; Reg Param: 0.0360; Time: 0.68\n",
      "Epoch 1113/8192 --- L(Train): 0.3809; L(Val): 0.4333; Reg Param: 0.0360; Time: 0.68\n",
      "Epoch 1114/8192 --- L(Train): 0.3853; L(Val): 0.4318; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1115/8192 --- L(Train): 0.3963; L(Val): 0.4354; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1116/8192 --- L(Train): 0.3775; L(Val): 0.4354; Reg Param: 0.0360; Time: 0.71\n",
      "Epoch 1117/8192 --- L(Train): 0.3829; L(Val): 0.4343; Reg Param: 0.0360; Time: 0.83\n",
      "Epoch 1118/8192 --- L(Train): 0.3917; L(Val): 0.4335; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1119/8192 --- L(Train): 0.3710; L(Val): 0.4346; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1120/8192 --- L(Train): 0.3788; L(Val): 0.4362; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1121/8192 --- L(Train): 0.3942; L(Val): 0.4329; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1122/8192 --- L(Train): 0.3751; L(Val): 0.4343; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1123/8192 --- L(Train): 0.3936; L(Val): 0.4341; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1124/8192 --- L(Train): 0.3973; L(Val): 0.4347; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1125/8192 --- L(Train): 0.3851; L(Val): 0.4353; Reg Param: 0.0360; Time: 0.79\n",
      "Epoch 1126/8192 --- L(Train): 0.3852; L(Val): 0.4353; Reg Param: 0.0360; Time: 0.68\n",
      "Epoch 1127/8192 --- L(Train): 0.3719; L(Val): 0.4366; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1128/8192 --- L(Train): 0.3797; L(Val): 0.4370; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1129/8192 --- L(Train): 0.3828; L(Val): 0.4381; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1130/8192 --- L(Train): 0.3863; L(Val): 0.4367; Reg Param: 0.0360; Time: 0.68\n",
      "Epoch 1131/8192 --- L(Train): 0.3704; L(Val): 0.4339; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1132/8192 --- L(Train): 0.3794; L(Val): 0.4333; Reg Param: 0.0360; Time: 0.68\n",
      "Epoch 1133/8192 --- L(Train): 0.3828; L(Val): 0.4329; Reg Param: 0.0360; Time: 0.71\n",
      "Epoch 1134/8192 --- L(Train): 0.3821; L(Val): 0.4348; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1135/8192 --- L(Train): 0.3769; L(Val): 0.4339; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1136/8192 --- L(Train): 0.3708; L(Val): 0.4332; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1137/8192 --- L(Train): 0.3737; L(Val): 0.4331; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1138/8192 --- L(Train): 0.3803; L(Val): 0.4333; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1139/8192 --- L(Train): 0.3692; L(Val): 0.4332; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1140/8192 --- L(Train): 0.3767; L(Val): 0.4323; Reg Param: 0.0360; Time: 0.68\n",
      "Epoch 1141/8192 --- L(Train): 0.3896; L(Val): 0.4310; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1142/8192 --- L(Train): 0.3810; L(Val): 0.4313; Reg Param: 0.0360; Time: 0.69\n",
      "Epoch 1143/8192 --- L(Train): 0.3787; L(Val): 0.4323; Reg Param: 0.0360; Time: 0.74\n",
      "Epoch 1144/8192 --- L(Train): 0.3895; L(Val): 0.4340; Reg Param: 0.0360; Time: 0.79\n",
      "Epoch 1145/8192 --- L(Train): 0.3874; L(Val): 0.4354; Reg Param: 0.0360; Time: 0.73\n",
      "Epoch 1146/8192 --- L(Train): 0.3804; L(Val): 0.4347; Reg Param: 0.0360; Time: 0.82\n",
      "Epoch 1147/8192 --- L(Train): 0.3841; L(Val): 0.4336; Reg Param: 0.0360; Time: 0.77\n",
      "Epoch 1148/8192 --- L(Train): 0.3804; L(Val): 0.4328; Reg Param: 0.0360; Time: 0.71\n",
      "Epoch 1149/8192 --- L(Train): 0.3842; L(Val): 0.4333; Reg Param: 0.0360; Time: 0.70\n",
      "Epoch 1150/8192 --- L(Train): 0.3751; L(Val): 0.4333; Reg Param: 0.0401; Time: 7.37\n",
      "Epoch 1151/8192 --- L(Train): 0.3735; L(Val): 0.4326; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1152/8192 --- L(Train): 0.3818; L(Val): 0.4323; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1153/8192 --- L(Train): 0.3826; L(Val): 0.4329; Reg Param: 0.0401; Time: 0.67\n",
      "Epoch 1154/8192 --- L(Train): 0.3897; L(Val): 0.4343; Reg Param: 0.0401; Time: 0.75\n",
      "Epoch 1155/8192 --- L(Train): 0.3725; L(Val): 0.4357; Reg Param: 0.0401; Time: 0.67\n",
      "Epoch 1156/8192 --- L(Train): 0.3788; L(Val): 0.4365; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1157/8192 --- L(Train): 0.3843; L(Val): 0.4364; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1158/8192 --- L(Train): 0.3726; L(Val): 0.4361; Reg Param: 0.0401; Time: 0.75\n",
      "Epoch 1159/8192 --- L(Train): 0.3797; L(Val): 0.4360; Reg Param: 0.0401; Time: 0.72\n",
      "Epoch 1160/8192 --- L(Train): 0.3759; L(Val): 0.4363; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1161/8192 --- L(Train): 0.3918; L(Val): 0.4353; Reg Param: 0.0401; Time: 0.70\n",
      "Epoch 1162/8192 --- L(Train): 0.3788; L(Val): 0.4347; Reg Param: 0.0401; Time: 0.67\n",
      "Epoch 1163/8192 --- L(Train): 0.3694; L(Val): 0.4334; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1164/8192 --- L(Train): 0.3801; L(Val): 0.4333; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1165/8192 --- L(Train): 0.3897; L(Val): 0.4332; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1166/8192 --- L(Train): 0.3862; L(Val): 0.4334; Reg Param: 0.0401; Time: 0.75\n",
      "Epoch 1167/8192 --- L(Train): 0.3779; L(Val): 0.4342; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1168/8192 --- L(Train): 0.3777; L(Val): 0.4353; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1169/8192 --- L(Train): 0.3839; L(Val): 0.4356; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1170/8192 --- L(Train): 0.3768; L(Val): 0.4358; Reg Param: 0.0401; Time: 0.76\n",
      "Epoch 1171/8192 --- L(Train): 0.3852; L(Val): 0.4362; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1172/8192 --- L(Train): 0.3739; L(Val): 0.4355; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1173/8192 --- L(Train): 0.3780; L(Val): 0.4348; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1174/8192 --- L(Train): 0.3747; L(Val): 0.4337; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1175/8192 --- L(Train): 0.3886; L(Val): 0.4322; Reg Param: 0.0401; Time: 0.70\n",
      "Epoch 1176/8192 --- L(Train): 0.3743; L(Val): 0.4306; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1177/8192 --- L(Train): 0.3724; L(Val): 0.4304; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1178/8192 --- L(Train): 0.3879; L(Val): 0.4317; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1179/8192 --- L(Train): 0.3894; L(Val): 0.4330; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1180/8192 --- L(Train): 0.3863; L(Val): 0.4342; Reg Param: 0.0401; Time: 0.76\n",
      "Epoch 1181/8192 --- L(Train): 0.3816; L(Val): 0.4355; Reg Param: 0.0401; Time: 0.76\n",
      "Epoch 1182/8192 --- L(Train): 0.3840; L(Val): 0.4359; Reg Param: 0.0401; Time: 0.80\n",
      "Epoch 1183/8192 --- L(Train): 0.3820; L(Val): 0.4356; Reg Param: 0.0401; Time: 0.70\n",
      "Epoch 1184/8192 --- L(Train): 0.3868; L(Val): 0.4343; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1185/8192 --- L(Train): 0.3751; L(Val): 0.4331; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1186/8192 --- L(Train): 0.3851; L(Val): 0.4329; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1187/8192 --- L(Train): 0.3941; L(Val): 0.4335; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1188/8192 --- L(Train): 0.3777; L(Val): 0.4345; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1189/8192 --- L(Train): 0.3673; L(Val): 0.4350; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1190/8192 --- L(Train): 0.3860; L(Val): 0.4341; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1191/8192 --- L(Train): 0.3706; L(Val): 0.4331; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1192/8192 --- L(Train): 0.3807; L(Val): 0.4330; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1193/8192 --- L(Train): 0.3875; L(Val): 0.4339; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1194/8192 --- L(Train): 0.3907; L(Val): 0.4346; Reg Param: 0.0401; Time: 0.69\n",
      "Epoch 1195/8192 --- L(Train): 0.3632; L(Val): 0.4353; Reg Param: 0.0401; Time: 0.82\n",
      "Epoch 1196/8192 --- L(Train): 0.3751; L(Val): 0.4359; Reg Param: 0.0401; Time: 0.73\n",
      "Epoch 1197/8192 --- L(Train): 0.3763; L(Val): 0.4359; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1198/8192 --- L(Train): 0.3822; L(Val): 0.4363; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1199/8192 --- L(Train): 0.3962; L(Val): 0.4354; Reg Param: 0.0401; Time: 0.68\n",
      "Epoch 1200/8192 --- L(Train): 0.3810; L(Val): 0.4354; Reg Param: 0.0428; Time: 7.45\n",
      "Epoch 1201/8192 --- L(Train): 0.3852; L(Val): 0.4347; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1202/8192 --- L(Train): 0.3838; L(Val): 0.4350; Reg Param: 0.0428; Time: 0.71\n",
      "Epoch 1203/8192 --- L(Train): 0.3802; L(Val): 0.4359; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1204/8192 --- L(Train): 0.3656; L(Val): 0.4360; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1205/8192 --- L(Train): 0.3802; L(Val): 0.4359; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1206/8192 --- L(Train): 0.3813; L(Val): 0.4363; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1207/8192 --- L(Train): 0.3643; L(Val): 0.4366; Reg Param: 0.0428; Time: 0.68\n",
      "Epoch 1208/8192 --- L(Train): 0.3752; L(Val): 0.4362; Reg Param: 0.0428; Time: 0.70\n",
      "Epoch 1209/8192 --- L(Train): 0.3781; L(Val): 0.4355; Reg Param: 0.0428; Time: 0.78\n",
      "Epoch 1210/8192 --- L(Train): 0.3709; L(Val): 0.4352; Reg Param: 0.0428; Time: 0.76\n",
      "Epoch 1211/8192 --- L(Train): 0.3759; L(Val): 0.4364; Reg Param: 0.0428; Time: 0.77\n",
      "Epoch 1212/8192 --- L(Train): 0.3773; L(Val): 0.4346; Reg Param: 0.0428; Time: 0.72\n",
      "Epoch 1213/8192 --- L(Train): 0.3806; L(Val): 0.4336; Reg Param: 0.0428; Time: 0.70\n",
      "Epoch 1214/8192 --- L(Train): 0.3891; L(Val): 0.4345; Reg Param: 0.0428; Time: 0.70\n",
      "Epoch 1215/8192 --- L(Train): 0.3678; L(Val): 0.4354; Reg Param: 0.0428; Time: 0.73\n",
      "Epoch 1216/8192 --- L(Train): 0.3820; L(Val): 0.4365; Reg Param: 0.0428; Time: 0.84\n",
      "Epoch 1217/8192 --- L(Train): 0.3891; L(Val): 0.4353; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1218/8192 --- L(Train): 0.3873; L(Val): 0.4349; Reg Param: 0.0428; Time: 0.70\n",
      "Epoch 1219/8192 --- L(Train): 0.3774; L(Val): 0.4350; Reg Param: 0.0428; Time: 0.70\n",
      "Epoch 1220/8192 --- L(Train): 0.3664; L(Val): 0.4345; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1221/8192 --- L(Train): 0.3793; L(Val): 0.4332; Reg Param: 0.0428; Time: 0.70\n",
      "Epoch 1222/8192 --- L(Train): 0.3814; L(Val): 0.4329; Reg Param: 0.0428; Time: 0.68\n",
      "Epoch 1223/8192 --- L(Train): 0.3770; L(Val): 0.4332; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1224/8192 --- L(Train): 0.3846; L(Val): 0.4333; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1225/8192 --- L(Train): 0.3973; L(Val): 0.4328; Reg Param: 0.0428; Time: 0.70\n",
      "Epoch 1226/8192 --- L(Train): 0.3785; L(Val): 0.4328; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1227/8192 --- L(Train): 0.3794; L(Val): 0.4331; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1228/8192 --- L(Train): 0.3718; L(Val): 0.4337; Reg Param: 0.0428; Time: 0.68\n",
      "Epoch 1229/8192 --- L(Train): 0.3752; L(Val): 0.4338; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1230/8192 --- L(Train): 0.3848; L(Val): 0.4337; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1231/8192 --- L(Train): 0.3673; L(Val): 0.4338; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1232/8192 --- L(Train): 0.3796; L(Val): 0.4343; Reg Param: 0.0428; Time: 0.71\n",
      "Epoch 1233/8192 --- L(Train): 0.3730; L(Val): 0.4346; Reg Param: 0.0428; Time: 0.70\n",
      "Epoch 1234/8192 --- L(Train): 0.3718; L(Val): 0.4336; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1235/8192 --- L(Train): 0.3762; L(Val): 0.4337; Reg Param: 0.0428; Time: 0.78\n",
      "Epoch 1236/8192 --- L(Train): 0.3835; L(Val): 0.4339; Reg Param: 0.0428; Time: 0.71\n",
      "Epoch 1237/8192 --- L(Train): 0.3811; L(Val): 0.4349; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1238/8192 --- L(Train): 0.3820; L(Val): 0.4361; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1239/8192 --- L(Train): 0.3773; L(Val): 0.4357; Reg Param: 0.0428; Time: 0.78\n",
      "Epoch 1240/8192 --- L(Train): 0.3845; L(Val): 0.4352; Reg Param: 0.0428; Time: 0.68\n",
      "Epoch 1241/8192 --- L(Train): 0.3780; L(Val): 0.4347; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1242/8192 --- L(Train): 0.3779; L(Val): 0.4339; Reg Param: 0.0428; Time: 0.71\n",
      "Epoch 1243/8192 --- L(Train): 0.3714; L(Val): 0.4333; Reg Param: 0.0428; Time: 0.70\n",
      "Epoch 1244/8192 --- L(Train): 0.3849; L(Val): 0.4323; Reg Param: 0.0428; Time: 0.70\n",
      "Epoch 1245/8192 --- L(Train): 0.3741; L(Val): 0.4317; Reg Param: 0.0428; Time: 0.75\n",
      "Epoch 1246/8192 --- L(Train): 0.3773; L(Val): 0.4321; Reg Param: 0.0428; Time: 0.69\n",
      "Epoch 1247/8192 --- L(Train): 0.3830; L(Val): 0.4328; Reg Param: 0.0428; Time: 0.70\n",
      "Epoch 1248/8192 --- L(Train): 0.3730; L(Val): 0.4335; Reg Param: 0.0428; Time: 0.68\n",
      "Epoch 1249/8192 --- L(Train): 0.3736; L(Val): 0.4345; Reg Param: 0.0428; Time: 0.68\n",
      "Epoch 1250/8192 --- L(Train): 0.3753; L(Val): 0.4345; Reg Param: 0.0454; Time: 7.51\n",
      "Epoch 1251/8192 --- L(Train): 0.3716; L(Val): 0.4331; Reg Param: 0.0454; Time: 0.70\n",
      "Epoch 1252/8192 --- L(Train): 0.3731; L(Val): 0.4328; Reg Param: 0.0454; Time: 0.70\n",
      "Epoch 1253/8192 --- L(Train): 0.3932; L(Val): 0.4324; Reg Param: 0.0454; Time: 0.72\n",
      "Epoch 1254/8192 --- L(Train): 0.3496; L(Val): 0.4322; Reg Param: 0.0454; Time: 0.73\n",
      "Epoch 1255/8192 --- L(Train): 0.3824; L(Val): 0.4317; Reg Param: 0.0454; Time: 0.70\n",
      "Epoch 1256/8192 --- L(Train): 0.3693; L(Val): 0.4319; Reg Param: 0.0454; Time: 0.69\n",
      "Epoch 1257/8192 --- L(Train): 0.3861; L(Val): 0.4319; Reg Param: 0.0454; Time: 0.70\n",
      "Epoch 1258/8192 --- L(Train): 0.3725; L(Val): 0.4316; Reg Param: 0.0454; Time: 0.70\n",
      "Epoch 1259/8192 --- L(Train): 0.3925; L(Val): 0.4314; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1260/8192 --- L(Train): 0.3821; L(Val): 0.4315; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1261/8192 --- L(Train): 0.3746; L(Val): 0.4320; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1262/8192 --- L(Train): 0.3813; L(Val): 0.4313; Reg Param: 0.0454; Time: 0.69\n",
      "Epoch 1263/8192 --- L(Train): 0.3728; L(Val): 0.4311; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1264/8192 --- L(Train): 0.3634; L(Val): 0.4310; Reg Param: 0.0454; Time: 0.69\n",
      "Epoch 1265/8192 --- L(Train): 0.3918; L(Val): 0.4319; Reg Param: 0.0454; Time: 0.71\n",
      "Epoch 1266/8192 --- L(Train): 0.3854; L(Val): 0.4336; Reg Param: 0.0454; Time: 0.81\n",
      "Epoch 1267/8192 --- L(Train): 0.3900; L(Val): 0.4335; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1268/8192 --- L(Train): 0.3698; L(Val): 0.4339; Reg Param: 0.0454; Time: 0.69\n",
      "Epoch 1269/8192 --- L(Train): 0.3730; L(Val): 0.4352; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1270/8192 --- L(Train): 0.3793; L(Val): 0.4356; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1271/8192 --- L(Train): 0.3805; L(Val): 0.4348; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1272/8192 --- L(Train): 0.3759; L(Val): 0.4337; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1273/8192 --- L(Train): 0.3828; L(Val): 0.4320; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1274/8192 --- L(Train): 0.3816; L(Val): 0.4310; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1275/8192 --- L(Train): 0.3760; L(Val): 0.4308; Reg Param: 0.0454; Time: 0.79\n",
      "Epoch 1276/8192 --- L(Train): 0.3872; L(Val): 0.4308; Reg Param: 0.0454; Time: 0.67\n",
      "Epoch 1277/8192 --- L(Train): 0.3889; L(Val): 0.4311; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1278/8192 --- L(Train): 0.3883; L(Val): 0.4313; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1279/8192 --- L(Train): 0.3752; L(Val): 0.4313; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1280/8192 --- L(Train): 0.3833; L(Val): 0.4312; Reg Param: 0.0454; Time: 0.70\n",
      "Epoch 1281/8192 --- L(Train): 0.3766; L(Val): 0.4316; Reg Param: 0.0454; Time: 0.70\n",
      "Epoch 1282/8192 --- L(Train): 0.3764; L(Val): 0.4320; Reg Param: 0.0454; Time: 0.70\n",
      "Epoch 1283/8192 --- L(Train): 0.3736; L(Val): 0.4320; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1284/8192 --- L(Train): 0.3820; L(Val): 0.4322; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1285/8192 --- L(Train): 0.3789; L(Val): 0.4328; Reg Param: 0.0454; Time: 0.70\n",
      "Epoch 1286/8192 --- L(Train): 0.3824; L(Val): 0.4328; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1287/8192 --- L(Train): 0.3814; L(Val): 0.4326; Reg Param: 0.0454; Time: 0.69\n",
      "Epoch 1288/8192 --- L(Train): 0.3769; L(Val): 0.4320; Reg Param: 0.0454; Time: 0.77\n",
      "Epoch 1289/8192 --- L(Train): 0.3728; L(Val): 0.4314; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1290/8192 --- L(Train): 0.3695; L(Val): 0.4314; Reg Param: 0.0454; Time: 0.69\n",
      "Epoch 1291/8192 --- L(Train): 0.3750; L(Val): 0.4312; Reg Param: 0.0454; Time: 0.69\n",
      "Epoch 1292/8192 --- L(Train): 0.3828; L(Val): 0.4313; Reg Param: 0.0454; Time: 0.68\n",
      "Epoch 1293/8192 --- L(Train): 0.3708; L(Val): 0.4311; Reg Param: 0.0454; Time: 0.69\n",
      "Epoch 1294/8192 --- L(Train): 0.3797; L(Val): 0.4315; Reg Param: 0.0454; Time: 0.70\n",
      "Epoch 1295/8192 --- L(Train): 0.3779; L(Val): 0.4321; Reg Param: 0.0454; Time: 0.70\n",
      "Epoch 1296/8192 --- L(Train): 0.3753; L(Val): 0.4329; Reg Param: 0.0454; Time: 0.77\n",
      "Epoch 1297/8192 --- L(Train): 0.3808; L(Val): 0.4336; Reg Param: 0.0454; Time: 0.71\n",
      "Epoch 1298/8192 --- L(Train): 0.3785; L(Val): 0.4338; Reg Param: 0.0454; Time: 0.69\n",
      "Epoch 1299/8192 --- L(Train): 0.3688; L(Val): 0.4341; Reg Param: 0.0454; Time: 0.69\n",
      "Epoch 1300/8192 --- L(Train): 0.3777; L(Val): 0.4341; Reg Param: 0.0478; Time: 7.37\n",
      "Epoch 1301/8192 --- L(Train): 0.3796; L(Val): 0.4340; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1302/8192 --- L(Train): 0.3757; L(Val): 0.4339; Reg Param: 0.0478; Time: 0.79\n",
      "Epoch 1303/8192 --- L(Train): 0.3778; L(Val): 0.4339; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1304/8192 --- L(Train): 0.3849; L(Val): 0.4337; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1305/8192 --- L(Train): 0.3923; L(Val): 0.4338; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1306/8192 --- L(Train): 0.3802; L(Val): 0.4338; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1307/8192 --- L(Train): 0.3725; L(Val): 0.4339; Reg Param: 0.0478; Time: 0.70\n",
      "Epoch 1308/8192 --- L(Train): 0.3652; L(Val): 0.4344; Reg Param: 0.0478; Time: 0.70\n",
      "Epoch 1309/8192 --- L(Train): 0.3935; L(Val): 0.4342; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1310/8192 --- L(Train): 0.3787; L(Val): 0.4334; Reg Param: 0.0478; Time: 0.70\n",
      "Epoch 1311/8192 --- L(Train): 0.3803; L(Val): 0.4331; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1312/8192 --- L(Train): 0.3813; L(Val): 0.4332; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1313/8192 --- L(Train): 0.3643; L(Val): 0.4341; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1314/8192 --- L(Train): 0.3796; L(Val): 0.4350; Reg Param: 0.0478; Time: 0.74\n",
      "Epoch 1315/8192 --- L(Train): 0.3839; L(Val): 0.4347; Reg Param: 0.0478; Time: 0.75\n",
      "Epoch 1316/8192 --- L(Train): 0.3737; L(Val): 0.4344; Reg Param: 0.0478; Time: 0.72\n",
      "Epoch 1317/8192 --- L(Train): 0.3759; L(Val): 0.4339; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1318/8192 --- L(Train): 0.3709; L(Val): 0.4343; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1319/8192 --- L(Train): 0.3738; L(Val): 0.4344; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1320/8192 --- L(Train): 0.3615; L(Val): 0.4333; Reg Param: 0.0478; Time: 0.77\n",
      "Epoch 1321/8192 --- L(Train): 0.3804; L(Val): 0.4325; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1322/8192 --- L(Train): 0.3813; L(Val): 0.4325; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1323/8192 --- L(Train): 0.3791; L(Val): 0.4332; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1324/8192 --- L(Train): 0.3779; L(Val): 0.4338; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1325/8192 --- L(Train): 0.3762; L(Val): 0.4342; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1326/8192 --- L(Train): 0.3704; L(Val): 0.4345; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1327/8192 --- L(Train): 0.3837; L(Val): 0.4349; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1328/8192 --- L(Train): 0.3739; L(Val): 0.4357; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1329/8192 --- L(Train): 0.3751; L(Val): 0.4368; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1330/8192 --- L(Train): 0.3887; L(Val): 0.4369; Reg Param: 0.0478; Time: 0.71\n",
      "Epoch 1331/8192 --- L(Train): 0.3859; L(Val): 0.4363; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1332/8192 --- L(Train): 0.3762; L(Val): 0.4362; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1333/8192 --- L(Train): 0.3757; L(Val): 0.4363; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1334/8192 --- L(Train): 0.3744; L(Val): 0.4362; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1335/8192 --- L(Train): 0.3719; L(Val): 0.4352; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1336/8192 --- L(Train): 0.3909; L(Val): 0.4347; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1337/8192 --- L(Train): 0.3705; L(Val): 0.4349; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1338/8192 --- L(Train): 0.3764; L(Val): 0.4358; Reg Param: 0.0478; Time: 0.70\n",
      "Epoch 1339/8192 --- L(Train): 0.3762; L(Val): 0.4357; Reg Param: 0.0478; Time: 0.67\n",
      "Epoch 1340/8192 --- L(Train): 0.3844; L(Val): 0.4352; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1341/8192 --- L(Train): 0.3760; L(Val): 0.4348; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1342/8192 --- L(Train): 0.3749; L(Val): 0.4345; Reg Param: 0.0478; Time: 0.70\n",
      "Epoch 1343/8192 --- L(Train): 0.3751; L(Val): 0.4343; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1344/8192 --- L(Train): 0.3798; L(Val): 0.4344; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1345/8192 --- L(Train): 0.3775; L(Val): 0.4349; Reg Param: 0.0478; Time: 0.83\n",
      "Epoch 1346/8192 --- L(Train): 0.3798; L(Val): 0.4343; Reg Param: 0.0478; Time: 0.72\n",
      "Epoch 1347/8192 --- L(Train): 0.3785; L(Val): 0.4341; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1348/8192 --- L(Train): 0.3740; L(Val): 0.4346; Reg Param: 0.0478; Time: 0.68\n",
      "Epoch 1349/8192 --- L(Train): 0.3742; L(Val): 0.4347; Reg Param: 0.0478; Time: 0.69\n",
      "Epoch 1350/8192 --- L(Train): 0.3690; L(Val): 0.4347; Reg Param: 0.0499; Time: 7.34\n",
      "Epoch 1351/8192 --- L(Train): 0.3839; L(Val): 0.4335; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1352/8192 --- L(Train): 0.3735; L(Val): 0.4335; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1353/8192 --- L(Train): 0.3772; L(Val): 0.4345; Reg Param: 0.0499; Time: 0.67\n",
      "Epoch 1354/8192 --- L(Train): 0.3827; L(Val): 0.4358; Reg Param: 0.0499; Time: 0.75\n",
      "Epoch 1355/8192 --- L(Train): 0.3854; L(Val): 0.4360; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1356/8192 --- L(Train): 0.3821; L(Val): 0.4361; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1357/8192 --- L(Train): 0.3809; L(Val): 0.4361; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1358/8192 --- L(Train): 0.3820; L(Val): 0.4367; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1359/8192 --- L(Train): 0.3862; L(Val): 0.4370; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1360/8192 --- L(Train): 0.3746; L(Val): 0.4360; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1361/8192 --- L(Train): 0.3680; L(Val): 0.4354; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1362/8192 --- L(Train): 0.3748; L(Val): 0.4354; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1363/8192 --- L(Train): 0.3763; L(Val): 0.4354; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1364/8192 --- L(Train): 0.3837; L(Val): 0.4355; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1365/8192 --- L(Train): 0.3799; L(Val): 0.4352; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1366/8192 --- L(Train): 0.3869; L(Val): 0.4348; Reg Param: 0.0499; Time: 0.74\n",
      "Epoch 1367/8192 --- L(Train): 0.3746; L(Val): 0.4344; Reg Param: 0.0499; Time: 0.74\n",
      "Epoch 1368/8192 --- L(Train): 0.3808; L(Val): 0.4342; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1369/8192 --- L(Train): 0.3747; L(Val): 0.4340; Reg Param: 0.0499; Time: 0.77\n",
      "Epoch 1370/8192 --- L(Train): 0.3806; L(Val): 0.4339; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1371/8192 --- L(Train): 0.3724; L(Val): 0.4341; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1372/8192 --- L(Train): 0.3848; L(Val): 0.4343; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1373/8192 --- L(Train): 0.3782; L(Val): 0.4345; Reg Param: 0.0499; Time: 0.71\n",
      "Epoch 1374/8192 --- L(Train): 0.3858; L(Val): 0.4345; Reg Param: 0.0499; Time: 0.72\n",
      "Epoch 1375/8192 --- L(Train): 0.3851; L(Val): 0.4345; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1376/8192 --- L(Train): 0.3730; L(Val): 0.4344; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1377/8192 --- L(Train): 0.3819; L(Val): 0.4344; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1378/8192 --- L(Train): 0.3842; L(Val): 0.4347; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1379/8192 --- L(Train): 0.3673; L(Val): 0.4349; Reg Param: 0.0499; Time: 0.77\n",
      "Epoch 1380/8192 --- L(Train): 0.3803; L(Val): 0.4347; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1381/8192 --- L(Train): 0.3767; L(Val): 0.4348; Reg Param: 0.0499; Time: 0.70\n",
      "Epoch 1382/8192 --- L(Train): 0.3771; L(Val): 0.4348; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1383/8192 --- L(Train): 0.3741; L(Val): 0.4345; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1384/8192 --- L(Train): 0.3744; L(Val): 0.4345; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1385/8192 --- L(Train): 0.3712; L(Val): 0.4351; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1386/8192 --- L(Train): 0.3758; L(Val): 0.4357; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1387/8192 --- L(Train): 0.3685; L(Val): 0.4354; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1388/8192 --- L(Train): 0.3919; L(Val): 0.4351; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1389/8192 --- L(Train): 0.3779; L(Val): 0.4348; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1390/8192 --- L(Train): 0.3675; L(Val): 0.4349; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1391/8192 --- L(Train): 0.3861; L(Val): 0.4353; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1392/8192 --- L(Train): 0.3770; L(Val): 0.4353; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1393/8192 --- L(Train): 0.3690; L(Val): 0.4347; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1394/8192 --- L(Train): 0.3852; L(Val): 0.4346; Reg Param: 0.0499; Time: 0.69\n",
      "Epoch 1395/8192 --- L(Train): 0.3681; L(Val): 0.4337; Reg Param: 0.0499; Time: 0.71\n",
      "Epoch 1396/8192 --- L(Train): 0.3791; L(Val): 0.4335; Reg Param: 0.0499; Time: 0.79\n",
      "Epoch 1397/8192 --- L(Train): 0.3760; L(Val): 0.4334; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1398/8192 --- L(Train): 0.3680; L(Val): 0.4335; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1399/8192 --- L(Train): 0.3745; L(Val): 0.4336; Reg Param: 0.0499; Time: 0.68\n",
      "Epoch 1400/8192 --- L(Train): 0.3738; L(Val): 0.4336; Reg Param: 0.0520; Time: 7.31\n",
      "Epoch 1401/8192 --- L(Train): 0.3728; L(Val): 0.4344; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1402/8192 --- L(Train): 0.3732; L(Val): 0.4352; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1403/8192 --- L(Train): 0.3688; L(Val): 0.4360; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1404/8192 --- L(Train): 0.3704; L(Val): 0.4361; Reg Param: 0.0520; Time: 0.68\n",
      "Epoch 1405/8192 --- L(Train): 0.3778; L(Val): 0.4360; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1406/8192 --- L(Train): 0.3756; L(Val): 0.4359; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1407/8192 --- L(Train): 0.3864; L(Val): 0.4352; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1408/8192 --- L(Train): 0.3774; L(Val): 0.4344; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1409/8192 --- L(Train): 0.3873; L(Val): 0.4335; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1410/8192 --- L(Train): 0.3851; L(Val): 0.4335; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1411/8192 --- L(Train): 0.3849; L(Val): 0.4335; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1412/8192 --- L(Train): 0.3824; L(Val): 0.4339; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1413/8192 --- L(Train): 0.3797; L(Val): 0.4345; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1414/8192 --- L(Train): 0.3749; L(Val): 0.4355; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1415/8192 --- L(Train): 0.3701; L(Val): 0.4367; Reg Param: 0.0520; Time: 0.71\n",
      "Epoch 1416/8192 --- L(Train): 0.3865; L(Val): 0.4376; Reg Param: 0.0520; Time: 0.73\n",
      "Epoch 1417/8192 --- L(Train): 0.3695; L(Val): 0.4368; Reg Param: 0.0520; Time: 0.83\n",
      "Epoch 1418/8192 --- L(Train): 0.3941; L(Val): 0.4368; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1419/8192 --- L(Train): 0.3818; L(Val): 0.4367; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1420/8192 --- L(Train): 0.3666; L(Val): 0.4373; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1421/8192 --- L(Train): 0.3867; L(Val): 0.4365; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1422/8192 --- L(Train): 0.3773; L(Val): 0.4352; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1423/8192 --- L(Train): 0.3862; L(Val): 0.4342; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1424/8192 --- L(Train): 0.3697; L(Val): 0.4345; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1425/8192 --- L(Train): 0.3635; L(Val): 0.4351; Reg Param: 0.0520; Time: 0.81\n",
      "Epoch 1426/8192 --- L(Train): 0.3678; L(Val): 0.4359; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1427/8192 --- L(Train): 0.3888; L(Val): 0.4357; Reg Param: 0.0520; Time: 0.71\n",
      "Epoch 1428/8192 --- L(Train): 0.3693; L(Val): 0.4362; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1429/8192 --- L(Train): 0.3680; L(Val): 0.4369; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1430/8192 --- L(Train): 0.3796; L(Val): 0.4378; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1431/8192 --- L(Train): 0.3758; L(Val): 0.4349; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1432/8192 --- L(Train): 0.3887; L(Val): 0.4348; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1433/8192 --- L(Train): 0.3724; L(Val): 0.4330; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1434/8192 --- L(Train): 0.3948; L(Val): 0.4336; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1435/8192 --- L(Train): 0.3782; L(Val): 0.4317; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1436/8192 --- L(Train): 0.3873; L(Val): 0.4288; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1437/8192 --- L(Train): 0.3948; L(Val): 0.4276; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1438/8192 --- L(Train): 0.3851; L(Val): 0.4270; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1439/8192 --- L(Train): 0.3954; L(Val): 0.4273; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1440/8192 --- L(Train): 0.3928; L(Val): 0.4277; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1441/8192 --- L(Train): 0.3850; L(Val): 0.4283; Reg Param: 0.0520; Time: 0.68\n",
      "Epoch 1442/8192 --- L(Train): 0.3921; L(Val): 0.4290; Reg Param: 0.0520; Time: 0.68\n",
      "Epoch 1443/8192 --- L(Train): 0.3859; L(Val): 0.4296; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1444/8192 --- L(Train): 0.3840; L(Val): 0.4303; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1445/8192 --- L(Train): 0.3856; L(Val): 0.4311; Reg Param: 0.0520; Time: 0.70\n",
      "Epoch 1446/8192 --- L(Train): 0.3904; L(Val): 0.4318; Reg Param: 0.0520; Time: 0.71\n",
      "Epoch 1447/8192 --- L(Train): 0.3998; L(Val): 0.4324; Reg Param: 0.0520; Time: 0.79\n",
      "Epoch 1448/8192 --- L(Train): 0.3844; L(Val): 0.4328; Reg Param: 0.0520; Time: 0.71\n",
      "Epoch 1449/8192 --- L(Train): 0.3756; L(Val): 0.4335; Reg Param: 0.0520; Time: 0.69\n",
      "Epoch 1450/8192 --- L(Train): 0.3824; L(Val): 0.4335; Reg Param: 0.0538; Time: 7.38\n",
      "Epoch 1451/8192 --- L(Train): 0.3798; L(Val): 0.4340; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1452/8192 --- L(Train): 0.3851; L(Val): 0.4342; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1453/8192 --- L(Train): 0.3839; L(Val): 0.4348; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1454/8192 --- L(Train): 0.3764; L(Val): 0.4353; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1455/8192 --- L(Train): 0.3840; L(Val): 0.4352; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1456/8192 --- L(Train): 0.3790; L(Val): 0.4357; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1457/8192 --- L(Train): 0.3718; L(Val): 0.4366; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1458/8192 --- L(Train): 0.3829; L(Val): 0.4374; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1459/8192 --- L(Train): 0.3808; L(Val): 0.4375; Reg Param: 0.0538; Time: 0.68\n",
      "Epoch 1460/8192 --- L(Train): 0.3885; L(Val): 0.4374; Reg Param: 0.0538; Time: 0.68\n",
      "Epoch 1461/8192 --- L(Train): 0.3841; L(Val): 0.4369; Reg Param: 0.0538; Time: 0.68\n",
      "Epoch 1462/8192 --- L(Train): 0.4002; L(Val): 0.4363; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1463/8192 --- L(Train): 0.3845; L(Val): 0.4357; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1464/8192 --- L(Train): 0.3782; L(Val): 0.4353; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1465/8192 --- L(Train): 0.3661; L(Val): 0.4347; Reg Param: 0.0538; Time: 0.79\n",
      "Epoch 1466/8192 --- L(Train): 0.3868; L(Val): 0.4340; Reg Param: 0.0538; Time: 0.75\n",
      "Epoch 1467/8192 --- L(Train): 0.3700; L(Val): 0.4338; Reg Param: 0.0538; Time: 0.72\n",
      "Epoch 1468/8192 --- L(Train): 0.3846; L(Val): 0.4335; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1469/8192 --- L(Train): 0.3881; L(Val): 0.4337; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1470/8192 --- L(Train): 0.3757; L(Val): 0.4339; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1471/8192 --- L(Train): 0.3785; L(Val): 0.4336; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1472/8192 --- L(Train): 0.3735; L(Val): 0.4338; Reg Param: 0.0538; Time: 0.77\n",
      "Epoch 1473/8192 --- L(Train): 0.3841; L(Val): 0.4342; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1474/8192 --- L(Train): 0.3760; L(Val): 0.4347; Reg Param: 0.0538; Time: 0.72\n",
      "Epoch 1475/8192 --- L(Train): 0.3796; L(Val): 0.4354; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1476/8192 --- L(Train): 0.3908; L(Val): 0.4355; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1477/8192 --- L(Train): 0.3818; L(Val): 0.4350; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1478/8192 --- L(Train): 0.3858; L(Val): 0.4345; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1479/8192 --- L(Train): 0.3680; L(Val): 0.4342; Reg Param: 0.0538; Time: 0.71\n",
      "Epoch 1480/8192 --- L(Train): 0.3874; L(Val): 0.4343; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1481/8192 --- L(Train): 0.3799; L(Val): 0.4348; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1482/8192 --- L(Train): 0.3821; L(Val): 0.4343; Reg Param: 0.0538; Time: 0.77\n",
      "Epoch 1483/8192 --- L(Train): 0.3776; L(Val): 0.4340; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1484/8192 --- L(Train): 0.3836; L(Val): 0.4343; Reg Param: 0.0538; Time: 0.71\n",
      "Epoch 1485/8192 --- L(Train): 0.3863; L(Val): 0.4347; Reg Param: 0.0538; Time: 0.75\n",
      "Epoch 1486/8192 --- L(Train): 0.3692; L(Val): 0.4351; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1487/8192 --- L(Train): 0.3760; L(Val): 0.4359; Reg Param: 0.0538; Time: 0.74\n",
      "Epoch 1488/8192 --- L(Train): 0.3802; L(Val): 0.4365; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1489/8192 --- L(Train): 0.3808; L(Val): 0.4367; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1490/8192 --- L(Train): 0.3811; L(Val): 0.4366; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1491/8192 --- L(Train): 0.3783; L(Val): 0.4361; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1492/8192 --- L(Train): 0.3829; L(Val): 0.4360; Reg Param: 0.0538; Time: 0.68\n",
      "Epoch 1493/8192 --- L(Train): 0.3736; L(Val): 0.4360; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1494/8192 --- L(Train): 0.3817; L(Val): 0.4351; Reg Param: 0.0538; Time: 0.72\n",
      "Epoch 1495/8192 --- L(Train): 0.3831; L(Val): 0.4344; Reg Param: 0.0538; Time: 0.84\n",
      "Epoch 1496/8192 --- L(Train): 0.3762; L(Val): 0.4343; Reg Param: 0.0538; Time: 0.71\n",
      "Epoch 1497/8192 --- L(Train): 0.3828; L(Val): 0.4348; Reg Param: 0.0538; Time: 0.70\n",
      "Epoch 1498/8192 --- L(Train): 0.3730; L(Val): 0.4355; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1499/8192 --- L(Train): 0.3777; L(Val): 0.4357; Reg Param: 0.0538; Time: 0.69\n",
      "Epoch 1500/8192 --- L(Train): 0.3827; L(Val): 0.4357; Reg Param: 0.0555; Time: 7.66\n",
      "Epoch 1501/8192 --- L(Train): 0.3797; L(Val): 0.4359; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1502/8192 --- L(Train): 0.3843; L(Val): 0.4361; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1503/8192 --- L(Train): 0.3839; L(Val): 0.4362; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1504/8192 --- L(Train): 0.3742; L(Val): 0.4360; Reg Param: 0.0555; Time: 0.71\n",
      "Epoch 1505/8192 --- L(Train): 0.3868; L(Val): 0.4357; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1506/8192 --- L(Train): 0.3994; L(Val): 0.4356; Reg Param: 0.0555; Time: 0.71\n",
      "Epoch 1507/8192 --- L(Train): 0.3801; L(Val): 0.4358; Reg Param: 0.0555; Time: 0.71\n",
      "Epoch 1508/8192 --- L(Train): 0.3845; L(Val): 0.4359; Reg Param: 0.0555; Time: 0.71\n",
      "Epoch 1509/8192 --- L(Train): 0.3801; L(Val): 0.4362; Reg Param: 0.0555; Time: 0.71\n",
      "Epoch 1510/8192 --- L(Train): 0.3824; L(Val): 0.4363; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1511/8192 --- L(Train): 0.3873; L(Val): 0.4364; Reg Param: 0.0555; Time: 0.80\n",
      "Epoch 1512/8192 --- L(Train): 0.3798; L(Val): 0.4366; Reg Param: 0.0555; Time: 0.71\n",
      "Epoch 1513/8192 --- L(Train): 0.3586; L(Val): 0.4370; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1514/8192 --- L(Train): 0.3783; L(Val): 0.4370; Reg Param: 0.0555; Time: 0.75\n",
      "Epoch 1515/8192 --- L(Train): 0.3792; L(Val): 0.4367; Reg Param: 0.0555; Time: 0.72\n",
      "Epoch 1516/8192 --- L(Train): 0.3693; L(Val): 0.4368; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1517/8192 --- L(Train): 0.3892; L(Val): 0.4368; Reg Param: 0.0555; Time: 0.71\n",
      "Epoch 1518/8192 --- L(Train): 0.3840; L(Val): 0.4368; Reg Param: 0.0555; Time: 0.72\n",
      "Epoch 1519/8192 --- L(Train): 0.3828; L(Val): 0.4370; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1520/8192 --- L(Train): 0.3793; L(Val): 0.4372; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1521/8192 --- L(Train): 0.3774; L(Val): 0.4371; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1522/8192 --- L(Train): 0.3780; L(Val): 0.4368; Reg Param: 0.0555; Time: 0.77\n",
      "Epoch 1523/8192 --- L(Train): 0.3770; L(Val): 0.4364; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1524/8192 --- L(Train): 0.3876; L(Val): 0.4360; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1525/8192 --- L(Train): 0.3744; L(Val): 0.4362; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1526/8192 --- L(Train): 0.3737; L(Val): 0.4357; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1527/8192 --- L(Train): 0.3823; L(Val): 0.4354; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1528/8192 --- L(Train): 0.3767; L(Val): 0.4361; Reg Param: 0.0555; Time: 0.77\n",
      "Epoch 1529/8192 --- L(Train): 0.3721; L(Val): 0.4371; Reg Param: 0.0555; Time: 0.71\n",
      "Epoch 1530/8192 --- L(Train): 0.3697; L(Val): 0.4368; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1531/8192 --- L(Train): 0.3812; L(Val): 0.4363; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1532/8192 --- L(Train): 0.3808; L(Val): 0.4362; Reg Param: 0.0555; Time: 0.72\n",
      "Epoch 1533/8192 --- L(Train): 0.3820; L(Val): 0.4362; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1534/8192 --- L(Train): 0.3722; L(Val): 0.4360; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1535/8192 --- L(Train): 0.3848; L(Val): 0.4355; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1536/8192 --- L(Train): 0.3832; L(Val): 0.4347; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1537/8192 --- L(Train): 0.3663; L(Val): 0.4335; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1538/8192 --- L(Train): 0.3761; L(Val): 0.4331; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1539/8192 --- L(Train): 0.3645; L(Val): 0.4332; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1540/8192 --- L(Train): 0.3734; L(Val): 0.4337; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1541/8192 --- L(Train): 0.3669; L(Val): 0.4342; Reg Param: 0.0555; Time: 0.68\n",
      "Epoch 1542/8192 --- L(Train): 0.3681; L(Val): 0.4345; Reg Param: 0.0555; Time: 0.70\n",
      "Epoch 1543/8192 --- L(Train): 0.3840; L(Val): 0.4347; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1544/8192 --- L(Train): 0.3794; L(Val): 0.4347; Reg Param: 0.0555; Time: 0.72\n",
      "Epoch 1545/8192 --- L(Train): 0.3791; L(Val): 0.4348; Reg Param: 0.0555; Time: 0.76\n",
      "Epoch 1546/8192 --- L(Train): 0.3774; L(Val): 0.4352; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1547/8192 --- L(Train): 0.3806; L(Val): 0.4354; Reg Param: 0.0555; Time: 0.78\n",
      "Epoch 1548/8192 --- L(Train): 0.3777; L(Val): 0.4347; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1549/8192 --- L(Train): 0.3764; L(Val): 0.4340; Reg Param: 0.0555; Time: 0.69\n",
      "Epoch 1550/8192 --- L(Train): 0.3819; L(Val): 0.4340; Reg Param: 0.0571; Time: 7.36\n",
      "Epoch 1551/8192 --- L(Train): 0.3855; L(Val): 0.4338; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1552/8192 --- L(Train): 0.3753; L(Val): 0.4339; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1553/8192 --- L(Train): 0.3782; L(Val): 0.4339; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1554/8192 --- L(Train): 0.3775; L(Val): 0.4335; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1555/8192 --- L(Train): 0.3711; L(Val): 0.4331; Reg Param: 0.0571; Time: 0.71\n",
      "Epoch 1556/8192 --- L(Train): 0.3826; L(Val): 0.4327; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1557/8192 --- L(Train): 0.3776; L(Val): 0.4326; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1558/8192 --- L(Train): 0.3802; L(Val): 0.4326; Reg Param: 0.0571; Time: 0.68\n",
      "Epoch 1559/8192 --- L(Train): 0.3791; L(Val): 0.4326; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1560/8192 --- L(Train): 0.3719; L(Val): 0.4326; Reg Param: 0.0571; Time: 0.68\n",
      "Epoch 1561/8192 --- L(Train): 0.3745; L(Val): 0.4327; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1562/8192 --- L(Train): 0.3722; L(Val): 0.4329; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1563/8192 --- L(Train): 0.3827; L(Val): 0.4329; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1564/8192 --- L(Train): 0.3746; L(Val): 0.4329; Reg Param: 0.0571; Time: 0.80\n",
      "Epoch 1565/8192 --- L(Train): 0.3767; L(Val): 0.4329; Reg Param: 0.0571; Time: 0.74\n",
      "Epoch 1566/8192 --- L(Train): 0.3769; L(Val): 0.4332; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1567/8192 --- L(Train): 0.3721; L(Val): 0.4334; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1568/8192 --- L(Train): 0.3824; L(Val): 0.4335; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1569/8192 --- L(Train): 0.3749; L(Val): 0.4334; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1570/8192 --- L(Train): 0.3752; L(Val): 0.4333; Reg Param: 0.0571; Time: 0.68\n",
      "Epoch 1571/8192 --- L(Train): 0.3740; L(Val): 0.4333; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1572/8192 --- L(Train): 0.3822; L(Val): 0.4335; Reg Param: 0.0571; Time: 0.68\n",
      "Epoch 1573/8192 --- L(Train): 0.3747; L(Val): 0.4338; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1574/8192 --- L(Train): 0.3849; L(Val): 0.4341; Reg Param: 0.0571; Time: 0.80\n",
      "Epoch 1575/8192 --- L(Train): 0.3808; L(Val): 0.4343; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1576/8192 --- L(Train): 0.3823; L(Val): 0.4345; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1577/8192 --- L(Train): 0.3868; L(Val): 0.4348; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1578/8192 --- L(Train): 0.3834; L(Val): 0.4348; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1579/8192 --- L(Train): 0.3721; L(Val): 0.4346; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1580/8192 --- L(Train): 0.3734; L(Val): 0.4345; Reg Param: 0.0571; Time: 0.71\n",
      "Epoch 1581/8192 --- L(Train): 0.3891; L(Val): 0.4344; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1582/8192 --- L(Train): 0.3851; L(Val): 0.4340; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1583/8192 --- L(Train): 0.3771; L(Val): 0.4337; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1584/8192 --- L(Train): 0.3810; L(Val): 0.4335; Reg Param: 0.0571; Time: 0.68\n",
      "Epoch 1585/8192 --- L(Train): 0.3766; L(Val): 0.4333; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1586/8192 --- L(Train): 0.3759; L(Val): 0.4333; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1587/8192 --- L(Train): 0.3725; L(Val): 0.4335; Reg Param: 0.0571; Time: 0.77\n",
      "Epoch 1588/8192 --- L(Train): 0.3779; L(Val): 0.4339; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1589/8192 --- L(Train): 0.3871; L(Val): 0.4343; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1590/8192 --- L(Train): 0.3665; L(Val): 0.4346; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1591/8192 --- L(Train): 0.3942; L(Val): 0.4349; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1592/8192 --- L(Train): 0.3759; L(Val): 0.4352; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1593/8192 --- L(Train): 0.3771; L(Val): 0.4356; Reg Param: 0.0571; Time: 0.75\n",
      "Epoch 1594/8192 --- L(Train): 0.3812; L(Val): 0.4358; Reg Param: 0.0571; Time: 0.72\n",
      "Epoch 1595/8192 --- L(Train): 0.3841; L(Val): 0.4358; Reg Param: 0.0571; Time: 0.69\n",
      "Epoch 1596/8192 --- L(Train): 0.3814; L(Val): 0.4356; Reg Param: 0.0571; Time: 0.70\n",
      "Epoch 1597/8192 --- L(Train): 0.3812; L(Val): 0.4354; Reg Param: 0.0571; Time: 0.71\n",
      "Epoch 1598/8192 --- L(Train): 0.3784; L(Val): 0.4352; Reg Param: 0.0571; Time: 0.73\n",
      "Epoch 1599/8192 --- L(Train): 0.3783; L(Val): 0.4351; Reg Param: 0.0571; Time: 0.74\n",
      "Epoch 1600/8192 --- L(Train): 0.3693; L(Val): 0.4351; Reg Param: 0.0586; Time: 7.57\n",
      "Epoch 1601/8192 --- L(Train): 0.3751; L(Val): 0.4349; Reg Param: 0.0586; Time: 0.72\n",
      "Epoch 1602/8192 --- L(Train): 0.3754; L(Val): 0.4347; Reg Param: 0.0586; Time: 0.72\n",
      "Epoch 1603/8192 --- L(Train): 0.3809; L(Val): 0.4347; Reg Param: 0.0586; Time: 0.74\n",
      "Epoch 1604/8192 --- L(Train): 0.3762; L(Val): 0.4348; Reg Param: 0.0586; Time: 0.73\n",
      "Epoch 1605/8192 --- L(Train): 0.3816; L(Val): 0.4350; Reg Param: 0.0586; Time: 0.71\n",
      "Epoch 1606/8192 --- L(Train): 0.3709; L(Val): 0.4352; Reg Param: 0.0586; Time: 0.71\n",
      "Epoch 1607/8192 --- L(Train): 0.3799; L(Val): 0.4353; Reg Param: 0.0586; Time: 0.71\n",
      "Epoch 1608/8192 --- L(Train): 0.3724; L(Val): 0.4354; Reg Param: 0.0586; Time: 0.71\n",
      "Epoch 1609/8192 --- L(Train): 0.3748; L(Val): 0.4355; Reg Param: 0.0586; Time: 0.71\n",
      "Epoch 1610/8192 --- L(Train): 0.3744; L(Val): 0.4358; Reg Param: 0.0586; Time: 0.78\n",
      "Epoch 1611/8192 --- L(Train): 0.3800; L(Val): 0.4365; Reg Param: 0.0586; Time: 0.72\n",
      "Epoch 1612/8192 --- L(Train): 0.3852; L(Val): 0.4366; Reg Param: 0.0586; Time: 0.72\n",
      "Epoch 1613/8192 --- L(Train): 0.3720; L(Val): 0.4359; Reg Param: 0.0586; Time: 0.77\n",
      "Epoch 1614/8192 --- L(Train): 0.3842; L(Val): 0.4352; Reg Param: 0.0586; Time: 0.88\n",
      "Epoch 1615/8192 --- L(Train): 0.3763; L(Val): 0.4345; Reg Param: 0.0586; Time: 0.71\n",
      "Epoch 1616/8192 --- L(Train): 0.3739; L(Val): 0.4340; Reg Param: 0.0586; Time: 0.71\n",
      "Epoch 1617/8192 --- L(Train): 0.3740; L(Val): 0.4335; Reg Param: 0.0586; Time: 0.71\n",
      "Epoch 1618/8192 --- L(Train): 0.3746; L(Val): 0.4335; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1619/8192 --- L(Train): 0.3776; L(Val): 0.4339; Reg Param: 0.0586; Time: 0.69\n",
      "Epoch 1620/8192 --- L(Train): 0.3724; L(Val): 0.4345; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1621/8192 --- L(Train): 0.3654; L(Val): 0.4350; Reg Param: 0.0586; Time: 0.69\n",
      "Epoch 1622/8192 --- L(Train): 0.3803; L(Val): 0.4354; Reg Param: 0.0586; Time: 0.69\n",
      "Epoch 1623/8192 --- L(Train): 0.3737; L(Val): 0.4357; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1624/8192 --- L(Train): 0.3873; L(Val): 0.4358; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1625/8192 --- L(Train): 0.3849; L(Val): 0.4359; Reg Param: 0.0586; Time: 0.69\n",
      "Epoch 1626/8192 --- L(Train): 0.3698; L(Val): 0.4357; Reg Param: 0.0586; Time: 0.69\n",
      "Epoch 1627/8192 --- L(Train): 0.3730; L(Val): 0.4357; Reg Param: 0.0586; Time: 0.69\n",
      "Epoch 1628/8192 --- L(Train): 0.3887; L(Val): 0.4355; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1629/8192 --- L(Train): 0.3808; L(Val): 0.4352; Reg Param: 0.0586; Time: 0.71\n",
      "Epoch 1630/8192 --- L(Train): 0.3671; L(Val): 0.4350; Reg Param: 0.0586; Time: 0.69\n",
      "Epoch 1631/8192 --- L(Train): 0.3860; L(Val): 0.4347; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1632/8192 --- L(Train): 0.3765; L(Val): 0.4343; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1633/8192 --- L(Train): 0.3735; L(Val): 0.4341; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1634/8192 --- L(Train): 0.3682; L(Val): 0.4340; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1635/8192 --- L(Train): 0.3645; L(Val): 0.4341; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1636/8192 --- L(Train): 0.3726; L(Val): 0.4340; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1637/8192 --- L(Train): 0.3755; L(Val): 0.4339; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1638/8192 --- L(Train): 0.3811; L(Val): 0.4338; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1639/8192 --- L(Train): 0.3887; L(Val): 0.4339; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1640/8192 --- L(Train): 0.3844; L(Val): 0.4339; Reg Param: 0.0586; Time: 0.71\n",
      "Epoch 1641/8192 --- L(Train): 0.3803; L(Val): 0.4340; Reg Param: 0.0586; Time: 0.71\n",
      "Epoch 1642/8192 --- L(Train): 0.3838; L(Val): 0.4342; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1643/8192 --- L(Train): 0.3746; L(Val): 0.4342; Reg Param: 0.0586; Time: 0.84\n",
      "Epoch 1644/8192 --- L(Train): 0.3801; L(Val): 0.4345; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1645/8192 --- L(Train): 0.3870; L(Val): 0.4346; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1646/8192 --- L(Train): 0.3792; L(Val): 0.4343; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1647/8192 --- L(Train): 0.3805; L(Val): 0.4343; Reg Param: 0.0586; Time: 0.73\n",
      "Epoch 1648/8192 --- L(Train): 0.3627; L(Val): 0.4344; Reg Param: 0.0586; Time: 0.72\n",
      "Epoch 1649/8192 --- L(Train): 0.3801; L(Val): 0.4347; Reg Param: 0.0586; Time: 0.70\n",
      "Epoch 1650/8192 --- L(Train): 0.3615; L(Val): 0.4347; Reg Param: 0.0600; Time: 7.51\n",
      "Epoch 1651/8192 --- L(Train): 0.3736; L(Val): 0.4353; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1652/8192 --- L(Train): 0.3834; L(Val): 0.4358; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1653/8192 --- L(Train): 0.3862; L(Val): 0.4362; Reg Param: 0.0600; Time: 0.71\n",
      "Epoch 1654/8192 --- L(Train): 0.3756; L(Val): 0.4362; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1655/8192 --- L(Train): 0.3775; L(Val): 0.4358; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1656/8192 --- L(Train): 0.3777; L(Val): 0.4354; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1657/8192 --- L(Train): 0.3767; L(Val): 0.4352; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1658/8192 --- L(Train): 0.3844; L(Val): 0.4350; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1659/8192 --- L(Train): 0.3795; L(Val): 0.4351; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1660/8192 --- L(Train): 0.3686; L(Val): 0.4353; Reg Param: 0.0600; Time: 0.76\n",
      "Epoch 1661/8192 --- L(Train): 0.3727; L(Val): 0.4354; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1662/8192 --- L(Train): 0.3883; L(Val): 0.4355; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1663/8192 --- L(Train): 0.3758; L(Val): 0.4353; Reg Param: 0.0600; Time: 0.74\n",
      "Epoch 1664/8192 --- L(Train): 0.3718; L(Val): 0.4351; Reg Param: 0.0600; Time: 0.83\n",
      "Epoch 1665/8192 --- L(Train): 0.3758; L(Val): 0.4352; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1666/8192 --- L(Train): 0.3714; L(Val): 0.4350; Reg Param: 0.0600; Time: 0.71\n",
      "Epoch 1667/8192 --- L(Train): 0.3803; L(Val): 0.4350; Reg Param: 0.0600; Time: 0.71\n",
      "Epoch 1668/8192 --- L(Train): 0.3837; L(Val): 0.4351; Reg Param: 0.0600; Time: 0.71\n",
      "Epoch 1669/8192 --- L(Train): 0.3901; L(Val): 0.4353; Reg Param: 0.0600; Time: 0.71\n",
      "Epoch 1670/8192 --- L(Train): 0.3609; L(Val): 0.4355; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1671/8192 --- L(Train): 0.3632; L(Val): 0.4356; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1672/8192 --- L(Train): 0.3762; L(Val): 0.4357; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1673/8192 --- L(Train): 0.3620; L(Val): 0.4356; Reg Param: 0.0600; Time: 0.73\n",
      "Epoch 1674/8192 --- L(Train): 0.3763; L(Val): 0.4357; Reg Param: 0.0600; Time: 0.81\n",
      "Epoch 1675/8192 --- L(Train): 0.3858; L(Val): 0.4360; Reg Param: 0.0600; Time: 0.71\n",
      "Epoch 1676/8192 --- L(Train): 0.3655; L(Val): 0.4362; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1677/8192 --- L(Train): 0.3719; L(Val): 0.4362; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1678/8192 --- L(Train): 0.3753; L(Val): 0.4357; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1679/8192 --- L(Train): 0.3690; L(Val): 0.4356; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1680/8192 --- L(Train): 0.3755; L(Val): 0.4355; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1681/8192 --- L(Train): 0.3687; L(Val): 0.4352; Reg Param: 0.0600; Time: 0.75\n",
      "Epoch 1682/8192 --- L(Train): 0.3788; L(Val): 0.4351; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1683/8192 --- L(Train): 0.3683; L(Val): 0.4352; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1684/8192 --- L(Train): 0.3689; L(Val): 0.4354; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1685/8192 --- L(Train): 0.3836; L(Val): 0.4356; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1686/8192 --- L(Train): 0.3749; L(Val): 0.4357; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1687/8192 --- L(Train): 0.3799; L(Val): 0.4360; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1688/8192 --- L(Train): 0.3841; L(Val): 0.4362; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1689/8192 --- L(Train): 0.3806; L(Val): 0.4361; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1690/8192 --- L(Train): 0.3778; L(Val): 0.4360; Reg Param: 0.0600; Time: 0.69\n",
      "Epoch 1691/8192 --- L(Train): 0.3706; L(Val): 0.4359; Reg Param: 0.0600; Time: 0.72\n",
      "Epoch 1692/8192 --- L(Train): 0.3719; L(Val): 0.4356; Reg Param: 0.0600; Time: 0.84\n",
      "Epoch 1693/8192 --- L(Train): 0.3737; L(Val): 0.4352; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1694/8192 --- L(Train): 0.3567; L(Val): 0.4349; Reg Param: 0.0600; Time: 0.71\n",
      "Epoch 1695/8192 --- L(Train): 0.3808; L(Val): 0.4346; Reg Param: 0.0600; Time: 0.72\n",
      "Epoch 1696/8192 --- L(Train): 0.3693; L(Val): 0.4343; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1697/8192 --- L(Train): 0.3788; L(Val): 0.4342; Reg Param: 0.0600; Time: 0.70\n",
      "Epoch 1698/8192 --- L(Train): 0.3729; L(Val): 0.4340; Reg Param: 0.0600; Time: 0.71\n",
      "Epoch 1699/8192 --- L(Train): 0.3742; L(Val): 0.4337; Reg Param: 0.0600; Time: 0.71\n",
      "Epoch 1700/8192 --- L(Train): 0.3637; L(Val): 0.4337; Reg Param: 0.0613; Time: 7.31\n",
      "Epoch 1701/8192 --- L(Train): 0.3785; L(Val): 0.4336; Reg Param: 0.0613; Time: 0.68\n",
      "Epoch 1702/8192 --- L(Train): 0.3711; L(Val): 0.4336; Reg Param: 0.0613; Time: 0.68\n",
      "Epoch 1703/8192 --- L(Train): 0.3659; L(Val): 0.4338; Reg Param: 0.0613; Time: 0.71\n",
      "Epoch 1704/8192 --- L(Train): 0.3842; L(Val): 0.4341; Reg Param: 0.0613; Time: 0.69\n",
      "Epoch 1705/8192 --- L(Train): 0.3667; L(Val): 0.4345; Reg Param: 0.0613; Time: 0.68\n",
      "Epoch 1706/8192 --- L(Train): 0.3736; L(Val): 0.4351; Reg Param: 0.0613; Time: 0.69\n",
      "Epoch 1707/8192 --- L(Train): 0.3793; L(Val): 0.4354; Reg Param: 0.0613; Time: 0.68\n",
      "Epoch 1708/8192 --- L(Train): 0.3769; L(Val): 0.4354; Reg Param: 0.0613; Time: 0.68\n",
      "Epoch 1709/8192 --- L(Train): 0.3778; L(Val): 0.4352; Reg Param: 0.0613; Time: 0.68\n",
      "Epoch 1710/8192 --- L(Train): 0.3915; L(Val): 0.4351; Reg Param: 0.0613; Time: 0.69\n",
      "Epoch 1711/8192 --- L(Train): 0.3740; L(Val): 0.4351; Reg Param: 0.0613; Time: 0.68\n",
      "Epoch 1712/8192 --- L(Train): 0.3763; L(Val): 0.4351; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1713/8192 --- L(Train): 0.3843; L(Val): 0.4349; Reg Param: 0.0613; Time: 0.83\n",
      "Epoch 1714/8192 --- L(Train): 0.3835; L(Val): 0.4349; Reg Param: 0.0613; Time: 0.69\n",
      "Epoch 1715/8192 --- L(Train): 0.3760; L(Val): 0.4350; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1716/8192 --- L(Train): 0.3798; L(Val): 0.4351; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1717/8192 --- L(Train): 0.3866; L(Val): 0.4350; Reg Param: 0.0613; Time: 0.71\n",
      "Epoch 1718/8192 --- L(Train): 0.3837; L(Val): 0.4350; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1719/8192 --- L(Train): 0.3758; L(Val): 0.4348; Reg Param: 0.0613; Time: 0.71\n",
      "Epoch 1720/8192 --- L(Train): 0.3651; L(Val): 0.4348; Reg Param: 0.0613; Time: 0.71\n",
      "Epoch 1721/8192 --- L(Train): 0.3764; L(Val): 0.4348; Reg Param: 0.0613; Time: 0.71\n",
      "Epoch 1722/8192 --- L(Train): 0.3848; L(Val): 0.4348; Reg Param: 0.0613; Time: 0.71\n",
      "Epoch 1723/8192 --- L(Train): 0.3815; L(Val): 0.4350; Reg Param: 0.0613; Time: 0.79\n",
      "Epoch 1724/8192 --- L(Train): 0.3691; L(Val): 0.4352; Reg Param: 0.0613; Time: 0.68\n",
      "Epoch 1725/8192 --- L(Train): 0.3688; L(Val): 0.4355; Reg Param: 0.0613; Time: 0.68\n",
      "Epoch 1726/8192 --- L(Train): 0.3745; L(Val): 0.4358; Reg Param: 0.0613; Time: 0.69\n",
      "Epoch 1727/8192 --- L(Train): 0.3614; L(Val): 0.4360; Reg Param: 0.0613; Time: 0.69\n",
      "Epoch 1728/8192 --- L(Train): 0.3718; L(Val): 0.4361; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1729/8192 --- L(Train): 0.3785; L(Val): 0.4360; Reg Param: 0.0613; Time: 0.71\n",
      "Epoch 1730/8192 --- L(Train): 0.3824; L(Val): 0.4361; Reg Param: 0.0613; Time: 0.69\n",
      "Epoch 1731/8192 --- L(Train): 0.3730; L(Val): 0.4362; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1732/8192 --- L(Train): 0.3756; L(Val): 0.4363; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1733/8192 --- L(Train): 0.3709; L(Val): 0.4364; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1734/8192 --- L(Train): 0.3709; L(Val): 0.4365; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1735/8192 --- L(Train): 0.3882; L(Val): 0.4367; Reg Param: 0.0613; Time: 0.77\n",
      "Epoch 1736/8192 --- L(Train): 0.3794; L(Val): 0.4368; Reg Param: 0.0613; Time: 0.71\n",
      "Epoch 1737/8192 --- L(Train): 0.3757; L(Val): 0.4368; Reg Param: 0.0613; Time: 0.71\n",
      "Epoch 1738/8192 --- L(Train): 0.3756; L(Val): 0.4368; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1739/8192 --- L(Train): 0.3731; L(Val): 0.4367; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1740/8192 --- L(Train): 0.3786; L(Val): 0.4365; Reg Param: 0.0613; Time: 0.77\n",
      "Epoch 1741/8192 --- L(Train): 0.3599; L(Val): 0.4364; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1742/8192 --- L(Train): 0.3750; L(Val): 0.4364; Reg Param: 0.0613; Time: 0.75\n",
      "Epoch 1743/8192 --- L(Train): 0.3683; L(Val): 0.4364; Reg Param: 0.0613; Time: 0.70\n",
      "Epoch 1744/8192 --- L(Train): 0.3650; L(Val): 0.4364; Reg Param: 0.0613; Time: 0.69\n",
      "Epoch 1745/8192 --- L(Train): 0.3771; L(Val): 0.4365; Reg Param: 0.0613; Time: 0.68\n",
      "Epoch 1746/8192 --- L(Train): 0.3770; L(Val): 0.4365; Reg Param: 0.0613; Time: 0.69\n",
      "Epoch 1747/8192 --- L(Train): 0.3653; L(Val): 0.4365; Reg Param: 0.0613; Time: 0.69\n",
      "Epoch 1748/8192 --- L(Train): 0.3678; L(Val): 0.4366; Reg Param: 0.0613; Time: 0.69\n",
      "Epoch 1749/8192 --- L(Train): 0.3705; L(Val): 0.4367; Reg Param: 0.0613; Time: 0.69\n",
      "Epoch 1750/8192 --- L(Train): 0.3782; L(Val): 0.4367; Reg Param: 0.0625; Time: 7.48\n",
      "Epoch 1751/8192 --- L(Train): 0.3790; L(Val): 0.4368; Reg Param: 0.0625; Time: 0.70\n",
      "Epoch 1752/8192 --- L(Train): 0.3702; L(Val): 0.4368; Reg Param: 0.0625; Time: 0.70\n",
      "Epoch 1753/8192 --- L(Train): 0.3685; L(Val): 0.4366; Reg Param: 0.0625; Time: 0.71\n",
      "Epoch 1754/8192 --- L(Train): 0.3819; L(Val): 0.4365; Reg Param: 0.0625; Time: 0.72\n",
      "Epoch 1755/8192 --- L(Train): 0.3810; L(Val): 0.4363; Reg Param: 0.0625; Time: 0.70\n",
      "Epoch 1756/8192 --- L(Train): 0.3661; L(Val): 0.4362; Reg Param: 0.0625; Time: 0.69\n",
      "Epoch 1757/8192 --- L(Train): 0.3696; L(Val): 0.4362; Reg Param: 0.0625; Time: 0.71\n",
      "Epoch 1758/8192 --- L(Train): 0.3783; L(Val): 0.4361; Reg Param: 0.0625; Time: 0.70\n",
      "Epoch 1759/8192 --- L(Train): 0.3815; L(Val): 0.4359; Reg Param: 0.0625; Time: 0.70\n",
      "Epoch 1760/8192 --- L(Train): 0.3758; L(Val): 0.4360; Reg Param: 0.0625; Time: 0.70\n",
      "Epoch 1761/8192 --- L(Train): 0.3629; L(Val): 0.4362; Reg Param: 0.0625; Time: 0.74\n",
      "Epoch 1762/8192 --- L(Train): 0.3634; L(Val): 0.4362; Reg Param: 0.0625; Time: 0.72\n",
      "Epoch 1763/8192 --- L(Train): 0.3763; L(Val): 0.4362; Reg Param: 0.0625; Time: 0.71\n",
      "Epoch 1764/8192 --- L(Train): 0.3766; L(Val): 0.4361; Reg Param: 0.0625; Time: 0.69\n",
      "Epoch 1765/8192 --- L(Train): 0.3782; L(Val): 0.4361; Reg Param: 0.0625; Time: 0.68\n",
      "Epoch 1766/8192 --- L(Train): 0.3670; L(Val): 0.4361; Reg Param: 0.0625; Time: 0.70\n",
      "Epoch 1767/8192 --- L(Train): 0.3684; L(Val): 0.4361; Reg Param: 0.0625; Time: 0.71\n",
      "Epoch 1768/8192 --- L(Train): 0.3766; L(Val): 0.4361; Reg Param: 0.0625; Time: 0.70\n",
      "Epoch 1769/8192 --- L(Train): 0.3699; L(Val): 0.4361; Reg Param: 0.0625; Time: 0.77\n",
      "Epoch 1770/8192 --- L(Train): 0.3764; L(Val): 0.4360; Reg Param: 0.0625; Time: 0.69\n",
      "Epoch 1771/8192 --- L(Train): 0.3712; L(Val): 0.4360; Reg Param: 0.0625; Time: 0.69\n",
      "Epoch 1772/8192 --- L(Train): 0.3794; L(Val): 0.4359; Reg Param: 0.0625; Time: 0.70\n",
      "Epoch 1773/8192 --- L(Train): 0.3834; L(Val): 0.4357; Reg Param: 0.0625; Time: 0.69\n",
      "Epoch 1774/8192 --- L(Train): 0.3762; L(Val): 0.4354; Reg Param: 0.0625; Time: 0.69\n",
      "Epoch 1775/8192 --- L(Train): 0.3672; L(Val): 0.4353; Reg Param: 0.0625; Time: 0.69\n",
      "Epoch 1776/8192 --- L(Train): 0.3802; L(Val): 0.4354; Reg Param: 0.0625; Time: 0.69\n",
      "Epoch 1777/8192 --- L(Train): 0.3734; L(Val): 0.4354; Reg Param: 0.0625; Time: 0.70\n",
      "Epoch 1778/8192 --- L(Train): 0.3672; L(Val): 0.4355; Reg Param: 0.0625; Time: 0.71\n",
      "Epoch 1779/8192 --- L(Train): 0.3716; L(Val): 0.4356; Reg Param: 0.0625; Time: 0.72\n",
      "Epoch 1780/8192 --- L(Train): 0.3817; L(Val): 0.4357; Reg Param: 0.0625; Time: 0.75\n",
      "Epoch 1781/8192 --- L(Train): 0.3782; L(Val): 0.4357; Reg Param: 0.0625; Time: 0.87\n",
      "Epoch 1782/8192 --- L(Train): 0.3729; L(Val): 0.4359; Reg Param: 0.0625; Time: 0.79\n",
      "Epoch 1783/8192 --- L(Train): 0.3803; L(Val): 0.4361; Reg Param: 0.0625; Time: 0.78\n",
      "Epoch 1784/8192 --- L(Train): 0.3725; L(Val): 0.4363; Reg Param: 0.0625; Time: 0.77\n",
      "Epoch 1785/8192 --- L(Train): 0.3788; L(Val): 0.4366; Reg Param: 0.0625; Time: 0.78\n",
      "Epoch 1786/8192 --- L(Train): 0.3704; L(Val): 0.4367; Reg Param: 0.0625; Time: 0.81\n",
      "Epoch 1787/8192 --- L(Train): 0.3687; L(Val): 0.4369; Reg Param: 0.0625; Time: 0.82\n",
      "Epoch 1788/8192 --- L(Train): 0.3717; L(Val): 0.4370; Reg Param: 0.0625; Time: 0.79\n",
      "Epoch 1789/8192 --- L(Train): 0.3656; L(Val): 0.4372; Reg Param: 0.0625; Time: 0.85\n",
      "Epoch 1790/8192 --- L(Train): 0.3684; L(Val): 0.4372; Reg Param: 0.0625; Time: 0.88\n",
      "Epoch 1791/8192 --- L(Train): 0.3830; L(Val): 0.4372; Reg Param: 0.0625; Time: 0.79\n",
      "Epoch 1792/8192 --- L(Train): 0.3596; L(Val): 0.4372; Reg Param: 0.0625; Time: 0.81\n",
      "Epoch 1793/8192 --- L(Train): 0.3756; L(Val): 0.4371; Reg Param: 0.0625; Time: 0.84\n",
      "Epoch 1794/8192 --- L(Train): 0.3733; L(Val): 0.4369; Reg Param: 0.0625; Time: 0.80\n",
      "Epoch 1795/8192 --- L(Train): 0.3784; L(Val): 0.4367; Reg Param: 0.0625; Time: 0.89\n",
      "Epoch 1796/8192 --- L(Train): 0.3750; L(Val): 0.4366; Reg Param: 0.0625; Time: 0.70\n",
      "Epoch 1797/8192 --- L(Train): 0.3826; L(Val): 0.4366; Reg Param: 0.0625; Time: 0.71\n",
      "Epoch 1798/8192 --- L(Train): 0.3661; L(Val): 0.4365; Reg Param: 0.0625; Time: 0.70\n",
      "Epoch 1799/8192 --- L(Train): 0.3651; L(Val): 0.4363; Reg Param: 0.0625; Time: 0.71\n",
      "Epoch 1800/8192 --- L(Train): 0.3739; L(Val): 0.4363; Reg Param: 0.0636; Time: 7.60\n",
      "Epoch 1801/8192 --- L(Train): 0.3712; L(Val): 0.4361; Reg Param: 0.0636; Time: 0.74\n",
      "Epoch 1802/8192 --- L(Train): 0.3677; L(Val): 0.4360; Reg Param: 0.0636; Time: 0.73\n",
      "Epoch 1803/8192 --- L(Train): 0.3775; L(Val): 0.4359; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1804/8192 --- L(Train): 0.3866; L(Val): 0.4357; Reg Param: 0.0636; Time: 0.79\n",
      "Epoch 1805/8192 --- L(Train): 0.3709; L(Val): 0.4357; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1806/8192 --- L(Train): 0.3741; L(Val): 0.4358; Reg Param: 0.0636; Time: 0.72\n",
      "Epoch 1807/8192 --- L(Train): 0.3698; L(Val): 0.4359; Reg Param: 0.0636; Time: 0.72\n",
      "Epoch 1808/8192 --- L(Train): 0.3682; L(Val): 0.4360; Reg Param: 0.0636; Time: 0.77\n",
      "Epoch 1809/8192 --- L(Train): 0.3730; L(Val): 0.4363; Reg Param: 0.0636; Time: 0.82\n",
      "Epoch 1810/8192 --- L(Train): 0.3734; L(Val): 0.4365; Reg Param: 0.0636; Time: 0.72\n",
      "Epoch 1811/8192 --- L(Train): 0.3772; L(Val): 0.4366; Reg Param: 0.0636; Time: 0.73\n",
      "Epoch 1812/8192 --- L(Train): 0.3857; L(Val): 0.4366; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1813/8192 --- L(Train): 0.3814; L(Val): 0.4366; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1814/8192 --- L(Train): 0.3774; L(Val): 0.4367; Reg Param: 0.0636; Time: 0.70\n",
      "Epoch 1815/8192 --- L(Train): 0.3813; L(Val): 0.4366; Reg Param: 0.0636; Time: 0.69\n",
      "Epoch 1816/8192 --- L(Train): 0.3714; L(Val): 0.4366; Reg Param: 0.0636; Time: 0.70\n",
      "Epoch 1817/8192 --- L(Train): 0.3668; L(Val): 0.4366; Reg Param: 0.0636; Time: 0.70\n",
      "Epoch 1818/8192 --- L(Train): 0.3595; L(Val): 0.4367; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1819/8192 --- L(Train): 0.3769; L(Val): 0.4367; Reg Param: 0.0636; Time: 0.70\n",
      "Epoch 1820/8192 --- L(Train): 0.3701; L(Val): 0.4368; Reg Param: 0.0636; Time: 0.70\n",
      "Epoch 1821/8192 --- L(Train): 0.3706; L(Val): 0.4369; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1822/8192 --- L(Train): 0.3696; L(Val): 0.4370; Reg Param: 0.0636; Time: 0.75\n",
      "Epoch 1823/8192 --- L(Train): 0.3660; L(Val): 0.4371; Reg Param: 0.0636; Time: 0.73\n",
      "Epoch 1824/8192 --- L(Train): 0.3808; L(Val): 0.4372; Reg Param: 0.0636; Time: 0.76\n",
      "Epoch 1825/8192 --- L(Train): 0.3632; L(Val): 0.4372; Reg Param: 0.0636; Time: 0.74\n",
      "Epoch 1826/8192 --- L(Train): 0.3701; L(Val): 0.4372; Reg Param: 0.0636; Time: 0.75\n",
      "Epoch 1827/8192 --- L(Train): 0.3675; L(Val): 0.4371; Reg Param: 0.0636; Time: 0.72\n",
      "Epoch 1828/8192 --- L(Train): 0.3665; L(Val): 0.4370; Reg Param: 0.0636; Time: 0.72\n",
      "Epoch 1829/8192 --- L(Train): 0.3845; L(Val): 0.4369; Reg Param: 0.0636; Time: 0.72\n",
      "Epoch 1830/8192 --- L(Train): 0.3694; L(Val): 0.4368; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1831/8192 --- L(Train): 0.3791; L(Val): 0.4368; Reg Param: 0.0636; Time: 0.72\n",
      "Epoch 1832/8192 --- L(Train): 0.3800; L(Val): 0.4368; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1833/8192 --- L(Train): 0.3678; L(Val): 0.4369; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1834/8192 --- L(Train): 0.3712; L(Val): 0.4370; Reg Param: 0.0636; Time: 0.74\n",
      "Epoch 1835/8192 --- L(Train): 0.3723; L(Val): 0.4370; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1836/8192 --- L(Train): 0.3886; L(Val): 0.4371; Reg Param: 0.0636; Time: 0.79\n",
      "Epoch 1837/8192 --- L(Train): 0.3836; L(Val): 0.4372; Reg Param: 0.0636; Time: 0.76\n",
      "Epoch 1838/8192 --- L(Train): 0.3719; L(Val): 0.4373; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1839/8192 --- L(Train): 0.3671; L(Val): 0.4374; Reg Param: 0.0636; Time: 0.70\n",
      "Epoch 1840/8192 --- L(Train): 0.3738; L(Val): 0.4374; Reg Param: 0.0636; Time: 0.70\n",
      "Epoch 1841/8192 --- L(Train): 0.3783; L(Val): 0.4374; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1842/8192 --- L(Train): 0.3701; L(Val): 0.4374; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1843/8192 --- L(Train): 0.3686; L(Val): 0.4374; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1844/8192 --- L(Train): 0.3743; L(Val): 0.4374; Reg Param: 0.0636; Time: 0.70\n",
      "Epoch 1845/8192 --- L(Train): 0.3711; L(Val): 0.4373; Reg Param: 0.0636; Time: 0.70\n",
      "Epoch 1846/8192 --- L(Train): 0.3703; L(Val): 0.4372; Reg Param: 0.0636; Time: 0.69\n",
      "Epoch 1847/8192 --- L(Train): 0.3596; L(Val): 0.4371; Reg Param: 0.0636; Time: 0.72\n",
      "Epoch 1848/8192 --- L(Train): 0.3683; L(Val): 0.4370; Reg Param: 0.0636; Time: 0.72\n",
      "Epoch 1849/8192 --- L(Train): 0.3727; L(Val): 0.4370; Reg Param: 0.0636; Time: 0.71\n",
      "Epoch 1850/8192 --- L(Train): 0.3685; L(Val): 0.4370; Reg Param: 0.0647; Time: 7.59\n",
      "Epoch 1851/8192 --- L(Train): 0.3870; L(Val): 0.4368; Reg Param: 0.0647; Time: 0.75\n",
      "Epoch 1852/8192 --- L(Train): 0.3799; L(Val): 0.4367; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 1853/8192 --- L(Train): 0.3712; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.72\n",
      "Epoch 1854/8192 --- L(Train): 0.3820; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1855/8192 --- L(Train): 0.3806; L(Val): 0.4365; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1856/8192 --- L(Train): 0.3766; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.79\n",
      "Epoch 1857/8192 --- L(Train): 0.3742; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.74\n",
      "Epoch 1858/8192 --- L(Train): 0.3697; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.70\n",
      "Epoch 1859/8192 --- L(Train): 0.3800; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.72\n",
      "Epoch 1860/8192 --- L(Train): 0.3693; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1861/8192 --- L(Train): 0.3772; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1862/8192 --- L(Train): 0.3770; L(Val): 0.4367; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1863/8192 --- L(Train): 0.3901; L(Val): 0.4367; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1864/8192 --- L(Train): 0.3696; L(Val): 0.4367; Reg Param: 0.0647; Time: 0.70\n",
      "Epoch 1865/8192 --- L(Train): 0.3894; L(Val): 0.4367; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1866/8192 --- L(Train): 0.3772; L(Val): 0.4367; Reg Param: 0.0647; Time: 0.76\n",
      "Epoch 1867/8192 --- L(Train): 0.3707; L(Val): 0.4368; Reg Param: 0.0647; Time: 0.72\n",
      "Epoch 1868/8192 --- L(Train): 0.3779; L(Val): 0.4368; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1869/8192 --- L(Train): 0.3707; L(Val): 0.4368; Reg Param: 0.0647; Time: 0.72\n",
      "Epoch 1870/8192 --- L(Train): 0.3818; L(Val): 0.4368; Reg Param: 0.0647; Time: 0.69\n",
      "Epoch 1871/8192 --- L(Train): 0.3669; L(Val): 0.4369; Reg Param: 0.0647; Time: 0.85\n",
      "Epoch 1872/8192 --- L(Train): 0.3713; L(Val): 0.4368; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1873/8192 --- L(Train): 0.3795; L(Val): 0.4368; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 1874/8192 --- L(Train): 0.3770; L(Val): 0.4367; Reg Param: 0.0647; Time: 0.69\n",
      "Epoch 1875/8192 --- L(Train): 0.3838; L(Val): 0.4367; Reg Param: 0.0647; Time: 0.69\n",
      "Epoch 1876/8192 --- L(Train): 0.3762; L(Val): 0.4368; Reg Param: 0.0647; Time: 0.69\n",
      "Epoch 1877/8192 --- L(Train): 0.3760; L(Val): 0.4368; Reg Param: 0.0647; Time: 0.69\n",
      "Epoch 1878/8192 --- L(Train): 0.3762; L(Val): 0.4367; Reg Param: 0.0647; Time: 0.80\n",
      "Epoch 1879/8192 --- L(Train): 0.3682; L(Val): 0.4367; Reg Param: 0.0647; Time: 0.72\n",
      "Epoch 1880/8192 --- L(Train): 0.3703; L(Val): 0.4367; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1881/8192 --- L(Train): 0.3762; L(Val): 0.4367; Reg Param: 0.0647; Time: 0.72\n",
      "Epoch 1882/8192 --- L(Train): 0.3698; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.70\n",
      "Epoch 1883/8192 --- L(Train): 0.3834; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.70\n",
      "Epoch 1884/8192 --- L(Train): 0.3761; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.70\n",
      "Epoch 1885/8192 --- L(Train): 0.3816; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1886/8192 --- L(Train): 0.3709; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.80\n",
      "Epoch 1887/8192 --- L(Train): 0.3746; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1888/8192 --- L(Train): 0.3699; L(Val): 0.4365; Reg Param: 0.0647; Time: 0.70\n",
      "Epoch 1889/8192 --- L(Train): 0.3686; L(Val): 0.4365; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1890/8192 --- L(Train): 0.3692; L(Val): 0.4365; Reg Param: 0.0647; Time: 0.78\n",
      "Epoch 1891/8192 --- L(Train): 0.3818; L(Val): 0.4364; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1892/8192 --- L(Train): 0.3685; L(Val): 0.4365; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1893/8192 --- L(Train): 0.3816; L(Val): 0.4365; Reg Param: 0.0647; Time: 0.70\n",
      "Epoch 1894/8192 --- L(Train): 0.3847; L(Val): 0.4365; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1895/8192 --- L(Train): 0.3720; L(Val): 0.4365; Reg Param: 0.0647; Time: 0.71\n",
      "Epoch 1896/8192 --- L(Train): 0.3703; L(Val): 0.4365; Reg Param: 0.0647; Time: 0.70\n",
      "Epoch 1897/8192 --- L(Train): 0.3749; L(Val): 0.4365; Reg Param: 0.0647; Time: 0.69\n",
      "Epoch 1898/8192 --- L(Train): 0.3679; L(Val): 0.4365; Reg Param: 0.0647; Time: 0.70\n",
      "Epoch 1899/8192 --- L(Train): 0.3726; L(Val): 0.4366; Reg Param: 0.0647; Time: 0.69\n",
      "Epoch 1900/8192 --- L(Train): 0.3713; L(Val): 0.4366; Reg Param: 0.0657; Time: 7.43\n",
      "Epoch 1901/8192 --- L(Train): 0.3641; L(Val): 0.4367; Reg Param: 0.0657; Time: 0.75\n",
      "Epoch 1902/8192 --- L(Train): 0.3756; L(Val): 0.4367; Reg Param: 0.0657; Time: 0.75\n",
      "Epoch 1903/8192 --- L(Train): 0.3813; L(Val): 0.4367; Reg Param: 0.0657; Time: 0.74\n",
      "Epoch 1904/8192 --- L(Train): 0.3734; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.81\n",
      "Epoch 1905/8192 --- L(Train): 0.3754; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.79\n",
      "Epoch 1906/8192 --- L(Train): 0.3753; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.83\n",
      "Epoch 1907/8192 --- L(Train): 0.3703; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.74\n",
      "Epoch 1908/8192 --- L(Train): 0.3731; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.74\n",
      "Epoch 1909/8192 --- L(Train): 0.3760; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.75\n",
      "Epoch 1910/8192 --- L(Train): 0.3532; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1911/8192 --- L(Train): 0.3720; L(Val): 0.4370; Reg Param: 0.0657; Time: 0.73\n",
      "Epoch 1912/8192 --- L(Train): 0.3701; L(Val): 0.4370; Reg Param: 0.0657; Time: 0.74\n",
      "Epoch 1913/8192 --- L(Train): 0.3672; L(Val): 0.4370; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1914/8192 --- L(Train): 0.3593; L(Val): 0.4370; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1915/8192 --- L(Train): 0.3648; L(Val): 0.4370; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1916/8192 --- L(Train): 0.3730; L(Val): 0.4370; Reg Param: 0.0657; Time: 0.70\n",
      "Epoch 1917/8192 --- L(Train): 0.3842; L(Val): 0.4370; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1918/8192 --- L(Train): 0.3744; L(Val): 0.4370; Reg Param: 0.0657; Time: 0.70\n",
      "Epoch 1919/8192 --- L(Train): 0.3585; L(Val): 0.4370; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1920/8192 --- L(Train): 0.3717; L(Val): 0.4370; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1921/8192 --- L(Train): 0.3663; L(Val): 0.4370; Reg Param: 0.0657; Time: 0.70\n",
      "Epoch 1922/8192 --- L(Train): 0.3775; L(Val): 0.4370; Reg Param: 0.0657; Time: 0.70\n",
      "Epoch 1923/8192 --- L(Train): 0.3717; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1924/8192 --- L(Train): 0.3792; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.70\n",
      "Epoch 1925/8192 --- L(Train): 0.3764; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1926/8192 --- L(Train): 0.3745; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.73\n",
      "Epoch 1927/8192 --- L(Train): 0.3608; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1928/8192 --- L(Train): 0.3663; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1929/8192 --- L(Train): 0.3729; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1930/8192 --- L(Train): 0.3695; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1931/8192 --- L(Train): 0.3805; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1932/8192 --- L(Train): 0.3791; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1933/8192 --- L(Train): 0.3870; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.85\n",
      "Epoch 1934/8192 --- L(Train): 0.3616; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.69\n",
      "Epoch 1935/8192 --- L(Train): 0.3598; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.70\n",
      "Epoch 1936/8192 --- L(Train): 0.3772; L(Val): 0.4369; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1937/8192 --- L(Train): 0.3694; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1938/8192 --- L(Train): 0.3743; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1939/8192 --- L(Train): 0.3739; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1940/8192 --- L(Train): 0.3677; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.74\n",
      "Epoch 1941/8192 --- L(Train): 0.3820; L(Val): 0.4367; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1942/8192 --- L(Train): 0.3663; L(Val): 0.4367; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1943/8192 --- L(Train): 0.3707; L(Val): 0.4367; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1944/8192 --- L(Train): 0.3688; L(Val): 0.4367; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1945/8192 --- L(Train): 0.3780; L(Val): 0.4367; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1946/8192 --- L(Train): 0.3712; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1947/8192 --- L(Train): 0.3729; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 1948/8192 --- L(Train): 0.3729; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1949/8192 --- L(Train): 0.3720; L(Val): 0.4368; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 1950/8192 --- L(Train): 0.3650; L(Val): 0.4368; Reg Param: 0.0666; Time: 7.52\n",
      "Epoch 1951/8192 --- L(Train): 0.3745; L(Val): 0.4368; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1952/8192 --- L(Train): 0.3777; L(Val): 0.4369; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1953/8192 --- L(Train): 0.3900; L(Val): 0.4369; Reg Param: 0.0666; Time: 0.82\n",
      "Epoch 1954/8192 --- L(Train): 0.3727; L(Val): 0.4369; Reg Param: 0.0666; Time: 0.68\n",
      "Epoch 1955/8192 --- L(Train): 0.3631; L(Val): 0.4369; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1956/8192 --- L(Train): 0.3706; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1957/8192 --- L(Train): 0.3708; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.71\n",
      "Epoch 1958/8192 --- L(Train): 0.3780; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.71\n",
      "Epoch 1959/8192 --- L(Train): 0.3785; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1960/8192 --- L(Train): 0.3671; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1961/8192 --- L(Train): 0.3569; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.75\n",
      "Epoch 1962/8192 --- L(Train): 0.3686; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.71\n",
      "Epoch 1963/8192 --- L(Train): 0.3747; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.71\n",
      "Epoch 1964/8192 --- L(Train): 0.3723; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1965/8192 --- L(Train): 0.3688; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1966/8192 --- L(Train): 0.3644; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.74\n",
      "Epoch 1967/8192 --- L(Train): 0.3731; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1968/8192 --- L(Train): 0.3783; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1969/8192 --- L(Train): 0.3658; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1970/8192 --- L(Train): 0.3668; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1971/8192 --- L(Train): 0.3661; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1972/8192 --- L(Train): 0.3653; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1973/8192 --- L(Train): 0.3836; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.71\n",
      "Epoch 1974/8192 --- L(Train): 0.3691; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1975/8192 --- L(Train): 0.3722; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1976/8192 --- L(Train): 0.3683; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.71\n",
      "Epoch 1977/8192 --- L(Train): 0.3798; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1978/8192 --- L(Train): 0.3690; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1979/8192 --- L(Train): 0.3783; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1980/8192 --- L(Train): 0.3673; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1981/8192 --- L(Train): 0.3708; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.71\n",
      "Epoch 1982/8192 --- L(Train): 0.3743; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.86\n",
      "Epoch 1983/8192 --- L(Train): 0.3628; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1984/8192 --- L(Train): 0.3782; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1985/8192 --- L(Train): 0.3805; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1986/8192 --- L(Train): 0.3747; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.71\n",
      "Epoch 1987/8192 --- L(Train): 0.3687; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1988/8192 --- L(Train): 0.3702; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.74\n",
      "Epoch 1989/8192 --- L(Train): 0.3701; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1990/8192 --- L(Train): 0.3803; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1991/8192 --- L(Train): 0.3757; L(Val): 0.4370; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1992/8192 --- L(Train): 0.3716; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.71\n",
      "Epoch 1993/8192 --- L(Train): 0.3842; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1994/8192 --- L(Train): 0.3798; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1995/8192 --- L(Train): 0.3746; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1996/8192 --- L(Train): 0.3757; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 1997/8192 --- L(Train): 0.3727; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1998/8192 --- L(Train): 0.3795; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 1999/8192 --- L(Train): 0.3725; L(Val): 0.4371; Reg Param: 0.0666; Time: 0.69\n",
      "Epoch 2000/8192 --- L(Train): 0.3689; L(Val): 0.4371; Reg Param: 0.0674; Time: 7.47\n",
      "Epoch 2001/8192 --- L(Train): 0.3685; L(Val): 0.4371; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2002/8192 --- L(Train): 0.3813; L(Val): 0.4371; Reg Param: 0.0674; Time: 0.88\n",
      "Epoch 2003/8192 --- L(Train): 0.3807; L(Val): 0.4371; Reg Param: 0.0674; Time: 0.77\n",
      "Epoch 2004/8192 --- L(Train): 0.3677; L(Val): 0.4371; Reg Param: 0.0674; Time: 0.71\n",
      "Epoch 2005/8192 --- L(Train): 0.3751; L(Val): 0.4371; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2006/8192 --- L(Train): 0.3681; L(Val): 0.4371; Reg Param: 0.0674; Time: 0.72\n",
      "Epoch 2007/8192 --- L(Train): 0.3684; L(Val): 0.4371; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2008/8192 --- L(Train): 0.3728; L(Val): 0.4371; Reg Param: 0.0674; Time: 0.71\n",
      "Epoch 2009/8192 --- L(Train): 0.3734; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2010/8192 --- L(Train): 0.3763; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.82\n",
      "Epoch 2011/8192 --- L(Train): 0.3723; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2012/8192 --- L(Train): 0.3684; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2013/8192 --- L(Train): 0.3692; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2014/8192 --- L(Train): 0.3749; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2015/8192 --- L(Train): 0.3752; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.69\n",
      "Epoch 2016/8192 --- L(Train): 0.3737; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2017/8192 --- L(Train): 0.3752; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.71\n",
      "Epoch 2018/8192 --- L(Train): 0.3862; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.72\n",
      "Epoch 2019/8192 --- L(Train): 0.3795; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.76\n",
      "Epoch 2020/8192 --- L(Train): 0.3690; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.69\n",
      "Epoch 2021/8192 --- L(Train): 0.3717; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2022/8192 --- L(Train): 0.3794; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.69\n",
      "Epoch 2023/8192 --- L(Train): 0.3701; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.69\n",
      "Epoch 2024/8192 --- L(Train): 0.3738; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.75\n",
      "Epoch 2025/8192 --- L(Train): 0.3743; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.69\n",
      "Epoch 2026/8192 --- L(Train): 0.3747; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2027/8192 --- L(Train): 0.3779; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2028/8192 --- L(Train): 0.3729; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.71\n",
      "Epoch 2029/8192 --- L(Train): 0.3669; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.71\n",
      "Epoch 2030/8192 --- L(Train): 0.3668; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2031/8192 --- L(Train): 0.3700; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.83\n",
      "Epoch 2032/8192 --- L(Train): 0.3683; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.73\n",
      "Epoch 2033/8192 --- L(Train): 0.3567; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.69\n",
      "Epoch 2034/8192 --- L(Train): 0.3716; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.71\n",
      "Epoch 2035/8192 --- L(Train): 0.3673; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2036/8192 --- L(Train): 0.3701; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2037/8192 --- L(Train): 0.3752; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2038/8192 --- L(Train): 0.3705; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2039/8192 --- L(Train): 0.3774; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.71\n",
      "Epoch 2040/8192 --- L(Train): 0.3731; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.71\n",
      "Epoch 2041/8192 --- L(Train): 0.3738; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.71\n",
      "Epoch 2042/8192 --- L(Train): 0.3850; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2043/8192 --- L(Train): 0.3650; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2044/8192 --- L(Train): 0.3789; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2045/8192 --- L(Train): 0.3851; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2046/8192 --- L(Train): 0.3829; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2047/8192 --- L(Train): 0.3704; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2048/8192 --- L(Train): 0.3834; L(Val): 0.4372; Reg Param: 0.0674; Time: 0.71\n",
      "Epoch 2049/8192 --- L(Train): 0.3739; L(Val): 0.4371; Reg Param: 0.0674; Time: 0.70\n",
      "Epoch 2050/8192 --- L(Train): 0.3785; L(Val): 0.4371; Reg Param: 0.0681; Time: 7.38\n",
      "Epoch 2051/8192 --- L(Train): 0.3766; L(Val): 0.4375; Reg Param: 0.0681; Time: 0.79\n",
      "Epoch 2052/8192 --- L(Train): 0.3748; L(Val): 0.4377; Reg Param: 0.0681; Time: 0.72\n",
      "Epoch 2053/8192 --- L(Train): 0.3912; L(Val): 0.4377; Reg Param: 0.0681; Time: 0.69\n",
      "Epoch 2054/8192 --- L(Train): 0.3747; L(Val): 0.4373; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2055/8192 --- L(Train): 0.3687; L(Val): 0.4369; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2056/8192 --- L(Train): 0.3796; L(Val): 0.4357; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2057/8192 --- L(Train): 0.3881; L(Val): 0.4346; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2058/8192 --- L(Train): 0.3787; L(Val): 0.4341; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2059/8192 --- L(Train): 0.3792; L(Val): 0.4350; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2060/8192 --- L(Train): 0.3805; L(Val): 0.4365; Reg Param: 0.0681; Time: 0.78\n",
      "Epoch 2061/8192 --- L(Train): 0.3717; L(Val): 0.4362; Reg Param: 0.0681; Time: 0.72\n",
      "Epoch 2062/8192 --- L(Train): 0.3693; L(Val): 0.4342; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2063/8192 --- L(Train): 0.3799; L(Val): 0.4342; Reg Param: 0.0681; Time: 0.69\n",
      "Epoch 2064/8192 --- L(Train): 0.3785; L(Val): 0.4351; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2065/8192 --- L(Train): 0.3673; L(Val): 0.4363; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2066/8192 --- L(Train): 0.3826; L(Val): 0.4365; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2067/8192 --- L(Train): 0.3880; L(Val): 0.4359; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2068/8192 --- L(Train): 0.3887; L(Val): 0.4363; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2069/8192 --- L(Train): 0.3797; L(Val): 0.4366; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2070/8192 --- L(Train): 0.3783; L(Val): 0.4359; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2071/8192 --- L(Train): 0.3781; L(Val): 0.4361; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2072/8192 --- L(Train): 0.3922; L(Val): 0.4378; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2073/8192 --- L(Train): 0.3699; L(Val): 0.4359; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2074/8192 --- L(Train): 0.3768; L(Val): 0.4366; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2075/8192 --- L(Train): 0.3721; L(Val): 0.4359; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2076/8192 --- L(Train): 0.3767; L(Val): 0.4353; Reg Param: 0.0681; Time: 0.69\n",
      "Epoch 2077/8192 --- L(Train): 0.3949; L(Val): 0.4352; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2078/8192 --- L(Train): 0.3793; L(Val): 0.4355; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2079/8192 --- L(Train): 0.3806; L(Val): 0.4349; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2080/8192 --- L(Train): 0.3800; L(Val): 0.4352; Reg Param: 0.0681; Time: 0.75\n",
      "Epoch 2081/8192 --- L(Train): 0.3883; L(Val): 0.4347; Reg Param: 0.0681; Time: 0.85\n",
      "Epoch 2082/8192 --- L(Train): 0.3762; L(Val): 0.4337; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2083/8192 --- L(Train): 0.3805; L(Val): 0.4325; Reg Param: 0.0681; Time: 0.69\n",
      "Epoch 2084/8192 --- L(Train): 0.3890; L(Val): 0.4319; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2085/8192 --- L(Train): 0.3952; L(Val): 0.4320; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2086/8192 --- L(Train): 0.3932; L(Val): 0.4340; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2087/8192 --- L(Train): 0.3922; L(Val): 0.4354; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2088/8192 --- L(Train): 0.3779; L(Val): 0.4351; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2089/8192 --- L(Train): 0.3916; L(Val): 0.4363; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2090/8192 --- L(Train): 0.3791; L(Val): 0.4363; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2091/8192 --- L(Train): 0.3745; L(Val): 0.4369; Reg Param: 0.0681; Time: 0.69\n",
      "Epoch 2092/8192 --- L(Train): 0.3840; L(Val): 0.4355; Reg Param: 0.0681; Time: 0.82\n",
      "Epoch 2093/8192 --- L(Train): 0.3890; L(Val): 0.4333; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2094/8192 --- L(Train): 0.3947; L(Val): 0.4320; Reg Param: 0.0681; Time: 0.78\n",
      "Epoch 2095/8192 --- L(Train): 0.3874; L(Val): 0.4316; Reg Param: 0.0681; Time: 0.69\n",
      "Epoch 2096/8192 --- L(Train): 0.3887; L(Val): 0.4341; Reg Param: 0.0681; Time: 0.70\n",
      "Epoch 2097/8192 --- L(Train): 0.3814; L(Val): 0.4329; Reg Param: 0.0681; Time: 0.69\n",
      "Epoch 2098/8192 --- L(Train): 0.3817; L(Val): 0.4323; Reg Param: 0.0681; Time: 0.71\n",
      "Epoch 2099/8192 --- L(Train): 0.3765; L(Val): 0.4328; Reg Param: 0.0681; Time: 0.69\n",
      "Epoch 2100/8192 --- L(Train): 0.3905; L(Val): 0.4328; Reg Param: 0.0687; Time: 7.52\n",
      "Epoch 2101/8192 --- L(Train): 0.3722; L(Val): 0.4361; Reg Param: 0.0687; Time: 0.81\n",
      "Epoch 2102/8192 --- L(Train): 0.3963; L(Val): 0.4357; Reg Param: 0.0687; Time: 0.76\n",
      "Epoch 2103/8192 --- L(Train): 0.3801; L(Val): 0.4341; Reg Param: 0.0687; Time: 0.71\n",
      "Epoch 2104/8192 --- L(Train): 0.3877; L(Val): 0.4339; Reg Param: 0.0687; Time: 0.71\n",
      "Epoch 2105/8192 --- L(Train): 0.3869; L(Val): 0.4342; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2106/8192 --- L(Train): 0.3874; L(Val): 0.4356; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2107/8192 --- L(Train): 0.3817; L(Val): 0.4366; Reg Param: 0.0687; Time: 0.77\n",
      "Epoch 2108/8192 --- L(Train): 0.3756; L(Val): 0.4361; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2109/8192 --- L(Train): 0.3738; L(Val): 0.4365; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2110/8192 --- L(Train): 0.3766; L(Val): 0.4368; Reg Param: 0.0687; Time: 0.70\n",
      "Epoch 2111/8192 --- L(Train): 0.3893; L(Val): 0.4373; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2112/8192 --- L(Train): 0.3883; L(Val): 0.4365; Reg Param: 0.0687; Time: 0.70\n",
      "Epoch 2113/8192 --- L(Train): 0.3818; L(Val): 0.4351; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2114/8192 --- L(Train): 0.3798; L(Val): 0.4349; Reg Param: 0.0687; Time: 0.71\n",
      "Epoch 2115/8192 --- L(Train): 0.3831; L(Val): 0.4344; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2116/8192 --- L(Train): 0.3921; L(Val): 0.4341; Reg Param: 0.0687; Time: 0.70\n",
      "Epoch 2117/8192 --- L(Train): 0.3732; L(Val): 0.4339; Reg Param: 0.0687; Time: 0.70\n",
      "Epoch 2118/8192 --- L(Train): 0.3757; L(Val): 0.4339; Reg Param: 0.0687; Time: 0.70\n",
      "Epoch 2119/8192 --- L(Train): 0.3878; L(Val): 0.4344; Reg Param: 0.0687; Time: 0.70\n",
      "Epoch 2120/8192 --- L(Train): 0.3718; L(Val): 0.4354; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2121/8192 --- L(Train): 0.3724; L(Val): 0.4357; Reg Param: 0.0687; Time: 0.70\n",
      "Epoch 2122/8192 --- L(Train): 0.3787; L(Val): 0.4343; Reg Param: 0.0687; Time: 0.71\n",
      "Epoch 2123/8192 --- L(Train): 0.3663; L(Val): 0.4341; Reg Param: 0.0687; Time: 0.72\n",
      "Epoch 2124/8192 --- L(Train): 0.3757; L(Val): 0.4347; Reg Param: 0.0687; Time: 0.71\n",
      "Epoch 2125/8192 --- L(Train): 0.3841; L(Val): 0.4349; Reg Param: 0.0687; Time: 0.72\n",
      "Epoch 2126/8192 --- L(Train): 0.3816; L(Val): 0.4336; Reg Param: 0.0687; Time: 0.70\n",
      "Epoch 2127/8192 --- L(Train): 0.3859; L(Val): 0.4326; Reg Param: 0.0687; Time: 0.72\n",
      "Epoch 2128/8192 --- L(Train): 0.3807; L(Val): 0.4321; Reg Param: 0.0687; Time: 0.70\n",
      "Epoch 2129/8192 --- L(Train): 0.3805; L(Val): 0.4325; Reg Param: 0.0687; Time: 0.73\n",
      "Epoch 2130/8192 --- L(Train): 0.3917; L(Val): 0.4340; Reg Param: 0.0687; Time: 0.72\n",
      "Epoch 2131/8192 --- L(Train): 0.3672; L(Val): 0.4330; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2132/8192 --- L(Train): 0.3752; L(Val): 0.4334; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2133/8192 --- L(Train): 0.3958; L(Val): 0.4336; Reg Param: 0.0687; Time: 0.70\n",
      "Epoch 2134/8192 --- L(Train): 0.3828; L(Val): 0.4347; Reg Param: 0.0687; Time: 0.68\n",
      "Epoch 2135/8192 --- L(Train): 0.3893; L(Val): 0.4344; Reg Param: 0.0687; Time: 0.78\n",
      "Epoch 2136/8192 --- L(Train): 0.3819; L(Val): 0.4346; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2137/8192 --- L(Train): 0.3877; L(Val): 0.4355; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2138/8192 --- L(Train): 0.3791; L(Val): 0.4360; Reg Param: 0.0687; Time: 0.70\n",
      "Epoch 2139/8192 --- L(Train): 0.3760; L(Val): 0.4361; Reg Param: 0.0687; Time: 0.71\n",
      "Epoch 2140/8192 --- L(Train): 0.3766; L(Val): 0.4366; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2141/8192 --- L(Train): 0.3784; L(Val): 0.4361; Reg Param: 0.0687; Time: 0.71\n",
      "Epoch 2142/8192 --- L(Train): 0.3780; L(Val): 0.4368; Reg Param: 0.0687; Time: 0.71\n",
      "Epoch 2143/8192 --- L(Train): 0.3832; L(Val): 0.4380; Reg Param: 0.0687; Time: 0.71\n",
      "Epoch 2144/8192 --- L(Train): 0.3847; L(Val): 0.4386; Reg Param: 0.0687; Time: 0.71\n",
      "Epoch 2145/8192 --- L(Train): 0.3776; L(Val): 0.4377; Reg Param: 0.0687; Time: 0.70\n",
      "Epoch 2146/8192 --- L(Train): 0.3733; L(Val): 0.4370; Reg Param: 0.0687; Time: 0.73\n",
      "Epoch 2147/8192 --- L(Train): 0.3876; L(Val): 0.4371; Reg Param: 0.0687; Time: 0.73\n",
      "Epoch 2148/8192 --- L(Train): 0.3845; L(Val): 0.4371; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2149/8192 --- L(Train): 0.3813; L(Val): 0.4374; Reg Param: 0.0687; Time: 0.69\n",
      "Epoch 2150/8192 --- L(Train): 0.3835; L(Val): 0.4374; Reg Param: 0.0692; Time: 7.55\n",
      "Epoch 2151/8192 --- L(Train): 0.3749; L(Val): 0.4383; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2152/8192 --- L(Train): 0.3704; L(Val): 0.4387; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2153/8192 --- L(Train): 0.3689; L(Val): 0.4378; Reg Param: 0.0692; Time: 0.81\n",
      "Epoch 2154/8192 --- L(Train): 0.3855; L(Val): 0.4377; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2155/8192 --- L(Train): 0.3703; L(Val): 0.4370; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2156/8192 --- L(Train): 0.3853; L(Val): 0.4364; Reg Param: 0.0692; Time: 0.77\n",
      "Epoch 2157/8192 --- L(Train): 0.3902; L(Val): 0.4370; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2158/8192 --- L(Train): 0.3887; L(Val): 0.4374; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2159/8192 --- L(Train): 0.3867; L(Val): 0.4367; Reg Param: 0.0692; Time: 0.70\n",
      "Epoch 2160/8192 --- L(Train): 0.3738; L(Val): 0.4370; Reg Param: 0.0692; Time: 0.77\n",
      "Epoch 2161/8192 --- L(Train): 0.3700; L(Val): 0.4368; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2162/8192 --- L(Train): 0.3918; L(Val): 0.4370; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2163/8192 --- L(Train): 0.3769; L(Val): 0.4369; Reg Param: 0.0692; Time: 0.70\n",
      "Epoch 2164/8192 --- L(Train): 0.3903; L(Val): 0.4359; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2165/8192 --- L(Train): 0.3840; L(Val): 0.4360; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2166/8192 --- L(Train): 0.3797; L(Val): 0.4358; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2167/8192 --- L(Train): 0.3715; L(Val): 0.4375; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2168/8192 --- L(Train): 0.3875; L(Val): 0.4379; Reg Param: 0.0692; Time: 0.70\n",
      "Epoch 2169/8192 --- L(Train): 0.3808; L(Val): 0.4366; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2170/8192 --- L(Train): 0.3891; L(Val): 0.4350; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2171/8192 --- L(Train): 0.3765; L(Val): 0.4340; Reg Param: 0.0692; Time: 0.70\n",
      "Epoch 2172/8192 --- L(Train): 0.3745; L(Val): 0.4344; Reg Param: 0.0692; Time: 0.70\n",
      "Epoch 2173/8192 --- L(Train): 0.3816; L(Val): 0.4347; Reg Param: 0.0692; Time: 0.70\n",
      "Epoch 2174/8192 --- L(Train): 0.3853; L(Val): 0.4342; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2175/8192 --- L(Train): 0.3943; L(Val): 0.4335; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2176/8192 --- L(Train): 0.3823; L(Val): 0.4336; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2177/8192 --- L(Train): 0.3817; L(Val): 0.4345; Reg Param: 0.0692; Time: 0.70\n",
      "Epoch 2178/8192 --- L(Train): 0.3750; L(Val): 0.4357; Reg Param: 0.0692; Time: 0.86\n",
      "Epoch 2179/8192 --- L(Train): 0.3776; L(Val): 0.4357; Reg Param: 0.0692; Time: 0.77\n",
      "Epoch 2180/8192 --- L(Train): 0.3759; L(Val): 0.4357; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2181/8192 --- L(Train): 0.3916; L(Val): 0.4370; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2182/8192 --- L(Train): 0.3792; L(Val): 0.4376; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2183/8192 --- L(Train): 0.3684; L(Val): 0.4368; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2184/8192 --- L(Train): 0.3817; L(Val): 0.4352; Reg Param: 0.0692; Time: 0.75\n",
      "Epoch 2185/8192 --- L(Train): 0.3754; L(Val): 0.4347; Reg Param: 0.0692; Time: 0.77\n",
      "Epoch 2186/8192 --- L(Train): 0.3909; L(Val): 0.4354; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2187/8192 --- L(Train): 0.3927; L(Val): 0.4370; Reg Param: 0.0692; Time: 0.70\n",
      "Epoch 2188/8192 --- L(Train): 0.3851; L(Val): 0.4367; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2189/8192 --- L(Train): 0.3861; L(Val): 0.4361; Reg Param: 0.0692; Time: 0.71\n",
      "Epoch 2190/8192 --- L(Train): 0.3862; L(Val): 0.4358; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2191/8192 --- L(Train): 0.3791; L(Val): 0.4351; Reg Param: 0.0692; Time: 0.68\n",
      "Epoch 2192/8192 --- L(Train): 0.3776; L(Val): 0.4359; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2193/8192 --- L(Train): 0.3815; L(Val): 0.4356; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2194/8192 --- L(Train): 0.3841; L(Val): 0.4343; Reg Param: 0.0692; Time: 0.70\n",
      "Epoch 2195/8192 --- L(Train): 0.3714; L(Val): 0.4348; Reg Param: 0.0692; Time: 0.69\n",
      "Epoch 2196/8192 --- L(Train): 0.3788; L(Val): 0.4354; Reg Param: 0.0692; Time: 0.68\n",
      "Epoch 2197/8192 --- L(Train): 0.3739; L(Val): 0.4362; Reg Param: 0.0692; Time: 0.78\n",
      "Epoch 2198/8192 --- L(Train): 0.3776; L(Val): 0.4363; Reg Param: 0.0692; Time: 0.77\n",
      "Epoch 2199/8192 --- L(Train): 0.3691; L(Val): 0.4362; Reg Param: 0.0692; Time: 0.71\n",
      "Epoch 2200/8192 --- L(Train): 0.3691; L(Val): 0.4362; Reg Param: 0.0697; Time: 7.76\n",
      "Epoch 2201/8192 --- L(Train): 0.3786; L(Val): 0.4361; Reg Param: 0.0697; Time: 0.78\n",
      "Epoch 2202/8192 --- L(Train): 0.3783; L(Val): 0.4370; Reg Param: 0.0697; Time: 0.71\n",
      "Epoch 2203/8192 --- L(Train): 0.3817; L(Val): 0.4375; Reg Param: 0.0697; Time: 0.82\n",
      "Epoch 2204/8192 --- L(Train): 0.3817; L(Val): 0.4362; Reg Param: 0.0697; Time: 0.74\n",
      "Epoch 2205/8192 --- L(Train): 0.3759; L(Val): 0.4357; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2206/8192 --- L(Train): 0.3649; L(Val): 0.4356; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2207/8192 --- L(Train): 0.3752; L(Val): 0.4354; Reg Param: 0.0697; Time: 0.71\n",
      "Epoch 2208/8192 --- L(Train): 0.3849; L(Val): 0.4358; Reg Param: 0.0697; Time: 0.73\n",
      "Epoch 2209/8192 --- L(Train): 0.3863; L(Val): 0.4358; Reg Param: 0.0697; Time: 0.72\n",
      "Epoch 2210/8192 --- L(Train): 0.3795; L(Val): 0.4359; Reg Param: 0.0697; Time: 0.72\n",
      "Epoch 2211/8192 --- L(Train): 0.3917; L(Val): 0.4357; Reg Param: 0.0697; Time: 0.72\n",
      "Epoch 2212/8192 --- L(Train): 0.3715; L(Val): 0.4353; Reg Param: 0.0697; Time: 0.72\n",
      "Epoch 2213/8192 --- L(Train): 0.3781; L(Val): 0.4354; Reg Param: 0.0697; Time: 0.71\n",
      "Epoch 2214/8192 --- L(Train): 0.3803; L(Val): 0.4357; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2215/8192 --- L(Train): 0.3738; L(Val): 0.4340; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2216/8192 --- L(Train): 0.3863; L(Val): 0.4333; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2217/8192 --- L(Train): 0.3814; L(Val): 0.4325; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2218/8192 --- L(Train): 0.3711; L(Val): 0.4325; Reg Param: 0.0697; Time: 0.69\n",
      "Epoch 2219/8192 --- L(Train): 0.3806; L(Val): 0.4325; Reg Param: 0.0697; Time: 0.72\n",
      "Epoch 2220/8192 --- L(Train): 0.3738; L(Val): 0.4325; Reg Param: 0.0697; Time: 0.72\n",
      "Epoch 2221/8192 --- L(Train): 0.3749; L(Val): 0.4328; Reg Param: 0.0697; Time: 0.71\n",
      "Epoch 2222/8192 --- L(Train): 0.3840; L(Val): 0.4341; Reg Param: 0.0697; Time: 0.79\n",
      "Epoch 2223/8192 --- L(Train): 0.3740; L(Val): 0.4351; Reg Param: 0.0697; Time: 0.73\n",
      "Epoch 2224/8192 --- L(Train): 0.3769; L(Val): 0.4361; Reg Param: 0.0697; Time: 0.72\n",
      "Epoch 2225/8192 --- L(Train): 0.3841; L(Val): 0.4360; Reg Param: 0.0697; Time: 0.71\n",
      "Epoch 2226/8192 --- L(Train): 0.3833; L(Val): 0.4357; Reg Param: 0.0697; Time: 0.72\n",
      "Epoch 2227/8192 --- L(Train): 0.3510; L(Val): 0.4361; Reg Param: 0.0697; Time: 0.79\n",
      "Epoch 2228/8192 --- L(Train): 0.3806; L(Val): 0.4371; Reg Param: 0.0697; Time: 0.81\n",
      "Epoch 2229/8192 --- L(Train): 0.3723; L(Val): 0.4385; Reg Param: 0.0697; Time: 0.72\n",
      "Epoch 2230/8192 --- L(Train): 0.3782; L(Val): 0.4388; Reg Param: 0.0697; Time: 0.72\n",
      "Epoch 2231/8192 --- L(Train): 0.3886; L(Val): 0.4383; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2232/8192 --- L(Train): 0.3815; L(Val): 0.4374; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2233/8192 --- L(Train): 0.3816; L(Val): 0.4371; Reg Param: 0.0697; Time: 0.82\n",
      "Epoch 2234/8192 --- L(Train): 0.3812; L(Val): 0.4365; Reg Param: 0.0697; Time: 0.72\n",
      "Epoch 2235/8192 --- L(Train): 0.3932; L(Val): 0.4364; Reg Param: 0.0697; Time: 0.78\n",
      "Epoch 2236/8192 --- L(Train): 0.3815; L(Val): 0.4373; Reg Param: 0.0697; Time: 0.71\n",
      "Epoch 2237/8192 --- L(Train): 0.3794; L(Val): 0.4383; Reg Param: 0.0697; Time: 0.72\n",
      "Epoch 2238/8192 --- L(Train): 0.3744; L(Val): 0.4395; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2239/8192 --- L(Train): 0.3747; L(Val): 0.4401; Reg Param: 0.0697; Time: 0.69\n",
      "Epoch 2240/8192 --- L(Train): 0.3882; L(Val): 0.4396; Reg Param: 0.0697; Time: 0.69\n",
      "Epoch 2241/8192 --- L(Train): 0.3716; L(Val): 0.4390; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2242/8192 --- L(Train): 0.3734; L(Val): 0.4377; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2243/8192 --- L(Train): 0.3793; L(Val): 0.4372; Reg Param: 0.0697; Time: 0.69\n",
      "Epoch 2244/8192 --- L(Train): 0.3836; L(Val): 0.4368; Reg Param: 0.0697; Time: 0.71\n",
      "Epoch 2245/8192 --- L(Train): 0.3772; L(Val): 0.4371; Reg Param: 0.0697; Time: 0.71\n",
      "Epoch 2246/8192 --- L(Train): 0.3788; L(Val): 0.4376; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2247/8192 --- L(Train): 0.3726; L(Val): 0.4384; Reg Param: 0.0697; Time: 0.69\n",
      "Epoch 2248/8192 --- L(Train): 0.3824; L(Val): 0.4388; Reg Param: 0.0697; Time: 0.70\n",
      "Epoch 2249/8192 --- L(Train): 0.3937; L(Val): 0.4391; Reg Param: 0.0697; Time: 0.69\n",
      "Epoch 2250/8192 --- L(Train): 0.3821; L(Val): 0.4391; Reg Param: 0.0700; Time: 7.50\n",
      "Epoch 2251/8192 --- L(Train): 0.3686; L(Val): 0.4380; Reg Param: 0.0700; Time: 0.71\n",
      "Epoch 2252/8192 --- L(Train): 0.3688; L(Val): 0.4377; Reg Param: 0.0700; Time: 0.71\n",
      "Epoch 2253/8192 --- L(Train): 0.3741; L(Val): 0.4376; Reg Param: 0.0700; Time: 0.73\n",
      "Epoch 2254/8192 --- L(Train): 0.3827; L(Val): 0.4376; Reg Param: 0.0700; Time: 0.80\n",
      "Epoch 2255/8192 --- L(Train): 0.3787; L(Val): 0.4385; Reg Param: 0.0700; Time: 0.71\n",
      "Epoch 2256/8192 --- L(Train): 0.3854; L(Val): 0.4386; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2257/8192 --- L(Train): 0.3772; L(Val): 0.4378; Reg Param: 0.0700; Time: 0.69\n",
      "Epoch 2258/8192 --- L(Train): 0.3661; L(Val): 0.4377; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2259/8192 --- L(Train): 0.3706; L(Val): 0.4378; Reg Param: 0.0700; Time: 0.69\n",
      "Epoch 2260/8192 --- L(Train): 0.3808; L(Val): 0.4377; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2261/8192 --- L(Train): 0.3821; L(Val): 0.4379; Reg Param: 0.0700; Time: 0.73\n",
      "Epoch 2262/8192 --- L(Train): 0.3897; L(Val): 0.4379; Reg Param: 0.0700; Time: 0.72\n",
      "Epoch 2263/8192 --- L(Train): 0.3787; L(Val): 0.4379; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2264/8192 --- L(Train): 0.3753; L(Val): 0.4382; Reg Param: 0.0700; Time: 0.71\n",
      "Epoch 2265/8192 --- L(Train): 0.3840; L(Val): 0.4386; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2266/8192 --- L(Train): 0.3743; L(Val): 0.4380; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2267/8192 --- L(Train): 0.3865; L(Val): 0.4377; Reg Param: 0.0700; Time: 0.71\n",
      "Epoch 2268/8192 --- L(Train): 0.3758; L(Val): 0.4381; Reg Param: 0.0700; Time: 0.71\n",
      "Epoch 2269/8192 --- L(Train): 0.3735; L(Val): 0.4384; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2270/8192 --- L(Train): 0.3890; L(Val): 0.4376; Reg Param: 0.0700; Time: 0.71\n",
      "Epoch 2271/8192 --- L(Train): 0.3831; L(Val): 0.4364; Reg Param: 0.0700; Time: 0.71\n",
      "Epoch 2272/8192 --- L(Train): 0.3891; L(Val): 0.4354; Reg Param: 0.0700; Time: 0.71\n",
      "Epoch 2273/8192 --- L(Train): 0.3775; L(Val): 0.4351; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2274/8192 --- L(Train): 0.3817; L(Val): 0.4366; Reg Param: 0.0700; Time: 0.71\n",
      "Epoch 2275/8192 --- L(Train): 0.3892; L(Val): 0.4366; Reg Param: 0.0700; Time: 0.76\n",
      "Epoch 2276/8192 --- L(Train): 0.3816; L(Val): 0.4367; Reg Param: 0.0700; Time: 0.84\n",
      "Epoch 2277/8192 --- L(Train): 0.3813; L(Val): 0.4374; Reg Param: 0.0700; Time: 0.78\n",
      "Epoch 2278/8192 --- L(Train): 0.3699; L(Val): 0.4376; Reg Param: 0.0700; Time: 0.77\n",
      "Epoch 2279/8192 --- L(Train): 0.3920; L(Val): 0.4375; Reg Param: 0.0700; Time: 0.77\n",
      "Epoch 2280/8192 --- L(Train): 0.3808; L(Val): 0.4363; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2281/8192 --- L(Train): 0.3893; L(Val): 0.4359; Reg Param: 0.0700; Time: 0.72\n",
      "Epoch 2282/8192 --- L(Train): 0.3729; L(Val): 0.4362; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2283/8192 --- L(Train): 0.3839; L(Val): 0.4374; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2284/8192 --- L(Train): 0.3739; L(Val): 0.4366; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2285/8192 --- L(Train): 0.3712; L(Val): 0.4360; Reg Param: 0.0700; Time: 0.72\n",
      "Epoch 2286/8192 --- L(Train): 0.3904; L(Val): 0.4361; Reg Param: 0.0700; Time: 0.71\n",
      "Epoch 2287/8192 --- L(Train): 0.3841; L(Val): 0.4358; Reg Param: 0.0700; Time: 0.69\n",
      "Epoch 2288/8192 --- L(Train): 0.3683; L(Val): 0.4364; Reg Param: 0.0700; Time: 0.69\n",
      "Epoch 2289/8192 --- L(Train): 0.3865; L(Val): 0.4362; Reg Param: 0.0700; Time: 0.69\n",
      "Epoch 2290/8192 --- L(Train): 0.3789; L(Val): 0.4360; Reg Param: 0.0700; Time: 0.69\n",
      "Epoch 2291/8192 --- L(Train): 0.3828; L(Val): 0.4354; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2292/8192 --- L(Train): 0.3629; L(Val): 0.4357; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2293/8192 --- L(Train): 0.3782; L(Val): 0.4373; Reg Param: 0.0700; Time: 0.69\n",
      "Epoch 2294/8192 --- L(Train): 0.3844; L(Val): 0.4376; Reg Param: 0.0700; Time: 0.69\n",
      "Epoch 2295/8192 --- L(Train): 0.3907; L(Val): 0.4372; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2296/8192 --- L(Train): 0.3690; L(Val): 0.4368; Reg Param: 0.0700; Time: 0.72\n",
      "Epoch 2297/8192 --- L(Train): 0.3805; L(Val): 0.4367; Reg Param: 0.0700; Time: 0.70\n",
      "Epoch 2298/8192 --- L(Train): 0.3908; L(Val): 0.4373; Reg Param: 0.0700; Time: 0.69\n",
      "Epoch 2299/8192 --- L(Train): 0.3844; L(Val): 0.4370; Reg Param: 0.0700; Time: 0.69\n",
      "Epoch 2300/8192 --- L(Train): 0.3823; L(Val): 0.4370; Reg Param: 0.0703; Time: 7.64\n",
      "Epoch 2301/8192 --- L(Train): 0.3833; L(Val): 0.4378; Reg Param: 0.0703; Time: 0.72\n",
      "Epoch 2302/8192 --- L(Train): 0.3819; L(Val): 0.4383; Reg Param: 0.0703; Time: 0.69\n",
      "Epoch 2303/8192 --- L(Train): 0.3775; L(Val): 0.4387; Reg Param: 0.0703; Time: 0.69\n",
      "Epoch 2304/8192 --- L(Train): 0.3810; L(Val): 0.4383; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2305/8192 --- L(Train): 0.3775; L(Val): 0.4366; Reg Param: 0.0703; Time: 0.69\n",
      "Epoch 2306/8192 --- L(Train): 0.3920; L(Val): 0.4360; Reg Param: 0.0703; Time: 0.69\n",
      "Epoch 2307/8192 --- L(Train): 0.3703; L(Val): 0.4359; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2308/8192 --- L(Train): 0.3810; L(Val): 0.4360; Reg Param: 0.0703; Time: 0.69\n",
      "Epoch 2309/8192 --- L(Train): 0.3757; L(Val): 0.4369; Reg Param: 0.0703; Time: 0.69\n",
      "Epoch 2310/8192 --- L(Train): 0.3734; L(Val): 0.4368; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2311/8192 --- L(Train): 0.3747; L(Val): 0.4364; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2312/8192 --- L(Train): 0.3712; L(Val): 0.4363; Reg Param: 0.0703; Time: 0.71\n",
      "Epoch 2313/8192 --- L(Train): 0.3802; L(Val): 0.4367; Reg Param: 0.0703; Time: 0.71\n",
      "Epoch 2314/8192 --- L(Train): 0.3661; L(Val): 0.4372; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2315/8192 --- L(Train): 0.3794; L(Val): 0.4374; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2316/8192 --- L(Train): 0.3919; L(Val): 0.4366; Reg Param: 0.0703; Time: 0.75\n",
      "Epoch 2317/8192 --- L(Train): 0.3791; L(Val): 0.4362; Reg Param: 0.0703; Time: 0.71\n",
      "Epoch 2318/8192 --- L(Train): 0.3819; L(Val): 0.4363; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2319/8192 --- L(Train): 0.3882; L(Val): 0.4366; Reg Param: 0.0703; Time: 0.71\n",
      "Epoch 2320/8192 --- L(Train): 0.3791; L(Val): 0.4370; Reg Param: 0.0703; Time: 0.74\n",
      "Epoch 2321/8192 --- L(Train): 0.3766; L(Val): 0.4374; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2322/8192 --- L(Train): 0.3632; L(Val): 0.4374; Reg Param: 0.0703; Time: 0.79\n",
      "Epoch 2323/8192 --- L(Train): 0.3766; L(Val): 0.4380; Reg Param: 0.0703; Time: 0.71\n",
      "Epoch 2324/8192 --- L(Train): 0.3710; L(Val): 0.4386; Reg Param: 0.0703; Time: 0.78\n",
      "Epoch 2325/8192 --- L(Train): 0.3690; L(Val): 0.4389; Reg Param: 0.0703; Time: 0.73\n",
      "Epoch 2326/8192 --- L(Train): 0.3765; L(Val): 0.4388; Reg Param: 0.0703; Time: 0.69\n",
      "Epoch 2327/8192 --- L(Train): 0.3812; L(Val): 0.4382; Reg Param: 0.0703; Time: 0.69\n",
      "Epoch 2328/8192 --- L(Train): 0.3611; L(Val): 0.4376; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2329/8192 --- L(Train): 0.3787; L(Val): 0.4371; Reg Param: 0.0703; Time: 0.78\n",
      "Epoch 2330/8192 --- L(Train): 0.3852; L(Val): 0.4368; Reg Param: 0.0703; Time: 0.69\n",
      "Epoch 2331/8192 --- L(Train): 0.3708; L(Val): 0.4372; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2332/8192 --- L(Train): 0.3861; L(Val): 0.4373; Reg Param: 0.0703; Time: 0.69\n",
      "Epoch 2333/8192 --- L(Train): 0.3776; L(Val): 0.4360; Reg Param: 0.0703; Time: 0.71\n",
      "Epoch 2334/8192 --- L(Train): 0.3878; L(Val): 0.4358; Reg Param: 0.0703; Time: 0.72\n",
      "Epoch 2335/8192 --- L(Train): 0.3714; L(Val): 0.4362; Reg Param: 0.0703; Time: 0.71\n",
      "Epoch 2336/8192 --- L(Train): 0.3789; L(Val): 0.4373; Reg Param: 0.0703; Time: 0.72\n",
      "Epoch 2337/8192 --- L(Train): 0.3738; L(Val): 0.4379; Reg Param: 0.0703; Time: 0.72\n",
      "Epoch 2338/8192 --- L(Train): 0.3821; L(Val): 0.4376; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2339/8192 --- L(Train): 0.3689; L(Val): 0.4380; Reg Param: 0.0703; Time: 0.71\n",
      "Epoch 2340/8192 --- L(Train): 0.3784; L(Val): 0.4386; Reg Param: 0.0703; Time: 0.71\n",
      "Epoch 2341/8192 --- L(Train): 0.3819; L(Val): 0.4395; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2342/8192 --- L(Train): 0.3758; L(Val): 0.4389; Reg Param: 0.0703; Time: 0.76\n",
      "Epoch 2343/8192 --- L(Train): 0.3938; L(Val): 0.4388; Reg Param: 0.0703; Time: 0.71\n",
      "Epoch 2344/8192 --- L(Train): 0.3686; L(Val): 0.4391; Reg Param: 0.0703; Time: 0.72\n",
      "Epoch 2345/8192 --- L(Train): 0.3752; L(Val): 0.4388; Reg Param: 0.0703; Time: 0.71\n",
      "Epoch 2346/8192 --- L(Train): 0.3793; L(Val): 0.4386; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2347/8192 --- L(Train): 0.3773; L(Val): 0.4379; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2348/8192 --- L(Train): 0.3861; L(Val): 0.4373; Reg Param: 0.0703; Time: 0.70\n",
      "Epoch 2349/8192 --- L(Train): 0.3846; L(Val): 0.4369; Reg Param: 0.0703; Time: 0.69\n",
      "Epoch 2350/8192 --- L(Train): 0.3843; L(Val): 0.4369; Reg Param: 0.0705; Time: 7.54\n",
      "Epoch 2351/8192 --- L(Train): 0.3840; L(Val): 0.4366; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2352/8192 --- L(Train): 0.3814; L(Val): 0.4363; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2353/8192 --- L(Train): 0.3855; L(Val): 0.4356; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2354/8192 --- L(Train): 0.3835; L(Val): 0.4351; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2355/8192 --- L(Train): 0.3708; L(Val): 0.4354; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2356/8192 --- L(Train): 0.3762; L(Val): 0.4357; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2357/8192 --- L(Train): 0.3765; L(Val): 0.4355; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2358/8192 --- L(Train): 0.3817; L(Val): 0.4358; Reg Param: 0.0705; Time: 0.68\n",
      "Epoch 2359/8192 --- L(Train): 0.3799; L(Val): 0.4364; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2360/8192 --- L(Train): 0.3826; L(Val): 0.4370; Reg Param: 0.0705; Time: 0.69\n",
      "Epoch 2361/8192 --- L(Train): 0.3688; L(Val): 0.4371; Reg Param: 0.0705; Time: 0.69\n",
      "Epoch 2362/8192 --- L(Train): 0.3789; L(Val): 0.4366; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2363/8192 --- L(Train): 0.3805; L(Val): 0.4362; Reg Param: 0.0705; Time: 0.69\n",
      "Epoch 2364/8192 --- L(Train): 0.3814; L(Val): 0.4365; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2365/8192 --- L(Train): 0.3832; L(Val): 0.4370; Reg Param: 0.0705; Time: 0.69\n",
      "Epoch 2366/8192 --- L(Train): 0.3801; L(Val): 0.4378; Reg Param: 0.0705; Time: 0.69\n",
      "Epoch 2367/8192 --- L(Train): 0.3736; L(Val): 0.4382; Reg Param: 0.0705; Time: 0.69\n",
      "Epoch 2368/8192 --- L(Train): 0.3794; L(Val): 0.4382; Reg Param: 0.0705; Time: 0.69\n",
      "Epoch 2369/8192 --- L(Train): 0.3877; L(Val): 0.4379; Reg Param: 0.0705; Time: 0.69\n",
      "Epoch 2370/8192 --- L(Train): 0.3748; L(Val): 0.4379; Reg Param: 0.0705; Time: 0.69\n",
      "Epoch 2371/8192 --- L(Train): 0.3729; L(Val): 0.4383; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2372/8192 --- L(Train): 0.3734; L(Val): 0.4377; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2373/8192 --- L(Train): 0.3784; L(Val): 0.4370; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2374/8192 --- L(Train): 0.3892; L(Val): 0.4369; Reg Param: 0.0705; Time: 0.84\n",
      "Epoch 2375/8192 --- L(Train): 0.3729; L(Val): 0.4375; Reg Param: 0.0705; Time: 0.77\n",
      "Epoch 2376/8192 --- L(Train): 0.3847; L(Val): 0.4380; Reg Param: 0.0705; Time: 0.78\n",
      "Epoch 2377/8192 --- L(Train): 0.3817; L(Val): 0.4382; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2378/8192 --- L(Train): 0.3816; L(Val): 0.4383; Reg Param: 0.0705; Time: 0.69\n",
      "Epoch 2379/8192 --- L(Train): 0.3931; L(Val): 0.4386; Reg Param: 0.0705; Time: 0.69\n",
      "Epoch 2380/8192 --- L(Train): 0.3700; L(Val): 0.4378; Reg Param: 0.0705; Time: 0.80\n",
      "Epoch 2381/8192 --- L(Train): 0.3929; L(Val): 0.4369; Reg Param: 0.0705; Time: 0.69\n",
      "Epoch 2382/8192 --- L(Train): 0.3847; L(Val): 0.4364; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2383/8192 --- L(Train): 0.3758; L(Val): 0.4361; Reg Param: 0.0705; Time: 0.73\n",
      "Epoch 2384/8192 --- L(Train): 0.3798; L(Val): 0.4374; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2385/8192 --- L(Train): 0.3895; L(Val): 0.4382; Reg Param: 0.0705; Time: 0.72\n",
      "Epoch 2386/8192 --- L(Train): 0.3856; L(Val): 0.4370; Reg Param: 0.0705; Time: 0.78\n",
      "Epoch 2387/8192 --- L(Train): 0.3778; L(Val): 0.4362; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2388/8192 --- L(Train): 0.3818; L(Val): 0.4360; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2389/8192 --- L(Train): 0.3787; L(Val): 0.4360; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2390/8192 --- L(Train): 0.3928; L(Val): 0.4362; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2391/8192 --- L(Train): 0.3780; L(Val): 0.4363; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2392/8192 --- L(Train): 0.3708; L(Val): 0.4374; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2393/8192 --- L(Train): 0.3769; L(Val): 0.4383; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2394/8192 --- L(Train): 0.3761; L(Val): 0.4394; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2395/8192 --- L(Train): 0.3757; L(Val): 0.4391; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2396/8192 --- L(Train): 0.3918; L(Val): 0.4375; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2397/8192 --- L(Train): 0.3823; L(Val): 0.4365; Reg Param: 0.0705; Time: 0.71\n",
      "Epoch 2398/8192 --- L(Train): 0.3745; L(Val): 0.4369; Reg Param: 0.0705; Time: 0.70\n",
      "Epoch 2399/8192 --- L(Train): 0.3769; L(Val): 0.4380; Reg Param: 0.0705; Time: 0.75\n",
      "Epoch 2400/8192 --- L(Train): 0.3788; L(Val): 0.4380; Reg Param: 0.0708; Time: 7.45\n",
      "Epoch 2401/8192 --- L(Train): 0.3775; L(Val): 0.4381; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2402/8192 --- L(Train): 0.3771; L(Val): 0.4387; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2403/8192 --- L(Train): 0.3750; L(Val): 0.4380; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2404/8192 --- L(Train): 0.3853; L(Val): 0.4365; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2405/8192 --- L(Train): 0.3720; L(Val): 0.4344; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2406/8192 --- L(Train): 0.3771; L(Val): 0.4334; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2407/8192 --- L(Train): 0.3804; L(Val): 0.4330; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2408/8192 --- L(Train): 0.3807; L(Val): 0.4333; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2409/8192 --- L(Train): 0.3812; L(Val): 0.4340; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2410/8192 --- L(Train): 0.3838; L(Val): 0.4354; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2411/8192 --- L(Train): 0.3649; L(Val): 0.4370; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2412/8192 --- L(Train): 0.3707; L(Val): 0.4376; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2413/8192 --- L(Train): 0.3805; L(Val): 0.4371; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2414/8192 --- L(Train): 0.3868; L(Val): 0.4358; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2415/8192 --- L(Train): 0.3881; L(Val): 0.4350; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2416/8192 --- L(Train): 0.3800; L(Val): 0.4349; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2417/8192 --- L(Train): 0.3815; L(Val): 0.4353; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2418/8192 --- L(Train): 0.3705; L(Val): 0.4353; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2419/8192 --- L(Train): 0.3727; L(Val): 0.4353; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2420/8192 --- L(Train): 0.3753; L(Val): 0.4359; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2421/8192 --- L(Train): 0.3760; L(Val): 0.4362; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2422/8192 --- L(Train): 0.3755; L(Val): 0.4366; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2423/8192 --- L(Train): 0.3874; L(Val): 0.4367; Reg Param: 0.0708; Time: 0.84\n",
      "Epoch 2424/8192 --- L(Train): 0.3915; L(Val): 0.4363; Reg Param: 0.0708; Time: 0.77\n",
      "Epoch 2425/8192 --- L(Train): 0.3758; L(Val): 0.4365; Reg Param: 0.0708; Time: 0.72\n",
      "Epoch 2426/8192 --- L(Train): 0.3785; L(Val): 0.4373; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2427/8192 --- L(Train): 0.3869; L(Val): 0.4380; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2428/8192 --- L(Train): 0.3816; L(Val): 0.4379; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2429/8192 --- L(Train): 0.3707; L(Val): 0.4377; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2430/8192 --- L(Train): 0.3818; L(Val): 0.4369; Reg Param: 0.0708; Time: 0.78\n",
      "Epoch 2431/8192 --- L(Train): 0.3716; L(Val): 0.4371; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2432/8192 --- L(Train): 0.3726; L(Val): 0.4377; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2433/8192 --- L(Train): 0.3698; L(Val): 0.4382; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2434/8192 --- L(Train): 0.3737; L(Val): 0.4381; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2435/8192 --- L(Train): 0.3825; L(Val): 0.4383; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2436/8192 --- L(Train): 0.3798; L(Val): 0.4387; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2437/8192 --- L(Train): 0.3793; L(Val): 0.4393; Reg Param: 0.0708; Time: 0.72\n",
      "Epoch 2438/8192 --- L(Train): 0.3874; L(Val): 0.4398; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2439/8192 --- L(Train): 0.3725; L(Val): 0.4395; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2440/8192 --- L(Train): 0.3782; L(Val): 0.4383; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2441/8192 --- L(Train): 0.3777; L(Val): 0.4378; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2442/8192 --- L(Train): 0.3838; L(Val): 0.4383; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2443/8192 --- L(Train): 0.3859; L(Val): 0.4399; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2444/8192 --- L(Train): 0.3835; L(Val): 0.4398; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2445/8192 --- L(Train): 0.3821; L(Val): 0.4388; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2446/8192 --- L(Train): 0.3662; L(Val): 0.4385; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2447/8192 --- L(Train): 0.3690; L(Val): 0.4385; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2448/8192 --- L(Train): 0.3831; L(Val): 0.4386; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2449/8192 --- L(Train): 0.3866; L(Val): 0.4381; Reg Param: 0.0708; Time: 0.68\n",
      "Epoch 2450/8192 --- L(Train): 0.3890; L(Val): 0.4381; Reg Param: 0.0710; Time: 7.65\n",
      "Epoch 2451/8192 --- L(Train): 0.3757; L(Val): 0.4362; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2452/8192 --- L(Train): 0.3756; L(Val): 0.4363; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2453/8192 --- L(Train): 0.3600; L(Val): 0.4370; Reg Param: 0.0710; Time: 0.70\n",
      "Epoch 2454/8192 --- L(Train): 0.3774; L(Val): 0.4381; Reg Param: 0.0710; Time: 0.70\n",
      "Epoch 2455/8192 --- L(Train): 0.3823; L(Val): 0.4380; Reg Param: 0.0710; Time: 0.70\n",
      "Epoch 2456/8192 --- L(Train): 0.3834; L(Val): 0.4374; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2457/8192 --- L(Train): 0.3672; L(Val): 0.4373; Reg Param: 0.0710; Time: 0.78\n",
      "Epoch 2458/8192 --- L(Train): 0.3713; L(Val): 0.4378; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2459/8192 --- L(Train): 0.3732; L(Val): 0.4387; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2460/8192 --- L(Train): 0.3675; L(Val): 0.4390; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2461/8192 --- L(Train): 0.3878; L(Val): 0.4382; Reg Param: 0.0710; Time: 0.70\n",
      "Epoch 2462/8192 --- L(Train): 0.3765; L(Val): 0.4380; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2463/8192 --- L(Train): 0.3670; L(Val): 0.4382; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2464/8192 --- L(Train): 0.3760; L(Val): 0.4392; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2465/8192 --- L(Train): 0.3759; L(Val): 0.4394; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2466/8192 --- L(Train): 0.3764; L(Val): 0.4397; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2467/8192 --- L(Train): 0.3800; L(Val): 0.4400; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2468/8192 --- L(Train): 0.3787; L(Val): 0.4398; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2469/8192 --- L(Train): 0.3765; L(Val): 0.4391; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2470/8192 --- L(Train): 0.3881; L(Val): 0.4383; Reg Param: 0.0710; Time: 0.72\n",
      "Epoch 2471/8192 --- L(Train): 0.3823; L(Val): 0.4379; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2472/8192 --- L(Train): 0.3772; L(Val): 0.4371; Reg Param: 0.0710; Time: 0.70\n",
      "Epoch 2473/8192 --- L(Train): 0.3688; L(Val): 0.4366; Reg Param: 0.0710; Time: 0.72\n",
      "Epoch 2474/8192 --- L(Train): 0.3828; L(Val): 0.4364; Reg Param: 0.0710; Time: 0.75\n",
      "Epoch 2475/8192 --- L(Train): 0.3646; L(Val): 0.4369; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2476/8192 --- L(Train): 0.3786; L(Val): 0.4378; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2477/8192 --- L(Train): 0.3754; L(Val): 0.4375; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2478/8192 --- L(Train): 0.3745; L(Val): 0.4369; Reg Param: 0.0710; Time: 0.77\n",
      "Epoch 2479/8192 --- L(Train): 0.3930; L(Val): 0.4357; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2480/8192 --- L(Train): 0.3838; L(Val): 0.4351; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2481/8192 --- L(Train): 0.3756; L(Val): 0.4350; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2482/8192 --- L(Train): 0.3785; L(Val): 0.4344; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2483/8192 --- L(Train): 0.3667; L(Val): 0.4337; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2484/8192 --- L(Train): 0.3910; L(Val): 0.4336; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2485/8192 --- L(Train): 0.3706; L(Val): 0.4340; Reg Param: 0.0710; Time: 0.72\n",
      "Epoch 2486/8192 --- L(Train): 0.3869; L(Val): 0.4338; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2487/8192 --- L(Train): 0.3791; L(Val): 0.4337; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2488/8192 --- L(Train): 0.3798; L(Val): 0.4342; Reg Param: 0.0710; Time: 0.69\n",
      "Epoch 2489/8192 --- L(Train): 0.3903; L(Val): 0.4345; Reg Param: 0.0710; Time: 0.70\n",
      "Epoch 2490/8192 --- L(Train): 0.3858; L(Val): 0.4353; Reg Param: 0.0710; Time: 0.70\n",
      "Epoch 2491/8192 --- L(Train): 0.3690; L(Val): 0.4354; Reg Param: 0.0710; Time: 0.70\n",
      "Epoch 2492/8192 --- L(Train): 0.3839; L(Val): 0.4350; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2493/8192 --- L(Train): 0.3750; L(Val): 0.4340; Reg Param: 0.0710; Time: 0.75\n",
      "Epoch 2494/8192 --- L(Train): 0.3758; L(Val): 0.4336; Reg Param: 0.0710; Time: 0.70\n",
      "Epoch 2495/8192 --- L(Train): 0.3932; L(Val): 0.4343; Reg Param: 0.0710; Time: 0.71\n",
      "Epoch 2496/8192 --- L(Train): 0.3867; L(Val): 0.4351; Reg Param: 0.0710; Time: 0.70\n",
      "Epoch 2497/8192 --- L(Train): 0.3835; L(Val): 0.4347; Reg Param: 0.0710; Time: 0.70\n",
      "Epoch 2498/8192 --- L(Train): 0.3883; L(Val): 0.4352; Reg Param: 0.0710; Time: 0.72\n",
      "Epoch 2499/8192 --- L(Train): 0.3690; L(Val): 0.4358; Reg Param: 0.0710; Time: 0.72\n",
      "Epoch 2500/8192 --- L(Train): 0.3860; L(Val): 0.4358; Reg Param: 0.0711; Time: 7.64\n",
      "Epoch 2501/8192 --- L(Train): 0.3808; L(Val): 0.4369; Reg Param: 0.0711; Time: 0.69\n",
      "Epoch 2502/8192 --- L(Train): 0.3869; L(Val): 0.4359; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2503/8192 --- L(Train): 0.3704; L(Val): 0.4363; Reg Param: 0.0711; Time: 0.72\n",
      "Epoch 2504/8192 --- L(Train): 0.3803; L(Val): 0.4369; Reg Param: 0.0711; Time: 0.77\n",
      "Epoch 2505/8192 --- L(Train): 0.3795; L(Val): 0.4375; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2506/8192 --- L(Train): 0.3814; L(Val): 0.4378; Reg Param: 0.0711; Time: 0.74\n",
      "Epoch 2507/8192 --- L(Train): 0.3762; L(Val): 0.4373; Reg Param: 0.0711; Time: 0.72\n",
      "Epoch 2508/8192 --- L(Train): 0.3784; L(Val): 0.4367; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2509/8192 --- L(Train): 0.3715; L(Val): 0.4367; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2510/8192 --- L(Train): 0.3734; L(Val): 0.4373; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2511/8192 --- L(Train): 0.3816; L(Val): 0.4382; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2512/8192 --- L(Train): 0.3833; L(Val): 0.4393; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2513/8192 --- L(Train): 0.3948; L(Val): 0.4396; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2514/8192 --- L(Train): 0.3662; L(Val): 0.4397; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2515/8192 --- L(Train): 0.3716; L(Val): 0.4398; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2516/8192 --- L(Train): 0.3839; L(Val): 0.4396; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2517/8192 --- L(Train): 0.3784; L(Val): 0.4393; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2518/8192 --- L(Train): 0.3845; L(Val): 0.4386; Reg Param: 0.0711; Time: 0.78\n",
      "Epoch 2519/8192 --- L(Train): 0.3762; L(Val): 0.4383; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2520/8192 --- L(Train): 0.3863; L(Val): 0.4383; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2521/8192 --- L(Train): 0.3859; L(Val): 0.4386; Reg Param: 0.0711; Time: 0.75\n",
      "Epoch 2522/8192 --- L(Train): 0.3827; L(Val): 0.4395; Reg Param: 0.0711; Time: 0.83\n",
      "Epoch 2523/8192 --- L(Train): 0.3764; L(Val): 0.4400; Reg Param: 0.0711; Time: 0.69\n",
      "Epoch 2524/8192 --- L(Train): 0.3804; L(Val): 0.4398; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2525/8192 --- L(Train): 0.3738; L(Val): 0.4394; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2526/8192 --- L(Train): 0.3801; L(Val): 0.4385; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2527/8192 --- L(Train): 0.3775; L(Val): 0.4378; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2528/8192 --- L(Train): 0.3727; L(Val): 0.4370; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2529/8192 --- L(Train): 0.3838; L(Val): 0.4366; Reg Param: 0.0711; Time: 0.80\n",
      "Epoch 2530/8192 --- L(Train): 0.3709; L(Val): 0.4364; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2531/8192 --- L(Train): 0.3784; L(Val): 0.4359; Reg Param: 0.0711; Time: 0.72\n",
      "Epoch 2532/8192 --- L(Train): 0.3758; L(Val): 0.4358; Reg Param: 0.0711; Time: 0.72\n",
      "Epoch 2533/8192 --- L(Train): 0.3826; L(Val): 0.4357; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2534/8192 --- L(Train): 0.3727; L(Val): 0.4352; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2535/8192 --- L(Train): 0.3716; L(Val): 0.4347; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2536/8192 --- L(Train): 0.3991; L(Val): 0.4350; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2537/8192 --- L(Train): 0.3662; L(Val): 0.4352; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2538/8192 --- L(Train): 0.3793; L(Val): 0.4352; Reg Param: 0.0711; Time: 0.69\n",
      "Epoch 2539/8192 --- L(Train): 0.3719; L(Val): 0.4354; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2540/8192 --- L(Train): 0.3635; L(Val): 0.4357; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2541/8192 --- L(Train): 0.3878; L(Val): 0.4359; Reg Param: 0.0711; Time: 0.79\n",
      "Epoch 2542/8192 --- L(Train): 0.3741; L(Val): 0.4359; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2543/8192 --- L(Train): 0.3687; L(Val): 0.4364; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2544/8192 --- L(Train): 0.3734; L(Val): 0.4370; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2545/8192 --- L(Train): 0.3796; L(Val): 0.4366; Reg Param: 0.0711; Time: 0.72\n",
      "Epoch 2546/8192 --- L(Train): 0.3826; L(Val): 0.4357; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2547/8192 --- L(Train): 0.3840; L(Val): 0.4354; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2548/8192 --- L(Train): 0.3722; L(Val): 0.4359; Reg Param: 0.0711; Time: 0.71\n",
      "Epoch 2549/8192 --- L(Train): 0.3776; L(Val): 0.4366; Reg Param: 0.0711; Time: 0.70\n",
      "Epoch 2550/8192 --- L(Train): 0.3754; L(Val): 0.4366; Reg Param: 0.0709; Time: 7.60\n",
      "Epoch 2551/8192 --- L(Train): 0.3770; L(Val): 0.4362; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2552/8192 --- L(Train): 0.3749; L(Val): 0.4354; Reg Param: 0.0709; Time: 0.69\n",
      "Epoch 2553/8192 --- L(Train): 0.3842; L(Val): 0.4358; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2554/8192 --- L(Train): 0.3655; L(Val): 0.4382; Reg Param: 0.0709; Time: 0.75\n",
      "Epoch 2555/8192 --- L(Train): 0.3766; L(Val): 0.4397; Reg Param: 0.0709; Time: 0.71\n",
      "Epoch 2556/8192 --- L(Train): 0.3732; L(Val): 0.4391; Reg Param: 0.0709; Time: 0.71\n",
      "Epoch 2557/8192 --- L(Train): 0.3804; L(Val): 0.4391; Reg Param: 0.0709; Time: 0.75\n",
      "Epoch 2558/8192 --- L(Train): 0.3683; L(Val): 0.4392; Reg Param: 0.0709; Time: 0.77\n",
      "Epoch 2559/8192 --- L(Train): 0.3758; L(Val): 0.4395; Reg Param: 0.0709; Time: 0.74\n",
      "Epoch 2560/8192 --- L(Train): 0.3766; L(Val): 0.4400; Reg Param: 0.0709; Time: 0.71\n",
      "Epoch 2561/8192 --- L(Train): 0.3774; L(Val): 0.4387; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2562/8192 --- L(Train): 0.3789; L(Val): 0.4372; Reg Param: 0.0709; Time: 0.71\n",
      "Epoch 2563/8192 --- L(Train): 0.3826; L(Val): 0.4353; Reg Param: 0.0709; Time: 0.72\n",
      "Epoch 2564/8192 --- L(Train): 0.3692; L(Val): 0.4340; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2565/8192 --- L(Train): 0.3823; L(Val): 0.4333; Reg Param: 0.0709; Time: 0.71\n",
      "Epoch 2566/8192 --- L(Train): 0.3725; L(Val): 0.4326; Reg Param: 0.0709; Time: 0.69\n",
      "Epoch 2567/8192 --- L(Train): 0.3765; L(Val): 0.4323; Reg Param: 0.0709; Time: 0.71\n",
      "Epoch 2568/8192 --- L(Train): 0.3804; L(Val): 0.4320; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2569/8192 --- L(Train): 0.3818; L(Val): 0.4322; Reg Param: 0.0709; Time: 0.82\n",
      "Epoch 2570/8192 --- L(Train): 0.3791; L(Val): 0.4325; Reg Param: 0.0709; Time: 0.78\n",
      "Epoch 2571/8192 --- L(Train): 0.3780; L(Val): 0.4331; Reg Param: 0.0709; Time: 0.73\n",
      "Epoch 2572/8192 --- L(Train): 0.3782; L(Val): 0.4338; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2573/8192 --- L(Train): 0.3837; L(Val): 0.4342; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2574/8192 --- L(Train): 0.3788; L(Val): 0.4337; Reg Param: 0.0709; Time: 0.69\n",
      "Epoch 2575/8192 --- L(Train): 0.3817; L(Val): 0.4337; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2576/8192 --- L(Train): 0.3720; L(Val): 0.4334; Reg Param: 0.0709; Time: 0.78\n",
      "Epoch 2577/8192 --- L(Train): 0.3816; L(Val): 0.4339; Reg Param: 0.0709; Time: 0.72\n",
      "Epoch 2578/8192 --- L(Train): 0.3905; L(Val): 0.4329; Reg Param: 0.0709; Time: 0.73\n",
      "Epoch 2579/8192 --- L(Train): 0.3877; L(Val): 0.4321; Reg Param: 0.0709; Time: 0.72\n",
      "Epoch 2580/8192 --- L(Train): 0.3798; L(Val): 0.4318; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2581/8192 --- L(Train): 0.3823; L(Val): 0.4319; Reg Param: 0.0709; Time: 0.69\n",
      "Epoch 2582/8192 --- L(Train): 0.3790; L(Val): 0.4328; Reg Param: 0.0709; Time: 0.69\n",
      "Epoch 2583/8192 --- L(Train): 0.3879; L(Val): 0.4324; Reg Param: 0.0709; Time: 0.71\n",
      "Epoch 2584/8192 --- L(Train): 0.3815; L(Val): 0.4309; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2585/8192 --- L(Train): 0.3763; L(Val): 0.4312; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2586/8192 --- L(Train): 0.3639; L(Val): 0.4317; Reg Param: 0.0709; Time: 0.69\n",
      "Epoch 2587/8192 --- L(Train): 0.3845; L(Val): 0.4321; Reg Param: 0.0709; Time: 0.69\n",
      "Epoch 2588/8192 --- L(Train): 0.3652; L(Val): 0.4328; Reg Param: 0.0709; Time: 0.68\n",
      "Epoch 2589/8192 --- L(Train): 0.3920; L(Val): 0.4331; Reg Param: 0.0709; Time: 0.69\n",
      "Epoch 2590/8192 --- L(Train): 0.3698; L(Val): 0.4324; Reg Param: 0.0709; Time: 0.71\n",
      "Epoch 2591/8192 --- L(Train): 0.3701; L(Val): 0.4322; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2592/8192 --- L(Train): 0.3768; L(Val): 0.4325; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2593/8192 --- L(Train): 0.3839; L(Val): 0.4331; Reg Param: 0.0709; Time: 0.79\n",
      "Epoch 2594/8192 --- L(Train): 0.3781; L(Val): 0.4335; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2595/8192 --- L(Train): 0.3865; L(Val): 0.4330; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2596/8192 --- L(Train): 0.3839; L(Val): 0.4323; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2597/8192 --- L(Train): 0.3851; L(Val): 0.4319; Reg Param: 0.0709; Time: 0.70\n",
      "Epoch 2598/8192 --- L(Train): 0.3704; L(Val): 0.4317; Reg Param: 0.0709; Time: 0.71\n",
      "Epoch 2599/8192 --- L(Train): 0.3776; L(Val): 0.4312; Reg Param: 0.0709; Time: 0.84\n",
      "Epoch 2600/8192 --- L(Train): 0.3842; L(Val): 0.4312; Reg Param: 0.0708; Time: 7.55\n",
      "Epoch 2601/8192 --- L(Train): 0.3854; L(Val): 0.4319; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2602/8192 --- L(Train): 0.3783; L(Val): 0.4321; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2603/8192 --- L(Train): 0.3746; L(Val): 0.4325; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2604/8192 --- L(Train): 0.3818; L(Val): 0.4328; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2605/8192 --- L(Train): 0.3738; L(Val): 0.4333; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2606/8192 --- L(Train): 0.3749; L(Val): 0.4337; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2607/8192 --- L(Train): 0.3711; L(Val): 0.4335; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2608/8192 --- L(Train): 0.3829; L(Val): 0.4322; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2609/8192 --- L(Train): 0.3836; L(Val): 0.4311; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2610/8192 --- L(Train): 0.3699; L(Val): 0.4308; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2611/8192 --- L(Train): 0.3799; L(Val): 0.4309; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2612/8192 --- L(Train): 0.3800; L(Val): 0.4316; Reg Param: 0.0708; Time: 0.79\n",
      "Epoch 2613/8192 --- L(Train): 0.3837; L(Val): 0.4326; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2614/8192 --- L(Train): 0.3773; L(Val): 0.4329; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2615/8192 --- L(Train): 0.3709; L(Val): 0.4335; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2616/8192 --- L(Train): 0.3849; L(Val): 0.4342; Reg Param: 0.0708; Time: 0.72\n",
      "Epoch 2617/8192 --- L(Train): 0.3839; L(Val): 0.4345; Reg Param: 0.0708; Time: 0.72\n",
      "Epoch 2618/8192 --- L(Train): 0.3698; L(Val): 0.4349; Reg Param: 0.0708; Time: 0.78\n",
      "Epoch 2619/8192 --- L(Train): 0.3747; L(Val): 0.4353; Reg Param: 0.0708; Time: 0.75\n",
      "Epoch 2620/8192 --- L(Train): 0.3851; L(Val): 0.4351; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2621/8192 --- L(Train): 0.3804; L(Val): 0.4353; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2622/8192 --- L(Train): 0.3793; L(Val): 0.4346; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2623/8192 --- L(Train): 0.3864; L(Val): 0.4344; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2624/8192 --- L(Train): 0.3756; L(Val): 0.4341; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2625/8192 --- L(Train): 0.3828; L(Val): 0.4343; Reg Param: 0.0708; Time: 0.80\n",
      "Epoch 2626/8192 --- L(Train): 0.3800; L(Val): 0.4344; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2627/8192 --- L(Train): 0.3830; L(Val): 0.4336; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2628/8192 --- L(Train): 0.3732; L(Val): 0.4329; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2629/8192 --- L(Train): 0.3736; L(Val): 0.4325; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2630/8192 --- L(Train): 0.3883; L(Val): 0.4327; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2631/8192 --- L(Train): 0.3818; L(Val): 0.4329; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2632/8192 --- L(Train): 0.3804; L(Val): 0.4333; Reg Param: 0.0708; Time: 0.72\n",
      "Epoch 2633/8192 --- L(Train): 0.3809; L(Val): 0.4337; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2634/8192 --- L(Train): 0.3824; L(Val): 0.4338; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2635/8192 --- L(Train): 0.3711; L(Val): 0.4343; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2636/8192 --- L(Train): 0.3713; L(Val): 0.4349; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2637/8192 --- L(Train): 0.3832; L(Val): 0.4354; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2638/8192 --- L(Train): 0.3834; L(Val): 0.4354; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2639/8192 --- L(Train): 0.3669; L(Val): 0.4345; Reg Param: 0.0708; Time: 0.72\n",
      "Epoch 2640/8192 --- L(Train): 0.3725; L(Val): 0.4347; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2641/8192 --- L(Train): 0.3844; L(Val): 0.4351; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2642/8192 --- L(Train): 0.3718; L(Val): 0.4356; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2643/8192 --- L(Train): 0.3649; L(Val): 0.4355; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2644/8192 --- L(Train): 0.3802; L(Val): 0.4351; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2645/8192 --- L(Train): 0.3822; L(Val): 0.4354; Reg Param: 0.0708; Time: 0.69\n",
      "Epoch 2646/8192 --- L(Train): 0.3688; L(Val): 0.4358; Reg Param: 0.0708; Time: 0.70\n",
      "Epoch 2647/8192 --- L(Train): 0.3676; L(Val): 0.4365; Reg Param: 0.0708; Time: 0.74\n",
      "Epoch 2648/8192 --- L(Train): 0.3876; L(Val): 0.4370; Reg Param: 0.0708; Time: 0.85\n",
      "Epoch 2649/8192 --- L(Train): 0.3705; L(Val): 0.4366; Reg Param: 0.0708; Time: 0.71\n",
      "Epoch 2650/8192 --- L(Train): 0.3786; L(Val): 0.4366; Reg Param: 0.0707; Time: 7.39\n",
      "Epoch 2651/8192 --- L(Train): 0.3819; L(Val): 0.4353; Reg Param: 0.0707; Time: 0.77\n",
      "Epoch 2652/8192 --- L(Train): 0.3745; L(Val): 0.4349; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2653/8192 --- L(Train): 0.3795; L(Val): 0.4347; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2654/8192 --- L(Train): 0.3878; L(Val): 0.4348; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2655/8192 --- L(Train): 0.3873; L(Val): 0.4347; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2656/8192 --- L(Train): 0.3749; L(Val): 0.4348; Reg Param: 0.0707; Time: 0.69\n",
      "Epoch 2657/8192 --- L(Train): 0.3800; L(Val): 0.4352; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2658/8192 --- L(Train): 0.3780; L(Val): 0.4358; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2659/8192 --- L(Train): 0.3684; L(Val): 0.4359; Reg Param: 0.0707; Time: 0.77\n",
      "Epoch 2660/8192 --- L(Train): 0.3801; L(Val): 0.4362; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2661/8192 --- L(Train): 0.3662; L(Val): 0.4365; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2662/8192 --- L(Train): 0.3750; L(Val): 0.4367; Reg Param: 0.0707; Time: 0.69\n",
      "Epoch 2663/8192 --- L(Train): 0.3756; L(Val): 0.4368; Reg Param: 0.0707; Time: 0.69\n",
      "Epoch 2664/8192 --- L(Train): 0.3914; L(Val): 0.4373; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2665/8192 --- L(Train): 0.3772; L(Val): 0.4372; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2666/8192 --- L(Train): 0.3826; L(Val): 0.4372; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2667/8192 --- L(Train): 0.3759; L(Val): 0.4371; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2668/8192 --- L(Train): 0.3836; L(Val): 0.4370; Reg Param: 0.0707; Time: 0.77\n",
      "Epoch 2669/8192 --- L(Train): 0.3678; L(Val): 0.4370; Reg Param: 0.0707; Time: 0.81\n",
      "Epoch 2670/8192 --- L(Train): 0.3712; L(Val): 0.4375; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2671/8192 --- L(Train): 0.3914; L(Val): 0.4372; Reg Param: 0.0707; Time: 0.76\n",
      "Epoch 2672/8192 --- L(Train): 0.3778; L(Val): 0.4370; Reg Param: 0.0707; Time: 0.72\n",
      "Epoch 2673/8192 --- L(Train): 0.3705; L(Val): 0.4372; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2674/8192 --- L(Train): 0.3845; L(Val): 0.4372; Reg Param: 0.0707; Time: 0.82\n",
      "Epoch 2675/8192 --- L(Train): 0.3739; L(Val): 0.4376; Reg Param: 0.0707; Time: 0.74\n",
      "Epoch 2676/8192 --- L(Train): 0.3839; L(Val): 0.4379; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2677/8192 --- L(Train): 0.3636; L(Val): 0.4380; Reg Param: 0.0707; Time: 0.72\n",
      "Epoch 2678/8192 --- L(Train): 0.3858; L(Val): 0.4366; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2679/8192 --- L(Train): 0.3739; L(Val): 0.4362; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2680/8192 --- L(Train): 0.3736; L(Val): 0.4363; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2681/8192 --- L(Train): 0.3765; L(Val): 0.4365; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2682/8192 --- L(Train): 0.3665; L(Val): 0.4378; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2683/8192 --- L(Train): 0.3809; L(Val): 0.4392; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2684/8192 --- L(Train): 0.3717; L(Val): 0.4393; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2685/8192 --- L(Train): 0.3827; L(Val): 0.4393; Reg Param: 0.0707; Time: 0.72\n",
      "Epoch 2686/8192 --- L(Train): 0.3822; L(Val): 0.4393; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2687/8192 --- L(Train): 0.3705; L(Val): 0.4387; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2688/8192 --- L(Train): 0.3803; L(Val): 0.4383; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2689/8192 --- L(Train): 0.3726; L(Val): 0.4372; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2690/8192 --- L(Train): 0.3814; L(Val): 0.4357; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2691/8192 --- L(Train): 0.3788; L(Val): 0.4345; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2692/8192 --- L(Train): 0.3768; L(Val): 0.4344; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2693/8192 --- L(Train): 0.3728; L(Val): 0.4346; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2694/8192 --- L(Train): 0.3814; L(Val): 0.4346; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2695/8192 --- L(Train): 0.3756; L(Val): 0.4346; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2696/8192 --- L(Train): 0.3605; L(Val): 0.4346; Reg Param: 0.0707; Time: 0.84\n",
      "Epoch 2697/8192 --- L(Train): 0.3714; L(Val): 0.4350; Reg Param: 0.0707; Time: 0.71\n",
      "Epoch 2698/8192 --- L(Train): 0.3853; L(Val): 0.4351; Reg Param: 0.0707; Time: 0.70\n",
      "Epoch 2699/8192 --- L(Train): 0.3688; L(Val): 0.4352; Reg Param: 0.0707; Time: 0.73\n",
      "Epoch 2700/8192 --- L(Train): 0.3806; L(Val): 0.4352; Reg Param: 0.0706; Time: 7.37\n",
      "Epoch 2701/8192 --- L(Train): 0.3825; L(Val): 0.4345; Reg Param: 0.0706; Time: 0.70\n",
      "Epoch 2702/8192 --- L(Train): 0.3821; L(Val): 0.4346; Reg Param: 0.0706; Time: 0.70\n",
      "Epoch 2703/8192 --- L(Train): 0.3823; L(Val): 0.4340; Reg Param: 0.0706; Time: 0.71\n",
      "Epoch 2704/8192 --- L(Train): 0.3692; L(Val): 0.4339; Reg Param: 0.0706; Time: 0.71\n",
      "Epoch 2705/8192 --- L(Train): 0.3887; L(Val): 0.4341; Reg Param: 0.0706; Time: 0.71\n",
      "Epoch 2706/8192 --- L(Train): 0.3790; L(Val): 0.4342; Reg Param: 0.0706; Time: 0.70\n",
      "Epoch 2707/8192 --- L(Train): 0.3813; L(Val): 0.4348; Reg Param: 0.0706; Time: 0.69\n",
      "Epoch 2708/8192 --- L(Train): 0.3713; L(Val): 0.4354; Reg Param: 0.0706; Time: 0.69\n",
      "Epoch 2709/8192 --- L(Train): 0.3783; L(Val): 0.4354; Reg Param: 0.0706; Time: 0.70\n",
      "Epoch 2710/8192 --- L(Train): 0.3879; L(Val): 0.4351; Reg Param: 0.0706; Time: 0.71\n",
      "Epoch 2711/8192 --- L(Train): 0.3958; L(Val): 0.4350; Reg Param: 0.0706; Time: 0.71\n",
      "Epoch 2712/8192 --- L(Train): 0.3808; L(Val): 0.4350; Reg Param: 0.0706; Time: 0.71\n",
      "Epoch 2713/8192 --- L(Train): 0.3856; L(Val): 0.4348; Reg Param: 0.0706; Time: 0.70\n",
      "Epoch 2714/8192 --- L(Train): 0.3719; L(Val): 0.4343; Reg Param: 0.0706; Time: 0.71\n",
      "Epoch 2715/8192 --- L(Train): 0.3691; L(Val): 0.4339; Reg Param: 0.0706; Time: 0.69\n",
      "Epoch 2716/8192 --- L(Train): 0.3783; L(Val): 0.4338; Reg Param: 0.0706; Time: 0.74\n",
      "Epoch 2717/8192 --- L(Train): 0.3735; L(Val): 0.4342; Reg Param: 0.0706; Time: 0.86\n",
      "Epoch 2718/8192 --- L(Train): 0.3728; L(Val): 0.4350; Reg Param: 0.0706; Time: 0.74\n",
      "Epoch 2719/8192 --- L(Train): 0.3722; L(Val): 0.4357; Reg Param: 0.0706; Time: 0.68\n",
      "Epoch 2720/8192 --- L(Train): 0.3706; L(Val): 0.4361; Reg Param: 0.0706; Time: 0.70\n",
      "Epoch 2721/8192 --- L(Train): 0.3787; L(Val): 0.4363; Reg Param: 0.0706; Time: 0.69\n",
      "Epoch 2722/8192 --- L(Train): 0.3783; L(Val): 0.4365; Reg Param: 0.0706; Time: 0.70\n",
      "Epoch 2723/8192 --- L(Train): 0.3848; L(Val): 0.4366; Reg Param: 0.0706; Time: 0.71\n",
      "Epoch 2724/8192 --- L(Train): 0.3798; L(Val): 0.4366; Reg Param: 0.0706; Time: 0.94\n",
      "Epoch 2725/8192 --- L(Train): 0.3669; L(Val): 0.4367; Reg Param: 0.0706; Time: 1.35\n",
      "Epoch 2726/8192 --- L(Train): 0.3830; L(Val): 0.4370; Reg Param: 0.0706; Time: 0.80\n",
      "Epoch 2727/8192 --- L(Train): 0.3721; L(Val): 0.4373; Reg Param: 0.0706; Time: 0.73\n",
      "Epoch 2728/8192 --- L(Train): 0.3705; L(Val): 0.4380; Reg Param: 0.0706; Time: 0.72\n",
      "Epoch 2729/8192 --- L(Train): 0.3839; L(Val): 0.4383; Reg Param: 0.0706; Time: 1.72\n",
      "Epoch 2730/8192 --- L(Train): 0.3811; L(Val): 0.4379; Reg Param: 0.0706; Time: 0.87\n",
      "Epoch 2731/8192 --- L(Train): 0.3738; L(Val): 0.4377; Reg Param: 0.0706; Time: 1.09\n",
      "Epoch 2732/8192 --- L(Train): 0.3701; L(Val): 0.4373; Reg Param: 0.0706; Time: 0.87\n",
      "Epoch 2733/8192 --- L(Train): 0.3690; L(Val): 0.4376; Reg Param: 0.0706; Time: 0.98\n",
      "Epoch 2734/8192 --- L(Train): 0.3654; L(Val): 0.4381; Reg Param: 0.0706; Time: 0.98\n",
      "Epoch 2735/8192 --- L(Train): 0.3704; L(Val): 0.4374; Reg Param: 0.0706; Time: 0.78\n",
      "Epoch 2736/8192 --- L(Train): 0.3825; L(Val): 0.4365; Reg Param: 0.0706; Time: 0.76\n",
      "Epoch 2737/8192 --- L(Train): 0.3781; L(Val): 0.4364; Reg Param: 0.0706; Time: 0.78\n",
      "Epoch 2738/8192 --- L(Train): 0.3785; L(Val): 0.4366; Reg Param: 0.0706; Time: 0.77\n",
      "Epoch 2739/8192 --- L(Train): 0.3814; L(Val): 0.4373; Reg Param: 0.0706; Time: 0.75\n",
      "Epoch 2740/8192 --- L(Train): 0.3702; L(Val): 0.4384; Reg Param: 0.0706; Time: 0.77\n",
      "Epoch 2741/8192 --- L(Train): 0.3712; L(Val): 0.4385; Reg Param: 0.0706; Time: 0.78\n",
      "Epoch 2742/8192 --- L(Train): 0.3759; L(Val): 0.4384; Reg Param: 0.0706; Time: 0.73\n",
      "Epoch 2743/8192 --- L(Train): 0.3762; L(Val): 0.4387; Reg Param: 0.0706; Time: 0.74\n",
      "Epoch 2744/8192 --- L(Train): 0.3739; L(Val): 0.4387; Reg Param: 0.0706; Time: 0.85\n",
      "Epoch 2745/8192 --- L(Train): 0.3727; L(Val): 0.4393; Reg Param: 0.0706; Time: 0.75\n",
      "Epoch 2746/8192 --- L(Train): 0.3683; L(Val): 0.4401; Reg Param: 0.0706; Time: 0.82\n",
      "Epoch 2747/8192 --- L(Train): 0.3898; L(Val): 0.4392; Reg Param: 0.0706; Time: 0.75\n",
      "Epoch 2748/8192 --- L(Train): 0.3741; L(Val): 0.4386; Reg Param: 0.0706; Time: 0.75\n",
      "Epoch 2749/8192 --- L(Train): 0.3688; L(Val): 0.4383; Reg Param: 0.0706; Time: 0.74\n",
      "Epoch 2750/8192 --- L(Train): 0.3804; L(Val): 0.4383; Reg Param: 0.0704; Time: 8.13\n",
      "Epoch 2751/8192 --- L(Train): 0.3722; L(Val): 0.4388; Reg Param: 0.0704; Time: 0.81\n",
      "Epoch 2752/8192 --- L(Train): 0.3778; L(Val): 0.4385; Reg Param: 0.0704; Time: 0.93\n",
      "Epoch 2753/8192 --- L(Train): 0.3895; L(Val): 0.4378; Reg Param: 0.0704; Time: 0.77\n",
      "Epoch 2754/8192 --- L(Train): 0.3818; L(Val): 0.4374; Reg Param: 0.0704; Time: 0.74\n",
      "Epoch 2755/8192 --- L(Train): 0.3711; L(Val): 0.4373; Reg Param: 0.0704; Time: 0.83\n",
      "Epoch 2756/8192 --- L(Train): 0.3729; L(Val): 0.4375; Reg Param: 0.0704; Time: 0.97\n",
      "Epoch 2757/8192 --- L(Train): 0.3783; L(Val): 0.4378; Reg Param: 0.0704; Time: 0.80\n",
      "Epoch 2758/8192 --- L(Train): 0.3541; L(Val): 0.4379; Reg Param: 0.0704; Time: 0.78\n",
      "Epoch 2759/8192 --- L(Train): 0.3665; L(Val): 0.4377; Reg Param: 0.0704; Time: 0.77\n",
      "Epoch 2760/8192 --- L(Train): 0.3737; L(Val): 0.4376; Reg Param: 0.0704; Time: 0.82\n",
      "Epoch 2761/8192 --- L(Train): 0.3841; L(Val): 0.4380; Reg Param: 0.0704; Time: 0.79\n",
      "Epoch 2762/8192 --- L(Train): 0.3731; L(Val): 0.4381; Reg Param: 0.0704; Time: 0.87\n",
      "Epoch 2763/8192 --- L(Train): 0.3709; L(Val): 0.4382; Reg Param: 0.0704; Time: 0.76\n",
      "Epoch 2764/8192 --- L(Train): 0.3706; L(Val): 0.4383; Reg Param: 0.0704; Time: 0.76\n",
      "Epoch 2765/8192 --- L(Train): 0.3771; L(Val): 0.4384; Reg Param: 0.0704; Time: 0.76\n",
      "Epoch 2766/8192 --- L(Train): 0.3760; L(Val): 0.4381; Reg Param: 0.0704; Time: 0.78\n",
      "Epoch 2767/8192 --- L(Train): 0.3690; L(Val): 0.4377; Reg Param: 0.0704; Time: 0.74\n",
      "Epoch 2768/8192 --- L(Train): 0.3785; L(Val): 0.4375; Reg Param: 0.0704; Time: 0.76\n",
      "Epoch 2769/8192 --- L(Train): 0.3739; L(Val): 0.4374; Reg Param: 0.0704; Time: 0.78\n",
      "Epoch 2770/8192 --- L(Train): 0.3837; L(Val): 0.4369; Reg Param: 0.0704; Time: 0.82\n",
      "Epoch 2771/8192 --- L(Train): 0.3742; L(Val): 0.4363; Reg Param: 0.0704; Time: 0.79\n",
      "Epoch 2772/8192 --- L(Train): 0.3834; L(Val): 0.4361; Reg Param: 0.0704; Time: 0.79\n",
      "Epoch 2773/8192 --- L(Train): 0.3777; L(Val): 0.4361; Reg Param: 0.0704; Time: 0.76\n",
      "Epoch 2774/8192 --- L(Train): 0.3745; L(Val): 0.4361; Reg Param: 0.0704; Time: 0.76\n",
      "Epoch 2775/8192 --- L(Train): 0.3754; L(Val): 0.4362; Reg Param: 0.0704; Time: 0.84\n",
      "Epoch 2776/8192 --- L(Train): 0.3740; L(Val): 0.4360; Reg Param: 0.0704; Time: 0.75\n",
      "Epoch 2777/8192 --- L(Train): 0.3856; L(Val): 0.4359; Reg Param: 0.0704; Time: 0.75\n",
      "Epoch 2778/8192 --- L(Train): 0.3748; L(Val): 0.4360; Reg Param: 0.0704; Time: 0.83\n",
      "Epoch 2779/8192 --- L(Train): 0.3783; L(Val): 0.4362; Reg Param: 0.0704; Time: 0.77\n",
      "Epoch 2780/8192 --- L(Train): 0.3726; L(Val): 0.4365; Reg Param: 0.0704; Time: 0.82\n",
      "Epoch 2781/8192 --- L(Train): 0.3801; L(Val): 0.4370; Reg Param: 0.0704; Time: 0.91\n",
      "Epoch 2782/8192 --- L(Train): 0.3757; L(Val): 0.4384; Reg Param: 0.0704; Time: 1.00\n",
      "Epoch 2783/8192 --- L(Train): 0.3721; L(Val): 0.4385; Reg Param: 0.0704; Time: 0.87\n",
      "Epoch 2784/8192 --- L(Train): 0.3886; L(Val): 0.4385; Reg Param: 0.0704; Time: 0.81\n",
      "Epoch 2785/8192 --- L(Train): 0.3776; L(Val): 0.4385; Reg Param: 0.0704; Time: 0.78\n",
      "Epoch 2786/8192 --- L(Train): 0.3779; L(Val): 0.4392; Reg Param: 0.0704; Time: 0.77\n",
      "Epoch 2787/8192 --- L(Train): 0.3765; L(Val): 0.4399; Reg Param: 0.0704; Time: 0.76\n",
      "Epoch 2788/8192 --- L(Train): 0.3722; L(Val): 0.4404; Reg Param: 0.0704; Time: 0.71\n",
      "Epoch 2789/8192 --- L(Train): 0.3756; L(Val): 0.4411; Reg Param: 0.0704; Time: 0.70\n",
      "Epoch 2790/8192 --- L(Train): 0.3721; L(Val): 0.4415; Reg Param: 0.0704; Time: 0.90\n",
      "Epoch 2791/8192 --- L(Train): 0.3785; L(Val): 0.4415; Reg Param: 0.0704; Time: 0.79\n",
      "Epoch 2792/8192 --- L(Train): 0.3692; L(Val): 0.4410; Reg Param: 0.0704; Time: 0.71\n",
      "Epoch 2793/8192 --- L(Train): 0.3724; L(Val): 0.4402; Reg Param: 0.0704; Time: 0.71\n",
      "Epoch 2794/8192 --- L(Train): 0.3765; L(Val): 0.4395; Reg Param: 0.0704; Time: 0.70\n",
      "Epoch 2795/8192 --- L(Train): 0.3880; L(Val): 0.4392; Reg Param: 0.0704; Time: 0.77\n",
      "Epoch 2796/8192 --- L(Train): 0.3753; L(Val): 0.4397; Reg Param: 0.0704; Time: 0.69\n",
      "Epoch 2797/8192 --- L(Train): 0.3786; L(Val): 0.4400; Reg Param: 0.0704; Time: 0.76\n",
      "Epoch 2798/8192 --- L(Train): 0.3838; L(Val): 0.4399; Reg Param: 0.0704; Time: 0.87\n",
      "Epoch 2799/8192 --- L(Train): 0.3754; L(Val): 0.4401; Reg Param: 0.0704; Time: 0.76\n",
      "Epoch 2800/8192 --- L(Train): 0.3659; L(Val): 0.4401; Reg Param: 0.0701; Time: 8.43\n",
      "Epoch 2801/8192 --- L(Train): 0.3830; L(Val): 0.4407; Reg Param: 0.0701; Time: 0.76\n",
      "Epoch 2802/8192 --- L(Train): 0.3844; L(Val): 0.4407; Reg Param: 0.0701; Time: 0.77\n",
      "Epoch 2803/8192 --- L(Train): 0.3688; L(Val): 0.4401; Reg Param: 0.0701; Time: 0.76\n",
      "Epoch 2804/8192 --- L(Train): 0.3741; L(Val): 0.4393; Reg Param: 0.0701; Time: 0.74\n",
      "Epoch 2805/8192 --- L(Train): 0.3747; L(Val): 0.4383; Reg Param: 0.0701; Time: 0.76\n",
      "Epoch 2806/8192 --- L(Train): 0.3743; L(Val): 0.4376; Reg Param: 0.0701; Time: 0.73\n",
      "Epoch 2807/8192 --- L(Train): 0.3770; L(Val): 0.4374; Reg Param: 0.0701; Time: 0.74\n",
      "Epoch 2808/8192 --- L(Train): 0.3654; L(Val): 0.4373; Reg Param: 0.0701; Time: 0.74\n",
      "Epoch 2809/8192 --- L(Train): 0.3646; L(Val): 0.4373; Reg Param: 0.0701; Time: 0.84\n",
      "Epoch 2810/8192 --- L(Train): 0.3813; L(Val): 0.4369; Reg Param: 0.0701; Time: 0.76\n",
      "Epoch 2811/8192 --- L(Train): 0.3779; L(Val): 0.4367; Reg Param: 0.0701; Time: 0.71\n",
      "Epoch 2812/8192 --- L(Train): 0.3799; L(Val): 0.4371; Reg Param: 0.0701; Time: 0.69\n",
      "Epoch 2813/8192 --- L(Train): 0.3711; L(Val): 0.4376; Reg Param: 0.0701; Time: 0.68\n",
      "Epoch 2814/8192 --- L(Train): 0.3616; L(Val): 0.4379; Reg Param: 0.0701; Time: 0.77\n",
      "Epoch 2815/8192 --- L(Train): 0.3806; L(Val): 0.4379; Reg Param: 0.0701; Time: 0.71\n",
      "Epoch 2816/8192 --- L(Train): 0.3706; L(Val): 0.4376; Reg Param: 0.0701; Time: 0.70\n",
      "Epoch 2817/8192 --- L(Train): 0.3816; L(Val): 0.4373; Reg Param: 0.0701; Time: 0.81\n",
      "Epoch 2818/8192 --- L(Train): 0.3705; L(Val): 0.4367; Reg Param: 0.0701; Time: 0.74\n",
      "Epoch 2819/8192 --- L(Train): 0.3725; L(Val): 0.4365; Reg Param: 0.0701; Time: 0.80\n",
      "Epoch 2820/8192 --- L(Train): 0.3719; L(Val): 0.4359; Reg Param: 0.0701; Time: 0.78\n",
      "Epoch 2821/8192 --- L(Train): 0.3815; L(Val): 0.4357; Reg Param: 0.0701; Time: 0.82\n",
      "Epoch 2822/8192 --- L(Train): 0.3897; L(Val): 0.4358; Reg Param: 0.0701; Time: 0.87\n",
      "Epoch 2823/8192 --- L(Train): 0.3891; L(Val): 0.4359; Reg Param: 0.0701; Time: 1.05\n",
      "Epoch 2824/8192 --- L(Train): 0.3731; L(Val): 0.4366; Reg Param: 0.0701; Time: 0.79\n",
      "Epoch 2825/8192 --- L(Train): 0.3830; L(Val): 0.4373; Reg Param: 0.0701; Time: 0.79\n",
      "Epoch 2826/8192 --- L(Train): 0.3706; L(Val): 0.4375; Reg Param: 0.0701; Time: 0.79\n",
      "Epoch 2827/8192 --- L(Train): 0.3678; L(Val): 0.4374; Reg Param: 0.0701; Time: 0.80\n",
      "Epoch 2828/8192 --- L(Train): 0.3719; L(Val): 0.4380; Reg Param: 0.0701; Time: 0.78\n",
      "Epoch 2829/8192 --- L(Train): 0.3572; L(Val): 0.4384; Reg Param: 0.0701; Time: 0.88\n",
      "Epoch 2830/8192 --- L(Train): 0.3680; L(Val): 0.4384; Reg Param: 0.0701; Time: 0.77\n",
      "Epoch 2831/8192 --- L(Train): 0.3861; L(Val): 0.4384; Reg Param: 0.0701; Time: 0.76\n",
      "Epoch 2832/8192 --- L(Train): 0.3832; L(Val): 0.4383; Reg Param: 0.0701; Time: 0.77\n",
      "Epoch 2833/8192 --- L(Train): 0.3764; L(Val): 0.4380; Reg Param: 0.0701; Time: 0.79\n",
      "Epoch 2834/8192 --- L(Train): 0.3646; L(Val): 0.4380; Reg Param: 0.0701; Time: 0.79\n",
      "Epoch 2835/8192 --- L(Train): 0.3766; L(Val): 0.4379; Reg Param: 0.0701; Time: 0.78\n",
      "Epoch 2836/8192 --- L(Train): 0.3797; L(Val): 0.4379; Reg Param: 0.0701; Time: 0.77\n",
      "Epoch 2837/8192 --- L(Train): 0.3768; L(Val): 0.4375; Reg Param: 0.0701; Time: 0.79\n",
      "Epoch 2838/8192 --- L(Train): 0.3735; L(Val): 0.4368; Reg Param: 0.0701; Time: 0.83\n",
      "Epoch 2839/8192 --- L(Train): 0.3712; L(Val): 0.4365; Reg Param: 0.0701; Time: 0.80\n",
      "Epoch 2840/8192 --- L(Train): 0.3726; L(Val): 0.4365; Reg Param: 0.0701; Time: 0.78\n",
      "Epoch 2841/8192 --- L(Train): 0.3633; L(Val): 0.4368; Reg Param: 0.0701; Time: 0.76\n",
      "Epoch 2842/8192 --- L(Train): 0.3809; L(Val): 0.4373; Reg Param: 0.0701; Time: 0.87\n",
      "Epoch 2843/8192 --- L(Train): 0.3743; L(Val): 0.4372; Reg Param: 0.0701; Time: 0.76\n",
      "Epoch 2844/8192 --- L(Train): 0.3716; L(Val): 0.4373; Reg Param: 0.0701; Time: 0.78\n",
      "Epoch 2845/8192 --- L(Train): 0.3714; L(Val): 0.4377; Reg Param: 0.0701; Time: 0.79\n",
      "Epoch 2846/8192 --- L(Train): 0.3769; L(Val): 0.4379; Reg Param: 0.0701; Time: 0.76\n",
      "Epoch 2847/8192 --- L(Train): 0.3794; L(Val): 0.4376; Reg Param: 0.0701; Time: 0.74\n",
      "Epoch 2848/8192 --- L(Train): 0.3758; L(Val): 0.4374; Reg Param: 0.0701; Time: 0.77\n",
      "Epoch 2849/8192 --- L(Train): 0.3721; L(Val): 0.4371; Reg Param: 0.0701; Time: 0.75\n",
      "Epoch 2850/8192 --- L(Train): 0.3776; L(Val): 0.4371; Reg Param: 0.0699; Time: 7.87\n",
      "Epoch 2851/8192 --- L(Train): 0.3718; L(Val): 0.4373; Reg Param: 0.0699; Time: 0.71\n",
      "Epoch 2852/8192 --- L(Train): 0.3712; L(Val): 0.4376; Reg Param: 0.0699; Time: 0.73\n",
      "Epoch 2853/8192 --- L(Train): 0.3730; L(Val): 0.4381; Reg Param: 0.0699; Time: 0.73\n",
      "Epoch 2854/8192 --- L(Train): 0.3757; L(Val): 0.4378; Reg Param: 0.0699; Time: 0.70\n",
      "Epoch 2855/8192 --- L(Train): 0.3691; L(Val): 0.4380; Reg Param: 0.0699; Time: 0.72\n",
      "Epoch 2856/8192 --- L(Train): 0.3758; L(Val): 0.4381; Reg Param: 0.0699; Time: 0.93\n",
      "Epoch 2857/8192 --- L(Train): 0.3638; L(Val): 0.4383; Reg Param: 0.0699; Time: 0.85\n",
      "Epoch 2858/8192 --- L(Train): 0.3790; L(Val): 0.4386; Reg Param: 0.0699; Time: 0.78\n",
      "Epoch 2859/8192 --- L(Train): 0.3794; L(Val): 0.4383; Reg Param: 0.0699; Time: 0.78\n",
      "Epoch 2860/8192 --- L(Train): 0.3706; L(Val): 0.4378; Reg Param: 0.0699; Time: 0.85\n",
      "Epoch 2861/8192 --- L(Train): 0.3662; L(Val): 0.4373; Reg Param: 0.0699; Time: 0.80\n",
      "Epoch 2862/8192 --- L(Train): 0.3734; L(Val): 0.4374; Reg Param: 0.0699; Time: 0.81\n",
      "Epoch 2863/8192 --- L(Train): 0.3736; L(Val): 0.4376; Reg Param: 0.0699; Time: 0.77\n",
      "Epoch 2864/8192 --- L(Train): 0.3592; L(Val): 0.4377; Reg Param: 0.0699; Time: 0.87\n",
      "Epoch 2865/8192 --- L(Train): 0.3691; L(Val): 0.4377; Reg Param: 0.0699; Time: 0.78\n",
      "Epoch 2866/8192 --- L(Train): 0.3786; L(Val): 0.4375; Reg Param: 0.0699; Time: 0.80\n",
      "Epoch 2867/8192 --- L(Train): 0.3749; L(Val): 0.4374; Reg Param: 0.0699; Time: 0.74\n",
      "Epoch 2868/8192 --- L(Train): 0.3793; L(Val): 0.4370; Reg Param: 0.0699; Time: 0.75\n",
      "Epoch 2869/8192 --- L(Train): 0.3720; L(Val): 0.4369; Reg Param: 0.0699; Time: 0.77\n",
      "Epoch 2870/8192 --- L(Train): 0.3872; L(Val): 0.4367; Reg Param: 0.0699; Time: 0.78\n",
      "Epoch 2871/8192 --- L(Train): 0.3807; L(Val): 0.4357; Reg Param: 0.0699; Time: 0.77\n",
      "Epoch 2872/8192 --- L(Train): 0.3743; L(Val): 0.4353; Reg Param: 0.0699; Time: 0.76\n",
      "Epoch 2873/8192 --- L(Train): 0.3813; L(Val): 0.4353; Reg Param: 0.0699; Time: 0.73\n",
      "Epoch 2874/8192 --- L(Train): 0.3760; L(Val): 0.4358; Reg Param: 0.0699; Time: 0.70\n",
      "Epoch 2875/8192 --- L(Train): 0.3762; L(Val): 0.4367; Reg Param: 0.0699; Time: 0.87\n",
      "Epoch 2876/8192 --- L(Train): 0.3692; L(Val): 0.4371; Reg Param: 0.0699; Time: 0.75\n",
      "Epoch 2877/8192 --- L(Train): 0.3656; L(Val): 0.4367; Reg Param: 0.0699; Time: 0.73\n",
      "Epoch 2878/8192 --- L(Train): 0.3609; L(Val): 0.4362; Reg Param: 0.0699; Time: 0.73\n",
      "Epoch 2879/8192 --- L(Train): 0.3750; L(Val): 0.4359; Reg Param: 0.0699; Time: 0.74\n",
      "Epoch 2880/8192 --- L(Train): 0.3794; L(Val): 0.4361; Reg Param: 0.0699; Time: 0.75\n",
      "Epoch 2881/8192 --- L(Train): 0.3777; L(Val): 0.4371; Reg Param: 0.0699; Time: 0.68\n",
      "Epoch 2882/8192 --- L(Train): 0.3748; L(Val): 0.4376; Reg Param: 0.0699; Time: 0.77\n",
      "Epoch 2883/8192 --- L(Train): 0.3737; L(Val): 0.4378; Reg Param: 0.0699; Time: 0.73\n",
      "Epoch 2884/8192 --- L(Train): 0.3700; L(Val): 0.4373; Reg Param: 0.0699; Time: 0.70\n",
      "Epoch 2885/8192 --- L(Train): 0.3690; L(Val): 0.4370; Reg Param: 0.0699; Time: 0.70\n",
      "Epoch 2886/8192 --- L(Train): 0.3827; L(Val): 0.4367; Reg Param: 0.0699; Time: 0.71\n",
      "Epoch 2887/8192 --- L(Train): 0.3832; L(Val): 0.4366; Reg Param: 0.0699; Time: 0.70\n",
      "Epoch 2888/8192 --- L(Train): 0.3656; L(Val): 0.4369; Reg Param: 0.0699; Time: 0.74\n",
      "Epoch 2889/8192 --- L(Train): 0.3758; L(Val): 0.4368; Reg Param: 0.0699; Time: 0.70\n",
      "Epoch 2890/8192 --- L(Train): 0.3757; L(Val): 0.4366; Reg Param: 0.0699; Time: 0.70\n",
      "Epoch 2891/8192 --- L(Train): 0.3754; L(Val): 0.4364; Reg Param: 0.0699; Time: 0.77\n",
      "Epoch 2892/8192 --- L(Train): 0.3662; L(Val): 0.4364; Reg Param: 0.0699; Time: 0.70\n",
      "Epoch 2893/8192 --- L(Train): 0.3740; L(Val): 0.4361; Reg Param: 0.0699; Time: 0.71\n",
      "Epoch 2894/8192 --- L(Train): 0.3690; L(Val): 0.4363; Reg Param: 0.0699; Time: 0.70\n",
      "Epoch 2895/8192 --- L(Train): 0.3779; L(Val): 0.4363; Reg Param: 0.0699; Time: 0.68\n",
      "Epoch 2896/8192 --- L(Train): 0.3874; L(Val): 0.4362; Reg Param: 0.0699; Time: 0.71\n",
      "Epoch 2897/8192 --- L(Train): 0.3652; L(Val): 0.4363; Reg Param: 0.0699; Time: 0.73\n",
      "Epoch 2898/8192 --- L(Train): 0.3781; L(Val): 0.4368; Reg Param: 0.0699; Time: 0.70\n",
      "Epoch 2899/8192 --- L(Train): 0.3827; L(Val): 0.4378; Reg Param: 0.0699; Time: 0.76\n",
      "Epoch 2900/8192 --- L(Train): 0.3863; L(Val): 0.4378; Reg Param: 0.0695; Time: 7.84\n",
      "Epoch 2901/8192 --- L(Train): 0.3730; L(Val): 0.4389; Reg Param: 0.0695; Time: 0.76\n",
      "Epoch 2902/8192 --- L(Train): 0.3675; L(Val): 0.4386; Reg Param: 0.0695; Time: 0.74\n",
      "Epoch 2903/8192 --- L(Train): 0.3687; L(Val): 0.4384; Reg Param: 0.0695; Time: 0.73\n",
      "Epoch 2904/8192 --- L(Train): 0.3705; L(Val): 0.4382; Reg Param: 0.0695; Time: 0.74\n",
      "Epoch 2905/8192 --- L(Train): 0.3870; L(Val): 0.4377; Reg Param: 0.0695; Time: 0.73\n",
      "Epoch 2906/8192 --- L(Train): 0.3697; L(Val): 0.4374; Reg Param: 0.0695; Time: 0.74\n",
      "Epoch 2907/8192 --- L(Train): 0.3829; L(Val): 0.4372; Reg Param: 0.0695; Time: 0.67\n",
      "Epoch 2908/8192 --- L(Train): 0.3623; L(Val): 0.4367; Reg Param: 0.0695; Time: 0.67\n",
      "Epoch 2909/8192 --- L(Train): 0.3826; L(Val): 0.4365; Reg Param: 0.0695; Time: 0.67\n",
      "Epoch 2910/8192 --- L(Train): 0.3732; L(Val): 0.4365; Reg Param: 0.0695; Time: 0.67\n",
      "Epoch 2911/8192 --- L(Train): 0.3772; L(Val): 0.4368; Reg Param: 0.0695; Time: 0.69\n",
      "Epoch 2912/8192 --- L(Train): 0.3712; L(Val): 0.4368; Reg Param: 0.0695; Time: 0.69\n",
      "Epoch 2913/8192 --- L(Train): 0.3803; L(Val): 0.4367; Reg Param: 0.0695; Time: 0.72\n",
      "Epoch 2914/8192 --- L(Train): 0.3889; L(Val): 0.4375; Reg Param: 0.0695; Time: 0.71\n",
      "Epoch 2915/8192 --- L(Train): 0.3759; L(Val): 0.4380; Reg Param: 0.0695; Time: 0.69\n",
      "Epoch 2916/8192 --- L(Train): 0.3838; L(Val): 0.4384; Reg Param: 0.0695; Time: 0.70\n",
      "Epoch 2917/8192 --- L(Train): 0.3779; L(Val): 0.4386; Reg Param: 0.0695; Time: 0.77\n",
      "Epoch 2918/8192 --- L(Train): 0.3713; L(Val): 0.4385; Reg Param: 0.0695; Time: 0.70\n",
      "Epoch 2919/8192 --- L(Train): 0.3714; L(Val): 0.4385; Reg Param: 0.0695; Time: 0.75\n",
      "Epoch 2920/8192 --- L(Train): 0.3926; L(Val): 0.4385; Reg Param: 0.0695; Time: 0.77\n",
      "Epoch 2921/8192 --- L(Train): 0.3756; L(Val): 0.4387; Reg Param: 0.0695; Time: 0.76\n",
      "Epoch 2922/8192 --- L(Train): 0.3732; L(Val): 0.4389; Reg Param: 0.0695; Time: 0.78\n",
      "Epoch 2923/8192 --- L(Train): 0.3693; L(Val): 0.4394; Reg Param: 0.0695; Time: 0.74\n",
      "Epoch 2924/8192 --- L(Train): 0.3624; L(Val): 0.4397; Reg Param: 0.0695; Time: 0.70\n",
      "Epoch 2925/8192 --- L(Train): 0.3711; L(Val): 0.4396; Reg Param: 0.0695; Time: 0.72\n",
      "Epoch 2926/8192 --- L(Train): 0.3807; L(Val): 0.4397; Reg Param: 0.0695; Time: 0.75\n",
      "Epoch 2927/8192 --- L(Train): 0.3787; L(Val): 0.4397; Reg Param: 0.0695; Time: 0.73\n",
      "Epoch 2928/8192 --- L(Train): 0.3814; L(Val): 0.4401; Reg Param: 0.0695; Time: 0.74\n",
      "Epoch 2929/8192 --- L(Train): 0.3812; L(Val): 0.4401; Reg Param: 0.0695; Time: 0.84\n",
      "Epoch 2930/8192 --- L(Train): 0.3864; L(Val): 0.4395; Reg Param: 0.0695; Time: 0.77\n",
      "Epoch 2931/8192 --- L(Train): 0.3736; L(Val): 0.4389; Reg Param: 0.0695; Time: 0.73\n",
      "Epoch 2932/8192 --- L(Train): 0.3787; L(Val): 0.4388; Reg Param: 0.0695; Time: 0.72\n",
      "Epoch 2933/8192 --- L(Train): 0.3843; L(Val): 0.4387; Reg Param: 0.0695; Time: 0.71\n",
      "Epoch 2934/8192 --- L(Train): 0.3859; L(Val): 0.4390; Reg Param: 0.0695; Time: 0.71\n",
      "Epoch 2935/8192 --- L(Train): 0.3706; L(Val): 0.4394; Reg Param: 0.0695; Time: 0.74\n",
      "Epoch 2936/8192 --- L(Train): 0.3712; L(Val): 0.4397; Reg Param: 0.0695; Time: 0.70\n",
      "Epoch 2937/8192 --- L(Train): 0.3626; L(Val): 0.4396; Reg Param: 0.0695; Time: 0.70\n",
      "Epoch 2938/8192 --- L(Train): 0.3725; L(Val): 0.4398; Reg Param: 0.0695; Time: 0.69\n",
      "Epoch 2939/8192 --- L(Train): 0.3839; L(Val): 0.4398; Reg Param: 0.0695; Time: 0.72\n",
      "Epoch 2940/8192 --- L(Train): 0.3693; L(Val): 0.4401; Reg Param: 0.0695; Time: 0.69\n",
      "Epoch 2941/8192 --- L(Train): 0.3698; L(Val): 0.4403; Reg Param: 0.0695; Time: 0.68\n",
      "Epoch 2942/8192 --- L(Train): 0.3774; L(Val): 0.4400; Reg Param: 0.0695; Time: 0.75\n",
      "Epoch 2943/8192 --- L(Train): 0.3738; L(Val): 0.4401; Reg Param: 0.0695; Time: 0.69\n",
      "Epoch 2944/8192 --- L(Train): 0.3693; L(Val): 0.4400; Reg Param: 0.0695; Time: 0.68\n",
      "Epoch 2945/8192 --- L(Train): 0.3733; L(Val): 0.4395; Reg Param: 0.0695; Time: 0.69\n",
      "Epoch 2946/8192 --- L(Train): 0.3758; L(Val): 0.4390; Reg Param: 0.0695; Time: 0.70\n",
      "Epoch 2947/8192 --- L(Train): 0.3653; L(Val): 0.4387; Reg Param: 0.0695; Time: 0.68\n",
      "Epoch 2948/8192 --- L(Train): 0.3723; L(Val): 0.4390; Reg Param: 0.0695; Time: 0.67\n",
      "Epoch 2949/8192 --- L(Train): 0.3716; L(Val): 0.4391; Reg Param: 0.0695; Time: 0.67\n",
      "Epoch 2950/8192 --- L(Train): 0.3755; L(Val): 0.4391; Reg Param: 0.0691; Time: 7.34\n",
      "Epoch 2951/8192 --- L(Train): 0.3771; L(Val): 0.4394; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2952/8192 --- L(Train): 0.3635; L(Val): 0.4394; Reg Param: 0.0691; Time: 0.69\n",
      "Epoch 2953/8192 --- L(Train): 0.3678; L(Val): 0.4388; Reg Param: 0.0691; Time: 0.72\n",
      "Epoch 2954/8192 --- L(Train): 0.3752; L(Val): 0.4384; Reg Param: 0.0691; Time: 0.85\n",
      "Epoch 2955/8192 --- L(Train): 0.3821; L(Val): 0.4383; Reg Param: 0.0691; Time: 0.71\n",
      "Epoch 2956/8192 --- L(Train): 0.3863; L(Val): 0.4384; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2957/8192 --- L(Train): 0.3774; L(Val): 0.4385; Reg Param: 0.0691; Time: 0.72\n",
      "Epoch 2958/8192 --- L(Train): 0.3767; L(Val): 0.4387; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2959/8192 --- L(Train): 0.3712; L(Val): 0.4386; Reg Param: 0.0691; Time: 0.71\n",
      "Epoch 2960/8192 --- L(Train): 0.3867; L(Val): 0.4387; Reg Param: 0.0691; Time: 0.81\n",
      "Epoch 2961/8192 --- L(Train): 0.3726; L(Val): 0.4386; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2962/8192 --- L(Train): 0.3645; L(Val): 0.4382; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2963/8192 --- L(Train): 0.3781; L(Val): 0.4378; Reg Param: 0.0691; Time: 0.69\n",
      "Epoch 2964/8192 --- L(Train): 0.3827; L(Val): 0.4375; Reg Param: 0.0691; Time: 0.78\n",
      "Epoch 2965/8192 --- L(Train): 0.3799; L(Val): 0.4372; Reg Param: 0.0691; Time: 0.71\n",
      "Epoch 2966/8192 --- L(Train): 0.3845; L(Val): 0.4368; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2967/8192 --- L(Train): 0.3923; L(Val): 0.4369; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2968/8192 --- L(Train): 0.3711; L(Val): 0.4371; Reg Param: 0.0691; Time: 0.72\n",
      "Epoch 2969/8192 --- L(Train): 0.3777; L(Val): 0.4375; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2970/8192 --- L(Train): 0.3696; L(Val): 0.4380; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2971/8192 --- L(Train): 0.3787; L(Val): 0.4383; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2972/8192 --- L(Train): 0.3846; L(Val): 0.4382; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2973/8192 --- L(Train): 0.3706; L(Val): 0.4375; Reg Param: 0.0691; Time: 0.69\n",
      "Epoch 2974/8192 --- L(Train): 0.3783; L(Val): 0.4369; Reg Param: 0.0691; Time: 0.68\n",
      "Epoch 2975/8192 --- L(Train): 0.3859; L(Val): 0.4360; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2976/8192 --- L(Train): 0.3746; L(Val): 0.4355; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2977/8192 --- L(Train): 0.3816; L(Val): 0.4356; Reg Param: 0.0691; Time: 0.71\n",
      "Epoch 2978/8192 --- L(Train): 0.3715; L(Val): 0.4359; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2979/8192 --- L(Train): 0.3743; L(Val): 0.4359; Reg Param: 0.0691; Time: 0.71\n",
      "Epoch 2980/8192 --- L(Train): 0.3806; L(Val): 0.4362; Reg Param: 0.0691; Time: 0.71\n",
      "Epoch 2981/8192 --- L(Train): 0.3658; L(Val): 0.4365; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2982/8192 --- L(Train): 0.3718; L(Val): 0.4369; Reg Param: 0.0691; Time: 0.84\n",
      "Epoch 2983/8192 --- L(Train): 0.3702; L(Val): 0.4373; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2984/8192 --- L(Train): 0.3758; L(Val): 0.4377; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2985/8192 --- L(Train): 0.3700; L(Val): 0.4379; Reg Param: 0.0691; Time: 0.72\n",
      "Epoch 2986/8192 --- L(Train): 0.3714; L(Val): 0.4382; Reg Param: 0.0691; Time: 0.71\n",
      "Epoch 2987/8192 --- L(Train): 0.3801; L(Val): 0.4383; Reg Param: 0.0691; Time: 0.71\n",
      "Epoch 2988/8192 --- L(Train): 0.3676; L(Val): 0.4382; Reg Param: 0.0691; Time: 0.69\n",
      "Epoch 2989/8192 --- L(Train): 0.3599; L(Val): 0.4378; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2990/8192 --- L(Train): 0.3827; L(Val): 0.4374; Reg Param: 0.0691; Time: 0.69\n",
      "Epoch 2991/8192 --- L(Train): 0.3893; L(Val): 0.4372; Reg Param: 0.0691; Time: 0.69\n",
      "Epoch 2992/8192 --- L(Train): 0.3792; L(Val): 0.4376; Reg Param: 0.0691; Time: 0.77\n",
      "Epoch 2993/8192 --- L(Train): 0.3848; L(Val): 0.4380; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2994/8192 --- L(Train): 0.3715; L(Val): 0.4385; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2995/8192 --- L(Train): 0.3790; L(Val): 0.4387; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2996/8192 --- L(Train): 0.3719; L(Val): 0.4388; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2997/8192 --- L(Train): 0.3712; L(Val): 0.4388; Reg Param: 0.0691; Time: 0.70\n",
      "Epoch 2998/8192 --- L(Train): 0.3825; L(Val): 0.4387; Reg Param: 0.0691; Time: 0.73\n",
      "Epoch 2999/8192 --- L(Train): 0.3700; L(Val): 0.4386; Reg Param: 0.0691; Time: 0.75\n",
      "Epoch 3000/8192 --- L(Train): 0.3751; L(Val): 0.4386; Reg Param: 0.0686; Time: 7.77\n",
      "Epoch 3001/8192 --- L(Train): 0.3710; L(Val): 0.4390; Reg Param: 0.0686; Time: 0.75\n",
      "Epoch 3002/8192 --- L(Train): 0.3622; L(Val): 0.4389; Reg Param: 0.0686; Time: 0.75\n",
      "Epoch 3003/8192 --- L(Train): 0.3654; L(Val): 0.4386; Reg Param: 0.0686; Time: 0.89\n",
      "Epoch 3004/8192 --- L(Train): 0.3744; L(Val): 0.4384; Reg Param: 0.0686; Time: 0.72\n",
      "Epoch 3005/8192 --- L(Train): 0.3723; L(Val): 0.4386; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3006/8192 --- L(Train): 0.3863; L(Val): 0.4388; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3007/8192 --- L(Train): 0.3762; L(Val): 0.4393; Reg Param: 0.0686; Time: 0.84\n",
      "Epoch 3008/8192 --- L(Train): 0.3684; L(Val): 0.4400; Reg Param: 0.0686; Time: 0.71\n",
      "Epoch 3009/8192 --- L(Train): 0.3694; L(Val): 0.4399; Reg Param: 0.0686; Time: 0.72\n",
      "Epoch 3010/8192 --- L(Train): 0.3799; L(Val): 0.4395; Reg Param: 0.0686; Time: 0.74\n",
      "Epoch 3011/8192 --- L(Train): 0.3723; L(Val): 0.4394; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3012/8192 --- L(Train): 0.3634; L(Val): 0.4393; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3013/8192 --- L(Train): 0.3732; L(Val): 0.4392; Reg Param: 0.0686; Time: 0.74\n",
      "Epoch 3014/8192 --- L(Train): 0.3641; L(Val): 0.4391; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3015/8192 --- L(Train): 0.3719; L(Val): 0.4396; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3016/8192 --- L(Train): 0.3793; L(Val): 0.4393; Reg Param: 0.0686; Time: 0.71\n",
      "Epoch 3017/8192 --- L(Train): 0.3713; L(Val): 0.4390; Reg Param: 0.0686; Time: 0.72\n",
      "Epoch 3018/8192 --- L(Train): 0.3718; L(Val): 0.4389; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3019/8192 --- L(Train): 0.3704; L(Val): 0.4388; Reg Param: 0.0686; Time: 0.71\n",
      "Epoch 3020/8192 --- L(Train): 0.3722; L(Val): 0.4387; Reg Param: 0.0686; Time: 0.74\n",
      "Epoch 3021/8192 --- L(Train): 0.3757; L(Val): 0.4388; Reg Param: 0.0686; Time: 0.75\n",
      "Epoch 3022/8192 --- L(Train): 0.3692; L(Val): 0.4386; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3023/8192 --- L(Train): 0.3725; L(Val): 0.4381; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3024/8192 --- L(Train): 0.3815; L(Val): 0.4378; Reg Param: 0.0686; Time: 0.72\n",
      "Epoch 3025/8192 --- L(Train): 0.3679; L(Val): 0.4376; Reg Param: 0.0686; Time: 0.82\n",
      "Epoch 3026/8192 --- L(Train): 0.3836; L(Val): 0.4376; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3027/8192 --- L(Train): 0.3781; L(Val): 0.4377; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3028/8192 --- L(Train): 0.3696; L(Val): 0.4382; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3029/8192 --- L(Train): 0.3667; L(Val): 0.4383; Reg Param: 0.0686; Time: 0.72\n",
      "Epoch 3030/8192 --- L(Train): 0.3696; L(Val): 0.4382; Reg Param: 0.0686; Time: 0.75\n",
      "Epoch 3031/8192 --- L(Train): 0.3687; L(Val): 0.4382; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3032/8192 --- L(Train): 0.3779; L(Val): 0.4382; Reg Param: 0.0686; Time: 0.73\n",
      "Epoch 3033/8192 --- L(Train): 0.3760; L(Val): 0.4382; Reg Param: 0.0686; Time: 0.80\n",
      "Epoch 3034/8192 --- L(Train): 0.3665; L(Val): 0.4381; Reg Param: 0.0686; Time: 0.81\n",
      "Epoch 3035/8192 --- L(Train): 0.3718; L(Val): 0.4379; Reg Param: 0.0686; Time: 0.77\n",
      "Epoch 3036/8192 --- L(Train): 0.3593; L(Val): 0.4380; Reg Param: 0.0686; Time: 0.83\n",
      "Epoch 3037/8192 --- L(Train): 0.3836; L(Val): 0.4381; Reg Param: 0.0686; Time: 0.77\n",
      "Epoch 3038/8192 --- L(Train): 0.3728; L(Val): 0.4379; Reg Param: 0.0686; Time: 0.74\n",
      "Epoch 3039/8192 --- L(Train): 0.3770; L(Val): 0.4378; Reg Param: 0.0686; Time: 0.72\n",
      "Epoch 3040/8192 --- L(Train): 0.3698; L(Val): 0.4379; Reg Param: 0.0686; Time: 0.70\n",
      "Epoch 3041/8192 --- L(Train): 0.3794; L(Val): 0.4380; Reg Param: 0.0686; Time: 0.92\n",
      "Epoch 3042/8192 --- L(Train): 0.3735; L(Val): 0.4379; Reg Param: 0.0686; Time: 0.76\n",
      "Epoch 3043/8192 --- L(Train): 0.3711; L(Val): 0.4382; Reg Param: 0.0686; Time: 1.14\n",
      "Epoch 3044/8192 --- L(Train): 0.3819; L(Val): 0.4385; Reg Param: 0.0686; Time: 0.79\n",
      "Epoch 3045/8192 --- L(Train): 0.3671; L(Val): 0.4387; Reg Param: 0.0686; Time: 0.71\n",
      "Epoch 3046/8192 --- L(Train): 0.3747; L(Val): 0.4391; Reg Param: 0.0686; Time: 0.71\n",
      "Epoch 3047/8192 --- L(Train): 0.3779; L(Val): 0.4398; Reg Param: 0.0686; Time: 0.69\n",
      "Epoch 3048/8192 --- L(Train): 0.3758; L(Val): 0.4404; Reg Param: 0.0686; Time: 0.91\n",
      "Epoch 3049/8192 --- L(Train): 0.3623; L(Val): 0.4406; Reg Param: 0.0686; Time: 0.88\n",
      "Epoch 3050/8192 --- L(Train): 0.3833; L(Val): 0.4406; Reg Param: 0.0682; Time: 8.00\n",
      "Epoch 3051/8192 --- L(Train): 0.3720; L(Val): 0.4403; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3052/8192 --- L(Train): 0.3750; L(Val): 0.4403; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3053/8192 --- L(Train): 0.3774; L(Val): 0.4402; Reg Param: 0.0682; Time: 0.71\n",
      "Epoch 3054/8192 --- L(Train): 0.3815; L(Val): 0.4403; Reg Param: 0.0682; Time: 0.71\n",
      "Epoch 3055/8192 --- L(Train): 0.3684; L(Val): 0.4404; Reg Param: 0.0682; Time: 0.71\n",
      "Epoch 3056/8192 --- L(Train): 0.3697; L(Val): 0.4406; Reg Param: 0.0682; Time: 0.88\n",
      "Epoch 3057/8192 --- L(Train): 0.3674; L(Val): 0.4401; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3058/8192 --- L(Train): 0.3686; L(Val): 0.4397; Reg Param: 0.0682; Time: 0.71\n",
      "Epoch 3059/8192 --- L(Train): 0.3733; L(Val): 0.4396; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3060/8192 --- L(Train): 0.3792; L(Val): 0.4398; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3061/8192 --- L(Train): 0.3804; L(Val): 0.4399; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3062/8192 --- L(Train): 0.3765; L(Val): 0.4398; Reg Param: 0.0682; Time: 0.69\n",
      "Epoch 3063/8192 --- L(Train): 0.3760; L(Val): 0.4402; Reg Param: 0.0682; Time: 0.69\n",
      "Epoch 3064/8192 --- L(Train): 0.3670; L(Val): 0.4400; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3065/8192 --- L(Train): 0.3687; L(Val): 0.4394; Reg Param: 0.0682; Time: 0.75\n",
      "Epoch 3066/8192 --- L(Train): 0.3681; L(Val): 0.4390; Reg Param: 0.0682; Time: 0.83\n",
      "Epoch 3067/8192 --- L(Train): 0.3794; L(Val): 0.4392; Reg Param: 0.0682; Time: 0.75\n",
      "Epoch 3068/8192 --- L(Train): 0.3741; L(Val): 0.4391; Reg Param: 0.0682; Time: 0.74\n",
      "Epoch 3069/8192 --- L(Train): 0.3689; L(Val): 0.4392; Reg Param: 0.0682; Time: 0.73\n",
      "Epoch 3070/8192 --- L(Train): 0.3778; L(Val): 0.4394; Reg Param: 0.0682; Time: 0.73\n",
      "Epoch 3071/8192 --- L(Train): 0.3737; L(Val): 0.4397; Reg Param: 0.0682; Time: 0.71\n",
      "Epoch 3072/8192 --- L(Train): 0.3710; L(Val): 0.4397; Reg Param: 0.0682; Time: 0.73\n",
      "Epoch 3073/8192 --- L(Train): 0.3803; L(Val): 0.4395; Reg Param: 0.0682; Time: 0.73\n",
      "Epoch 3074/8192 --- L(Train): 0.3711; L(Val): 0.4393; Reg Param: 0.0682; Time: 0.73\n",
      "Epoch 3075/8192 --- L(Train): 0.3754; L(Val): 0.4391; Reg Param: 0.0682; Time: 0.72\n",
      "Epoch 3076/8192 --- L(Train): 0.3664; L(Val): 0.4389; Reg Param: 0.0682; Time: 0.74\n",
      "Epoch 3077/8192 --- L(Train): 0.3743; L(Val): 0.4388; Reg Param: 0.0682; Time: 0.73\n",
      "Epoch 3078/8192 --- L(Train): 0.3825; L(Val): 0.4391; Reg Param: 0.0682; Time: 0.73\n",
      "Epoch 3079/8192 --- L(Train): 0.3700; L(Val): 0.4392; Reg Param: 0.0682; Time: 0.72\n",
      "Epoch 3080/8192 --- L(Train): 0.3693; L(Val): 0.4391; Reg Param: 0.0682; Time: 0.72\n",
      "Epoch 3081/8192 --- L(Train): 0.3837; L(Val): 0.4393; Reg Param: 0.0682; Time: 0.81\n",
      "Epoch 3082/8192 --- L(Train): 0.3740; L(Val): 0.4397; Reg Param: 0.0682; Time: 0.74\n",
      "Epoch 3083/8192 --- L(Train): 0.3739; L(Val): 0.4402; Reg Param: 0.0682; Time: 0.73\n",
      "Epoch 3084/8192 --- L(Train): 0.3688; L(Val): 0.4403; Reg Param: 0.0682; Time: 0.72\n",
      "Epoch 3085/8192 --- L(Train): 0.3700; L(Val): 0.4403; Reg Param: 0.0682; Time: 0.72\n",
      "Epoch 3086/8192 --- L(Train): 0.3734; L(Val): 0.4399; Reg Param: 0.0682; Time: 0.99\n",
      "Epoch 3087/8192 --- L(Train): 0.3748; L(Val): 0.4395; Reg Param: 0.0682; Time: 0.81\n",
      "Epoch 3088/8192 --- L(Train): 0.3663; L(Val): 0.4394; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3089/8192 --- L(Train): 0.3648; L(Val): 0.4393; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3090/8192 --- L(Train): 0.3810; L(Val): 0.4394; Reg Param: 0.0682; Time: 0.69\n",
      "Epoch 3091/8192 --- L(Train): 0.3656; L(Val): 0.4392; Reg Param: 0.0682; Time: 0.69\n",
      "Epoch 3092/8192 --- L(Train): 0.3624; L(Val): 0.4387; Reg Param: 0.0682; Time: 0.69\n",
      "Epoch 3093/8192 --- L(Train): 0.3832; L(Val): 0.4383; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3094/8192 --- L(Train): 0.3670; L(Val): 0.4385; Reg Param: 0.0682; Time: 0.69\n",
      "Epoch 3095/8192 --- L(Train): 0.3750; L(Val): 0.4387; Reg Param: 0.0682; Time: 0.69\n",
      "Epoch 3096/8192 --- L(Train): 0.3607; L(Val): 0.4393; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3097/8192 --- L(Train): 0.3619; L(Val): 0.4402; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3098/8192 --- L(Train): 0.3695; L(Val): 0.4408; Reg Param: 0.0682; Time: 0.70\n",
      "Epoch 3099/8192 --- L(Train): 0.3731; L(Val): 0.4410; Reg Param: 0.0682; Time: 0.77\n",
      "Epoch 3100/8192 --- L(Train): 0.3688; L(Val): 0.4410; Reg Param: 0.0678; Time: 7.45\n",
      "Epoch 3101/8192 --- L(Train): 0.3781; L(Val): 0.4413; Reg Param: 0.0678; Time: 0.71\n",
      "Epoch 3102/8192 --- L(Train): 0.3683; L(Val): 0.4413; Reg Param: 0.0678; Time: 0.71\n",
      "Epoch 3103/8192 --- L(Train): 0.3680; L(Val): 0.4414; Reg Param: 0.0678; Time: 0.82\n",
      "Epoch 3104/8192 --- L(Train): 0.3726; L(Val): 0.4414; Reg Param: 0.0678; Time: 0.71\n",
      "Epoch 3105/8192 --- L(Train): 0.3823; L(Val): 0.4407; Reg Param: 0.0678; Time: 0.72\n",
      "Epoch 3106/8192 --- L(Train): 0.3732; L(Val): 0.4400; Reg Param: 0.0678; Time: 0.74\n",
      "Epoch 3107/8192 --- L(Train): 0.3739; L(Val): 0.4395; Reg Param: 0.0678; Time: 0.93\n",
      "Epoch 3108/8192 --- L(Train): 0.3854; L(Val): 0.4394; Reg Param: 0.0678; Time: 0.76\n",
      "Epoch 3109/8192 --- L(Train): 0.3825; L(Val): 0.4397; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3110/8192 --- L(Train): 0.3633; L(Val): 0.4399; Reg Param: 0.0678; Time: 0.72\n",
      "Epoch 3111/8192 --- L(Train): 0.3786; L(Val): 0.4402; Reg Param: 0.0678; Time: 0.72\n",
      "Epoch 3112/8192 --- L(Train): 0.3614; L(Val): 0.4403; Reg Param: 0.0678; Time: 0.73\n",
      "Epoch 3113/8192 --- L(Train): 0.3739; L(Val): 0.4398; Reg Param: 0.0678; Time: 0.75\n",
      "Epoch 3114/8192 --- L(Train): 0.3708; L(Val): 0.4392; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3115/8192 --- L(Train): 0.3739; L(Val): 0.4386; Reg Param: 0.0678; Time: 0.75\n",
      "Epoch 3116/8192 --- L(Train): 0.3845; L(Val): 0.4383; Reg Param: 0.0678; Time: 0.74\n",
      "Epoch 3117/8192 --- L(Train): 0.3741; L(Val): 0.4382; Reg Param: 0.0678; Time: 0.76\n",
      "Epoch 3118/8192 --- L(Train): 0.3828; L(Val): 0.4385; Reg Param: 0.0678; Time: 0.82\n",
      "Epoch 3119/8192 --- L(Train): 0.3706; L(Val): 0.4387; Reg Param: 0.0678; Time: 0.88\n",
      "Epoch 3120/8192 --- L(Train): 0.3652; L(Val): 0.4389; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3121/8192 --- L(Train): 0.3640; L(Val): 0.4391; Reg Param: 0.0678; Time: 0.79\n",
      "Epoch 3122/8192 --- L(Train): 0.3765; L(Val): 0.4391; Reg Param: 0.0678; Time: 0.79\n",
      "Epoch 3123/8192 --- L(Train): 0.3747; L(Val): 0.4391; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3124/8192 --- L(Train): 0.3638; L(Val): 0.4389; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3125/8192 --- L(Train): 0.3644; L(Val): 0.4390; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3126/8192 --- L(Train): 0.3721; L(Val): 0.4391; Reg Param: 0.0678; Time: 0.76\n",
      "Epoch 3127/8192 --- L(Train): 0.3641; L(Val): 0.4390; Reg Param: 0.0678; Time: 0.79\n",
      "Epoch 3128/8192 --- L(Train): 0.3764; L(Val): 0.4389; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3129/8192 --- L(Train): 0.3667; L(Val): 0.4389; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3130/8192 --- L(Train): 0.3820; L(Val): 0.4390; Reg Param: 0.0678; Time: 0.86\n",
      "Epoch 3131/8192 --- L(Train): 0.3692; L(Val): 0.4392; Reg Param: 0.0678; Time: 0.77\n",
      "Epoch 3132/8192 --- L(Train): 0.3585; L(Val): 0.4397; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3133/8192 --- L(Train): 0.3783; L(Val): 0.4402; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3134/8192 --- L(Train): 0.3681; L(Val): 0.4402; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3135/8192 --- L(Train): 0.3709; L(Val): 0.4402; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3136/8192 --- L(Train): 0.3781; L(Val): 0.4402; Reg Param: 0.0678; Time: 1.21\n",
      "Epoch 3137/8192 --- L(Train): 0.3807; L(Val): 0.4401; Reg Param: 0.0678; Time: 0.79\n",
      "Epoch 3138/8192 --- L(Train): 0.3736; L(Val): 0.4401; Reg Param: 0.0678; Time: 0.83\n",
      "Epoch 3139/8192 --- L(Train): 0.3778; L(Val): 0.4403; Reg Param: 0.0678; Time: 0.75\n",
      "Epoch 3140/8192 --- L(Train): 0.3717; L(Val): 0.4403; Reg Param: 0.0678; Time: 0.74\n",
      "Epoch 3141/8192 --- L(Train): 0.3765; L(Val): 0.4402; Reg Param: 0.0678; Time: 0.75\n",
      "Epoch 3142/8192 --- L(Train): 0.3707; L(Val): 0.4403; Reg Param: 0.0678; Time: 0.75\n",
      "Epoch 3143/8192 --- L(Train): 0.3683; L(Val): 0.4404; Reg Param: 0.0678; Time: 0.77\n",
      "Epoch 3144/8192 --- L(Train): 0.3659; L(Val): 0.4406; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3145/8192 --- L(Train): 0.3805; L(Val): 0.4409; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3146/8192 --- L(Train): 0.3782; L(Val): 0.4413; Reg Param: 0.0678; Time: 0.86\n",
      "Epoch 3147/8192 --- L(Train): 0.3678; L(Val): 0.4417; Reg Param: 0.0678; Time: 0.78\n",
      "Epoch 3148/8192 --- L(Train): 0.3713; L(Val): 0.4417; Reg Param: 0.0678; Time: 0.74\n",
      "Epoch 3149/8192 --- L(Train): 0.3754; L(Val): 0.4414; Reg Param: 0.0678; Time: 0.73\n",
      "Epoch 3150/8192 --- L(Train): 0.3743; L(Val): 0.4414; Reg Param: 0.0674; Time: 7.98\n",
      "Epoch 3151/8192 --- L(Train): 0.3649; L(Val): 0.4405; Reg Param: 0.0674; Time: 0.76\n",
      "Epoch 3152/8192 --- L(Train): 0.3704; L(Val): 0.4402; Reg Param: 0.0674; Time: 0.76\n",
      "Epoch 3153/8192 --- L(Train): 0.3685; L(Val): 0.4401; Reg Param: 0.0674; Time: 0.99\n",
      "Epoch 3154/8192 --- L(Train): 0.3911; L(Val): 0.4407; Reg Param: 0.0674; Time: 0.80\n",
      "Epoch 3155/8192 --- L(Train): 0.3733; L(Val): 0.4410; Reg Param: 0.0674; Time: 0.75\n",
      "Epoch 3156/8192 --- L(Train): 0.3786; L(Val): 0.4413; Reg Param: 0.0674; Time: 0.74\n",
      "Epoch 3157/8192 --- L(Train): 0.3772; L(Val): 0.4413; Reg Param: 0.0674; Time: 0.75\n",
      "Epoch 3158/8192 --- L(Train): 0.3707; L(Val): 0.4414; Reg Param: 0.0674; Time: 0.85\n",
      "Epoch 3159/8192 --- L(Train): 0.3945; L(Val): 0.4413; Reg Param: 0.0674; Time: 0.74\n",
      "Epoch 3160/8192 --- L(Train): 0.3769; L(Val): 0.4412; Reg Param: 0.0674; Time: 0.76\n",
      "Epoch 3161/8192 --- L(Train): 0.3675; L(Val): 0.4411; Reg Param: 0.0674; Time: 0.75\n",
      "Epoch 3162/8192 --- L(Train): 0.3551; L(Val): 0.4412; Reg Param: 0.0674; Time: 0.76\n",
      "Epoch 3163/8192 --- L(Train): 0.3773; L(Val): 0.4410; Reg Param: 0.0674; Time: 0.78\n",
      "Epoch 3164/8192 --- L(Train): 0.3763; L(Val): 0.4404; Reg Param: 0.0674; Time: 0.73\n",
      "Epoch 3165/8192 --- L(Train): 0.3802; L(Val): 0.4400; Reg Param: 0.0674; Time: 0.76\n",
      "Epoch 3166/8192 --- L(Train): 0.3698; L(Val): 0.4399; Reg Param: 0.0674; Time: 0.77\n",
      "Epoch 3167/8192 --- L(Train): 0.3726; L(Val): 0.4399; Reg Param: 0.0674; Time: 0.77\n",
      "Epoch 3168/8192 --- L(Train): 0.3649; L(Val): 0.4401; Reg Param: 0.0674; Time: 0.75\n",
      "Epoch 3169/8192 --- L(Train): 0.3859; L(Val): 0.4406; Reg Param: 0.0674; Time: 0.90\n",
      "Epoch 3170/8192 --- L(Train): 0.3851; L(Val): 0.4410; Reg Param: 0.0674; Time: 0.78\n",
      "Epoch 3171/8192 --- L(Train): 0.3719; L(Val): 0.4414; Reg Param: 0.0674; Time: 0.80\n",
      "Epoch 3172/8192 --- L(Train): 0.3606; L(Val): 0.4414; Reg Param: 0.0674; Time: 0.85\n",
      "Epoch 3173/8192 --- L(Train): 0.3808; L(Val): 0.4412; Reg Param: 0.0674; Time: 0.91\n",
      "Epoch 3174/8192 --- L(Train): 0.3682; L(Val): 0.4412; Reg Param: 0.0674; Time: 0.97\n",
      "Epoch 3175/8192 --- L(Train): 0.3698; L(Val): 0.4411; Reg Param: 0.0674; Time: 0.89\n",
      "Epoch 3176/8192 --- L(Train): 0.3715; L(Val): 0.4409; Reg Param: 0.0674; Time: 0.93\n",
      "Epoch 3177/8192 --- L(Train): 0.3752; L(Val): 0.4409; Reg Param: 0.0674; Time: 0.91\n",
      "Epoch 3178/8192 --- L(Train): 0.3712; L(Val): 0.4408; Reg Param: 0.0674; Time: 0.95\n",
      "Epoch 3179/8192 --- L(Train): 0.3810; L(Val): 0.4405; Reg Param: 0.0674; Time: 0.97\n",
      "Epoch 3180/8192 --- L(Train): 0.3762; L(Val): 0.4401; Reg Param: 0.0674; Time: 0.97\n",
      "Epoch 3181/8192 --- L(Train): 0.3840; L(Val): 0.4399; Reg Param: 0.0674; Time: 1.40\n",
      "Epoch 3182/8192 --- L(Train): 0.3888; L(Val): 0.4397; Reg Param: 0.0674; Time: 0.96\n",
      "Epoch 3183/8192 --- L(Train): 0.3858; L(Val): 0.4397; Reg Param: 0.0674; Time: 0.93\n",
      "Epoch 3184/8192 --- L(Train): 0.3811; L(Val): 0.4399; Reg Param: 0.0674; Time: 1.00\n",
      "Epoch 3185/8192 --- L(Train): 0.3747; L(Val): 0.4401; Reg Param: 0.0674; Time: 0.98\n",
      "Epoch 3186/8192 --- L(Train): 0.3785; L(Val): 0.4405; Reg Param: 0.0674; Time: 1.04\n",
      "Epoch 3187/8192 --- L(Train): 0.3702; L(Val): 0.4409; Reg Param: 0.0674; Time: 0.78\n",
      "Epoch 3188/8192 --- L(Train): 0.3734; L(Val): 0.4410; Reg Param: 0.0674; Time: 0.82\n",
      "Epoch 3189/8192 --- L(Train): 0.3765; L(Val): 0.4412; Reg Param: 0.0674; Time: 0.85\n",
      "Epoch 3190/8192 --- L(Train): 0.3760; L(Val): 0.4414; Reg Param: 0.0674; Time: 0.78\n",
      "Epoch 3191/8192 --- L(Train): 0.3685; L(Val): 0.4416; Reg Param: 0.0674; Time: 0.79\n",
      "Epoch 3192/8192 --- L(Train): 0.3815; L(Val): 0.4414; Reg Param: 0.0674; Time: 0.79\n",
      "Epoch 3193/8192 --- L(Train): 0.3791; L(Val): 0.4411; Reg Param: 0.0674; Time: 0.76\n",
      "Epoch 3194/8192 --- L(Train): 0.3753; L(Val): 0.4409; Reg Param: 0.0674; Time: 0.77\n",
      "Epoch 3195/8192 --- L(Train): 0.3800; L(Val): 0.4404; Reg Param: 0.0674; Time: 0.73\n",
      "Epoch 3196/8192 --- L(Train): 0.3789; L(Val): 0.4400; Reg Param: 0.0674; Time: 0.74\n",
      "Epoch 3197/8192 --- L(Train): 0.3743; L(Val): 0.4401; Reg Param: 0.0674; Time: 0.75\n",
      "Epoch 3198/8192 --- L(Train): 0.3746; L(Val): 0.4400; Reg Param: 0.0674; Time: 0.75\n",
      "Epoch 3199/8192 --- L(Train): 0.3658; L(Val): 0.4397; Reg Param: 0.0674; Time: 0.75\n",
      "Epoch 3200/8192 --- L(Train): 0.3660; L(Val): 0.4397; Reg Param: 0.0670; Time: 7.75\n",
      "Epoch 3201/8192 --- L(Train): 0.3660; L(Val): 0.4393; Reg Param: 0.0670; Time: 0.78\n",
      "Epoch 3202/8192 --- L(Train): 0.3748; L(Val): 0.4393; Reg Param: 0.0670; Time: 0.96\n",
      "Epoch 3203/8192 --- L(Train): 0.3752; L(Val): 0.4396; Reg Param: 0.0670; Time: 0.73\n",
      "Epoch 3204/8192 --- L(Train): 0.3756; L(Val): 0.4398; Reg Param: 0.0670; Time: 0.72\n",
      "Epoch 3205/8192 --- L(Train): 0.3809; L(Val): 0.4399; Reg Param: 0.0670; Time: 0.73\n",
      "Epoch 3206/8192 --- L(Train): 0.3690; L(Val): 0.4398; Reg Param: 0.0670; Time: 0.75\n",
      "Epoch 3207/8192 --- L(Train): 0.3672; L(Val): 0.4398; Reg Param: 0.0670; Time: 0.73\n",
      "Epoch 3208/8192 --- L(Train): 0.3792; L(Val): 0.4397; Reg Param: 0.0670; Time: 0.75\n",
      "Epoch 3209/8192 --- L(Train): 0.3681; L(Val): 0.4397; Reg Param: 0.0670; Time: 0.73\n",
      "Epoch 3210/8192 --- L(Train): 0.3682; L(Val): 0.4396; Reg Param: 0.0670; Time: 0.74\n",
      "Epoch 3211/8192 --- L(Train): 0.3691; L(Val): 0.4396; Reg Param: 0.0670; Time: 0.83\n",
      "Epoch 3212/8192 --- L(Train): 0.3699; L(Val): 0.4399; Reg Param: 0.0670; Time: 0.75\n",
      "Epoch 3213/8192 --- L(Train): 0.3736; L(Val): 0.4399; Reg Param: 0.0670; Time: 0.73\n",
      "Epoch 3214/8192 --- L(Train): 0.3745; L(Val): 0.4395; Reg Param: 0.0670; Time: 0.74\n",
      "Epoch 3215/8192 --- L(Train): 0.3699; L(Val): 0.4397; Reg Param: 0.0670; Time: 0.75\n",
      "Epoch 3216/8192 --- L(Train): 0.3819; L(Val): 0.4398; Reg Param: 0.0670; Time: 0.74\n",
      "Epoch 3217/8192 --- L(Train): 0.3780; L(Val): 0.4407; Reg Param: 0.0670; Time: 0.75\n",
      "Epoch 3218/8192 --- L(Train): 0.3639; L(Val): 0.4405; Reg Param: 0.0670; Time: 0.74\n",
      "Epoch 3219/8192 --- L(Train): 0.3778; L(Val): 0.4408; Reg Param: 0.0670; Time: 0.82\n",
      "Epoch 3220/8192 --- L(Train): 0.3848; L(Val): 0.4408; Reg Param: 0.0670; Time: 0.73\n",
      "Epoch 3221/8192 --- L(Train): 0.3653; L(Val): 0.4407; Reg Param: 0.0670; Time: 0.76\n",
      "Epoch 3222/8192 --- L(Train): 0.3716; L(Val): 0.4409; Reg Param: 0.0670; Time: 0.75\n",
      "Epoch 3223/8192 --- L(Train): 0.3723; L(Val): 0.4408; Reg Param: 0.0670; Time: 0.74\n",
      "Epoch 3224/8192 --- L(Train): 0.3701; L(Val): 0.4408; Reg Param: 0.0670; Time: 0.74\n",
      "Epoch 3225/8192 --- L(Train): 0.3787; L(Val): 0.4409; Reg Param: 0.0670; Time: 0.74\n",
      "Epoch 3226/8192 --- L(Train): 0.3748; L(Val): 0.4411; Reg Param: 0.0670; Time: 0.79\n",
      "Epoch 3227/8192 --- L(Train): 0.3799; L(Val): 0.4410; Reg Param: 0.0670; Time: 0.95\n",
      "Epoch 3228/8192 --- L(Train): 0.3658; L(Val): 0.4410; Reg Param: 0.0670; Time: 0.80\n",
      "Epoch 3229/8192 --- L(Train): 0.3744; L(Val): 0.4408; Reg Param: 0.0670; Time: 0.76\n",
      "Epoch 3230/8192 --- L(Train): 0.3677; L(Val): 0.4410; Reg Param: 0.0670; Time: 0.78\n",
      "Epoch 3231/8192 --- L(Train): 0.3596; L(Val): 0.4412; Reg Param: 0.0670; Time: 0.80\n",
      "Epoch 3232/8192 --- L(Train): 0.3672; L(Val): 0.4411; Reg Param: 0.0670; Time: 0.78\n",
      "Epoch 3233/8192 --- L(Train): 0.3674; L(Val): 0.4409; Reg Param: 0.0670; Time: 0.74\n",
      "Epoch 3234/8192 --- L(Train): 0.3785; L(Val): 0.4408; Reg Param: 0.0670; Time: 0.75\n",
      "Epoch 3235/8192 --- L(Train): 0.3705; L(Val): 0.4406; Reg Param: 0.0670; Time: 0.73\n",
      "Epoch 3236/8192 --- L(Train): 0.3882; L(Val): 0.4402; Reg Param: 0.0670; Time: 0.73\n",
      "Epoch 3237/8192 --- L(Train): 0.3822; L(Val): 0.4401; Reg Param: 0.0670; Time: 0.71\n",
      "Epoch 3238/8192 --- L(Train): 0.3760; L(Val): 0.4399; Reg Param: 0.0670; Time: 0.72\n",
      "Epoch 3239/8192 --- L(Train): 0.3727; L(Val): 0.4399; Reg Param: 0.0670; Time: 0.83\n",
      "Epoch 3240/8192 --- L(Train): 0.3693; L(Val): 0.4400; Reg Param: 0.0670; Time: 0.73\n",
      "Epoch 3241/8192 --- L(Train): 0.3768; L(Val): 0.4401; Reg Param: 0.0670; Time: 0.73\n",
      "Epoch 3242/8192 --- L(Train): 0.3621; L(Val): 0.4402; Reg Param: 0.0670; Time: 0.73\n",
      "Epoch 3243/8192 --- L(Train): 0.3767; L(Val): 0.4402; Reg Param: 0.0670; Time: 0.73\n",
      "Epoch 3244/8192 --- L(Train): 0.3817; L(Val): 0.4405; Reg Param: 0.0670; Time: 0.76\n",
      "Epoch 3245/8192 --- L(Train): 0.3716; L(Val): 0.4408; Reg Param: 0.0670; Time: 0.76\n",
      "Epoch 3246/8192 --- L(Train): 0.3745; L(Val): 0.4409; Reg Param: 0.0670; Time: 0.75\n",
      "Epoch 3247/8192 --- L(Train): 0.3745; L(Val): 0.4407; Reg Param: 0.0670; Time: 0.78\n",
      "Epoch 3248/8192 --- L(Train): 0.3605; L(Val): 0.4405; Reg Param: 0.0670; Time: 0.81\n",
      "Epoch 3249/8192 --- L(Train): 0.3764; L(Val): 0.4401; Reg Param: 0.0670; Time: 0.75\n",
      "Epoch 3250/8192 --- L(Train): 0.3708; L(Val): 0.4401; Reg Param: 0.0666; Time: 8.32\n",
      "Epoch 3251/8192 --- L(Train): 0.3788; L(Val): 0.4391; Reg Param: 0.0666; Time: 0.87\n",
      "Epoch 3252/8192 --- L(Train): 0.3634; L(Val): 0.4389; Reg Param: 0.0666; Time: 0.72\n",
      "Epoch 3253/8192 --- L(Train): 0.3743; L(Val): 0.4390; Reg Param: 0.0666; Time: 0.76\n",
      "Epoch 3254/8192 --- L(Train): 0.3797; L(Val): 0.4389; Reg Param: 0.0666; Time: 0.78\n",
      "Epoch 3255/8192 --- L(Train): 0.3782; L(Val): 0.4391; Reg Param: 0.0666; Time: 0.79\n",
      "Epoch 3256/8192 --- L(Train): 0.3720; L(Val): 0.4392; Reg Param: 0.0666; Time: 0.76\n",
      "Epoch 3257/8192 --- L(Train): 0.3682; L(Val): 0.4395; Reg Param: 0.0666; Time: 0.77\n",
      "Epoch 3258/8192 --- L(Train): 0.3696; L(Val): 0.4395; Reg Param: 0.0666; Time: 0.77\n",
      "Epoch 3259/8192 --- L(Train): 0.3700; L(Val): 0.4395; Reg Param: 0.0666; Time: 0.76\n",
      "Epoch 3260/8192 --- L(Train): 0.3719; L(Val): 0.4393; Reg Param: 0.0666; Time: 0.76\n",
      "Epoch 3261/8192 --- L(Train): 0.3801; L(Val): 0.4390; Reg Param: 0.0666; Time: 0.85\n",
      "Epoch 3262/8192 --- L(Train): 0.3756; L(Val): 0.4389; Reg Param: 0.0666; Time: 0.78\n",
      "Epoch 3263/8192 --- L(Train): 0.3701; L(Val): 0.4391; Reg Param: 0.0666; Time: 0.78\n",
      "Epoch 3264/8192 --- L(Train): 0.3892; L(Val): 0.4391; Reg Param: 0.0666; Time: 0.77\n",
      "Epoch 3265/8192 --- L(Train): 0.3821; L(Val): 0.4391; Reg Param: 0.0666; Time: 0.75\n",
      "Epoch 3266/8192 --- L(Train): 0.3704; L(Val): 0.4394; Reg Param: 0.0666; Time: 0.80\n",
      "Epoch 3267/8192 --- L(Train): 0.3663; L(Val): 0.4397; Reg Param: 0.0666; Time: 0.75\n",
      "Epoch 3268/8192 --- L(Train): 0.3748; L(Val): 0.4396; Reg Param: 0.0666; Time: 0.74\n",
      "Epoch 3269/8192 --- L(Train): 0.3775; L(Val): 0.4394; Reg Param: 0.0666; Time: 0.74\n",
      "Epoch 3270/8192 --- L(Train): 0.3789; L(Val): 0.4393; Reg Param: 0.0666; Time: 0.74\n",
      "Epoch 3271/8192 --- L(Train): 0.3771; L(Val): 0.4394; Reg Param: 0.0666; Time: 0.77\n",
      "Epoch 3272/8192 --- L(Train): 0.3664; L(Val): 0.4395; Reg Param: 0.0666; Time: 0.97\n",
      "Epoch 3273/8192 --- L(Train): 0.3809; L(Val): 0.4394; Reg Param: 0.0666; Time: 0.72\n",
      "Epoch 3274/8192 --- L(Train): 0.3722; L(Val): 0.4395; Reg Param: 0.0666; Time: 0.72\n",
      "Epoch 3275/8192 --- L(Train): 0.3686; L(Val): 0.4398; Reg Param: 0.0666; Time: 0.74\n",
      "Epoch 3276/8192 --- L(Train): 0.3639; L(Val): 0.4403; Reg Param: 0.0666; Time: 0.74\n",
      "Epoch 3277/8192 --- L(Train): 0.3629; L(Val): 0.4405; Reg Param: 0.0666; Time: 0.73\n",
      "Epoch 3278/8192 --- L(Train): 0.3620; L(Val): 0.4406; Reg Param: 0.0666; Time: 0.75\n",
      "Epoch 3279/8192 --- L(Train): 0.3784; L(Val): 0.4407; Reg Param: 0.0666; Time: 0.74\n",
      "Epoch 3280/8192 --- L(Train): 0.3671; L(Val): 0.4408; Reg Param: 0.0666; Time: 0.73\n",
      "Epoch 3281/8192 --- L(Train): 0.3765; L(Val): 0.4406; Reg Param: 0.0666; Time: 0.72\n",
      "Epoch 3282/8192 --- L(Train): 0.3795; L(Val): 0.4403; Reg Param: 0.0666; Time: 0.77\n",
      "Epoch 3283/8192 --- L(Train): 0.3770; L(Val): 0.4406; Reg Param: 0.0666; Time: 0.78\n",
      "Epoch 3284/8192 --- L(Train): 0.3783; L(Val): 0.4406; Reg Param: 0.0666; Time: 0.75\n",
      "Epoch 3285/8192 --- L(Train): 0.3831; L(Val): 0.4402; Reg Param: 0.0666; Time: 0.76\n",
      "Epoch 3286/8192 --- L(Train): 0.3728; L(Val): 0.4396; Reg Param: 0.0666; Time: 0.76\n",
      "Epoch 3287/8192 --- L(Train): 0.3702; L(Val): 0.4390; Reg Param: 0.0666; Time: 0.77\n",
      "Epoch 3288/8192 --- L(Train): 0.3788; L(Val): 0.4388; Reg Param: 0.0666; Time: 0.86\n",
      "Epoch 3289/8192 --- L(Train): 0.3794; L(Val): 0.4386; Reg Param: 0.0666; Time: 0.73\n",
      "Epoch 3290/8192 --- L(Train): 0.3746; L(Val): 0.4388; Reg Param: 0.0666; Time: 0.77\n",
      "Epoch 3291/8192 --- L(Train): 0.3721; L(Val): 0.4388; Reg Param: 0.0666; Time: 0.77\n",
      "Epoch 3292/8192 --- L(Train): 0.3891; L(Val): 0.4390; Reg Param: 0.0666; Time: 0.75\n",
      "Epoch 3293/8192 --- L(Train): 0.3716; L(Val): 0.4392; Reg Param: 0.0666; Time: 0.75\n",
      "Epoch 3294/8192 --- L(Train): 0.3791; L(Val): 0.4393; Reg Param: 0.0666; Time: 0.71\n",
      "Epoch 3295/8192 --- L(Train): 0.3720; L(Val): 0.4393; Reg Param: 0.0666; Time: 0.71\n",
      "Epoch 3296/8192 --- L(Train): 0.3690; L(Val): 0.4392; Reg Param: 0.0666; Time: 0.74\n",
      "Epoch 3297/8192 --- L(Train): 0.3706; L(Val): 0.4391; Reg Param: 0.0666; Time: 0.75\n",
      "Epoch 3298/8192 --- L(Train): 0.3797; L(Val): 0.4392; Reg Param: 0.0666; Time: 0.73\n",
      "Epoch 3299/8192 --- L(Train): 0.3672; L(Val): 0.4393; Reg Param: 0.0666; Time: 0.70\n",
      "Epoch 3300/8192 --- L(Train): 0.3844; L(Val): 0.4393; Reg Param: 0.0662; Time: 7.67\n",
      "Epoch 3301/8192 --- L(Train): 0.3678; L(Val): 0.4396; Reg Param: 0.0662; Time: 0.69\n",
      "Epoch 3302/8192 --- L(Train): 0.3659; L(Val): 0.4399; Reg Param: 0.0662; Time: 0.68\n",
      "Epoch 3303/8192 --- L(Train): 0.3744; L(Val): 0.4400; Reg Param: 0.0662; Time: 0.69\n",
      "Epoch 3304/8192 --- L(Train): 0.3674; L(Val): 0.4400; Reg Param: 0.0662; Time: 0.70\n",
      "Epoch 3305/8192 --- L(Train): 0.3637; L(Val): 0.4401; Reg Param: 0.0662; Time: 0.72\n",
      "Epoch 3306/8192 --- L(Train): 0.3683; L(Val): 0.4402; Reg Param: 0.0662; Time: 0.77\n",
      "Epoch 3307/8192 --- L(Train): 0.3643; L(Val): 0.4403; Reg Param: 0.0662; Time: 0.73\n",
      "Epoch 3308/8192 --- L(Train): 0.3694; L(Val): 0.4404; Reg Param: 0.0662; Time: 0.70\n",
      "Epoch 3309/8192 --- L(Train): 0.3827; L(Val): 0.4406; Reg Param: 0.0662; Time: 0.85\n",
      "Epoch 3310/8192 --- L(Train): 0.3732; L(Val): 0.4406; Reg Param: 0.0662; Time: 0.82\n",
      "Epoch 3311/8192 --- L(Train): 0.3791; L(Val): 0.4406; Reg Param: 0.0662; Time: 0.73\n",
      "Epoch 3312/8192 --- L(Train): 0.3744; L(Val): 0.4404; Reg Param: 0.0662; Time: 0.78\n",
      "Epoch 3313/8192 --- L(Train): 0.3863; L(Val): 0.4403; Reg Param: 0.0662; Time: 0.72\n",
      "Epoch 3314/8192 --- L(Train): 0.3733; L(Val): 0.4402; Reg Param: 0.0662; Time: 0.78\n",
      "Epoch 3315/8192 --- L(Train): 0.3734; L(Val): 0.4401; Reg Param: 0.0662; Time: 0.73\n",
      "Epoch 3316/8192 --- L(Train): 0.3713; L(Val): 0.4402; Reg Param: 0.0662; Time: 0.72\n",
      "Epoch 3317/8192 --- L(Train): 0.3685; L(Val): 0.4405; Reg Param: 0.0662; Time: 0.71\n",
      "Epoch 3318/8192 --- L(Train): 0.3719; L(Val): 0.4409; Reg Param: 0.0662; Time: 0.77\n",
      "Epoch 3319/8192 --- L(Train): 0.3543; L(Val): 0.4414; Reg Param: 0.0662; Time: 0.78\n",
      "Epoch 3320/8192 --- L(Train): 0.3739; L(Val): 0.4417; Reg Param: 0.0662; Time: 0.76\n",
      "Epoch 3321/8192 --- L(Train): 0.3707; L(Val): 0.4418; Reg Param: 0.0662; Time: 0.73\n",
      "Epoch 3322/8192 --- L(Train): 0.3744; L(Val): 0.4417; Reg Param: 0.0662; Time: 0.77\n",
      "Epoch 3323/8192 --- L(Train): 0.3641; L(Val): 0.4413; Reg Param: 0.0662; Time: 1.13\n",
      "Epoch 3324/8192 --- L(Train): 0.3688; L(Val): 0.4409; Reg Param: 0.0662; Time: 0.74\n",
      "Epoch 3325/8192 --- L(Train): 0.3687; L(Val): 0.4404; Reg Param: 0.0662; Time: 0.73\n",
      "Epoch 3326/8192 --- L(Train): 0.3686; L(Val): 0.4402; Reg Param: 0.0662; Time: 0.77\n",
      "Epoch 3327/8192 --- L(Train): 0.3704; L(Val): 0.4400; Reg Param: 0.0662; Time: 0.78\n",
      "Epoch 3328/8192 --- L(Train): 0.3651; L(Val): 0.4403; Reg Param: 0.0662; Time: 0.71\n",
      "Epoch 3329/8192 --- L(Train): 0.3813; L(Val): 0.4405; Reg Param: 0.0662; Time: 0.68\n",
      "Epoch 3330/8192 --- L(Train): 0.3635; L(Val): 0.4409; Reg Param: 0.0662; Time: 0.68\n",
      "Epoch 3331/8192 --- L(Train): 0.3707; L(Val): 0.4411; Reg Param: 0.0662; Time: 0.75\n",
      "Epoch 3332/8192 --- L(Train): 0.3761; L(Val): 0.4411; Reg Param: 0.0662; Time: 0.80\n",
      "Epoch 3333/8192 --- L(Train): 0.3617; L(Val): 0.4409; Reg Param: 0.0662; Time: 0.68\n",
      "Epoch 3334/8192 --- L(Train): 0.3756; L(Val): 0.4407; Reg Param: 0.0662; Time: 0.85\n",
      "Epoch 3335/8192 --- L(Train): 0.3776; L(Val): 0.4403; Reg Param: 0.0662; Time: 0.71\n",
      "Epoch 3336/8192 --- L(Train): 0.3651; L(Val): 0.4398; Reg Param: 0.0662; Time: 0.72\n",
      "Epoch 3337/8192 --- L(Train): 0.3702; L(Val): 0.4397; Reg Param: 0.0662; Time: 0.83\n",
      "Epoch 3338/8192 --- L(Train): 0.3890; L(Val): 0.4397; Reg Param: 0.0662; Time: 0.75\n",
      "Epoch 3339/8192 --- L(Train): 0.3781; L(Val): 0.4399; Reg Param: 0.0662; Time: 0.71\n",
      "Epoch 3340/8192 --- L(Train): 0.3588; L(Val): 0.4401; Reg Param: 0.0662; Time: 0.69\n",
      "Epoch 3341/8192 --- L(Train): 0.3695; L(Val): 0.4403; Reg Param: 0.0662; Time: 0.69\n",
      "Epoch 3342/8192 --- L(Train): 0.3798; L(Val): 0.4406; Reg Param: 0.0662; Time: 0.69\n",
      "Epoch 3343/8192 --- L(Train): 0.3797; L(Val): 0.4408; Reg Param: 0.0662; Time: 0.72\n",
      "Epoch 3344/8192 --- L(Train): 0.3739; L(Val): 0.4410; Reg Param: 0.0662; Time: 0.72\n",
      "Epoch 3345/8192 --- L(Train): 0.3590; L(Val): 0.4413; Reg Param: 0.0662; Time: 0.70\n",
      "Epoch 3346/8192 --- L(Train): 0.3776; L(Val): 0.4416; Reg Param: 0.0662; Time: 0.69\n",
      "Epoch 3347/8192 --- L(Train): 0.3735; L(Val): 0.4417; Reg Param: 0.0662; Time: 0.68\n",
      "Epoch 3348/8192 --- L(Train): 0.3653; L(Val): 0.4418; Reg Param: 0.0662; Time: 0.72\n",
      "Epoch 3349/8192 --- L(Train): 0.3754; L(Val): 0.4416; Reg Param: 0.0662; Time: 0.70\n",
      "Epoch 3350/8192 --- L(Train): 0.3779; L(Val): 0.4416; Reg Param: 0.0657; Time: 7.92\n",
      "Epoch 3351/8192 --- L(Train): 0.3592; L(Val): 0.4417; Reg Param: 0.0657; Time: 0.72\n",
      "Epoch 3352/8192 --- L(Train): 0.3731; L(Val): 0.4417; Reg Param: 0.0657; Time: 0.74\n",
      "Epoch 3353/8192 --- L(Train): 0.3751; L(Val): 0.4418; Reg Param: 0.0657; Time: 0.74\n",
      "Epoch 3354/8192 --- L(Train): 0.3675; L(Val): 0.4418; Reg Param: 0.0657; Time: 0.81\n",
      "Epoch 3355/8192 --- L(Train): 0.3823; L(Val): 0.4417; Reg Param: 0.0657; Time: 0.74\n",
      "Epoch 3356/8192 --- L(Train): 0.3717; L(Val): 0.4415; Reg Param: 0.0657; Time: 0.77\n",
      "Epoch 3357/8192 --- L(Train): 0.3650; L(Val): 0.4413; Reg Param: 0.0657; Time: 0.79\n",
      "Epoch 3358/8192 --- L(Train): 0.3876; L(Val): 0.4411; Reg Param: 0.0657; Time: 0.76\n",
      "Epoch 3359/8192 --- L(Train): 0.3767; L(Val): 0.4410; Reg Param: 0.0657; Time: 0.74\n",
      "Epoch 3360/8192 --- L(Train): 0.3741; L(Val): 0.4410; Reg Param: 0.0657; Time: 0.73\n",
      "Epoch 3361/8192 --- L(Train): 0.3775; L(Val): 0.4410; Reg Param: 0.0657; Time: 0.75\n",
      "Epoch 3362/8192 --- L(Train): 0.3770; L(Val): 0.4413; Reg Param: 0.0657; Time: 0.74\n",
      "Epoch 3363/8192 --- L(Train): 0.3785; L(Val): 0.4417; Reg Param: 0.0657; Time: 0.75\n",
      "Epoch 3364/8192 --- L(Train): 0.3709; L(Val): 0.4420; Reg Param: 0.0657; Time: 0.76\n",
      "Epoch 3365/8192 --- L(Train): 0.3731; L(Val): 0.4421; Reg Param: 0.0657; Time: 0.82\n",
      "Epoch 3366/8192 --- L(Train): 0.3691; L(Val): 0.4423; Reg Param: 0.0657; Time: 0.83\n",
      "Epoch 3367/8192 --- L(Train): 0.3730; L(Val): 0.4419; Reg Param: 0.0657; Time: 0.90\n",
      "Epoch 3368/8192 --- L(Train): 0.3796; L(Val): 0.4416; Reg Param: 0.0657; Time: 0.77\n",
      "Epoch 3369/8192 --- L(Train): 0.3886; L(Val): 0.4415; Reg Param: 0.0657; Time: 0.83\n",
      "Epoch 3370/8192 --- L(Train): 0.3780; L(Val): 0.4415; Reg Param: 0.0657; Time: 0.76\n",
      "Epoch 3371/8192 --- L(Train): 0.3598; L(Val): 0.4417; Reg Param: 0.0657; Time: 0.78\n",
      "Epoch 3372/8192 --- L(Train): 0.3736; L(Val): 0.4421; Reg Param: 0.0657; Time: 0.75\n",
      "Epoch 3373/8192 --- L(Train): 0.3678; L(Val): 0.4424; Reg Param: 0.0657; Time: 0.81\n",
      "Epoch 3374/8192 --- L(Train): 0.3811; L(Val): 0.4425; Reg Param: 0.0657; Time: 0.81\n",
      "Epoch 3375/8192 --- L(Train): 0.3784; L(Val): 0.4426; Reg Param: 0.0657; Time: 1.10\n",
      "Epoch 3376/8192 --- L(Train): 0.3796; L(Val): 0.4426; Reg Param: 0.0657; Time: 0.76\n",
      "Epoch 3377/8192 --- L(Train): 0.3712; L(Val): 0.4427; Reg Param: 0.0657; Time: 0.79\n",
      "Epoch 3378/8192 --- L(Train): 0.3625; L(Val): 0.4426; Reg Param: 0.0657; Time: 0.78\n",
      "Epoch 3379/8192 --- L(Train): 0.3854; L(Val): 0.4425; Reg Param: 0.0657; Time: 0.80\n",
      "Epoch 3380/8192 --- L(Train): 0.3687; L(Val): 0.4422; Reg Param: 0.0657; Time: 0.77\n",
      "Epoch 3381/8192 --- L(Train): 0.3647; L(Val): 0.4421; Reg Param: 0.0657; Time: 0.71\n",
      "Epoch 3382/8192 --- L(Train): 0.3848; L(Val): 0.4421; Reg Param: 0.0657; Time: 0.88\n",
      "Epoch 3383/8192 --- L(Train): 0.3692; L(Val): 0.4421; Reg Param: 0.0657; Time: 0.77\n",
      "Epoch 3384/8192 --- L(Train): 0.3661; L(Val): 0.4422; Reg Param: 0.0657; Time: 0.80\n",
      "Epoch 3385/8192 --- L(Train): 0.3727; L(Val): 0.4423; Reg Param: 0.0657; Time: 0.77\n",
      "Epoch 3386/8192 --- L(Train): 0.3733; L(Val): 0.4427; Reg Param: 0.0657; Time: 0.76\n",
      "Epoch 3387/8192 --- L(Train): 0.3738; L(Val): 0.4430; Reg Param: 0.0657; Time: 0.77\n",
      "Epoch 3388/8192 --- L(Train): 0.3816; L(Val): 0.4431; Reg Param: 0.0657; Time: 0.78\n",
      "Epoch 3389/8192 --- L(Train): 0.3664; L(Val): 0.4434; Reg Param: 0.0657; Time: 0.77\n",
      "Epoch 3390/8192 --- L(Train): 0.3736; L(Val): 0.4436; Reg Param: 0.0657; Time: 0.78\n",
      "Epoch 3391/8192 --- L(Train): 0.3744; L(Val): 0.4438; Reg Param: 0.0657; Time: 0.76\n",
      "Epoch 3392/8192 --- L(Train): 0.3754; L(Val): 0.4439; Reg Param: 0.0657; Time: 0.76\n",
      "Epoch 3393/8192 --- L(Train): 0.3625; L(Val): 0.4439; Reg Param: 0.0657; Time: 0.80\n",
      "Epoch 3394/8192 --- L(Train): 0.3740; L(Val): 0.4437; Reg Param: 0.0657; Time: 0.86\n",
      "Epoch 3395/8192 --- L(Train): 0.3865; L(Val): 0.4434; Reg Param: 0.0657; Time: 0.76\n",
      "Epoch 3396/8192 --- L(Train): 0.3694; L(Val): 0.4432; Reg Param: 0.0657; Time: 0.77\n",
      "Epoch 3397/8192 --- L(Train): 0.3650; L(Val): 0.4429; Reg Param: 0.0657; Time: 0.81\n",
      "Epoch 3398/8192 --- L(Train): 0.3647; L(Val): 0.4425; Reg Param: 0.0657; Time: 0.76\n",
      "Epoch 3399/8192 --- L(Train): 0.3810; L(Val): 0.4423; Reg Param: 0.0657; Time: 0.76\n",
      "Epoch 3400/8192 --- L(Train): 0.3675; L(Val): 0.4423; Reg Param: 0.0652; Time: 8.85\n",
      "Epoch 3401/8192 --- L(Train): 0.3714; L(Val): 0.4420; Reg Param: 0.0652; Time: 0.79\n",
      "Epoch 3402/8192 --- L(Train): 0.3698; L(Val): 0.4421; Reg Param: 0.0652; Time: 0.77\n",
      "Epoch 3403/8192 --- L(Train): 0.3693; L(Val): 0.4422; Reg Param: 0.0652; Time: 0.76\n",
      "Epoch 3404/8192 --- L(Train): 0.3791; L(Val): 0.4423; Reg Param: 0.0652; Time: 0.76\n",
      "Epoch 3405/8192 --- L(Train): 0.3569; L(Val): 0.4425; Reg Param: 0.0652; Time: 0.78\n",
      "Epoch 3406/8192 --- L(Train): 0.3673; L(Val): 0.4425; Reg Param: 0.0652; Time: 0.78\n",
      "Epoch 3407/8192 --- L(Train): 0.3651; L(Val): 0.4424; Reg Param: 0.0652; Time: 0.79\n",
      "Epoch 3408/8192 --- L(Train): 0.3511; L(Val): 0.4425; Reg Param: 0.0652; Time: 0.77\n",
      "Epoch 3409/8192 --- L(Train): 0.3684; L(Val): 0.4425; Reg Param: 0.0652; Time: 0.75\n",
      "Epoch 3410/8192 --- L(Train): 0.3730; L(Val): 0.4424; Reg Param: 0.0652; Time: 0.75\n",
      "Epoch 3411/8192 --- L(Train): 0.3747; L(Val): 0.4424; Reg Param: 0.0652; Time: 0.74\n",
      "Epoch 3412/8192 --- L(Train): 0.3836; L(Val): 0.4424; Reg Param: 0.0652; Time: 0.88\n",
      "Epoch 3413/8192 --- L(Train): 0.3738; L(Val): 0.4424; Reg Param: 0.0652; Time: 0.77\n",
      "Epoch 3414/8192 --- L(Train): 0.3725; L(Val): 0.4426; Reg Param: 0.0652; Time: 0.76\n",
      "Epoch 3415/8192 --- L(Train): 0.3664; L(Val): 0.4427; Reg Param: 0.0652; Time: 0.78\n",
      "Epoch 3416/8192 --- L(Train): 0.3614; L(Val): 0.4427; Reg Param: 0.0652; Time: 0.77\n",
      "Epoch 3417/8192 --- L(Train): 0.3817; L(Val): 0.4427; Reg Param: 0.0652; Time: 0.77\n",
      "Epoch 3418/8192 --- L(Train): 0.3850; L(Val): 0.4427; Reg Param: 0.0652; Time: 0.76\n",
      "Epoch 3419/8192 --- L(Train): 0.3837; L(Val): 0.4429; Reg Param: 0.0652; Time: 0.80\n",
      "Epoch 3420/8192 --- L(Train): 0.3643; L(Val): 0.4430; Reg Param: 0.0652; Time: 0.77\n",
      "Epoch 3421/8192 --- L(Train): 0.3688; L(Val): 0.4431; Reg Param: 0.0652; Time: 0.78\n",
      "Epoch 3422/8192 --- L(Train): 0.3597; L(Val): 0.4431; Reg Param: 0.0652; Time: 0.75\n",
      "Epoch 3423/8192 --- L(Train): 0.3746; L(Val): 0.4430; Reg Param: 0.0652; Time: 1.10\n",
      "Epoch 3424/8192 --- L(Train): 0.3737; L(Val): 0.4430; Reg Param: 0.0652; Time: 0.77\n",
      "Epoch 3425/8192 --- L(Train): 0.3640; L(Val): 0.4430; Reg Param: 0.0652; Time: 0.75\n",
      "Epoch 3426/8192 --- L(Train): 0.3645; L(Val): 0.4429; Reg Param: 0.0652; Time: 0.72\n",
      "Epoch 3427/8192 --- L(Train): 0.3780; L(Val): 0.4428; Reg Param: 0.0652; Time: 0.74\n",
      "Epoch 3428/8192 --- L(Train): 0.3773; L(Val): 0.4428; Reg Param: 0.0652; Time: 0.73\n",
      "Epoch 3429/8192 --- L(Train): 0.3627; L(Val): 0.4428; Reg Param: 0.0652; Time: 0.81\n",
      "Epoch 3430/8192 --- L(Train): 0.3713; L(Val): 0.4428; Reg Param: 0.0652; Time: 0.74\n",
      "Epoch 3431/8192 --- L(Train): 0.3564; L(Val): 0.4429; Reg Param: 0.0652; Time: 0.80\n",
      "Epoch 3432/8192 --- L(Train): 0.3655; L(Val): 0.4429; Reg Param: 0.0652; Time: 0.81\n",
      "Epoch 3433/8192 --- L(Train): 0.3672; L(Val): 0.4428; Reg Param: 0.0652; Time: 0.79\n",
      "Epoch 3434/8192 --- L(Train): 0.3747; L(Val): 0.4427; Reg Param: 0.0652; Time: 0.79\n",
      "Epoch 3435/8192 --- L(Train): 0.3711; L(Val): 0.4425; Reg Param: 0.0652; Time: 0.78\n",
      "Epoch 3436/8192 --- L(Train): 0.3763; L(Val): 0.4424; Reg Param: 0.0652; Time: 0.75\n",
      "Epoch 3437/8192 --- L(Train): 0.3699; L(Val): 0.4424; Reg Param: 0.0652; Time: 0.79\n",
      "Epoch 3438/8192 --- L(Train): 0.3689; L(Val): 0.4423; Reg Param: 0.0652; Time: 0.76\n",
      "Epoch 3439/8192 --- L(Train): 0.3732; L(Val): 0.4424; Reg Param: 0.0652; Time: 0.77\n",
      "Epoch 3440/8192 --- L(Train): 0.3587; L(Val): 0.4425; Reg Param: 0.0652; Time: 0.76\n",
      "Epoch 3441/8192 --- L(Train): 0.3632; L(Val): 0.4425; Reg Param: 0.0652; Time: 0.79\n",
      "Epoch 3442/8192 --- L(Train): 0.3712; L(Val): 0.4423; Reg Param: 0.0652; Time: 0.76\n",
      "Epoch 3443/8192 --- L(Train): 0.3872; L(Val): 0.4420; Reg Param: 0.0652; Time: 0.86\n",
      "Epoch 3444/8192 --- L(Train): 0.3742; L(Val): 0.4419; Reg Param: 0.0652; Time: 0.77\n",
      "Epoch 3445/8192 --- L(Train): 0.3628; L(Val): 0.4418; Reg Param: 0.0652; Time: 0.78\n",
      "Epoch 3446/8192 --- L(Train): 0.3724; L(Val): 0.4417; Reg Param: 0.0652; Time: 0.79\n",
      "Epoch 3447/8192 --- L(Train): 0.3673; L(Val): 0.4417; Reg Param: 0.0652; Time: 0.77\n",
      "Epoch 3448/8192 --- L(Train): 0.3724; L(Val): 0.4419; Reg Param: 0.0652; Time: 0.76\n",
      "Epoch 3449/8192 --- L(Train): 0.3751; L(Val): 0.4422; Reg Param: 0.0652; Time: 0.76\n",
      "Epoch 3450/8192 --- L(Train): 0.3641; L(Val): 0.4422; Reg Param: 0.0647; Time: 8.39\n",
      "Epoch 3451/8192 --- L(Train): 0.3654; L(Val): 0.4426; Reg Param: 0.0647; Time: 0.76\n",
      "Epoch 3452/8192 --- L(Train): 0.3739; L(Val): 0.4428; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3453/8192 --- L(Train): 0.3690; L(Val): 0.4430; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3454/8192 --- L(Train): 0.3675; L(Val): 0.4430; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3455/8192 --- L(Train): 0.3699; L(Val): 0.4431; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3456/8192 --- L(Train): 0.3681; L(Val): 0.4431; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3457/8192 --- L(Train): 0.3716; L(Val): 0.4432; Reg Param: 0.0647; Time: 0.78\n",
      "Epoch 3458/8192 --- L(Train): 0.3725; L(Val): 0.4431; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3459/8192 --- L(Train): 0.3773; L(Val): 0.4431; Reg Param: 0.0647; Time: 0.89\n",
      "Epoch 3460/8192 --- L(Train): 0.3852; L(Val): 0.4432; Reg Param: 0.0647; Time: 0.76\n",
      "Epoch 3461/8192 --- L(Train): 0.3648; L(Val): 0.4432; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3462/8192 --- L(Train): 0.3607; L(Val): 0.4432; Reg Param: 0.0647; Time: 0.76\n",
      "Epoch 3463/8192 --- L(Train): 0.3711; L(Val): 0.4431; Reg Param: 0.0647; Time: 0.76\n",
      "Epoch 3464/8192 --- L(Train): 0.3623; L(Val): 0.4429; Reg Param: 0.0647; Time: 0.81\n",
      "Epoch 3465/8192 --- L(Train): 0.3747; L(Val): 0.4427; Reg Param: 0.0647; Time: 1.15\n",
      "Epoch 3466/8192 --- L(Train): 0.3767; L(Val): 0.4427; Reg Param: 0.0647; Time: 0.78\n",
      "Epoch 3467/8192 --- L(Train): 0.3673; L(Val): 0.4427; Reg Param: 0.0647; Time: 0.78\n",
      "Epoch 3468/8192 --- L(Train): 0.3728; L(Val): 0.4427; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3469/8192 --- L(Train): 0.3766; L(Val): 0.4427; Reg Param: 0.0647; Time: 0.78\n",
      "Epoch 3470/8192 --- L(Train): 0.3622; L(Val): 0.4427; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3471/8192 --- L(Train): 0.3664; L(Val): 0.4428; Reg Param: 0.0647; Time: 0.74\n",
      "Epoch 3472/8192 --- L(Train): 0.3660; L(Val): 0.4428; Reg Param: 0.0647; Time: 0.74\n",
      "Epoch 3473/8192 --- L(Train): 0.3725; L(Val): 0.4431; Reg Param: 0.0647; Time: 0.73\n",
      "Epoch 3474/8192 --- L(Train): 0.3700; L(Val): 0.4433; Reg Param: 0.0647; Time: 0.90\n",
      "Epoch 3475/8192 --- L(Train): 0.3739; L(Val): 0.4434; Reg Param: 0.0647; Time: 0.76\n",
      "Epoch 3476/8192 --- L(Train): 0.3697; L(Val): 0.4435; Reg Param: 0.0647; Time: 0.73\n",
      "Epoch 3477/8192 --- L(Train): 0.3589; L(Val): 0.4436; Reg Param: 0.0647; Time: 0.75\n",
      "Epoch 3478/8192 --- L(Train): 0.3651; L(Val): 0.4438; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3479/8192 --- L(Train): 0.3737; L(Val): 0.4440; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3480/8192 --- L(Train): 0.3674; L(Val): 0.4442; Reg Param: 0.0647; Time: 0.78\n",
      "Epoch 3481/8192 --- L(Train): 0.3749; L(Val): 0.4442; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3482/8192 --- L(Train): 0.3726; L(Val): 0.4440; Reg Param: 0.0647; Time: 0.74\n",
      "Epoch 3483/8192 --- L(Train): 0.3649; L(Val): 0.4436; Reg Param: 0.0647; Time: 0.72\n",
      "Epoch 3484/8192 --- L(Train): 0.3618; L(Val): 0.4434; Reg Param: 0.0647; Time: 0.73\n",
      "Epoch 3485/8192 --- L(Train): 0.3746; L(Val): 0.4432; Reg Param: 0.0647; Time: 0.83\n",
      "Epoch 3486/8192 --- L(Train): 0.3666; L(Val): 0.4430; Reg Param: 0.0647; Time: 0.72\n",
      "Epoch 3487/8192 --- L(Train): 0.3654; L(Val): 0.4428; Reg Param: 0.0647; Time: 0.69\n",
      "Epoch 3488/8192 --- L(Train): 0.3797; L(Val): 0.4427; Reg Param: 0.0647; Time: 0.68\n",
      "Epoch 3489/8192 --- L(Train): 0.3712; L(Val): 0.4424; Reg Param: 0.0647; Time: 0.69\n",
      "Epoch 3490/8192 --- L(Train): 0.3773; L(Val): 0.4423; Reg Param: 0.0647; Time: 0.68\n",
      "Epoch 3491/8192 --- L(Train): 0.3657; L(Val): 0.4422; Reg Param: 0.0647; Time: 0.68\n",
      "Epoch 3492/8192 --- L(Train): 0.3662; L(Val): 0.4419; Reg Param: 0.0647; Time: 0.72\n",
      "Epoch 3493/8192 --- L(Train): 0.3746; L(Val): 0.4418; Reg Param: 0.0647; Time: 0.75\n",
      "Epoch 3494/8192 --- L(Train): 0.3671; L(Val): 0.4417; Reg Param: 0.0647; Time: 0.75\n",
      "Epoch 3495/8192 --- L(Train): 0.3699; L(Val): 0.4417; Reg Param: 0.0647; Time: 0.77\n",
      "Epoch 3496/8192 --- L(Train): 0.3601; L(Val): 0.4418; Reg Param: 0.0647; Time: 0.79\n",
      "Epoch 3497/8192 --- L(Train): 0.3723; L(Val): 0.4418; Reg Param: 0.0647; Time: 0.76\n",
      "Epoch 3498/8192 --- L(Train): 0.3615; L(Val): 0.4420; Reg Param: 0.0647; Time: 0.76\n",
      "Epoch 3499/8192 --- L(Train): 0.3629; L(Val): 0.4421; Reg Param: 0.0647; Time: 1.02\n",
      "Epoch 3500/8192 --- L(Train): 0.3705; L(Val): 0.4421; Reg Param: 0.0637; Time: 8.17\n",
      "Epoch 3501/8192 --- L(Train): 0.3653; L(Val): 0.4420; Reg Param: 0.0637; Time: 0.74\n",
      "Epoch 3502/8192 --- L(Train): 0.3601; L(Val): 0.4420; Reg Param: 0.0637; Time: 0.77\n",
      "Epoch 3503/8192 --- L(Train): 0.3781; L(Val): 0.4420; Reg Param: 0.0637; Time: 0.78\n",
      "Epoch 3504/8192 --- L(Train): 0.3821; L(Val): 0.4421; Reg Param: 0.0637; Time: 0.75\n",
      "Epoch 3505/8192 --- L(Train): 0.3660; L(Val): 0.4422; Reg Param: 0.0637; Time: 0.78\n",
      "Epoch 3506/8192 --- L(Train): 0.3730; L(Val): 0.4423; Reg Param: 0.0637; Time: 0.77\n",
      "Epoch 3507/8192 --- L(Train): 0.3605; L(Val): 0.4424; Reg Param: 0.0637; Time: 0.76\n",
      "Epoch 3508/8192 --- L(Train): 0.3703; L(Val): 0.4425; Reg Param: 0.0637; Time: 0.76\n",
      "Epoch 3509/8192 --- L(Train): 0.3713; L(Val): 0.4426; Reg Param: 0.0637; Time: 0.75\n",
      "Epoch 3510/8192 --- L(Train): 0.3855; L(Val): 0.4428; Reg Param: 0.0637; Time: 0.73\n",
      "Epoch 3511/8192 --- L(Train): 0.3679; L(Val): 0.4430; Reg Param: 0.0637; Time: 0.69\n",
      "Epoch 3512/8192 --- L(Train): 0.3751; L(Val): 0.4430; Reg Param: 0.0637; Time: 0.72\n",
      "Epoch 3513/8192 --- L(Train): 0.3640; L(Val): 0.4431; Reg Param: 0.0637; Time: 0.77\n",
      "Epoch 3514/8192 --- L(Train): 0.3701; L(Val): 0.4431; Reg Param: 0.0637; Time: 0.77\n",
      "Epoch 3515/8192 --- L(Train): 0.3896; L(Val): 0.4431; Reg Param: 0.0637; Time: 0.72\n",
      "Epoch 3516/8192 --- L(Train): 0.3694; L(Val): 0.4432; Reg Param: 0.0637; Time: 0.76\n",
      "Epoch 3517/8192 --- L(Train): 0.3630; L(Val): 0.4433; Reg Param: 0.0637; Time: 0.74\n",
      "Epoch 3518/8192 --- L(Train): 0.3765; L(Val): 0.4433; Reg Param: 0.0637; Time: 0.76\n",
      "Epoch 3519/8192 --- L(Train): 0.3848; L(Val): 0.4434; Reg Param: 0.0637; Time: 0.80\n",
      "Epoch 3520/8192 --- L(Train): 0.3824; L(Val): 0.4434; Reg Param: 0.0637; Time: 0.75\n",
      "Epoch 3521/8192 --- L(Train): 0.3691; L(Val): 0.4434; Reg Param: 0.0637; Time: 0.76\n",
      "Epoch 3522/8192 --- L(Train): 0.3711; L(Val): 0.4431; Reg Param: 0.0637; Time: 0.83\n",
      "Epoch 3523/8192 --- L(Train): 0.3765; L(Val): 0.4429; Reg Param: 0.0637; Time: 0.90\n",
      "Epoch 3524/8192 --- L(Train): 0.3683; L(Val): 0.4427; Reg Param: 0.0637; Time: 0.73\n",
      "Epoch 3525/8192 --- L(Train): 0.3673; L(Val): 0.4426; Reg Param: 0.0637; Time: 0.75\n",
      "Epoch 3526/8192 --- L(Train): 0.3598; L(Val): 0.4426; Reg Param: 0.0637; Time: 0.76\n",
      "Epoch 3527/8192 --- L(Train): 0.3666; L(Val): 0.4427; Reg Param: 0.0637; Time: 0.77\n",
      "Epoch 3528/8192 --- L(Train): 0.3809; L(Val): 0.4427; Reg Param: 0.0637; Time: 0.83\n",
      "Epoch 3529/8192 --- L(Train): 0.3800; L(Val): 0.4427; Reg Param: 0.0637; Time: 0.73\n",
      "Epoch 3530/8192 --- L(Train): 0.3763; L(Val): 0.4426; Reg Param: 0.0637; Time: 0.76\n",
      "Epoch 3531/8192 --- L(Train): 0.3729; L(Val): 0.4424; Reg Param: 0.0637; Time: 0.75\n",
      "Epoch 3532/8192 --- L(Train): 0.3700; L(Val): 0.4423; Reg Param: 0.0637; Time: 0.76\n",
      "Epoch 3533/8192 --- L(Train): 0.3615; L(Val): 0.4423; Reg Param: 0.0637; Time: 0.75\n",
      "Epoch 3534/8192 --- L(Train): 0.3708; L(Val): 0.4423; Reg Param: 0.0637; Time: 0.75\n",
      "Epoch 3535/8192 --- L(Train): 0.3807; L(Val): 0.4424; Reg Param: 0.0637; Time: 0.75\n",
      "Epoch 3536/8192 --- L(Train): 0.3708; L(Val): 0.4425; Reg Param: 0.0637; Time: 0.77\n",
      "Epoch 3537/8192 --- L(Train): 0.3807; L(Val): 0.4426; Reg Param: 0.0637; Time: 0.73\n",
      "Epoch 3538/8192 --- L(Train): 0.3664; L(Val): 0.4428; Reg Param: 0.0637; Time: 0.83\n",
      "Epoch 3539/8192 --- L(Train): 0.3734; L(Val): 0.4429; Reg Param: 0.0637; Time: 0.72\n",
      "Epoch 3540/8192 --- L(Train): 0.3744; L(Val): 0.4432; Reg Param: 0.0637; Time: 0.72\n",
      "Epoch 3541/8192 --- L(Train): 0.3759; L(Val): 0.4434; Reg Param: 0.0637; Time: 0.76\n",
      "Epoch 3542/8192 --- L(Train): 0.3679; L(Val): 0.4436; Reg Param: 0.0637; Time: 0.75\n",
      "Epoch 3543/8192 --- L(Train): 0.3730; L(Val): 0.4438; Reg Param: 0.0637; Time: 0.73\n",
      "Epoch 3544/8192 --- L(Train): 0.3751; L(Val): 0.4439; Reg Param: 0.0637; Time: 0.75\n",
      "Epoch 3545/8192 --- L(Train): 0.3847; L(Val): 0.4439; Reg Param: 0.0637; Time: 0.76\n",
      "Epoch 3546/8192 --- L(Train): 0.3785; L(Val): 0.4441; Reg Param: 0.0637; Time: 0.72\n",
      "Epoch 3547/8192 --- L(Train): 0.3700; L(Val): 0.4442; Reg Param: 0.0637; Time: 0.73\n",
      "Epoch 3548/8192 --- L(Train): 0.3683; L(Val): 0.4444; Reg Param: 0.0637; Time: 0.77\n",
      "Epoch 3549/8192 --- L(Train): 0.3655; L(Val): 0.4446; Reg Param: 0.0637; Time: 0.76\n",
      "Epoch 3550/8192 --- L(Train): 0.3811; L(Val): 0.4446; Reg Param: 0.0626; Time: 8.19\n",
      "Epoch 3551/8192 --- L(Train): 0.3651; L(Val): 0.4447; Reg Param: 0.0626; Time: 0.84\n",
      "Epoch 3552/8192 --- L(Train): 0.3875; L(Val): 0.4448; Reg Param: 0.0626; Time: 0.74\n",
      "Epoch 3553/8192 --- L(Train): 0.3779; L(Val): 0.4447; Reg Param: 0.0626; Time: 0.73\n",
      "Epoch 3554/8192 --- L(Train): 0.3767; L(Val): 0.4446; Reg Param: 0.0626; Time: 1.52\n",
      "Epoch 3555/8192 --- L(Train): 0.3694; L(Val): 0.4443; Reg Param: 0.0626; Time: 1.03\n",
      "Epoch 3556/8192 --- L(Train): 0.3760; L(Val): 0.4441; Reg Param: 0.0626; Time: 0.77\n",
      "Epoch 3557/8192 --- L(Train): 0.3548; L(Val): 0.4439; Reg Param: 0.0626; Time: 0.79\n",
      "Epoch 3558/8192 --- L(Train): 0.3835; L(Val): 0.4437; Reg Param: 0.0626; Time: 0.78\n",
      "Epoch 3559/8192 --- L(Train): 0.3699; L(Val): 0.4435; Reg Param: 0.0626; Time: 0.79\n",
      "Epoch 3560/8192 --- L(Train): 0.3598; L(Val): 0.4434; Reg Param: 0.0626; Time: 0.78\n",
      "Epoch 3561/8192 --- L(Train): 0.3780; L(Val): 0.4432; Reg Param: 0.0626; Time: 0.78\n",
      "Epoch 3562/8192 --- L(Train): 0.3703; L(Val): 0.4431; Reg Param: 0.0626; Time: 0.75\n",
      "Epoch 3563/8192 --- L(Train): 0.3697; L(Val): 0.4431; Reg Param: 0.0626; Time: 0.75\n",
      "Epoch 3564/8192 --- L(Train): 0.3645; L(Val): 0.4431; Reg Param: 0.0626; Time: 0.75\n",
      "Epoch 3565/8192 --- L(Train): 0.3851; L(Val): 0.4430; Reg Param: 0.0626; Time: 0.74\n",
      "Epoch 3566/8192 --- L(Train): 0.3626; L(Val): 0.4429; Reg Param: 0.0626; Time: 0.75\n",
      "Epoch 3567/8192 --- L(Train): 0.3680; L(Val): 0.4429; Reg Param: 0.0626; Time: 0.74\n",
      "Epoch 3568/8192 --- L(Train): 0.3585; L(Val): 0.4429; Reg Param: 0.0626; Time: 0.74\n",
      "Epoch 3569/8192 --- L(Train): 0.3706; L(Val): 0.4428; Reg Param: 0.0626; Time: 0.73\n",
      "Epoch 3570/8192 --- L(Train): 0.3662; L(Val): 0.4428; Reg Param: 0.0626; Time: 0.74\n",
      "Epoch 3571/8192 --- L(Train): 0.3768; L(Val): 0.4428; Reg Param: 0.0626; Time: 0.73\n",
      "Epoch 3572/8192 --- L(Train): 0.3692; L(Val): 0.4429; Reg Param: 0.0626; Time: 0.73\n",
      "Epoch 3573/8192 --- L(Train): 0.3783; L(Val): 0.4430; Reg Param: 0.0626; Time: 0.74\n",
      "Epoch 3574/8192 --- L(Train): 0.3594; L(Val): 0.4431; Reg Param: 0.0626; Time: 0.92\n",
      "Epoch 3575/8192 --- L(Train): 0.3714; L(Val): 0.4433; Reg Param: 0.0626; Time: 0.85\n",
      "Epoch 3576/8192 --- L(Train): 0.3767; L(Val): 0.4434; Reg Param: 0.0626; Time: 0.73\n",
      "Epoch 3577/8192 --- L(Train): 0.3619; L(Val): 0.4435; Reg Param: 0.0626; Time: 0.73\n",
      "Epoch 3578/8192 --- L(Train): 0.3723; L(Val): 0.4436; Reg Param: 0.0626; Time: 0.73\n",
      "Epoch 3579/8192 --- L(Train): 0.3741; L(Val): 0.4436; Reg Param: 0.0626; Time: 0.78\n",
      "Epoch 3580/8192 --- L(Train): 0.3846; L(Val): 0.4435; Reg Param: 0.0626; Time: 0.79\n",
      "Epoch 3581/8192 --- L(Train): 0.3551; L(Val): 0.4435; Reg Param: 0.0626; Time: 0.75\n",
      "Epoch 3582/8192 --- L(Train): 0.3638; L(Val): 0.4434; Reg Param: 0.0626; Time: 0.76\n",
      "Epoch 3583/8192 --- L(Train): 0.3710; L(Val): 0.4434; Reg Param: 0.0626; Time: 0.76\n",
      "Epoch 3584/8192 --- L(Train): 0.3643; L(Val): 0.4433; Reg Param: 0.0626; Time: 0.78\n",
      "Epoch 3585/8192 --- L(Train): 0.3677; L(Val): 0.4432; Reg Param: 0.0626; Time: 0.75\n",
      "Epoch 3586/8192 --- L(Train): 0.3783; L(Val): 0.4431; Reg Param: 0.0626; Time: 0.79\n",
      "Epoch 3587/8192 --- L(Train): 0.3734; L(Val): 0.4431; Reg Param: 0.0626; Time: 0.77\n",
      "Epoch 3588/8192 --- L(Train): 0.3756; L(Val): 0.4431; Reg Param: 0.0626; Time: 0.76\n",
      "Epoch 3589/8192 --- L(Train): 0.3820; L(Val): 0.4430; Reg Param: 0.0626; Time: 0.78\n",
      "Epoch 3590/8192 --- L(Train): 0.3669; L(Val): 0.4429; Reg Param: 0.0626; Time: 0.76\n",
      "Epoch 3591/8192 --- L(Train): 0.3780; L(Val): 0.4430; Reg Param: 0.0626; Time: 0.75\n",
      "Epoch 3592/8192 --- L(Train): 0.3700; L(Val): 0.4430; Reg Param: 0.0626; Time: 0.73\n",
      "Epoch 3593/8192 --- L(Train): 0.3619; L(Val): 0.4432; Reg Param: 0.0626; Time: 0.85\n",
      "Epoch 3594/8192 --- L(Train): 0.3807; L(Val): 0.4433; Reg Param: 0.0626; Time: 0.77\n",
      "Epoch 3595/8192 --- L(Train): 0.3771; L(Val): 0.4434; Reg Param: 0.0626; Time: 0.76\n",
      "Epoch 3596/8192 --- L(Train): 0.3660; L(Val): 0.4429; Reg Param: 0.0626; Time: 0.82\n",
      "Epoch 3597/8192 --- L(Train): 0.3688; L(Val): 0.4426; Reg Param: 0.0626; Time: 0.81\n",
      "Epoch 3598/8192 --- L(Train): 0.3552; L(Val): 0.4424; Reg Param: 0.0626; Time: 0.78\n",
      "Epoch 3599/8192 --- L(Train): 0.3745; L(Val): 0.4425; Reg Param: 0.0626; Time: 0.79\n",
      "Epoch 3600/8192 --- L(Train): 0.3699; L(Val): 0.4425; Reg Param: 0.0615; Time: 8.17\n",
      "Epoch 3601/8192 --- L(Train): 0.3670; L(Val): 0.4426; Reg Param: 0.0615; Time: 0.80\n",
      "Epoch 3602/8192 --- L(Train): 0.3712; L(Val): 0.4426; Reg Param: 0.0615; Time: 0.68\n",
      "Epoch 3603/8192 --- L(Train): 0.3617; L(Val): 0.4426; Reg Param: 0.0615; Time: 0.72\n",
      "Epoch 3604/8192 --- L(Train): 0.3731; L(Val): 0.4427; Reg Param: 0.0615; Time: 0.74\n",
      "Epoch 3605/8192 --- L(Train): 0.3628; L(Val): 0.4427; Reg Param: 0.0615; Time: 0.73\n",
      "Epoch 3606/8192 --- L(Train): 0.3743; L(Val): 0.4427; Reg Param: 0.0615; Time: 0.75\n",
      "Epoch 3607/8192 --- L(Train): 0.3726; L(Val): 0.4426; Reg Param: 0.0615; Time: 0.74\n",
      "Epoch 3608/8192 --- L(Train): 0.3734; L(Val): 0.4425; Reg Param: 0.0615; Time: 0.75\n",
      "Epoch 3609/8192 --- L(Train): 0.3682; L(Val): 0.4425; Reg Param: 0.0615; Time: 0.73\n",
      "Epoch 3610/8192 --- L(Train): 0.3714; L(Val): 0.4425; Reg Param: 0.0615; Time: 0.73\n",
      "Epoch 3611/8192 --- L(Train): 0.3673; L(Val): 0.4425; Reg Param: 0.0615; Time: 0.73\n",
      "Epoch 3612/8192 --- L(Train): 0.3772; L(Val): 0.4426; Reg Param: 0.0615; Time: 0.73\n",
      "Epoch 3613/8192 --- L(Train): 0.3673; L(Val): 0.4426; Reg Param: 0.0615; Time: 0.73\n",
      "Epoch 3614/8192 --- L(Train): 0.3661; L(Val): 0.4426; Reg Param: 0.0615; Time: 0.72\n",
      "Epoch 3615/8192 --- L(Train): 0.3741; L(Val): 0.4427; Reg Param: 0.0615; Time: 0.84\n",
      "Epoch 3616/8192 --- L(Train): 0.3844; L(Val): 0.4427; Reg Param: 0.0615; Time: 0.74\n",
      "Epoch 3617/8192 --- L(Train): 0.3545; L(Val): 0.4427; Reg Param: 0.0615; Time: 0.73\n",
      "Epoch 3618/8192 --- L(Train): 0.3759; L(Val): 0.4427; Reg Param: 0.0615; Time: 0.74\n",
      "Epoch 3619/8192 --- L(Train): 0.3572; L(Val): 0.4427; Reg Param: 0.0615; Time: 0.75\n",
      "Epoch 3620/8192 --- L(Train): 0.3638; L(Val): 0.4427; Reg Param: 0.0615; Time: 0.85\n",
      "Epoch 3621/8192 --- L(Train): 0.3710; L(Val): 0.4426; Reg Param: 0.0615; Time: 0.73\n",
      "Epoch 3622/8192 --- L(Train): 0.3640; L(Val): 0.4425; Reg Param: 0.0615; Time: 0.75\n",
      "Epoch 3623/8192 --- L(Train): 0.3770; L(Val): 0.4429; Reg Param: 0.0615; Time: 1.04\n",
      "Epoch 3624/8192 --- L(Train): 0.3520; L(Val): 0.4434; Reg Param: 0.0615; Time: 0.78\n",
      "Epoch 3625/8192 --- L(Train): 0.3737; L(Val): 0.4439; Reg Param: 0.0615; Time: 0.76\n",
      "Epoch 3626/8192 --- L(Train): 0.3663; L(Val): 0.4441; Reg Param: 0.0615; Time: 0.75\n",
      "Epoch 3627/8192 --- L(Train): 0.3832; L(Val): 0.4440; Reg Param: 0.0615; Time: 0.75\n",
      "Epoch 3628/8192 --- L(Train): 0.3788; L(Val): 0.4438; Reg Param: 0.0615; Time: 0.76\n",
      "Epoch 3629/8192 --- L(Train): 0.3833; L(Val): 0.4435; Reg Param: 0.0615; Time: 0.78\n",
      "Epoch 3630/8192 --- L(Train): 0.3760; L(Val): 0.4432; Reg Param: 0.0615; Time: 0.79\n",
      "Epoch 3631/8192 --- L(Train): 0.3722; L(Val): 0.4430; Reg Param: 0.0615; Time: 0.73\n",
      "Epoch 3632/8192 --- L(Train): 0.3778; L(Val): 0.4428; Reg Param: 0.0615; Time: 0.82\n",
      "Epoch 3633/8192 --- L(Train): 0.3676; L(Val): 0.4427; Reg Param: 0.0615; Time: 0.74\n",
      "Epoch 3634/8192 --- L(Train): 0.3759; L(Val): 0.4425; Reg Param: 0.0615; Time: 0.76\n",
      "Epoch 3635/8192 --- L(Train): 0.3844; L(Val): 0.4423; Reg Param: 0.0615; Time: 0.76\n",
      "Epoch 3636/8192 --- L(Train): 0.3879; L(Val): 0.4422; Reg Param: 0.0615; Time: 0.77\n",
      "Epoch 3637/8192 --- L(Train): 0.3618; L(Val): 0.4422; Reg Param: 0.0615; Time: 0.74\n",
      "Epoch 3638/8192 --- L(Train): 0.3811; L(Val): 0.4421; Reg Param: 0.0615; Time: 0.74\n",
      "Epoch 3639/8192 --- L(Train): 0.3698; L(Val): 0.4421; Reg Param: 0.0615; Time: 0.76\n",
      "Epoch 3640/8192 --- L(Train): 0.3655; L(Val): 0.4420; Reg Param: 0.0615; Time: 0.71\n",
      "Epoch 3641/8192 --- L(Train): 0.3633; L(Val): 0.4419; Reg Param: 0.0615; Time: 0.69\n",
      "Epoch 3642/8192 --- L(Train): 0.3790; L(Val): 0.4417; Reg Param: 0.0615; Time: 0.70\n",
      "Epoch 3643/8192 --- L(Train): 0.3767; L(Val): 0.4415; Reg Param: 0.0615; Time: 0.70\n",
      "Epoch 3644/8192 --- L(Train): 0.3889; L(Val): 0.4413; Reg Param: 0.0615; Time: 0.72\n",
      "Epoch 3645/8192 --- L(Train): 0.3756; L(Val): 0.4410; Reg Param: 0.0615; Time: 0.76\n",
      "Epoch 3646/8192 --- L(Train): 0.3840; L(Val): 0.4407; Reg Param: 0.0615; Time: 0.75\n",
      "Epoch 3647/8192 --- L(Train): 0.3682; L(Val): 0.4405; Reg Param: 0.0615; Time: 0.78\n",
      "Epoch 3648/8192 --- L(Train): 0.3777; L(Val): 0.4403; Reg Param: 0.0615; Time: 0.73\n",
      "Epoch 3649/8192 --- L(Train): 0.3759; L(Val): 0.4401; Reg Param: 0.0615; Time: 0.74\n",
      "Epoch 3650/8192 --- L(Train): 0.3797; L(Val): 0.4401; Reg Param: 0.0606; Time: 8.40\n",
      "Epoch 3651/8192 --- L(Train): 0.3714; L(Val): 0.4399; Reg Param: 0.0606; Time: 0.76\n",
      "Epoch 3652/8192 --- L(Train): 0.3875; L(Val): 0.4399; Reg Param: 0.0606; Time: 0.77\n",
      "Epoch 3653/8192 --- L(Train): 0.3818; L(Val): 0.4398; Reg Param: 0.0606; Time: 0.77\n",
      "Epoch 3654/8192 --- L(Train): 0.3754; L(Val): 0.4398; Reg Param: 0.0606; Time: 0.75\n",
      "Epoch 3655/8192 --- L(Train): 0.3527; L(Val): 0.4399; Reg Param: 0.0606; Time: 0.72\n",
      "Epoch 3656/8192 --- L(Train): 0.3752; L(Val): 0.4399; Reg Param: 0.0606; Time: 0.75\n",
      "Epoch 3657/8192 --- L(Train): 0.3718; L(Val): 0.4400; Reg Param: 0.0606; Time: 0.78\n",
      "Epoch 3658/8192 --- L(Train): 0.3713; L(Val): 0.4401; Reg Param: 0.0606; Time: 0.73\n",
      "Epoch 3659/8192 --- L(Train): 0.3722; L(Val): 0.4401; Reg Param: 0.0606; Time: 0.70\n",
      "Epoch 3660/8192 --- L(Train): 0.3777; L(Val): 0.4402; Reg Param: 0.0606; Time: 0.78\n",
      "Epoch 3661/8192 --- L(Train): 0.3771; L(Val): 0.4402; Reg Param: 0.0606; Time: 0.80\n",
      "Epoch 3662/8192 --- L(Train): 0.3701; L(Val): 0.4403; Reg Param: 0.0606; Time: 0.88\n",
      "Epoch 3663/8192 --- L(Train): 0.3660; L(Val): 0.4404; Reg Param: 0.0606; Time: 0.73\n",
      "Epoch 3664/8192 --- L(Train): 0.3696; L(Val): 0.4404; Reg Param: 0.0606; Time: 0.72\n",
      "Epoch 3665/8192 --- L(Train): 0.3610; L(Val): 0.4406; Reg Param: 0.0606; Time: 0.74\n",
      "Epoch 3666/8192 --- L(Train): 0.3818; L(Val): 0.4408; Reg Param: 0.0606; Time: 0.75\n",
      "Epoch 3667/8192 --- L(Train): 0.3666; L(Val): 0.4409; Reg Param: 0.0606; Time: 0.72\n",
      "Epoch 3668/8192 --- L(Train): 0.3795; L(Val): 0.4410; Reg Param: 0.0606; Time: 0.70\n",
      "Epoch 3669/8192 --- L(Train): 0.3726; L(Val): 0.4411; Reg Param: 0.0606; Time: 0.74\n",
      "Epoch 3670/8192 --- L(Train): 0.3719; L(Val): 0.4411; Reg Param: 0.0606; Time: 0.75\n",
      "Epoch 3671/8192 --- L(Train): 0.3640; L(Val): 0.4411; Reg Param: 0.0606; Time: 0.74\n",
      "Epoch 3672/8192 --- L(Train): 0.3767; L(Val): 0.4411; Reg Param: 0.0606; Time: 0.99\n",
      "Epoch 3673/8192 --- L(Train): 0.3734; L(Val): 0.4410; Reg Param: 0.0606; Time: 0.76\n",
      "Epoch 3674/8192 --- L(Train): 0.3696; L(Val): 0.4410; Reg Param: 0.0606; Time: 0.76\n",
      "Epoch 3675/8192 --- L(Train): 0.3670; L(Val): 0.4409; Reg Param: 0.0606; Time: 0.76\n",
      "Epoch 3676/8192 --- L(Train): 0.3621; L(Val): 0.4409; Reg Param: 0.0606; Time: 0.75\n",
      "Epoch 3677/8192 --- L(Train): 0.3631; L(Val): 0.4409; Reg Param: 0.0606; Time: 0.77\n",
      "Epoch 3678/8192 --- L(Train): 0.3553; L(Val): 0.4409; Reg Param: 0.0606; Time: 0.73\n",
      "Epoch 3679/8192 --- L(Train): 0.3690; L(Val): 0.4408; Reg Param: 0.0606; Time: 0.76\n",
      "Epoch 3680/8192 --- L(Train): 0.3724; L(Val): 0.4408; Reg Param: 0.0606; Time: 0.76\n",
      "Epoch 3681/8192 --- L(Train): 0.3662; L(Val): 0.4408; Reg Param: 0.0606; Time: 0.73\n",
      "Epoch 3682/8192 --- L(Train): 0.3775; L(Val): 0.4408; Reg Param: 0.0606; Time: 0.83\n",
      "Epoch 3683/8192 --- L(Train): 0.3759; L(Val): 0.4409; Reg Param: 0.0606; Time: 0.77\n",
      "Epoch 3684/8192 --- L(Train): 0.3746; L(Val): 0.4409; Reg Param: 0.0606; Time: 0.76\n",
      "Epoch 3685/8192 --- L(Train): 0.3789; L(Val): 0.4409; Reg Param: 0.0606; Time: 0.74\n",
      "Epoch 3686/8192 --- L(Train): 0.3709; L(Val): 0.4409; Reg Param: 0.0606; Time: 0.75\n",
      "Epoch 3687/8192 --- L(Train): 0.3749; L(Val): 0.4409; Reg Param: 0.0606; Time: 0.72\n",
      "Epoch 3688/8192 --- L(Train): 0.3797; L(Val): 0.4409; Reg Param: 0.0606; Time: 0.78\n",
      "Epoch 3689/8192 --- L(Train): 0.3792; L(Val): 0.4408; Reg Param: 0.0606; Time: 0.74\n",
      "Epoch 3690/8192 --- L(Train): 0.3749; L(Val): 0.4407; Reg Param: 0.0606; Time: 0.77\n",
      "Epoch 3691/8192 --- L(Train): 0.3750; L(Val): 0.4406; Reg Param: 0.0606; Time: 0.74\n",
      "Epoch 3692/8192 --- L(Train): 0.3774; L(Val): 0.4406; Reg Param: 0.0606; Time: 0.70\n",
      "Epoch 3693/8192 --- L(Train): 0.3706; L(Val): 0.4405; Reg Param: 0.0606; Time: 0.69\n",
      "Epoch 3694/8192 --- L(Train): 0.3537; L(Val): 0.4405; Reg Param: 0.0606; Time: 0.76\n",
      "Epoch 3695/8192 --- L(Train): 0.3794; L(Val): 0.4404; Reg Param: 0.0606; Time: 0.77\n",
      "Epoch 3696/8192 --- L(Train): 0.3742; L(Val): 0.4404; Reg Param: 0.0606; Time: 0.76\n",
      "Epoch 3697/8192 --- L(Train): 0.3688; L(Val): 0.4404; Reg Param: 0.0606; Time: 0.78\n",
      "Epoch 3698/8192 --- L(Train): 0.3754; L(Val): 0.4405; Reg Param: 0.0606; Time: 0.77\n",
      "Epoch 3699/8192 --- L(Train): 0.3904; L(Val): 0.4405; Reg Param: 0.0606; Time: 0.87\n",
      "Epoch 3700/8192 --- L(Train): 0.3692; L(Val): 0.4405; Reg Param: 0.0598; Time: 8.61\n",
      "Epoch 3701/8192 --- L(Train): 0.3706; L(Val): 0.4406; Reg Param: 0.0598; Time: 0.77\n",
      "Epoch 3702/8192 --- L(Train): 0.3767; L(Val): 0.4406; Reg Param: 0.0598; Time: 0.76\n",
      "Epoch 3703/8192 --- L(Train): 0.3664; L(Val): 0.4407; Reg Param: 0.0598; Time: 0.73\n",
      "Epoch 3704/8192 --- L(Train): 0.3736; L(Val): 0.4407; Reg Param: 0.0598; Time: 0.74\n",
      "Epoch 3705/8192 --- L(Train): 0.3706; L(Val): 0.4407; Reg Param: 0.0598; Time: 0.75\n",
      "Epoch 3706/8192 --- L(Train): 0.3819; L(Val): 0.4407; Reg Param: 0.0598; Time: 0.74\n",
      "Epoch 3707/8192 --- L(Train): 0.3862; L(Val): 0.4408; Reg Param: 0.0598; Time: 0.73\n",
      "Epoch 3708/8192 --- L(Train): 0.3754; L(Val): 0.4408; Reg Param: 0.0598; Time: 0.74\n",
      "Epoch 3709/8192 --- L(Train): 0.3660; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.74\n",
      "Epoch 3710/8192 --- L(Train): 0.3721; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.76\n",
      "Epoch 3711/8192 --- L(Train): 0.3615; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.77\n",
      "Epoch 3712/8192 --- L(Train): 0.3694; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.75\n",
      "Epoch 3713/8192 --- L(Train): 0.3735; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.78\n",
      "Epoch 3714/8192 --- L(Train): 0.3625; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.74\n",
      "Epoch 3715/8192 --- L(Train): 0.3805; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.74\n",
      "Epoch 3716/8192 --- L(Train): 0.3658; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.72\n",
      "Epoch 3717/8192 --- L(Train): 0.3846; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.69\n",
      "Epoch 3718/8192 --- L(Train): 0.3625; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.71\n",
      "Epoch 3719/8192 --- L(Train): 0.3679; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.74\n",
      "Epoch 3720/8192 --- L(Train): 0.3701; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.85\n",
      "Epoch 3721/8192 --- L(Train): 0.3745; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.69\n",
      "Epoch 3722/8192 --- L(Train): 0.3717; L(Val): 0.4410; Reg Param: 0.0598; Time: 0.69\n",
      "Epoch 3723/8192 --- L(Train): 0.3746; L(Val): 0.4410; Reg Param: 0.0598; Time: 0.69\n",
      "Epoch 3724/8192 --- L(Train): 0.3803; L(Val): 0.4410; Reg Param: 0.0598; Time: 0.69\n",
      "Epoch 3725/8192 --- L(Train): 0.3714; L(Val): 0.4411; Reg Param: 0.0598; Time: 0.74\n",
      "Epoch 3726/8192 --- L(Train): 0.3771; L(Val): 0.4411; Reg Param: 0.0598; Time: 0.84\n",
      "Epoch 3727/8192 --- L(Train): 0.3672; L(Val): 0.4412; Reg Param: 0.0598; Time: 0.82\n",
      "Epoch 3728/8192 --- L(Train): 0.3577; L(Val): 0.4412; Reg Param: 0.0598; Time: 0.80\n",
      "Epoch 3729/8192 --- L(Train): 0.3781; L(Val): 0.4412; Reg Param: 0.0598; Time: 0.79\n",
      "Epoch 3730/8192 --- L(Train): 0.3690; L(Val): 0.4412; Reg Param: 0.0598; Time: 0.73\n",
      "Epoch 3731/8192 --- L(Train): 0.3781; L(Val): 0.4412; Reg Param: 0.0598; Time: 0.73\n",
      "Epoch 3732/8192 --- L(Train): 0.3639; L(Val): 0.4411; Reg Param: 0.0598; Time: 0.77\n",
      "Epoch 3733/8192 --- L(Train): 0.3622; L(Val): 0.4410; Reg Param: 0.0598; Time: 0.77\n",
      "Epoch 3734/8192 --- L(Train): 0.3612; L(Val): 0.4410; Reg Param: 0.0598; Time: 0.76\n",
      "Epoch 3735/8192 --- L(Train): 0.3761; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.76\n",
      "Epoch 3736/8192 --- L(Train): 0.3768; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.77\n",
      "Epoch 3737/8192 --- L(Train): 0.3741; L(Val): 0.4409; Reg Param: 0.0598; Time: 0.77\n",
      "Epoch 3738/8192 --- L(Train): 0.3618; L(Val): 0.4410; Reg Param: 0.0598; Time: 0.78\n",
      "Epoch 3739/8192 --- L(Train): 0.3762; L(Val): 0.4410; Reg Param: 0.0598; Time: 0.78\n",
      "Epoch 3740/8192 --- L(Train): 0.3721; L(Val): 0.4411; Reg Param: 0.0598; Time: 0.76\n",
      "Epoch 3741/8192 --- L(Train): 0.3678; L(Val): 0.4411; Reg Param: 0.0598; Time: 0.73\n",
      "Epoch 3742/8192 --- L(Train): 0.3684; L(Val): 0.4411; Reg Param: 0.0598; Time: 0.74\n",
      "Epoch 3743/8192 --- L(Train): 0.3631; L(Val): 0.4412; Reg Param: 0.0598; Time: 0.74\n",
      "Epoch 3744/8192 --- L(Train): 0.3704; L(Val): 0.4412; Reg Param: 0.0598; Time: 0.75\n",
      "Epoch 3745/8192 --- L(Train): 0.3647; L(Val): 0.4412; Reg Param: 0.0598; Time: 0.84\n",
      "Epoch 3746/8192 --- L(Train): 0.3682; L(Val): 0.4412; Reg Param: 0.0598; Time: 0.74\n",
      "Epoch 3747/8192 --- L(Train): 0.3785; L(Val): 0.4413; Reg Param: 0.0598; Time: 0.77\n",
      "Epoch 3748/8192 --- L(Train): 0.3658; L(Val): 0.4413; Reg Param: 0.0598; Time: 0.76\n",
      "Epoch 3749/8192 --- L(Train): 0.3779; L(Val): 0.4413; Reg Param: 0.0598; Time: 0.89\n",
      "Epoch 3750/8192 --- L(Train): 0.3750; L(Val): 0.4413; Reg Param: 0.0589; Time: 7.84\n",
      "Epoch 3751/8192 --- L(Train): 0.3810; L(Val): 0.4413; Reg Param: 0.0589; Time: 0.74\n",
      "Epoch 3752/8192 --- L(Train): 0.3825; L(Val): 0.4412; Reg Param: 0.0589; Time: 0.73\n",
      "Epoch 3753/8192 --- L(Train): 0.3765; L(Val): 0.4412; Reg Param: 0.0589; Time: 0.72\n",
      "Epoch 3754/8192 --- L(Train): 0.3648; L(Val): 0.4411; Reg Param: 0.0589; Time: 0.72\n",
      "Epoch 3755/8192 --- L(Train): 0.3792; L(Val): 0.4411; Reg Param: 0.0589; Time: 0.73\n",
      "Epoch 3756/8192 --- L(Train): 0.3744; L(Val): 0.4411; Reg Param: 0.0589; Time: 0.73\n",
      "Epoch 3757/8192 --- L(Train): 0.3633; L(Val): 0.4411; Reg Param: 0.0589; Time: 0.71\n",
      "Epoch 3758/8192 --- L(Train): 0.3756; L(Val): 0.4410; Reg Param: 0.0589; Time: 0.70\n",
      "Epoch 3759/8192 --- L(Train): 0.3733; L(Val): 0.4410; Reg Param: 0.0589; Time: 0.70\n",
      "Epoch 3760/8192 --- L(Train): 0.3748; L(Val): 0.4409; Reg Param: 0.0589; Time: 0.81\n",
      "Epoch 3761/8192 --- L(Train): 0.3773; L(Val): 0.4408; Reg Param: 0.0589; Time: 0.73\n",
      "Epoch 3762/8192 --- L(Train): 0.3803; L(Val): 0.4408; Reg Param: 0.0589; Time: 0.74\n",
      "Epoch 3763/8192 --- L(Train): 0.3703; L(Val): 0.4407; Reg Param: 0.0589; Time: 0.72\n",
      "Epoch 3764/8192 --- L(Train): 0.3717; L(Val): 0.4407; Reg Param: 0.0589; Time: 0.72\n",
      "Epoch 3765/8192 --- L(Train): 0.3687; L(Val): 0.4406; Reg Param: 0.0589; Time: 0.73\n",
      "Epoch 3766/8192 --- L(Train): 0.3653; L(Val): 0.4406; Reg Param: 0.0589; Time: 0.73\n",
      "Epoch 3767/8192 --- L(Train): 0.3742; L(Val): 0.4406; Reg Param: 0.0589; Time: 0.75\n",
      "Epoch 3768/8192 --- L(Train): 0.3687; L(Val): 0.4406; Reg Param: 0.0589; Time: 0.96\n",
      "Epoch 3769/8192 --- L(Train): 0.3792; L(Val): 0.4406; Reg Param: 0.0589; Time: 0.87\n",
      "Epoch 3770/8192 --- L(Train): 0.3648; L(Val): 0.4406; Reg Param: 0.0589; Time: 0.76\n",
      "Epoch 3771/8192 --- L(Train): 0.3774; L(Val): 0.4407; Reg Param: 0.0589; Time: 0.77\n",
      "Epoch 3772/8192 --- L(Train): 0.3713; L(Val): 0.4407; Reg Param: 0.0589; Time: 0.75\n",
      "Epoch 3773/8192 --- L(Train): 0.3713; L(Val): 0.4408; Reg Param: 0.0589; Time: 0.70\n",
      "Epoch 3774/8192 --- L(Train): 0.3722; L(Val): 0.4408; Reg Param: 0.0589; Time: 0.72\n",
      "Epoch 3775/8192 --- L(Train): 0.3722; L(Val): 0.4408; Reg Param: 0.0589; Time: 0.76\n",
      "Epoch 3776/8192 --- L(Train): 0.3708; L(Val): 0.4409; Reg Param: 0.0589; Time: 0.76\n",
      "Epoch 3777/8192 --- L(Train): 0.3787; L(Val): 0.4409; Reg Param: 0.0589; Time: 0.76\n",
      "Epoch 3778/8192 --- L(Train): 0.3659; L(Val): 0.4409; Reg Param: 0.0589; Time: 0.75\n",
      "Epoch 3779/8192 --- L(Train): 0.3658; L(Val): 0.4409; Reg Param: 0.0589; Time: 0.71\n",
      "Epoch 3780/8192 --- L(Train): 0.3729; L(Val): 0.4409; Reg Param: 0.0589; Time: 0.71\n",
      "Epoch 3781/8192 --- L(Train): 0.3751; L(Val): 0.4409; Reg Param: 0.0589; Time: 0.69\n",
      "Epoch 3782/8192 --- L(Train): 0.3718; L(Val): 0.4409; Reg Param: 0.0589; Time: 0.71\n",
      "Epoch 3783/8192 --- L(Train): 0.3592; L(Val): 0.4409; Reg Param: 0.0589; Time: 0.70\n",
      "Epoch 3784/8192 --- L(Train): 0.3769; L(Val): 0.4410; Reg Param: 0.0589; Time: 0.76\n",
      "Epoch 3785/8192 --- L(Train): 0.3801; L(Val): 0.4410; Reg Param: 0.0589; Time: 0.74\n",
      "Epoch 3786/8192 --- L(Train): 0.3664; L(Val): 0.4410; Reg Param: 0.0589; Time: 0.73\n",
      "Epoch 3787/8192 --- L(Train): 0.3773; L(Val): 0.4410; Reg Param: 0.0589; Time: 0.75\n",
      "Epoch 3788/8192 --- L(Train): 0.3788; L(Val): 0.4410; Reg Param: 0.0589; Time: 0.75\n",
      "Epoch 3789/8192 --- L(Train): 0.3671; L(Val): 0.4410; Reg Param: 0.0589; Time: 0.70\n",
      "Epoch 3790/8192 --- L(Train): 0.3654; L(Val): 0.4410; Reg Param: 0.0589; Time: 0.69\n",
      "Epoch 3791/8192 --- L(Train): 0.3702; L(Val): 0.4411; Reg Param: 0.0589; Time: 0.69\n",
      "Epoch 3792/8192 --- L(Train): 0.3729; L(Val): 0.4411; Reg Param: 0.0589; Time: 0.69\n",
      "Epoch 3793/8192 --- L(Train): 0.3697; L(Val): 0.4411; Reg Param: 0.0589; Time: 0.78\n",
      "Epoch 3794/8192 --- L(Train): 0.3692; L(Val): 0.4412; Reg Param: 0.0589; Time: 0.75\n",
      "Epoch 3795/8192 --- L(Train): 0.3603; L(Val): 0.4412; Reg Param: 0.0589; Time: 0.77\n",
      "Epoch 3796/8192 --- L(Train): 0.3797; L(Val): 0.4412; Reg Param: 0.0589; Time: 0.87\n",
      "Epoch 3797/8192 --- L(Train): 0.3647; L(Val): 0.4412; Reg Param: 0.0589; Time: 0.84\n",
      "Epoch 3798/8192 --- L(Train): 0.3677; L(Val): 0.4412; Reg Param: 0.0589; Time: 0.77\n",
      "Epoch 3799/8192 --- L(Train): 0.3724; L(Val): 0.4412; Reg Param: 0.0589; Time: 0.78\n",
      "Epoch 3800/8192 --- L(Train): 0.3777; L(Val): 0.4412; Reg Param: 0.0581; Time: 8.30\n",
      "Epoch 3801/8192 --- L(Train): 0.3841; L(Val): 0.4411; Reg Param: 0.0581; Time: 0.72\n",
      "Epoch 3802/8192 --- L(Train): 0.3778; L(Val): 0.4411; Reg Param: 0.0581; Time: 0.74\n",
      "Epoch 3803/8192 --- L(Train): 0.3804; L(Val): 0.4412; Reg Param: 0.0581; Time: 0.75\n",
      "Epoch 3804/8192 --- L(Train): 0.3751; L(Val): 0.4412; Reg Param: 0.0581; Time: 0.77\n",
      "Epoch 3805/8192 --- L(Train): 0.3854; L(Val): 0.4412; Reg Param: 0.0581; Time: 0.77\n",
      "Epoch 3806/8192 --- L(Train): 0.3760; L(Val): 0.4412; Reg Param: 0.0581; Time: 0.75\n",
      "Epoch 3807/8192 --- L(Train): 0.3749; L(Val): 0.4412; Reg Param: 0.0581; Time: 0.74\n",
      "Epoch 3808/8192 --- L(Train): 0.3735; L(Val): 0.4413; Reg Param: 0.0581; Time: 0.75\n",
      "Epoch 3809/8192 --- L(Train): 0.3633; L(Val): 0.4413; Reg Param: 0.0581; Time: 0.74\n",
      "Epoch 3810/8192 --- L(Train): 0.3709; L(Val): 0.4413; Reg Param: 0.0581; Time: 0.71\n",
      "Epoch 3811/8192 --- L(Train): 0.3660; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.72\n",
      "Epoch 3812/8192 --- L(Train): 0.3718; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.79\n",
      "Epoch 3813/8192 --- L(Train): 0.3528; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.77\n",
      "Epoch 3814/8192 --- L(Train): 0.3724; L(Val): 0.4415; Reg Param: 0.0581; Time: 0.72\n",
      "Epoch 3815/8192 --- L(Train): 0.3872; L(Val): 0.4415; Reg Param: 0.0581; Time: 0.72\n",
      "Epoch 3816/8192 --- L(Train): 0.3768; L(Val): 0.4415; Reg Param: 0.0581; Time: 0.71\n",
      "Epoch 3817/8192 --- L(Train): 0.3741; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.80\n",
      "Epoch 3818/8192 --- L(Train): 0.3933; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.75\n",
      "Epoch 3819/8192 --- L(Train): 0.3755; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.73\n",
      "Epoch 3820/8192 --- L(Train): 0.3789; L(Val): 0.4414; Reg Param: 0.0581; Time: 1.10\n",
      "Epoch 3821/8192 --- L(Train): 0.3828; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.82\n",
      "Epoch 3822/8192 --- L(Train): 0.3725; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.85\n",
      "Epoch 3823/8192 --- L(Train): 0.3639; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.83\n",
      "Epoch 3824/8192 --- L(Train): 0.3658; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.82\n",
      "Epoch 3825/8192 --- L(Train): 0.3793; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.82\n",
      "Epoch 3826/8192 --- L(Train): 0.3727; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.87\n",
      "Epoch 3827/8192 --- L(Train): 0.3743; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.83\n",
      "Epoch 3828/8192 --- L(Train): 0.3709; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.84\n",
      "Epoch 3829/8192 --- L(Train): 0.3724; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.83\n",
      "Epoch 3830/8192 --- L(Train): 0.3673; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.82\n",
      "Epoch 3831/8192 --- L(Train): 0.3796; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.86\n",
      "Epoch 3832/8192 --- L(Train): 0.3658; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.79\n",
      "Epoch 3833/8192 --- L(Train): 0.3744; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.83\n",
      "Epoch 3834/8192 --- L(Train): 0.3657; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.83\n",
      "Epoch 3835/8192 --- L(Train): 0.3728; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.80\n",
      "Epoch 3836/8192 --- L(Train): 0.3675; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.79\n",
      "Epoch 3837/8192 --- L(Train): 0.3736; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.77\n",
      "Epoch 3838/8192 --- L(Train): 0.3667; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.76\n",
      "Epoch 3839/8192 --- L(Train): 0.3671; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.75\n",
      "Epoch 3840/8192 --- L(Train): 0.3856; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.75\n",
      "Epoch 3841/8192 --- L(Train): 0.3736; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.72\n",
      "Epoch 3842/8192 --- L(Train): 0.3688; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.73\n",
      "Epoch 3843/8192 --- L(Train): 0.3751; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.79\n",
      "Epoch 3844/8192 --- L(Train): 0.3686; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.98\n",
      "Epoch 3845/8192 --- L(Train): 0.3651; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.73\n",
      "Epoch 3846/8192 --- L(Train): 0.3670; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.74\n",
      "Epoch 3847/8192 --- L(Train): 0.3710; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.73\n",
      "Epoch 3848/8192 --- L(Train): 0.3774; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.75\n",
      "Epoch 3849/8192 --- L(Train): 0.3707; L(Val): 0.4414; Reg Param: 0.0581; Time: 0.74\n",
      "Epoch 3850/8192 --- L(Train): 0.3686; L(Val): 0.4414; Reg Param: 0.0574; Time: 8.25\n",
      "Epoch 3851/8192 --- L(Train): 0.3762; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.75\n",
      "Epoch 3852/8192 --- L(Train): 0.3866; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.76\n",
      "Epoch 3853/8192 --- L(Train): 0.3616; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.80\n",
      "Epoch 3854/8192 --- L(Train): 0.3797; L(Val): 0.4415; Reg Param: 0.0574; Time: 0.74\n",
      "Epoch 3855/8192 --- L(Train): 0.3615; L(Val): 0.4415; Reg Param: 0.0574; Time: 0.71\n",
      "Epoch 3856/8192 --- L(Train): 0.3792; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.78\n",
      "Epoch 3857/8192 --- L(Train): 0.3730; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.72\n",
      "Epoch 3858/8192 --- L(Train): 0.3741; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.76\n",
      "Epoch 3859/8192 --- L(Train): 0.3676; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.79\n",
      "Epoch 3860/8192 --- L(Train): 0.3679; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.98\n",
      "Epoch 3861/8192 --- L(Train): 0.3761; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.77\n",
      "Epoch 3862/8192 --- L(Train): 0.3711; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.79\n",
      "Epoch 3863/8192 --- L(Train): 0.3632; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.77\n",
      "Epoch 3864/8192 --- L(Train): 0.3630; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.76\n",
      "Epoch 3865/8192 --- L(Train): 0.3888; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.75\n",
      "Epoch 3866/8192 --- L(Train): 0.3769; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.79\n",
      "Epoch 3867/8192 --- L(Train): 0.3726; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.79\n",
      "Epoch 3868/8192 --- L(Train): 0.3680; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.77\n",
      "Epoch 3869/8192 --- L(Train): 0.3765; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.74\n",
      "Epoch 3870/8192 --- L(Train): 0.3753; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.71\n",
      "Epoch 3871/8192 --- L(Train): 0.3803; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.79\n",
      "Epoch 3872/8192 --- L(Train): 0.3647; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.79\n",
      "Epoch 3873/8192 --- L(Train): 0.3772; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.76\n",
      "Epoch 3874/8192 --- L(Train): 0.3797; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.79\n",
      "Epoch 3875/8192 --- L(Train): 0.3714; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.86\n",
      "Epoch 3876/8192 --- L(Train): 0.3771; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.77\n",
      "Epoch 3877/8192 --- L(Train): 0.3727; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.79\n",
      "Epoch 3878/8192 --- L(Train): 0.3673; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.79\n",
      "Epoch 3879/8192 --- L(Train): 0.3669; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.78\n",
      "Epoch 3880/8192 --- L(Train): 0.3662; L(Val): 0.4415; Reg Param: 0.0574; Time: 0.78\n",
      "Epoch 3881/8192 --- L(Train): 0.3678; L(Val): 0.4415; Reg Param: 0.0574; Time: 0.77\n",
      "Epoch 3882/8192 --- L(Train): 0.3733; L(Val): 0.4415; Reg Param: 0.0574; Time: 0.77\n",
      "Epoch 3883/8192 --- L(Train): 0.3789; L(Val): 0.4415; Reg Param: 0.0574; Time: 0.78\n",
      "Epoch 3884/8192 --- L(Train): 0.3763; L(Val): 0.4415; Reg Param: 0.0574; Time: 0.84\n",
      "Epoch 3885/8192 --- L(Train): 0.3701; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.78\n",
      "Epoch 3886/8192 --- L(Train): 0.3800; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.78\n",
      "Epoch 3887/8192 --- L(Train): 0.3732; L(Val): 0.4414; Reg Param: 0.0574; Time: 0.82\n",
      "Epoch 3888/8192 --- L(Train): 0.3646; L(Val): 0.4413; Reg Param: 0.0574; Time: 0.80\n",
      "Epoch 3889/8192 --- L(Train): 0.3654; L(Val): 0.4413; Reg Param: 0.0574; Time: 0.80\n",
      "Epoch 3890/8192 --- L(Train): 0.3677; L(Val): 0.4413; Reg Param: 0.0574; Time: 0.98\n",
      "Epoch 3891/8192 --- L(Train): 0.3897; L(Val): 0.4412; Reg Param: 0.0574; Time: 0.86\n",
      "Epoch 3892/8192 --- L(Train): 0.3622; L(Val): 0.4412; Reg Param: 0.0574; Time: 0.89\n",
      "Epoch 3893/8192 --- L(Train): 0.3752; L(Val): 0.4412; Reg Param: 0.0574; Time: 0.85\n",
      "Epoch 3894/8192 --- L(Train): 0.3648; L(Val): 0.4411; Reg Param: 0.0574; Time: 0.78\n",
      "Epoch 3895/8192 --- L(Train): 0.3682; L(Val): 0.4411; Reg Param: 0.0574; Time: 0.75\n",
      "Epoch 3896/8192 --- L(Train): 0.3811; L(Val): 0.4411; Reg Param: 0.0574; Time: 0.78\n",
      "Epoch 3897/8192 --- L(Train): 0.3798; L(Val): 0.4410; Reg Param: 0.0574; Time: 0.77\n",
      "Epoch 3898/8192 --- L(Train): 0.3788; L(Val): 0.4410; Reg Param: 0.0574; Time: 0.75\n",
      "Epoch 3899/8192 --- L(Train): 0.3666; L(Val): 0.4409; Reg Param: 0.0574; Time: 0.75\n",
      "Epoch 3900/8192 --- L(Train): 0.3759; L(Val): 0.4409; Reg Param: 0.0567; Time: 8.98\n",
      "Epoch 3901/8192 --- L(Train): 0.3804; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.71\n",
      "Epoch 3902/8192 --- L(Train): 0.3743; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.74\n",
      "Epoch 3903/8192 --- L(Train): 0.3744; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.71\n",
      "Epoch 3904/8192 --- L(Train): 0.3789; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.71\n",
      "Epoch 3905/8192 --- L(Train): 0.3778; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.80\n",
      "Epoch 3906/8192 --- L(Train): 0.3678; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.80\n",
      "Epoch 3907/8192 --- L(Train): 0.3745; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.70\n",
      "Epoch 3908/8192 --- L(Train): 0.3651; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.99\n",
      "Epoch 3909/8192 --- L(Train): 0.3663; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.66\n",
      "Epoch 3910/8192 --- L(Train): 0.3725; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.70\n",
      "Epoch 3911/8192 --- L(Train): 0.3745; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.70\n",
      "Epoch 3912/8192 --- L(Train): 0.3709; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.78\n",
      "Epoch 3913/8192 --- L(Train): 0.3656; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.69\n",
      "Epoch 3914/8192 --- L(Train): 0.3762; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.67\n",
      "Epoch 3915/8192 --- L(Train): 0.3734; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.68\n",
      "Epoch 3916/8192 --- L(Train): 0.3685; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.72\n",
      "Epoch 3917/8192 --- L(Train): 0.3745; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.68\n",
      "Epoch 3918/8192 --- L(Train): 0.3740; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.64\n",
      "Epoch 3919/8192 --- L(Train): 0.3763; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.73\n",
      "Epoch 3920/8192 --- L(Train): 0.3765; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.68\n",
      "Epoch 3921/8192 --- L(Train): 0.3775; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.69\n",
      "Epoch 3922/8192 --- L(Train): 0.3656; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.79\n",
      "Epoch 3923/8192 --- L(Train): 0.3810; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.89\n",
      "Epoch 3924/8192 --- L(Train): 0.3761; L(Val): 0.4407; Reg Param: 0.0567; Time: 1.13\n",
      "Epoch 3925/8192 --- L(Train): 0.3714; L(Val): 0.4407; Reg Param: 0.0567; Time: 0.75\n",
      "Epoch 3926/8192 --- L(Train): 0.3728; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.74\n",
      "Epoch 3927/8192 --- L(Train): 0.3818; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.72\n",
      "Epoch 3928/8192 --- L(Train): 0.3735; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.72\n",
      "Epoch 3929/8192 --- L(Train): 0.3794; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.73\n",
      "Epoch 3930/8192 --- L(Train): 0.3609; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.76\n",
      "Epoch 3931/8192 --- L(Train): 0.3678; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.77\n",
      "Epoch 3932/8192 --- L(Train): 0.3859; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.74\n",
      "Epoch 3933/8192 --- L(Train): 0.3722; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.75\n",
      "Epoch 3934/8192 --- L(Train): 0.3765; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.72\n",
      "Epoch 3935/8192 --- L(Train): 0.3699; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.73\n",
      "Epoch 3936/8192 --- L(Train): 0.3677; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.73\n",
      "Epoch 3937/8192 --- L(Train): 0.3673; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.69\n",
      "Epoch 3938/8192 --- L(Train): 0.3693; L(Val): 0.4408; Reg Param: 0.0567; Time: 0.68\n",
      "Epoch 3939/8192 --- L(Train): 0.3769; L(Val): 0.4409; Reg Param: 0.0567; Time: 0.90\n",
      "Epoch 3940/8192 --- L(Train): 0.3760; L(Val): 0.4409; Reg Param: 0.0567; Time: 0.68\n",
      "Epoch 3941/8192 --- L(Train): 0.3682; L(Val): 0.4409; Reg Param: 0.0567; Time: 0.68\n",
      "Epoch 3942/8192 --- L(Train): 0.3728; L(Val): 0.4409; Reg Param: 0.0567; Time: 0.69\n",
      "Epoch 3943/8192 --- L(Train): 0.3633; L(Val): 0.4409; Reg Param: 0.0567; Time: 0.73\n",
      "Epoch 3944/8192 --- L(Train): 0.3845; L(Val): 0.4409; Reg Param: 0.0567; Time: 0.75\n",
      "Epoch 3945/8192 --- L(Train): 0.3666; L(Val): 0.4409; Reg Param: 0.0567; Time: 0.72\n",
      "Epoch 3946/8192 --- L(Train): 0.3779; L(Val): 0.4409; Reg Param: 0.0567; Time: 0.69\n",
      "Epoch 3947/8192 --- L(Train): 0.3585; L(Val): 0.4409; Reg Param: 0.0567; Time: 0.73\n",
      "Epoch 3948/8192 --- L(Train): 0.3751; L(Val): 0.4409; Reg Param: 0.0567; Time: 0.77\n",
      "Epoch 3949/8192 --- L(Train): 0.3728; L(Val): 0.4410; Reg Param: 0.0567; Time: 0.76\n",
      "Epoch 3950/8192 --- L(Train): 0.3635; L(Val): 0.4410; Reg Param: 0.0560; Time: 8.79\n",
      "Epoch 3951/8192 --- L(Train): 0.3794; L(Val): 0.4410; Reg Param: 0.0560; Time: 1.60\n",
      "Epoch 3952/8192 --- L(Train): 0.3737; L(Val): 0.4410; Reg Param: 0.0560; Time: 0.95\n",
      "Epoch 3953/8192 --- L(Train): 0.3794; L(Val): 0.4410; Reg Param: 0.0560; Time: 0.93\n",
      "Epoch 3954/8192 --- L(Train): 0.3844; L(Val): 0.4410; Reg Param: 0.0560; Time: 0.80\n",
      "Epoch 3955/8192 --- L(Train): 0.3726; L(Val): 0.4410; Reg Param: 0.0560; Time: 0.85\n",
      "Epoch 3956/8192 --- L(Train): 0.3772; L(Val): 0.4410; Reg Param: 0.0560; Time: 1.04\n",
      "Epoch 3957/8192 --- L(Train): 0.3767; L(Val): 0.4410; Reg Param: 0.0560; Time: 0.83\n",
      "Epoch 3958/8192 --- L(Train): 0.3706; L(Val): 0.4410; Reg Param: 0.0560; Time: 0.77\n",
      "Epoch 3959/8192 --- L(Train): 0.3700; L(Val): 0.4410; Reg Param: 0.0560; Time: 0.80\n",
      "Epoch 3960/8192 --- L(Train): 0.3730; L(Val): 0.4410; Reg Param: 0.0560; Time: 0.75\n",
      "Epoch 3961/8192 --- L(Train): 0.3794; L(Val): 0.4410; Reg Param: 0.0560; Time: 0.81\n",
      "Epoch 3962/8192 --- L(Train): 0.3733; L(Val): 0.4410; Reg Param: 0.0560; Time: 0.77\n",
      "Epoch 3963/8192 --- L(Train): 0.3675; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.90\n",
      "Epoch 3964/8192 --- L(Train): 0.3676; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.80\n",
      "Epoch 3965/8192 --- L(Train): 0.3731; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.84\n",
      "Epoch 3966/8192 --- L(Train): 0.3502; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.76\n",
      "Epoch 3967/8192 --- L(Train): 0.3563; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.86\n",
      "Epoch 3968/8192 --- L(Train): 0.3590; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.80\n",
      "Epoch 3969/8192 --- L(Train): 0.3788; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.82\n",
      "Epoch 3970/8192 --- L(Train): 0.3728; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.78\n",
      "Epoch 3971/8192 --- L(Train): 0.3698; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.79\n",
      "Epoch 3972/8192 --- L(Train): 0.3788; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.71\n",
      "Epoch 3973/8192 --- L(Train): 0.3742; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.76\n",
      "Epoch 3974/8192 --- L(Train): 0.3638; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.80\n",
      "Epoch 3975/8192 --- L(Train): 0.3692; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.77\n",
      "Epoch 3976/8192 --- L(Train): 0.3715; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.77\n",
      "Epoch 3977/8192 --- L(Train): 0.3799; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.75\n",
      "Epoch 3978/8192 --- L(Train): 0.3824; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.78\n",
      "Epoch 3979/8192 --- L(Train): 0.3768; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.75\n",
      "Epoch 3980/8192 --- L(Train): 0.3727; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.76\n",
      "Epoch 3981/8192 --- L(Train): 0.3642; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.87\n",
      "Epoch 3982/8192 --- L(Train): 0.3668; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.78\n",
      "Epoch 3983/8192 --- L(Train): 0.3695; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.79\n",
      "Epoch 3984/8192 --- L(Train): 0.3671; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.78\n",
      "Epoch 3985/8192 --- L(Train): 0.3795; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.77\n",
      "Epoch 3986/8192 --- L(Train): 0.3674; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.80\n",
      "Epoch 3987/8192 --- L(Train): 0.3624; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.94\n",
      "Epoch 3988/8192 --- L(Train): 0.3567; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.76\n",
      "Epoch 3989/8192 --- L(Train): 0.3806; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.82\n",
      "Epoch 3990/8192 --- L(Train): 0.3760; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.77\n",
      "Epoch 3991/8192 --- L(Train): 0.3757; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.72\n",
      "Epoch 3992/8192 --- L(Train): 0.3770; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.75\n",
      "Epoch 3993/8192 --- L(Train): 0.3571; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.86\n",
      "Epoch 3994/8192 --- L(Train): 0.3705; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.76\n",
      "Epoch 3995/8192 --- L(Train): 0.3704; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.78\n",
      "Epoch 3996/8192 --- L(Train): 0.3748; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.75\n",
      "Epoch 3997/8192 --- L(Train): 0.3676; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.74\n",
      "Epoch 3998/8192 --- L(Train): 0.3704; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.76\n",
      "Epoch 3999/8192 --- L(Train): 0.3704; L(Val): 0.4411; Reg Param: 0.0560; Time: 0.76\n",
      "Epoch 4000/8192 --- L(Train): 0.3699; L(Val): 0.4411; Reg Param: 0.0553; Time: 8.14\n",
      "Epoch 4001/8192 --- L(Train): 0.3742; L(Val): 0.4411; Reg Param: 0.0553; Time: 0.75\n",
      "Epoch 4002/8192 --- L(Train): 0.3700; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.71\n",
      "Epoch 4003/8192 --- L(Train): 0.3671; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.73\n",
      "Epoch 4004/8192 --- L(Train): 0.3702; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.75\n",
      "Epoch 4005/8192 --- L(Train): 0.3688; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.77\n",
      "Epoch 4006/8192 --- L(Train): 0.3703; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.75\n",
      "Epoch 4007/8192 --- L(Train): 0.3876; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.94\n",
      "Epoch 4008/8192 --- L(Train): 0.3773; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.74\n",
      "Epoch 4009/8192 --- L(Train): 0.3557; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.76\n",
      "Epoch 4010/8192 --- L(Train): 0.3619; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.74\n",
      "Epoch 4011/8192 --- L(Train): 0.3801; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.77\n",
      "Epoch 4012/8192 --- L(Train): 0.3742; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.75\n",
      "Epoch 4013/8192 --- L(Train): 0.3721; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.74\n",
      "Epoch 4014/8192 --- L(Train): 0.3672; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.72\n",
      "Epoch 4015/8192 --- L(Train): 0.3847; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.74\n",
      "Epoch 4016/8192 --- L(Train): 0.3715; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.74\n",
      "Epoch 4017/8192 --- L(Train): 0.3617; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.75\n",
      "Epoch 4018/8192 --- L(Train): 0.3631; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.73\n",
      "Epoch 4019/8192 --- L(Train): 0.3682; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.80\n",
      "Epoch 4020/8192 --- L(Train): 0.3796; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.75\n",
      "Epoch 4021/8192 --- L(Train): 0.3682; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.75\n",
      "Epoch 4022/8192 --- L(Train): 0.3759; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.73\n",
      "Epoch 4023/8192 --- L(Train): 0.3712; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.76\n",
      "Epoch 4024/8192 --- L(Train): 0.3690; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.69\n",
      "Epoch 4025/8192 --- L(Train): 0.3725; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.74\n",
      "Epoch 4026/8192 --- L(Train): 0.3759; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.74\n",
      "Epoch 4027/8192 --- L(Train): 0.3567; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.72\n",
      "Epoch 4028/8192 --- L(Train): 0.3780; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.71\n",
      "Epoch 4029/8192 --- L(Train): 0.3638; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.73\n",
      "Epoch 4030/8192 --- L(Train): 0.3708; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.76\n",
      "Epoch 4031/8192 --- L(Train): 0.3699; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.74\n",
      "Epoch 4032/8192 --- L(Train): 0.3755; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.69\n",
      "Epoch 4033/8192 --- L(Train): 0.3867; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.69\n",
      "Epoch 4034/8192 --- L(Train): 0.3756; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.79\n",
      "Epoch 4035/8192 --- L(Train): 0.3691; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.69\n",
      "Epoch 4036/8192 --- L(Train): 0.3626; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.71\n",
      "Epoch 4037/8192 --- L(Train): 0.3750; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.73\n",
      "Epoch 4038/8192 --- L(Train): 0.3674; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.71\n",
      "Epoch 4039/8192 --- L(Train): 0.3706; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.73\n",
      "Epoch 4040/8192 --- L(Train): 0.3696; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.97\n",
      "Epoch 4041/8192 --- L(Train): 0.3690; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.72\n",
      "Epoch 4042/8192 --- L(Train): 0.3623; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.73\n",
      "Epoch 4043/8192 --- L(Train): 0.3725; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.76\n",
      "Epoch 4044/8192 --- L(Train): 0.3691; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.77\n",
      "Epoch 4045/8192 --- L(Train): 0.3666; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.79\n",
      "Epoch 4046/8192 --- L(Train): 0.3709; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.79\n",
      "Epoch 4047/8192 --- L(Train): 0.3681; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.76\n",
      "Epoch 4048/8192 --- L(Train): 0.3603; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.79\n",
      "Epoch 4049/8192 --- L(Train): 0.3673; L(Val): 0.4412; Reg Param: 0.0553; Time: 0.77\n",
      "Epoch 4050/8192 --- L(Train): 0.3762; L(Val): 0.4412; Reg Param: 0.0547; Time: 7.93\n",
      "Epoch 4051/8192 --- L(Train): 0.3739; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.75\n",
      "Epoch 4052/8192 --- L(Train): 0.3747; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.72\n",
      "Epoch 4053/8192 --- L(Train): 0.3653; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.71\n",
      "Epoch 4054/8192 --- L(Train): 0.3715; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.69\n",
      "Epoch 4055/8192 --- L(Train): 0.3655; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.79\n",
      "Epoch 4056/8192 --- L(Train): 0.3818; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.68\n",
      "Epoch 4057/8192 --- L(Train): 0.3633; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.69\n",
      "Epoch 4058/8192 --- L(Train): 0.3819; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.68\n",
      "Epoch 4059/8192 --- L(Train): 0.3785; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.68\n",
      "Epoch 4060/8192 --- L(Train): 0.3578; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.90\n",
      "Epoch 4061/8192 --- L(Train): 0.3807; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.84\n",
      "Epoch 4062/8192 --- L(Train): 0.3632; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.71\n",
      "Epoch 4063/8192 --- L(Train): 0.3736; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.77\n",
      "Epoch 4064/8192 --- L(Train): 0.3578; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.74\n",
      "Epoch 4065/8192 --- L(Train): 0.3679; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.70\n",
      "Epoch 4066/8192 --- L(Train): 0.3677; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.71\n",
      "Epoch 4067/8192 --- L(Train): 0.3641; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.73\n",
      "Epoch 4068/8192 --- L(Train): 0.3682; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.75\n",
      "Epoch 4069/8192 --- L(Train): 0.3693; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.73\n",
      "Epoch 4070/8192 --- L(Train): 0.3699; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.70\n",
      "Epoch 4071/8192 --- L(Train): 0.3709; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.72\n",
      "Epoch 4072/8192 --- L(Train): 0.3737; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.71\n",
      "Epoch 4073/8192 --- L(Train): 0.3790; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.69\n",
      "Epoch 4074/8192 --- L(Train): 0.3674; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.69\n",
      "Epoch 4075/8192 --- L(Train): 0.3540; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.76\n",
      "Epoch 4076/8192 --- L(Train): 0.3645; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.78\n",
      "Epoch 4077/8192 --- L(Train): 0.3690; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.74\n",
      "Epoch 4078/8192 --- L(Train): 0.3684; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.76\n",
      "Epoch 4079/8192 --- L(Train): 0.3777; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.78\n",
      "Epoch 4080/8192 --- L(Train): 0.3663; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.74\n",
      "Epoch 4081/8192 --- L(Train): 0.3688; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.75\n",
      "Epoch 4082/8192 --- L(Train): 0.3625; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.76\n",
      "Epoch 4083/8192 --- L(Train): 0.3680; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.72\n",
      "Epoch 4084/8192 --- L(Train): 0.3690; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.85\n",
      "Epoch 4085/8192 --- L(Train): 0.3627; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.76\n",
      "Epoch 4086/8192 --- L(Train): 0.3754; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.75\n",
      "Epoch 4087/8192 --- L(Train): 0.3718; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.77\n",
      "Epoch 4088/8192 --- L(Train): 0.3691; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.76\n",
      "Epoch 4089/8192 --- L(Train): 0.3726; L(Val): 0.4412; Reg Param: 0.0547; Time: 1.04\n",
      "Epoch 4090/8192 --- L(Train): 0.3826; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.76\n",
      "Epoch 4091/8192 --- L(Train): 0.3778; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.76\n",
      "Epoch 4092/8192 --- L(Train): 0.3676; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.77\n",
      "Epoch 4093/8192 --- L(Train): 0.3581; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.74\n",
      "Epoch 4094/8192 --- L(Train): 0.3815; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.73\n",
      "Epoch 4095/8192 --- L(Train): 0.3751; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.74\n",
      "Epoch 4096/8192 --- L(Train): 0.3822; L(Val): 0.4412; Reg Param: 0.0547; Time: 0.75\n",
      "Epoch 4097/8192 --- L(Train): 0.3724; L(Val): 0.4424; Reg Param: 0.0547; Time: 0.73\n",
      "Epoch 4098/8192 --- L(Train): 0.3799; L(Val): 0.4436; Reg Param: 0.0547; Time: 0.74\n",
      "Epoch 4099/8192 --- L(Train): 0.3658; L(Val): 0.4440; Reg Param: 0.0547; Time: 0.76\n",
      "Epoch 4100/8192 --- L(Train): 0.3660; L(Val): 0.4440; Reg Param: 0.0541; Time: 7.79\n",
      "Epoch 4101/8192 --- L(Train): 0.3665; L(Val): 0.4433; Reg Param: 0.0541; Time: 0.77\n",
      "Epoch 4102/8192 --- L(Train): 0.3756; L(Val): 0.4429; Reg Param: 0.0541; Time: 0.76\n",
      "Epoch 4103/8192 --- L(Train): 0.3751; L(Val): 0.4428; Reg Param: 0.0541; Time: 0.78\n",
      "Epoch 4104/8192 --- L(Train): 0.3818; L(Val): 0.4432; Reg Param: 0.0541; Time: 0.73\n",
      "Epoch 4105/8192 --- L(Train): 0.3766; L(Val): 0.4426; Reg Param: 0.0541; Time: 0.84\n",
      "Epoch 4106/8192 --- L(Train): 0.3795; L(Val): 0.4428; Reg Param: 0.0541; Time: 0.96\n",
      "Epoch 4107/8192 --- L(Train): 0.3803; L(Val): 0.4435; Reg Param: 0.0541; Time: 0.76\n",
      "Epoch 4108/8192 --- L(Train): 0.3669; L(Val): 0.4432; Reg Param: 0.0541; Time: 0.79\n",
      "Epoch 4109/8192 --- L(Train): 0.3773; L(Val): 0.4428; Reg Param: 0.0541; Time: 0.79\n",
      "Epoch 4110/8192 --- L(Train): 0.3749; L(Val): 0.4427; Reg Param: 0.0541; Time: 0.80\n",
      "Epoch 4111/8192 --- L(Train): 0.3683; L(Val): 0.4434; Reg Param: 0.0541; Time: 0.96\n",
      "Epoch 4112/8192 --- L(Train): 0.3739; L(Val): 0.4425; Reg Param: 0.0541; Time: 0.72\n",
      "Epoch 4113/8192 --- L(Train): 0.3645; L(Val): 0.4436; Reg Param: 0.0541; Time: 0.74\n",
      "Epoch 4114/8192 --- L(Train): 0.3761; L(Val): 0.4438; Reg Param: 0.0541; Time: 0.73\n",
      "Epoch 4115/8192 --- L(Train): 0.3674; L(Val): 0.4428; Reg Param: 0.0541; Time: 0.72\n",
      "Epoch 4116/8192 --- L(Train): 0.3795; L(Val): 0.4412; Reg Param: 0.0541; Time: 0.71\n",
      "Epoch 4117/8192 --- L(Train): 0.3764; L(Val): 0.4391; Reg Param: 0.0541; Time: 0.69\n",
      "Epoch 4118/8192 --- L(Train): 0.3758; L(Val): 0.4384; Reg Param: 0.0541; Time: 0.69\n",
      "Epoch 4119/8192 --- L(Train): 0.3706; L(Val): 0.4379; Reg Param: 0.0541; Time: 0.69\n",
      "Epoch 4120/8192 --- L(Train): 0.3793; L(Val): 0.4383; Reg Param: 0.0541; Time: 0.68\n",
      "Epoch 4121/8192 --- L(Train): 0.3848; L(Val): 0.4379; Reg Param: 0.0541; Time: 0.75\n",
      "Epoch 4122/8192 --- L(Train): 0.3825; L(Val): 0.4379; Reg Param: 0.0541; Time: 0.71\n",
      "Epoch 4123/8192 --- L(Train): 0.3829; L(Val): 0.4378; Reg Param: 0.0541; Time: 0.73\n",
      "Epoch 4124/8192 --- L(Train): 0.3637; L(Val): 0.4381; Reg Param: 0.0541; Time: 0.71\n",
      "Epoch 4125/8192 --- L(Train): 0.3809; L(Val): 0.4396; Reg Param: 0.0541; Time: 0.70\n",
      "Epoch 4126/8192 --- L(Train): 0.3723; L(Val): 0.4403; Reg Param: 0.0541; Time: 0.75\n",
      "Epoch 4127/8192 --- L(Train): 0.3743; L(Val): 0.4401; Reg Param: 0.0541; Time: 0.74\n",
      "Epoch 4128/8192 --- L(Train): 0.3716; L(Val): 0.4388; Reg Param: 0.0541; Time: 0.70\n",
      "Epoch 4129/8192 --- L(Train): 0.3703; L(Val): 0.4388; Reg Param: 0.0541; Time: 0.73\n",
      "Epoch 4130/8192 --- L(Train): 0.3922; L(Val): 0.4394; Reg Param: 0.0541; Time: 0.74\n",
      "Epoch 4131/8192 --- L(Train): 0.3857; L(Val): 0.4401; Reg Param: 0.0541; Time: 0.72\n",
      "Epoch 4132/8192 --- L(Train): 0.3690; L(Val): 0.4427; Reg Param: 0.0541; Time: 0.75\n",
      "Epoch 4133/8192 --- L(Train): 0.3931; L(Val): 0.4413; Reg Param: 0.0541; Time: 0.75\n",
      "Epoch 4134/8192 --- L(Train): 0.3774; L(Val): 0.4407; Reg Param: 0.0541; Time: 0.89\n",
      "Epoch 4135/8192 --- L(Train): 0.3749; L(Val): 0.4402; Reg Param: 0.0541; Time: 0.78\n",
      "Epoch 4136/8192 --- L(Train): 0.3684; L(Val): 0.4403; Reg Param: 0.0541; Time: 0.88\n",
      "Epoch 4137/8192 --- L(Train): 0.3762; L(Val): 0.4397; Reg Param: 0.0541; Time: 0.77\n",
      "Epoch 4138/8192 --- L(Train): 0.3675; L(Val): 0.4365; Reg Param: 0.0541; Time: 0.74\n",
      "Epoch 4139/8192 --- L(Train): 0.3734; L(Val): 0.4352; Reg Param: 0.0541; Time: 0.75\n",
      "Epoch 4140/8192 --- L(Train): 0.3732; L(Val): 0.4354; Reg Param: 0.0541; Time: 0.76\n",
      "Epoch 4141/8192 --- L(Train): 0.3770; L(Val): 0.4367; Reg Param: 0.0541; Time: 0.76\n",
      "Epoch 4142/8192 --- L(Train): 0.3790; L(Val): 0.4388; Reg Param: 0.0541; Time: 0.74\n",
      "Epoch 4143/8192 --- L(Train): 0.3756; L(Val): 0.4380; Reg Param: 0.0541; Time: 0.80\n",
      "Epoch 4144/8192 --- L(Train): 0.3889; L(Val): 0.4384; Reg Param: 0.0541; Time: 0.85\n",
      "Epoch 4145/8192 --- L(Train): 0.3741; L(Val): 0.4391; Reg Param: 0.0541; Time: 0.75\n",
      "Epoch 4146/8192 --- L(Train): 0.3808; L(Val): 0.4408; Reg Param: 0.0541; Time: 0.75\n",
      "Epoch 4147/8192 --- L(Train): 0.3784; L(Val): 0.4406; Reg Param: 0.0541; Time: 0.74\n",
      "Epoch 4148/8192 --- L(Train): 0.3770; L(Val): 0.4388; Reg Param: 0.0541; Time: 0.75\n",
      "Epoch 4149/8192 --- L(Train): 0.3912; L(Val): 0.4385; Reg Param: 0.0541; Time: 0.71\n",
      "Epoch 4150/8192 --- L(Train): 0.3754; L(Val): 0.4385; Reg Param: 0.0535; Time: 7.83\n",
      "Epoch 4151/8192 --- L(Train): 0.3612; L(Val): 0.4393; Reg Param: 0.0535; Time: 0.77\n",
      "Epoch 4152/8192 --- L(Train): 0.3745; L(Val): 0.4400; Reg Param: 0.0535; Time: 0.75\n",
      "Epoch 4153/8192 --- L(Train): 0.3842; L(Val): 0.4393; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4154/8192 --- L(Train): 0.3833; L(Val): 0.4391; Reg Param: 0.0535; Time: 0.76\n",
      "Epoch 4155/8192 --- L(Train): 0.3806; L(Val): 0.4392; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4156/8192 --- L(Train): 0.3783; L(Val): 0.4392; Reg Param: 0.0535; Time: 0.74\n",
      "Epoch 4157/8192 --- L(Train): 0.3834; L(Val): 0.4406; Reg Param: 0.0535; Time: 0.74\n",
      "Epoch 4158/8192 --- L(Train): 0.3766; L(Val): 0.4409; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4159/8192 --- L(Train): 0.3819; L(Val): 0.4385; Reg Param: 0.0535; Time: 0.72\n",
      "Epoch 4160/8192 --- L(Train): 0.3786; L(Val): 0.4385; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4161/8192 --- L(Train): 0.3950; L(Val): 0.4401; Reg Param: 0.0535; Time: 0.75\n",
      "Epoch 4162/8192 --- L(Train): 0.3833; L(Val): 0.4404; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4163/8192 --- L(Train): 0.3818; L(Val): 0.4420; Reg Param: 0.0535; Time: 0.72\n",
      "Epoch 4164/8192 --- L(Train): 0.3841; L(Val): 0.4412; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4165/8192 --- L(Train): 0.3793; L(Val): 0.4388; Reg Param: 0.0535; Time: 1.09\n",
      "Epoch 4166/8192 --- L(Train): 0.3905; L(Val): 0.4382; Reg Param: 0.0535; Time: 0.74\n",
      "Epoch 4167/8192 --- L(Train): 0.3782; L(Val): 0.4368; Reg Param: 0.0535; Time: 0.74\n",
      "Epoch 4168/8192 --- L(Train): 0.3852; L(Val): 0.4378; Reg Param: 0.0535; Time: 0.76\n",
      "Epoch 4169/8192 --- L(Train): 0.3791; L(Val): 0.4388; Reg Param: 0.0535; Time: 0.74\n",
      "Epoch 4170/8192 --- L(Train): 0.3762; L(Val): 0.4370; Reg Param: 0.0535; Time: 0.75\n",
      "Epoch 4171/8192 --- L(Train): 0.3766; L(Val): 0.4374; Reg Param: 0.0535; Time: 0.75\n",
      "Epoch 4172/8192 --- L(Train): 0.3855; L(Val): 0.4367; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4173/8192 --- L(Train): 0.3778; L(Val): 0.4379; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4174/8192 --- L(Train): 0.3760; L(Val): 0.4391; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4175/8192 --- L(Train): 0.3847; L(Val): 0.4383; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4176/8192 --- L(Train): 0.3852; L(Val): 0.4381; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4177/8192 --- L(Train): 0.3752; L(Val): 0.4388; Reg Param: 0.0535; Time: 0.75\n",
      "Epoch 4178/8192 --- L(Train): 0.3789; L(Val): 0.4385; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4179/8192 --- L(Train): 0.3793; L(Val): 0.4399; Reg Param: 0.0535; Time: 0.75\n",
      "Epoch 4180/8192 --- L(Train): 0.3733; L(Val): 0.4395; Reg Param: 0.0535; Time: 0.70\n",
      "Epoch 4181/8192 --- L(Train): 0.3642; L(Val): 0.4378; Reg Param: 0.0535; Time: 0.70\n",
      "Epoch 4182/8192 --- L(Train): 0.3786; L(Val): 0.4375; Reg Param: 0.0535; Time: 0.70\n",
      "Epoch 4183/8192 --- L(Train): 0.3750; L(Val): 0.4368; Reg Param: 0.0535; Time: 0.84\n",
      "Epoch 4184/8192 --- L(Train): 0.3734; L(Val): 0.4371; Reg Param: 0.0535; Time: 0.74\n",
      "Epoch 4185/8192 --- L(Train): 0.3648; L(Val): 0.4384; Reg Param: 0.0535; Time: 0.75\n",
      "Epoch 4186/8192 --- L(Train): 0.3645; L(Val): 0.4364; Reg Param: 0.0535; Time: 0.76\n",
      "Epoch 4187/8192 --- L(Train): 0.3876; L(Val): 0.4349; Reg Param: 0.0535; Time: 0.74\n",
      "Epoch 4188/8192 --- L(Train): 0.3756; L(Val): 0.4346; Reg Param: 0.0535; Time: 0.76\n",
      "Epoch 4189/8192 --- L(Train): 0.3807; L(Val): 0.4338; Reg Param: 0.0535; Time: 0.74\n",
      "Epoch 4190/8192 --- L(Train): 0.3800; L(Val): 0.4349; Reg Param: 0.0535; Time: 0.72\n",
      "Epoch 4191/8192 --- L(Train): 0.3820; L(Val): 0.4357; Reg Param: 0.0535; Time: 0.71\n",
      "Epoch 4192/8192 --- L(Train): 0.3807; L(Val): 0.4350; Reg Param: 0.0535; Time: 0.80\n",
      "Epoch 4193/8192 --- L(Train): 0.3825; L(Val): 0.4347; Reg Param: 0.0535; Time: 0.97\n",
      "Epoch 4194/8192 --- L(Train): 0.3777; L(Val): 0.4347; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4195/8192 --- L(Train): 0.3825; L(Val): 0.4346; Reg Param: 0.0535; Time: 0.74\n",
      "Epoch 4196/8192 --- L(Train): 0.3790; L(Val): 0.4344; Reg Param: 0.0535; Time: 0.73\n",
      "Epoch 4197/8192 --- L(Train): 0.3779; L(Val): 0.4343; Reg Param: 0.0535; Time: 0.70\n",
      "Epoch 4198/8192 --- L(Train): 0.3773; L(Val): 0.4347; Reg Param: 0.0535; Time: 0.72\n",
      "Epoch 4199/8192 --- L(Train): 0.3733; L(Val): 0.4347; Reg Param: 0.0535; Time: 0.72\n",
      "Epoch 4200/8192 --- L(Train): 0.3668; L(Val): 0.4347; Reg Param: 0.0530; Time: 7.69\n",
      "Epoch 4201/8192 --- L(Train): 0.3798; L(Val): 0.4348; Reg Param: 0.0530; Time: 0.90\n",
      "Epoch 4202/8192 --- L(Train): 0.3827; L(Val): 0.4354; Reg Param: 0.0530; Time: 0.75\n",
      "Epoch 4203/8192 --- L(Train): 0.3837; L(Val): 0.4354; Reg Param: 0.0530; Time: 0.78\n",
      "Epoch 4204/8192 --- L(Train): 0.3646; L(Val): 0.4354; Reg Param: 0.0530; Time: 0.78\n",
      "Epoch 4205/8192 --- L(Train): 0.3801; L(Val): 0.4348; Reg Param: 0.0530; Time: 0.75\n",
      "Epoch 4206/8192 --- L(Train): 0.3813; L(Val): 0.4349; Reg Param: 0.0530; Time: 0.75\n",
      "Epoch 4207/8192 --- L(Train): 0.3886; L(Val): 0.4355; Reg Param: 0.0530; Time: 0.77\n",
      "Epoch 4208/8192 --- L(Train): 0.3892; L(Val): 0.4366; Reg Param: 0.0530; Time: 0.75\n",
      "Epoch 4209/8192 --- L(Train): 0.3660; L(Val): 0.4375; Reg Param: 0.0530; Time: 0.82\n",
      "Epoch 4210/8192 --- L(Train): 0.3886; L(Val): 0.4375; Reg Param: 0.0530; Time: 0.75\n",
      "Epoch 4211/8192 --- L(Train): 0.3939; L(Val): 0.4373; Reg Param: 0.0530; Time: 0.82\n",
      "Epoch 4212/8192 --- L(Train): 0.3825; L(Val): 0.4370; Reg Param: 0.0530; Time: 0.82\n",
      "Epoch 4213/8192 --- L(Train): 0.3867; L(Val): 0.4369; Reg Param: 0.0530; Time: 0.83\n",
      "Epoch 4214/8192 --- L(Train): 0.3753; L(Val): 0.4367; Reg Param: 0.0530; Time: 0.74\n",
      "Epoch 4215/8192 --- L(Train): 0.3827; L(Val): 0.4383; Reg Param: 0.0530; Time: 0.76\n",
      "Epoch 4216/8192 --- L(Train): 0.3863; L(Val): 0.4395; Reg Param: 0.0530; Time: 0.76\n",
      "Epoch 4217/8192 --- L(Train): 0.3829; L(Val): 0.4401; Reg Param: 0.0530; Time: 1.00\n",
      "Epoch 4218/8192 --- L(Train): 0.3797; L(Val): 0.4401; Reg Param: 0.0530; Time: 0.77\n",
      "Epoch 4219/8192 --- L(Train): 0.3818; L(Val): 0.4402; Reg Param: 0.0530; Time: 0.79\n",
      "Epoch 4220/8192 --- L(Train): 0.3728; L(Val): 0.4407; Reg Param: 0.0530; Time: 0.70\n",
      "Epoch 4221/8192 --- L(Train): 0.3689; L(Val): 0.4404; Reg Param: 0.0530; Time: 0.76\n",
      "Epoch 4222/8192 --- L(Train): 0.3737; L(Val): 0.4402; Reg Param: 0.0530; Time: 0.72\n",
      "Epoch 4223/8192 --- L(Train): 0.3930; L(Val): 0.4392; Reg Param: 0.0530; Time: 0.76\n",
      "Epoch 4224/8192 --- L(Train): 0.3890; L(Val): 0.4382; Reg Param: 0.0530; Time: 0.73\n",
      "Epoch 4225/8192 --- L(Train): 0.3883; L(Val): 0.4380; Reg Param: 0.0530; Time: 0.74\n",
      "Epoch 4226/8192 --- L(Train): 0.3635; L(Val): 0.4382; Reg Param: 0.0530; Time: 0.83\n",
      "Epoch 4227/8192 --- L(Train): 0.3743; L(Val): 0.4392; Reg Param: 0.0530; Time: 0.74\n",
      "Epoch 4228/8192 --- L(Train): 0.3651; L(Val): 0.4398; Reg Param: 0.0530; Time: 0.73\n",
      "Epoch 4229/8192 --- L(Train): 0.3886; L(Val): 0.4398; Reg Param: 0.0530; Time: 0.73\n",
      "Epoch 4230/8192 --- L(Train): 0.3723; L(Val): 0.4400; Reg Param: 0.0530; Time: 0.70\n",
      "Epoch 4231/8192 --- L(Train): 0.3796; L(Val): 0.4395; Reg Param: 0.0530; Time: 0.72\n",
      "Epoch 4232/8192 --- L(Train): 0.3654; L(Val): 0.4406; Reg Param: 0.0530; Time: 0.71\n",
      "Epoch 4233/8192 --- L(Train): 0.3757; L(Val): 0.4417; Reg Param: 0.0530; Time: 0.71\n",
      "Epoch 4234/8192 --- L(Train): 0.3769; L(Val): 0.4417; Reg Param: 0.0530; Time: 0.73\n",
      "Epoch 4235/8192 --- L(Train): 0.3733; L(Val): 0.4421; Reg Param: 0.0530; Time: 0.71\n",
      "Epoch 4236/8192 --- L(Train): 0.3843; L(Val): 0.4421; Reg Param: 0.0530; Time: 0.70\n",
      "Epoch 4237/8192 --- L(Train): 0.3653; L(Val): 0.4417; Reg Param: 0.0530; Time: 0.71\n",
      "Epoch 4238/8192 --- L(Train): 0.3795; L(Val): 0.4410; Reg Param: 0.0530; Time: 0.70\n",
      "Epoch 4239/8192 --- L(Train): 0.3777; L(Val): 0.4394; Reg Param: 0.0530; Time: 0.72\n",
      "Epoch 4240/8192 --- L(Train): 0.3774; L(Val): 0.4387; Reg Param: 0.0530; Time: 0.73\n",
      "Epoch 4241/8192 --- L(Train): 0.3724; L(Val): 0.4388; Reg Param: 0.0530; Time: 0.76\n",
      "Epoch 4242/8192 --- L(Train): 0.3752; L(Val): 0.4396; Reg Param: 0.0530; Time: 0.74\n",
      "Epoch 4243/8192 --- L(Train): 0.3791; L(Val): 0.4410; Reg Param: 0.0530; Time: 1.02\n",
      "Epoch 4244/8192 --- L(Train): 0.3641; L(Val): 0.4404; Reg Param: 0.0530; Time: 0.74\n",
      "Epoch 4245/8192 --- L(Train): 0.3759; L(Val): 0.4399; Reg Param: 0.0530; Time: 0.78\n",
      "Epoch 4246/8192 --- L(Train): 0.3700; L(Val): 0.4395; Reg Param: 0.0530; Time: 0.76\n",
      "Epoch 4247/8192 --- L(Train): 0.3718; L(Val): 0.4381; Reg Param: 0.0530; Time: 0.79\n",
      "Epoch 4248/8192 --- L(Train): 0.3739; L(Val): 0.4380; Reg Param: 0.0530; Time: 0.73\n",
      "Epoch 4249/8192 --- L(Train): 0.3811; L(Val): 0.4382; Reg Param: 0.0530; Time: 0.77\n",
      "Epoch 4250/8192 --- L(Train): 0.3890; L(Val): 0.4382; Reg Param: 0.0525; Time: 8.13\n",
      "Epoch 4251/8192 --- L(Train): 0.3738; L(Val): 0.4365; Reg Param: 0.0525; Time: 0.76\n",
      "Epoch 4252/8192 --- L(Train): 0.3747; L(Val): 0.4362; Reg Param: 0.0525; Time: 0.74\n",
      "Epoch 4253/8192 --- L(Train): 0.3742; L(Val): 0.4364; Reg Param: 0.0525; Time: 0.85\n",
      "Epoch 4254/8192 --- L(Train): 0.3768; L(Val): 0.4368; Reg Param: 0.0525; Time: 0.74\n",
      "Epoch 4255/8192 --- L(Train): 0.3803; L(Val): 0.4366; Reg Param: 0.0525; Time: 0.74\n",
      "Epoch 4256/8192 --- L(Train): 0.3734; L(Val): 0.4366; Reg Param: 0.0525; Time: 0.74\n",
      "Epoch 4257/8192 --- L(Train): 0.3648; L(Val): 0.4360; Reg Param: 0.0525; Time: 0.72\n",
      "Epoch 4258/8192 --- L(Train): 0.3713; L(Val): 0.4356; Reg Param: 0.0525; Time: 0.83\n",
      "Epoch 4259/8192 --- L(Train): 0.3697; L(Val): 0.4358; Reg Param: 0.0525; Time: 0.73\n",
      "Epoch 4260/8192 --- L(Train): 0.3893; L(Val): 0.4365; Reg Param: 0.0525; Time: 0.72\n",
      "Epoch 4261/8192 --- L(Train): 0.3848; L(Val): 0.4373; Reg Param: 0.0525; Time: 0.74\n",
      "Epoch 4262/8192 --- L(Train): 0.3720; L(Val): 0.4379; Reg Param: 0.0525; Time: 0.73\n",
      "Epoch 4263/8192 --- L(Train): 0.3849; L(Val): 0.4388; Reg Param: 0.0525; Time: 0.73\n",
      "Epoch 4264/8192 --- L(Train): 0.3893; L(Val): 0.4390; Reg Param: 0.0525; Time: 0.80\n",
      "Epoch 4265/8192 --- L(Train): 0.3633; L(Val): 0.4390; Reg Param: 0.0525; Time: 0.94\n",
      "Epoch 4266/8192 --- L(Train): 0.3764; L(Val): 0.4392; Reg Param: 0.0525; Time: 0.76\n",
      "Epoch 4267/8192 --- L(Train): 0.3666; L(Val): 0.4382; Reg Param: 0.0525; Time: 0.76\n",
      "Epoch 4268/8192 --- L(Train): 0.3819; L(Val): 0.4379; Reg Param: 0.0525; Time: 0.74\n",
      "Epoch 4269/8192 --- L(Train): 0.3754; L(Val): 0.4382; Reg Param: 0.0525; Time: 0.75\n",
      "Epoch 4270/8192 --- L(Train): 0.3865; L(Val): 0.4376; Reg Param: 0.0525; Time: 0.70\n",
      "Epoch 4271/8192 --- L(Train): 0.3827; L(Val): 0.4385; Reg Param: 0.0525; Time: 0.71\n",
      "Epoch 4272/8192 --- L(Train): 0.3779; L(Val): 0.4395; Reg Param: 0.0525; Time: 0.75\n",
      "Epoch 4273/8192 --- L(Train): 0.3646; L(Val): 0.4394; Reg Param: 0.0525; Time: 0.75\n",
      "Epoch 4274/8192 --- L(Train): 0.3755; L(Val): 0.4391; Reg Param: 0.0525; Time: 0.77\n",
      "Epoch 4275/8192 --- L(Train): 0.3706; L(Val): 0.4388; Reg Param: 0.0525; Time: 0.75\n",
      "Epoch 4276/8192 --- L(Train): 0.3749; L(Val): 0.4384; Reg Param: 0.0525; Time: 0.75\n",
      "Epoch 4277/8192 --- L(Train): 0.3759; L(Val): 0.4386; Reg Param: 0.0525; Time: 0.76\n",
      "Epoch 4278/8192 --- L(Train): 0.3858; L(Val): 0.4388; Reg Param: 0.0525; Time: 0.78\n",
      "Epoch 4279/8192 --- L(Train): 0.3718; L(Val): 0.4397; Reg Param: 0.0525; Time: 0.77\n",
      "Epoch 4280/8192 --- L(Train): 0.3717; L(Val): 0.4404; Reg Param: 0.0525; Time: 0.75\n",
      "Epoch 4281/8192 --- L(Train): 0.3768; L(Val): 0.4405; Reg Param: 0.0525; Time: 0.75\n",
      "Epoch 4282/8192 --- L(Train): 0.3672; L(Val): 0.4405; Reg Param: 0.0525; Time: 0.74\n",
      "Epoch 4283/8192 --- L(Train): 0.3752; L(Val): 0.4401; Reg Param: 0.0525; Time: 0.76\n",
      "Epoch 4284/8192 --- L(Train): 0.3845; L(Val): 0.4395; Reg Param: 0.0525; Time: 0.87\n",
      "Epoch 4285/8192 --- L(Train): 0.3892; L(Val): 0.4389; Reg Param: 0.0525; Time: 0.75\n",
      "Epoch 4286/8192 --- L(Train): 0.3666; L(Val): 0.4384; Reg Param: 0.0525; Time: 0.77\n",
      "Epoch 4287/8192 --- L(Train): 0.3638; L(Val): 0.4382; Reg Param: 0.0525; Time: 0.77\n",
      "Epoch 4288/8192 --- L(Train): 0.3808; L(Val): 0.4384; Reg Param: 0.0525; Time: 0.78\n",
      "Epoch 4289/8192 --- L(Train): 0.3631; L(Val): 0.4386; Reg Param: 0.0525; Time: 0.80\n",
      "Epoch 4290/8192 --- L(Train): 0.3801; L(Val): 0.4381; Reg Param: 0.0525; Time: 0.76\n",
      "Epoch 4291/8192 --- L(Train): 0.3725; L(Val): 0.4378; Reg Param: 0.0525; Time: 0.76\n",
      "Epoch 4292/8192 --- L(Train): 0.3697; L(Val): 0.4371; Reg Param: 0.0525; Time: 0.72\n",
      "Epoch 4293/8192 --- L(Train): 0.3751; L(Val): 0.4365; Reg Param: 0.0525; Time: 0.75\n",
      "Epoch 4294/8192 --- L(Train): 0.3796; L(Val): 0.4364; Reg Param: 0.0525; Time: 0.77\n",
      "Epoch 4295/8192 --- L(Train): 0.3693; L(Val): 0.4365; Reg Param: 0.0525; Time: 0.76\n",
      "Epoch 4296/8192 --- L(Train): 0.3837; L(Val): 0.4370; Reg Param: 0.0525; Time: 0.86\n",
      "Epoch 4297/8192 --- L(Train): 0.3791; L(Val): 0.4377; Reg Param: 0.0525; Time: 0.73\n",
      "Epoch 4298/8192 --- L(Train): 0.3732; L(Val): 0.4376; Reg Param: 0.0525; Time: 0.74\n",
      "Epoch 4299/8192 --- L(Train): 0.3817; L(Val): 0.4377; Reg Param: 0.0525; Time: 1.01\n",
      "Epoch 4300/8192 --- L(Train): 0.3740; L(Val): 0.4377; Reg Param: 0.0521; Time: 7.72\n",
      "Epoch 4301/8192 --- L(Train): 0.3765; L(Val): 0.4378; Reg Param: 0.0521; Time: 0.81\n",
      "Epoch 4302/8192 --- L(Train): 0.3736; L(Val): 0.4376; Reg Param: 0.0521; Time: 0.72\n",
      "Epoch 4303/8192 --- L(Train): 0.3827; L(Val): 0.4374; Reg Param: 0.0521; Time: 0.73\n",
      "Epoch 4304/8192 --- L(Train): 0.3536; L(Val): 0.4383; Reg Param: 0.0521; Time: 0.76\n",
      "Epoch 4305/8192 --- L(Train): 0.3768; L(Val): 0.4385; Reg Param: 0.0521; Time: 0.75\n",
      "Epoch 4306/8192 --- L(Train): 0.3788; L(Val): 0.4387; Reg Param: 0.0521; Time: 0.76\n",
      "Epoch 4307/8192 --- L(Train): 0.3743; L(Val): 0.4384; Reg Param: 0.0521; Time: 0.73\n",
      "Epoch 4308/8192 --- L(Train): 0.3793; L(Val): 0.4382; Reg Param: 0.0521; Time: 0.75\n",
      "Epoch 4309/8192 --- L(Train): 0.3828; L(Val): 0.4382; Reg Param: 0.0521; Time: 0.76\n",
      "Epoch 4310/8192 --- L(Train): 0.3688; L(Val): 0.4381; Reg Param: 0.0521; Time: 0.76\n",
      "Epoch 4311/8192 --- L(Train): 0.3737; L(Val): 0.4379; Reg Param: 0.0521; Time: 0.75\n",
      "Epoch 4312/8192 --- L(Train): 0.3819; L(Val): 0.4380; Reg Param: 0.0521; Time: 0.74\n",
      "Epoch 4313/8192 --- L(Train): 0.3716; L(Val): 0.4386; Reg Param: 0.0521; Time: 0.76\n",
      "Epoch 4314/8192 --- L(Train): 0.3735; L(Val): 0.4397; Reg Param: 0.0521; Time: 0.75\n",
      "Epoch 4315/8192 --- L(Train): 0.3794; L(Val): 0.4408; Reg Param: 0.0521; Time: 0.75\n",
      "Epoch 4316/8192 --- L(Train): 0.3808; L(Val): 0.4409; Reg Param: 0.0521; Time: 0.75\n",
      "Epoch 4317/8192 --- L(Train): 0.3866; L(Val): 0.4410; Reg Param: 0.0521; Time: 1.05\n",
      "Epoch 4318/8192 --- L(Train): 0.3602; L(Val): 0.4409; Reg Param: 0.0521; Time: 0.79\n",
      "Epoch 4319/8192 --- L(Train): 0.3811; L(Val): 0.4405; Reg Param: 0.0521; Time: 0.75\n",
      "Epoch 4320/8192 --- L(Train): 0.3819; L(Val): 0.4397; Reg Param: 0.0521; Time: 0.75\n",
      "Epoch 4321/8192 --- L(Train): 0.3750; L(Val): 0.4388; Reg Param: 0.0521; Time: 0.73\n",
      "Epoch 4322/8192 --- L(Train): 0.3726; L(Val): 0.4381; Reg Param: 0.0521; Time: 0.79\n",
      "Epoch 4323/8192 --- L(Train): 0.3833; L(Val): 0.4377; Reg Param: 0.0521; Time: 0.78\n",
      "Epoch 4324/8192 --- L(Train): 0.3798; L(Val): 0.4378; Reg Param: 0.0521; Time: 0.74\n",
      "Epoch 4325/8192 --- L(Train): 0.3837; L(Val): 0.4382; Reg Param: 0.0521; Time: 0.74\n",
      "Epoch 4326/8192 --- L(Train): 0.3792; L(Val): 0.4387; Reg Param: 0.0521; Time: 0.85\n",
      "Epoch 4327/8192 --- L(Train): 0.3791; L(Val): 0.4392; Reg Param: 0.0521; Time: 0.74\n",
      "Epoch 4328/8192 --- L(Train): 0.3829; L(Val): 0.4400; Reg Param: 0.0521; Time: 0.78\n",
      "Epoch 4329/8192 --- L(Train): 0.3831; L(Val): 0.4394; Reg Param: 0.0521; Time: 0.78\n",
      "Epoch 4330/8192 --- L(Train): 0.3770; L(Val): 0.4392; Reg Param: 0.0521; Time: 0.77\n",
      "Epoch 4331/8192 --- L(Train): 0.3812; L(Val): 0.4391; Reg Param: 0.0521; Time: 0.76\n",
      "Epoch 4332/8192 --- L(Train): 0.3704; L(Val): 0.4388; Reg Param: 0.0521; Time: 0.76\n",
      "Epoch 4333/8192 --- L(Train): 0.3812; L(Val): 0.4387; Reg Param: 0.0521; Time: 0.77\n",
      "Epoch 4334/8192 --- L(Train): 0.3702; L(Val): 0.4388; Reg Param: 0.0521; Time: 0.80\n",
      "Epoch 4335/8192 --- L(Train): 0.3651; L(Val): 0.4390; Reg Param: 0.0521; Time: 0.76\n",
      "Epoch 4336/8192 --- L(Train): 0.3767; L(Val): 0.4391; Reg Param: 0.0521; Time: 0.77\n",
      "Epoch 4337/8192 --- L(Train): 0.3734; L(Val): 0.4389; Reg Param: 0.0521; Time: 0.75\n",
      "Epoch 4338/8192 --- L(Train): 0.3828; L(Val): 0.4393; Reg Param: 0.0521; Time: 0.78\n",
      "Epoch 4339/8192 --- L(Train): 0.3790; L(Val): 0.4392; Reg Param: 0.0521; Time: 0.77\n",
      "Epoch 4340/8192 --- L(Train): 0.3873; L(Val): 0.4395; Reg Param: 0.0521; Time: 0.76\n",
      "Epoch 4341/8192 --- L(Train): 0.3767; L(Val): 0.4398; Reg Param: 0.0521; Time: 0.76\n",
      "Epoch 4342/8192 --- L(Train): 0.3810; L(Val): 0.4400; Reg Param: 0.0521; Time: 0.75\n",
      "Epoch 4343/8192 --- L(Train): 0.3799; L(Val): 0.4408; Reg Param: 0.0521; Time: 0.76\n",
      "Epoch 4344/8192 --- L(Train): 0.3774; L(Val): 0.4410; Reg Param: 0.0521; Time: 0.75\n",
      "Epoch 4345/8192 --- L(Train): 0.3734; L(Val): 0.4418; Reg Param: 0.0521; Time: 0.78\n",
      "Epoch 4346/8192 --- L(Train): 0.3650; L(Val): 0.4428; Reg Param: 0.0521; Time: 1.00\n",
      "Epoch 4347/8192 --- L(Train): 0.3751; L(Val): 0.4424; Reg Param: 0.0521; Time: 0.96\n",
      "Epoch 4348/8192 --- L(Train): 0.3830; L(Val): 0.4420; Reg Param: 0.0521; Time: 0.77\n",
      "Epoch 4349/8192 --- L(Train): 0.3752; L(Val): 0.4422; Reg Param: 0.0521; Time: 0.76\n",
      "Epoch 4350/8192 --- L(Train): 0.3768; L(Val): 0.4422; Reg Param: 0.0514; Time: 7.91\n",
      "Epoch 4351/8192 --- L(Train): 0.3735; L(Val): 0.4435; Reg Param: 0.0514; Time: 0.86\n",
      "Epoch 4352/8192 --- L(Train): 0.3768; L(Val): 0.4443; Reg Param: 0.0514; Time: 0.75\n",
      "Epoch 4353/8192 --- L(Train): 0.3750; L(Val): 0.4426; Reg Param: 0.0514; Time: 0.90\n",
      "Epoch 4354/8192 --- L(Train): 0.3656; L(Val): 0.4413; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4355/8192 --- L(Train): 0.3735; L(Val): 0.4404; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4356/8192 --- L(Train): 0.3703; L(Val): 0.4399; Reg Param: 0.0514; Time: 0.74\n",
      "Epoch 4357/8192 --- L(Train): 0.3845; L(Val): 0.4400; Reg Param: 0.0514; Time: 0.79\n",
      "Epoch 4358/8192 --- L(Train): 0.3906; L(Val): 0.4403; Reg Param: 0.0514; Time: 0.77\n",
      "Epoch 4359/8192 --- L(Train): 0.3710; L(Val): 0.4404; Reg Param: 0.0514; Time: 0.75\n",
      "Epoch 4360/8192 --- L(Train): 0.3810; L(Val): 0.4398; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4361/8192 --- L(Train): 0.3780; L(Val): 0.4402; Reg Param: 0.0514; Time: 0.75\n",
      "Epoch 4362/8192 --- L(Train): 0.3872; L(Val): 0.4406; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4363/8192 --- L(Train): 0.3703; L(Val): 0.4406; Reg Param: 0.0514; Time: 0.80\n",
      "Epoch 4364/8192 --- L(Train): 0.3917; L(Val): 0.4404; Reg Param: 0.0514; Time: 0.78\n",
      "Epoch 4365/8192 --- L(Train): 0.3794; L(Val): 0.4404; Reg Param: 0.0514; Time: 0.77\n",
      "Epoch 4366/8192 --- L(Train): 0.3691; L(Val): 0.4399; Reg Param: 0.0514; Time: 0.93\n",
      "Epoch 4367/8192 --- L(Train): 0.3884; L(Val): 0.4403; Reg Param: 0.0514; Time: 0.75\n",
      "Epoch 4368/8192 --- L(Train): 0.3836; L(Val): 0.4413; Reg Param: 0.0514; Time: 0.75\n",
      "Epoch 4369/8192 --- L(Train): 0.3748; L(Val): 0.4425; Reg Param: 0.0514; Time: 0.78\n",
      "Epoch 4370/8192 --- L(Train): 0.3872; L(Val): 0.4432; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4371/8192 --- L(Train): 0.3777; L(Val): 0.4431; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4372/8192 --- L(Train): 0.3689; L(Val): 0.4435; Reg Param: 0.0514; Time: 0.78\n",
      "Epoch 4373/8192 --- L(Train): 0.3757; L(Val): 0.4440; Reg Param: 0.0514; Time: 0.82\n",
      "Epoch 4374/8192 --- L(Train): 0.3849; L(Val): 0.4437; Reg Param: 0.0514; Time: 0.88\n",
      "Epoch 4375/8192 --- L(Train): 0.3771; L(Val): 0.4430; Reg Param: 0.0514; Time: 0.82\n",
      "Epoch 4376/8192 --- L(Train): 0.3789; L(Val): 0.4425; Reg Param: 0.0514; Time: 0.75\n",
      "Epoch 4377/8192 --- L(Train): 0.3661; L(Val): 0.4423; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4378/8192 --- L(Train): 0.3837; L(Val): 0.4422; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4379/8192 --- L(Train): 0.3789; L(Val): 0.4419; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4380/8192 --- L(Train): 0.3831; L(Val): 0.4416; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4381/8192 --- L(Train): 0.3729; L(Val): 0.4412; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4382/8192 --- L(Train): 0.3810; L(Val): 0.4412; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4383/8192 --- L(Train): 0.3740; L(Val): 0.4410; Reg Param: 0.0514; Time: 0.75\n",
      "Epoch 4384/8192 --- L(Train): 0.3680; L(Val): 0.4408; Reg Param: 0.0514; Time: 0.79\n",
      "Epoch 4385/8192 --- L(Train): 0.3728; L(Val): 0.4400; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4386/8192 --- L(Train): 0.3746; L(Val): 0.4389; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4387/8192 --- L(Train): 0.3702; L(Val): 0.4384; Reg Param: 0.0514; Time: 0.87\n",
      "Epoch 4388/8192 --- L(Train): 0.3769; L(Val): 0.4381; Reg Param: 0.0514; Time: 0.78\n",
      "Epoch 4389/8192 --- L(Train): 0.3832; L(Val): 0.4381; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4390/8192 --- L(Train): 0.3667; L(Val): 0.4380; Reg Param: 0.0514; Time: 0.79\n",
      "Epoch 4391/8192 --- L(Train): 0.3814; L(Val): 0.4379; Reg Param: 0.0514; Time: 0.84\n",
      "Epoch 4392/8192 --- L(Train): 0.3724; L(Val): 0.4383; Reg Param: 0.0514; Time: 0.78\n",
      "Epoch 4393/8192 --- L(Train): 0.3697; L(Val): 0.4386; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4394/8192 --- L(Train): 0.3741; L(Val): 0.4394; Reg Param: 0.0514; Time: 0.78\n",
      "Epoch 4395/8192 --- L(Train): 0.3685; L(Val): 0.4399; Reg Param: 0.0514; Time: 0.77\n",
      "Epoch 4396/8192 --- L(Train): 0.3671; L(Val): 0.4404; Reg Param: 0.0514; Time: 0.76\n",
      "Epoch 4397/8192 --- L(Train): 0.3836; L(Val): 0.4415; Reg Param: 0.0514; Time: 0.78\n",
      "Epoch 4398/8192 --- L(Train): 0.3699; L(Val): 0.4422; Reg Param: 0.0514; Time: 0.77\n",
      "Epoch 4399/8192 --- L(Train): 0.3736; L(Val): 0.4420; Reg Param: 0.0514; Time: 0.86\n",
      "Epoch 4400/8192 --- L(Train): 0.3716; L(Val): 0.4420; Reg Param: 0.0507; Time: 8.12\n",
      "Epoch 4401/8192 --- L(Train): 0.3818; L(Val): 0.4411; Reg Param: 0.0507; Time: 0.76\n",
      "Epoch 4402/8192 --- L(Train): 0.3738; L(Val): 0.4408; Reg Param: 0.0507; Time: 0.77\n",
      "Epoch 4403/8192 --- L(Train): 0.3787; L(Val): 0.4409; Reg Param: 0.0507; Time: 0.73\n",
      "Epoch 4404/8192 --- L(Train): 0.3712; L(Val): 0.4410; Reg Param: 0.0507; Time: 0.73\n",
      "Epoch 4405/8192 --- L(Train): 0.3811; L(Val): 0.4409; Reg Param: 0.0507; Time: 0.72\n",
      "Epoch 4406/8192 --- L(Train): 0.3842; L(Val): 0.4411; Reg Param: 0.0507; Time: 0.71\n",
      "Epoch 4407/8192 --- L(Train): 0.3803; L(Val): 0.4411; Reg Param: 0.0507; Time: 0.72\n",
      "Epoch 4408/8192 --- L(Train): 0.3817; L(Val): 0.4413; Reg Param: 0.0507; Time: 0.72\n",
      "Epoch 4409/8192 --- L(Train): 0.3722; L(Val): 0.4417; Reg Param: 0.0507; Time: 0.71\n",
      "Epoch 4410/8192 --- L(Train): 0.3737; L(Val): 0.4414; Reg Param: 0.0507; Time: 0.73\n",
      "Epoch 4411/8192 --- L(Train): 0.3764; L(Val): 0.4411; Reg Param: 0.0507; Time: 0.72\n",
      "Epoch 4412/8192 --- L(Train): 0.3824; L(Val): 0.4406; Reg Param: 0.0507; Time: 0.72\n",
      "Epoch 4413/8192 --- L(Train): 0.3741; L(Val): 0.4400; Reg Param: 0.0507; Time: 0.73\n",
      "Epoch 4414/8192 --- L(Train): 0.3681; L(Val): 0.4393; Reg Param: 0.0507; Time: 0.72\n",
      "Epoch 4415/8192 --- L(Train): 0.3749; L(Val): 0.4396; Reg Param: 0.0507; Time: 0.80\n",
      "Epoch 4416/8192 --- L(Train): 0.3847; L(Val): 0.4397; Reg Param: 0.0507; Time: 0.81\n",
      "Epoch 4417/8192 --- L(Train): 0.3758; L(Val): 0.4396; Reg Param: 0.0507; Time: 0.77\n",
      "Epoch 4418/8192 --- L(Train): 0.3753; L(Val): 0.4391; Reg Param: 0.0507; Time: 1.12\n",
      "Epoch 4419/8192 --- L(Train): 0.3832; L(Val): 0.4387; Reg Param: 0.0507; Time: 0.73\n",
      "Epoch 4420/8192 --- L(Train): 0.3821; L(Val): 0.4382; Reg Param: 0.0507; Time: 0.70\n",
      "Epoch 4421/8192 --- L(Train): 0.3747; L(Val): 0.4369; Reg Param: 0.0507; Time: 0.71\n",
      "Epoch 4422/8192 --- L(Train): 0.3735; L(Val): 0.4361; Reg Param: 0.0507; Time: 0.71\n",
      "Epoch 4423/8192 --- L(Train): 0.3732; L(Val): 0.4354; Reg Param: 0.0507; Time: 0.71\n",
      "Epoch 4424/8192 --- L(Train): 0.3744; L(Val): 0.4348; Reg Param: 0.0507; Time: 0.71\n",
      "Epoch 4425/8192 --- L(Train): 0.3813; L(Val): 0.4345; Reg Param: 0.0507; Time: 0.71\n",
      "Epoch 4426/8192 --- L(Train): 0.3788; L(Val): 0.4342; Reg Param: 0.0507; Time: 0.69\n",
      "Epoch 4427/8192 --- L(Train): 0.3803; L(Val): 0.4339; Reg Param: 0.0507; Time: 0.70\n",
      "Epoch 4428/8192 --- L(Train): 0.3680; L(Val): 0.4341; Reg Param: 0.0507; Time: 0.70\n",
      "Epoch 4429/8192 --- L(Train): 0.3667; L(Val): 0.4347; Reg Param: 0.0507; Time: 0.70\n",
      "Epoch 4430/8192 --- L(Train): 0.3763; L(Val): 0.4356; Reg Param: 0.0507; Time: 0.69\n",
      "Epoch 4431/8192 --- L(Train): 0.3805; L(Val): 0.4363; Reg Param: 0.0507; Time: 0.70\n",
      "Epoch 4432/8192 --- L(Train): 0.3707; L(Val): 0.4367; Reg Param: 0.0507; Time: 0.81\n",
      "Epoch 4433/8192 --- L(Train): 0.3771; L(Val): 0.4372; Reg Param: 0.0507; Time: 0.69\n",
      "Epoch 4434/8192 --- L(Train): 0.3743; L(Val): 0.4376; Reg Param: 0.0507; Time: 0.71\n",
      "Epoch 4435/8192 --- L(Train): 0.3794; L(Val): 0.4377; Reg Param: 0.0507; Time: 0.70\n",
      "Epoch 4436/8192 --- L(Train): 0.3824; L(Val): 0.4377; Reg Param: 0.0507; Time: 0.70\n",
      "Epoch 4437/8192 --- L(Train): 0.3814; L(Val): 0.4378; Reg Param: 0.0507; Time: 0.71\n",
      "Epoch 4438/8192 --- L(Train): 0.3591; L(Val): 0.4377; Reg Param: 0.0507; Time: 0.72\n",
      "Epoch 4439/8192 --- L(Train): 0.3749; L(Val): 0.4373; Reg Param: 0.0507; Time: 0.73\n",
      "Epoch 4440/8192 --- L(Train): 0.3875; L(Val): 0.4376; Reg Param: 0.0507; Time: 0.73\n",
      "Epoch 4441/8192 --- L(Train): 0.3701; L(Val): 0.4379; Reg Param: 0.0507; Time: 0.72\n",
      "Epoch 4442/8192 --- L(Train): 0.3720; L(Val): 0.4383; Reg Param: 0.0507; Time: 0.72\n",
      "Epoch 4443/8192 --- L(Train): 0.3694; L(Val): 0.4383; Reg Param: 0.0507; Time: 0.72\n",
      "Epoch 4444/8192 --- L(Train): 0.3690; L(Val): 0.4386; Reg Param: 0.0507; Time: 0.83\n",
      "Epoch 4445/8192 --- L(Train): 0.3900; L(Val): 0.4388; Reg Param: 0.0507; Time: 0.77\n",
      "Epoch 4446/8192 --- L(Train): 0.3748; L(Val): 0.4386; Reg Param: 0.0507; Time: 0.74\n",
      "Epoch 4447/8192 --- L(Train): 0.3854; L(Val): 0.4386; Reg Param: 0.0507; Time: 0.74\n",
      "Epoch 4448/8192 --- L(Train): 0.3810; L(Val): 0.4384; Reg Param: 0.0507; Time: 0.76\n",
      "Epoch 4449/8192 --- L(Train): 0.3615; L(Val): 0.4385; Reg Param: 0.0507; Time: 0.77\n",
      "Epoch 4450/8192 --- L(Train): 0.3747; L(Val): 0.4385; Reg Param: 0.0500; Time: 8.27\n",
      "Epoch 4451/8192 --- L(Train): 0.3729; L(Val): 0.4387; Reg Param: 0.0500; Time: 0.75\n",
      "Epoch 4452/8192 --- L(Train): 0.3787; L(Val): 0.4390; Reg Param: 0.0500; Time: 0.84\n",
      "Epoch 4453/8192 --- L(Train): 0.3668; L(Val): 0.4394; Reg Param: 0.0500; Time: 0.73\n",
      "Epoch 4454/8192 --- L(Train): 0.3782; L(Val): 0.4399; Reg Param: 0.0500; Time: 0.83\n",
      "Epoch 4455/8192 --- L(Train): 0.3742; L(Val): 0.4401; Reg Param: 0.0500; Time: 0.75\n",
      "Epoch 4456/8192 --- L(Train): 0.3876; L(Val): 0.4402; Reg Param: 0.0500; Time: 0.75\n",
      "Epoch 4457/8192 --- L(Train): 0.3727; L(Val): 0.4400; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4458/8192 --- L(Train): 0.3821; L(Val): 0.4397; Reg Param: 0.0500; Time: 0.79\n",
      "Epoch 4459/8192 --- L(Train): 0.3743; L(Val): 0.4394; Reg Param: 0.0500; Time: 0.79\n",
      "Epoch 4460/8192 --- L(Train): 0.3708; L(Val): 0.4396; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4461/8192 --- L(Train): 0.3691; L(Val): 0.4394; Reg Param: 0.0500; Time: 0.79\n",
      "Epoch 4462/8192 --- L(Train): 0.3854; L(Val): 0.4398; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4463/8192 --- L(Train): 0.3777; L(Val): 0.4394; Reg Param: 0.0500; Time: 0.79\n",
      "Epoch 4464/8192 --- L(Train): 0.3717; L(Val): 0.4398; Reg Param: 0.0500; Time: 0.76\n",
      "Epoch 4465/8192 --- L(Train): 0.3696; L(Val): 0.4404; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4466/8192 --- L(Train): 0.3848; L(Val): 0.4406; Reg Param: 0.0500; Time: 0.78\n",
      "Epoch 4467/8192 --- L(Train): 0.3749; L(Val): 0.4409; Reg Param: 0.0500; Time: 0.78\n",
      "Epoch 4468/8192 --- L(Train): 0.3721; L(Val): 0.4415; Reg Param: 0.0500; Time: 0.79\n",
      "Epoch 4469/8192 --- L(Train): 0.3858; L(Val): 0.4417; Reg Param: 0.0500; Time: 0.78\n",
      "Epoch 4470/8192 --- L(Train): 0.3883; L(Val): 0.4419; Reg Param: 0.0500; Time: 1.26\n",
      "Epoch 4471/8192 --- L(Train): 0.3749; L(Val): 0.4415; Reg Param: 0.0500; Time: 0.76\n",
      "Epoch 4472/8192 --- L(Train): 0.3763; L(Val): 0.4408; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4473/8192 --- L(Train): 0.3789; L(Val): 0.4401; Reg Param: 0.0500; Time: 0.76\n",
      "Epoch 4474/8192 --- L(Train): 0.3873; L(Val): 0.4396; Reg Param: 0.0500; Time: 0.79\n",
      "Epoch 4475/8192 --- L(Train): 0.3681; L(Val): 0.4395; Reg Param: 0.0500; Time: 0.74\n",
      "Epoch 4476/8192 --- L(Train): 0.3796; L(Val): 0.4391; Reg Param: 0.0500; Time: 0.79\n",
      "Epoch 4477/8192 --- L(Train): 0.3805; L(Val): 0.4391; Reg Param: 0.0500; Time: 0.78\n",
      "Epoch 4478/8192 --- L(Train): 0.3784; L(Val): 0.4395; Reg Param: 0.0500; Time: 0.78\n",
      "Epoch 4479/8192 --- L(Train): 0.3830; L(Val): 0.4392; Reg Param: 0.0500; Time: 0.81\n",
      "Epoch 4480/8192 --- L(Train): 0.3837; L(Val): 0.4385; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4481/8192 --- L(Train): 0.3808; L(Val): 0.4382; Reg Param: 0.0500; Time: 0.82\n",
      "Epoch 4482/8192 --- L(Train): 0.3913; L(Val): 0.4380; Reg Param: 0.0500; Time: 0.82\n",
      "Epoch 4483/8192 --- L(Train): 0.3712; L(Val): 0.4380; Reg Param: 0.0500; Time: 0.80\n",
      "Epoch 4484/8192 --- L(Train): 0.3701; L(Val): 0.4386; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4485/8192 --- L(Train): 0.3569; L(Val): 0.4389; Reg Param: 0.0500; Time: 0.78\n",
      "Epoch 4486/8192 --- L(Train): 0.3760; L(Val): 0.4387; Reg Param: 0.0500; Time: 0.87\n",
      "Epoch 4487/8192 --- L(Train): 0.3879; L(Val): 0.4383; Reg Param: 0.0500; Time: 0.76\n",
      "Epoch 4488/8192 --- L(Train): 0.3683; L(Val): 0.4379; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4489/8192 --- L(Train): 0.3763; L(Val): 0.4380; Reg Param: 0.0500; Time: 0.75\n",
      "Epoch 4490/8192 --- L(Train): 0.3739; L(Val): 0.4384; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4491/8192 --- L(Train): 0.3791; L(Val): 0.4387; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4492/8192 --- L(Train): 0.3703; L(Val): 0.4386; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4493/8192 --- L(Train): 0.3701; L(Val): 0.4379; Reg Param: 0.0500; Time: 0.76\n",
      "Epoch 4494/8192 --- L(Train): 0.3660; L(Val): 0.4372; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4495/8192 --- L(Train): 0.3845; L(Val): 0.4367; Reg Param: 0.0500; Time: 0.77\n",
      "Epoch 4496/8192 --- L(Train): 0.3723; L(Val): 0.4365; Reg Param: 0.0500; Time: 0.74\n",
      "Epoch 4497/8192 --- L(Train): 0.3718; L(Val): 0.4362; Reg Param: 0.0500; Time: 0.78\n",
      "Epoch 4498/8192 --- L(Train): 0.3721; L(Val): 0.4366; Reg Param: 0.0500; Time: 0.80\n",
      "Epoch 4499/8192 --- L(Train): 0.3696; L(Val): 0.4375; Reg Param: 0.0500; Time: 0.74\n",
      "Epoch 4500/8192 --- L(Train): 0.3838; L(Val): 0.4375; Reg Param: 0.0493; Time: 8.34\n",
      "Epoch 4501/8192 --- L(Train): 0.3750; L(Val): 0.4388; Reg Param: 0.0493; Time: 0.77\n",
      "Epoch 4502/8192 --- L(Train): 0.3655; L(Val): 0.4388; Reg Param: 0.0493; Time: 0.89\n",
      "Epoch 4503/8192 --- L(Train): 0.3837; L(Val): 0.4388; Reg Param: 0.0493; Time: 0.77\n",
      "Epoch 4504/8192 --- L(Train): 0.3693; L(Val): 0.4386; Reg Param: 0.0493; Time: 0.77\n",
      "Epoch 4505/8192 --- L(Train): 0.3892; L(Val): 0.4382; Reg Param: 0.0493; Time: 0.81\n",
      "Epoch 4506/8192 --- L(Train): 0.3789; L(Val): 0.4382; Reg Param: 0.0493; Time: 0.76\n",
      "Epoch 4507/8192 --- L(Train): 0.3773; L(Val): 0.4382; Reg Param: 0.0493; Time: 0.76\n",
      "Epoch 4508/8192 --- L(Train): 0.3705; L(Val): 0.4391; Reg Param: 0.0493; Time: 0.78\n",
      "Epoch 4509/8192 --- L(Train): 0.3704; L(Val): 0.4395; Reg Param: 0.0493; Time: 0.75\n",
      "Epoch 4510/8192 --- L(Train): 0.3840; L(Val): 0.4397; Reg Param: 0.0493; Time: 0.75\n",
      "Epoch 4511/8192 --- L(Train): 0.3785; L(Val): 0.4398; Reg Param: 0.0493; Time: 0.71\n",
      "Epoch 4512/8192 --- L(Train): 0.3743; L(Val): 0.4401; Reg Param: 0.0493; Time: 0.80\n",
      "Epoch 4513/8192 --- L(Train): 0.3814; L(Val): 0.4398; Reg Param: 0.0493; Time: 0.75\n",
      "Epoch 4514/8192 --- L(Train): 0.3859; L(Val): 0.4399; Reg Param: 0.0493; Time: 0.77\n",
      "Epoch 4515/8192 --- L(Train): 0.3784; L(Val): 0.4401; Reg Param: 0.0493; Time: 0.76\n",
      "Epoch 4516/8192 --- L(Train): 0.3664; L(Val): 0.4412; Reg Param: 0.0493; Time: 0.75\n",
      "Epoch 4517/8192 --- L(Train): 0.3809; L(Val): 0.4415; Reg Param: 0.0493; Time: 0.76\n",
      "Epoch 4518/8192 --- L(Train): 0.3724; L(Val): 0.4416; Reg Param: 0.0493; Time: 0.78\n",
      "Epoch 4519/8192 --- L(Train): 0.3682; L(Val): 0.4418; Reg Param: 0.0493; Time: 1.24\n",
      "Epoch 4520/8192 --- L(Train): 0.3712; L(Val): 0.4420; Reg Param: 0.0493; Time: 0.77\n",
      "Epoch 4521/8192 --- L(Train): 0.3712; L(Val): 0.4424; Reg Param: 0.0493; Time: 0.76\n",
      "Epoch 4522/8192 --- L(Train): 0.3774; L(Val): 0.4424; Reg Param: 0.0493; Time: 0.78\n",
      "Epoch 4523/8192 --- L(Train): 0.3862; L(Val): 0.4415; Reg Param: 0.0493; Time: 0.73\n",
      "Epoch 4524/8192 --- L(Train): 0.3827; L(Val): 0.4411; Reg Param: 0.0493; Time: 0.77\n",
      "Epoch 4525/8192 --- L(Train): 0.3752; L(Val): 0.4406; Reg Param: 0.0493; Time: 0.77\n",
      "Epoch 4526/8192 --- L(Train): 0.3831; L(Val): 0.4403; Reg Param: 0.0493; Time: 0.73\n",
      "Epoch 4527/8192 --- L(Train): 0.3718; L(Val): 0.4408; Reg Param: 0.0493; Time: 0.77\n",
      "Epoch 4528/8192 --- L(Train): 0.3746; L(Val): 0.4408; Reg Param: 0.0493; Time: 0.80\n",
      "Epoch 4529/8192 --- L(Train): 0.3709; L(Val): 0.4409; Reg Param: 0.0493; Time: 0.79\n",
      "Epoch 4530/8192 --- L(Train): 0.3733; L(Val): 0.4407; Reg Param: 0.0493; Time: 0.80\n",
      "Epoch 4531/8192 --- L(Train): 0.3656; L(Val): 0.4408; Reg Param: 0.0493; Time: 0.78\n",
      "Epoch 4532/8192 --- L(Train): 0.3690; L(Val): 0.4412; Reg Param: 0.0493; Time: 0.75\n",
      "Epoch 4533/8192 --- L(Train): 0.3759; L(Val): 0.4416; Reg Param: 0.0493; Time: 0.75\n",
      "Epoch 4534/8192 --- L(Train): 0.3833; L(Val): 0.4417; Reg Param: 0.0493; Time: 0.72\n",
      "Epoch 4535/8192 --- L(Train): 0.3733; L(Val): 0.4418; Reg Param: 0.0493; Time: 0.73\n",
      "Epoch 4536/8192 --- L(Train): 0.3738; L(Val): 0.4420; Reg Param: 0.0493; Time: 0.71\n",
      "Epoch 4537/8192 --- L(Train): 0.3774; L(Val): 0.4419; Reg Param: 0.0493; Time: 0.72\n",
      "Epoch 4538/8192 --- L(Train): 0.3940; L(Val): 0.4418; Reg Param: 0.0493; Time: 0.73\n",
      "Epoch 4539/8192 --- L(Train): 0.3716; L(Val): 0.4413; Reg Param: 0.0493; Time: 0.85\n",
      "Epoch 4540/8192 --- L(Train): 0.3849; L(Val): 0.4404; Reg Param: 0.0493; Time: 0.76\n",
      "Epoch 4541/8192 --- L(Train): 0.3758; L(Val): 0.4407; Reg Param: 0.0493; Time: 0.87\n",
      "Epoch 4542/8192 --- L(Train): 0.3819; L(Val): 0.4419; Reg Param: 0.0493; Time: 0.78\n",
      "Epoch 4543/8192 --- L(Train): 0.3794; L(Val): 0.4430; Reg Param: 0.0493; Time: 0.78\n",
      "Epoch 4544/8192 --- L(Train): 0.3738; L(Val): 0.4430; Reg Param: 0.0493; Time: 0.87\n",
      "Epoch 4545/8192 --- L(Train): 0.3726; L(Val): 0.4422; Reg Param: 0.0493; Time: 0.87\n",
      "Epoch 4546/8192 --- L(Train): 0.3669; L(Val): 0.4415; Reg Param: 0.0493; Time: 0.85\n",
      "Epoch 4547/8192 --- L(Train): 0.3784; L(Val): 0.4412; Reg Param: 0.0493; Time: 0.76\n",
      "Epoch 4548/8192 --- L(Train): 0.3774; L(Val): 0.4411; Reg Param: 0.0493; Time: 0.73\n",
      "Epoch 4549/8192 --- L(Train): 0.3751; L(Val): 0.4409; Reg Param: 0.0493; Time: 0.79\n",
      "Epoch 4550/8192 --- L(Train): 0.3789; L(Val): 0.4409; Reg Param: 0.0485; Time: 8.34\n",
      "Epoch 4551/8192 --- L(Train): 0.3878; L(Val): 0.4398; Reg Param: 0.0485; Time: 0.74\n",
      "Epoch 4552/8192 --- L(Train): 0.3888; L(Val): 0.4389; Reg Param: 0.0485; Time: 0.78\n",
      "Epoch 4553/8192 --- L(Train): 0.3874; L(Val): 0.4385; Reg Param: 0.0485; Time: 0.82\n",
      "Epoch 4554/8192 --- L(Train): 0.3864; L(Val): 0.4385; Reg Param: 0.0485; Time: 0.81\n",
      "Epoch 4555/8192 --- L(Train): 0.3777; L(Val): 0.4393; Reg Param: 0.0485; Time: 0.78\n",
      "Epoch 4556/8192 --- L(Train): 0.3822; L(Val): 0.4404; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4557/8192 --- L(Train): 0.3735; L(Val): 0.4406; Reg Param: 0.0485; Time: 0.76\n",
      "Epoch 4558/8192 --- L(Train): 0.3727; L(Val): 0.4406; Reg Param: 0.0485; Time: 0.79\n",
      "Epoch 4559/8192 --- L(Train): 0.3875; L(Val): 0.4404; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4560/8192 --- L(Train): 0.3785; L(Val): 0.4396; Reg Param: 0.0485; Time: 0.81\n",
      "Epoch 4561/8192 --- L(Train): 0.3759; L(Val): 0.4393; Reg Param: 0.0485; Time: 0.79\n",
      "Epoch 4562/8192 --- L(Train): 0.3814; L(Val): 0.4387; Reg Param: 0.0485; Time: 0.81\n",
      "Epoch 4563/8192 --- L(Train): 0.3796; L(Val): 0.4387; Reg Param: 0.0485; Time: 0.90\n",
      "Epoch 4564/8192 --- L(Train): 0.3765; L(Val): 0.4388; Reg Param: 0.0485; Time: 0.78\n",
      "Epoch 4565/8192 --- L(Train): 0.3747; L(Val): 0.4394; Reg Param: 0.0485; Time: 0.78\n",
      "Epoch 4566/8192 --- L(Train): 0.3801; L(Val): 0.4401; Reg Param: 0.0485; Time: 0.80\n",
      "Epoch 4567/8192 --- L(Train): 0.3784; L(Val): 0.4406; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4568/8192 --- L(Train): 0.3909; L(Val): 0.4412; Reg Param: 0.0485; Time: 0.75\n",
      "Epoch 4569/8192 --- L(Train): 0.3861; L(Val): 0.4421; Reg Param: 0.0485; Time: 0.86\n",
      "Epoch 4570/8192 --- L(Train): 0.3712; L(Val): 0.4423; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4571/8192 --- L(Train): 0.3727; L(Val): 0.4421; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4572/8192 --- L(Train): 0.3760; L(Val): 0.4420; Reg Param: 0.0485; Time: 0.78\n",
      "Epoch 4573/8192 --- L(Train): 0.3856; L(Val): 0.4415; Reg Param: 0.0485; Time: 0.78\n",
      "Epoch 4574/8192 --- L(Train): 0.3747; L(Val): 0.4408; Reg Param: 0.0485; Time: 0.79\n",
      "Epoch 4575/8192 --- L(Train): 0.3839; L(Val): 0.4402; Reg Param: 0.0485; Time: 0.78\n",
      "Epoch 4576/8192 --- L(Train): 0.3749; L(Val): 0.4394; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4577/8192 --- L(Train): 0.3744; L(Val): 0.4386; Reg Param: 0.0485; Time: 0.89\n",
      "Epoch 4578/8192 --- L(Train): 0.3697; L(Val): 0.4382; Reg Param: 0.0485; Time: 0.75\n",
      "Epoch 4579/8192 --- L(Train): 0.3809; L(Val): 0.4375; Reg Param: 0.0485; Time: 0.83\n",
      "Epoch 4580/8192 --- L(Train): 0.3782; L(Val): 0.4369; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4581/8192 --- L(Train): 0.3747; L(Val): 0.4365; Reg Param: 0.0485; Time: 0.78\n",
      "Epoch 4582/8192 --- L(Train): 0.3754; L(Val): 0.4368; Reg Param: 0.0485; Time: 0.88\n",
      "Epoch 4583/8192 --- L(Train): 0.3656; L(Val): 0.4375; Reg Param: 0.0485; Time: 0.78\n",
      "Epoch 4584/8192 --- L(Train): 0.3917; L(Val): 0.4384; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4585/8192 --- L(Train): 0.3879; L(Val): 0.4389; Reg Param: 0.0485; Time: 0.75\n",
      "Epoch 4586/8192 --- L(Train): 0.3728; L(Val): 0.4388; Reg Param: 0.0485; Time: 0.81\n",
      "Epoch 4587/8192 --- L(Train): 0.3734; L(Val): 0.4395; Reg Param: 0.0485; Time: 0.80\n",
      "Epoch 4588/8192 --- L(Train): 0.3717; L(Val): 0.4400; Reg Param: 0.0485; Time: 0.76\n",
      "Epoch 4589/8192 --- L(Train): 0.3768; L(Val): 0.4396; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4590/8192 --- L(Train): 0.3761; L(Val): 0.4400; Reg Param: 0.0485; Time: 0.75\n",
      "Epoch 4591/8192 --- L(Train): 0.3657; L(Val): 0.4407; Reg Param: 0.0485; Time: 0.74\n",
      "Epoch 4592/8192 --- L(Train): 0.3787; L(Val): 0.4401; Reg Param: 0.0485; Time: 0.78\n",
      "Epoch 4593/8192 --- L(Train): 0.3778; L(Val): 0.4399; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4594/8192 --- L(Train): 0.3750; L(Val): 0.4400; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4595/8192 --- L(Train): 0.3854; L(Val): 0.4394; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4596/8192 --- L(Train): 0.3774; L(Val): 0.4390; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4597/8192 --- L(Train): 0.3820; L(Val): 0.4389; Reg Param: 0.0485; Time: 0.77\n",
      "Epoch 4598/8192 --- L(Train): 0.3719; L(Val): 0.4383; Reg Param: 0.0485; Time: 0.78\n",
      "Epoch 4599/8192 --- L(Train): 0.3757; L(Val): 0.4383; Reg Param: 0.0485; Time: 0.94\n",
      "Epoch 4600/8192 --- L(Train): 0.3632; L(Val): 0.4383; Reg Param: 0.0477; Time: 8.03\n",
      "Epoch 4601/8192 --- L(Train): 0.3747; L(Val): 0.4393; Reg Param: 0.0477; Time: 0.74\n",
      "Epoch 4602/8192 --- L(Train): 0.3781; L(Val): 0.4405; Reg Param: 0.0477; Time: 0.76\n",
      "Epoch 4603/8192 --- L(Train): 0.3905; L(Val): 0.4419; Reg Param: 0.0477; Time: 0.72\n",
      "Epoch 4604/8192 --- L(Train): 0.3733; L(Val): 0.4429; Reg Param: 0.0477; Time: 0.72\n",
      "Epoch 4605/8192 --- L(Train): 0.3772; L(Val): 0.4429; Reg Param: 0.0477; Time: 0.72\n",
      "Epoch 4606/8192 --- L(Train): 0.3719; L(Val): 0.4423; Reg Param: 0.0477; Time: 0.70\n",
      "Epoch 4607/8192 --- L(Train): 0.3690; L(Val): 0.4414; Reg Param: 0.0477; Time: 0.71\n",
      "Epoch 4608/8192 --- L(Train): 0.3630; L(Val): 0.4401; Reg Param: 0.0477; Time: 0.71\n",
      "Epoch 4609/8192 --- L(Train): 0.3749; L(Val): 0.4392; Reg Param: 0.0477; Time: 0.70\n",
      "Epoch 4610/8192 --- L(Train): 0.3737; L(Val): 0.4379; Reg Param: 0.0477; Time: 0.72\n",
      "Epoch 4611/8192 --- L(Train): 0.3814; L(Val): 0.4376; Reg Param: 0.0477; Time: 0.70\n",
      "Epoch 4612/8192 --- L(Train): 0.3648; L(Val): 0.4383; Reg Param: 0.0477; Time: 0.70\n",
      "Epoch 4613/8192 --- L(Train): 0.3836; L(Val): 0.4394; Reg Param: 0.0477; Time: 0.70\n",
      "Epoch 4614/8192 --- L(Train): 0.3827; L(Val): 0.4401; Reg Param: 0.0477; Time: 0.69\n",
      "Epoch 4615/8192 --- L(Train): 0.3670; L(Val): 0.4403; Reg Param: 0.0477; Time: 0.88\n",
      "Epoch 4616/8192 --- L(Train): 0.3735; L(Val): 0.4406; Reg Param: 0.0477; Time: 0.71\n",
      "Epoch 4617/8192 --- L(Train): 0.3746; L(Val): 0.4407; Reg Param: 0.0477; Time: 0.71\n",
      "Epoch 4618/8192 --- L(Train): 0.3613; L(Val): 0.4402; Reg Param: 0.0477; Time: 0.71\n",
      "Epoch 4619/8192 --- L(Train): 0.3783; L(Val): 0.4398; Reg Param: 0.0477; Time: 0.73\n",
      "Epoch 4620/8192 --- L(Train): 0.3830; L(Val): 0.4392; Reg Param: 0.0477; Time: 0.72\n",
      "Epoch 4621/8192 --- L(Train): 0.3864; L(Val): 0.4388; Reg Param: 0.0477; Time: 0.76\n",
      "Epoch 4622/8192 --- L(Train): 0.3860; L(Val): 0.4387; Reg Param: 0.0477; Time: 0.77\n",
      "Epoch 4623/8192 --- L(Train): 0.3792; L(Val): 0.4385; Reg Param: 0.0477; Time: 0.75\n",
      "Epoch 4624/8192 --- L(Train): 0.3798; L(Val): 0.4392; Reg Param: 0.0477; Time: 0.72\n",
      "Epoch 4625/8192 --- L(Train): 0.3749; L(Val): 0.4399; Reg Param: 0.0477; Time: 0.72\n",
      "Epoch 4626/8192 --- L(Train): 0.3739; L(Val): 0.4408; Reg Param: 0.0477; Time: 0.71\n",
      "Epoch 4627/8192 --- L(Train): 0.3763; L(Val): 0.4412; Reg Param: 0.0477; Time: 0.71\n",
      "Epoch 4628/8192 --- L(Train): 0.3802; L(Val): 0.4417; Reg Param: 0.0477; Time: 0.73\n",
      "Epoch 4629/8192 --- L(Train): 0.3762; L(Val): 0.4409; Reg Param: 0.0477; Time: 0.71\n",
      "Epoch 4630/8192 --- L(Train): 0.3786; L(Val): 0.4402; Reg Param: 0.0477; Time: 0.73\n",
      "Epoch 4631/8192 --- L(Train): 0.3716; L(Val): 0.4395; Reg Param: 0.0477; Time: 0.73\n",
      "Epoch 4632/8192 --- L(Train): 0.3763; L(Val): 0.4392; Reg Param: 0.0477; Time: 0.67\n",
      "Epoch 4633/8192 --- L(Train): 0.3750; L(Val): 0.4394; Reg Param: 0.0477; Time: 0.82\n",
      "Epoch 4634/8192 --- L(Train): 0.3768; L(Val): 0.4398; Reg Param: 0.0477; Time: 0.73\n",
      "Epoch 4635/8192 --- L(Train): 0.3730; L(Val): 0.4396; Reg Param: 0.0477; Time: 0.72\n",
      "Epoch 4636/8192 --- L(Train): 0.3857; L(Val): 0.4397; Reg Param: 0.0477; Time: 0.73\n",
      "Epoch 4637/8192 --- L(Train): 0.3614; L(Val): 0.4402; Reg Param: 0.0477; Time: 0.73\n",
      "Epoch 4638/8192 --- L(Train): 0.3728; L(Val): 0.4407; Reg Param: 0.0477; Time: 0.71\n",
      "Epoch 4639/8192 --- L(Train): 0.3664; L(Val): 0.4414; Reg Param: 0.0477; Time: 0.73\n",
      "Epoch 4640/8192 --- L(Train): 0.3781; L(Val): 0.4419; Reg Param: 0.0477; Time: 0.73\n",
      "Epoch 4641/8192 --- L(Train): 0.3819; L(Val): 0.4419; Reg Param: 0.0477; Time: 0.74\n",
      "Epoch 4642/8192 --- L(Train): 0.3770; L(Val): 0.4421; Reg Param: 0.0477; Time: 0.70\n",
      "Epoch 4643/8192 --- L(Train): 0.3730; L(Val): 0.4422; Reg Param: 0.0477; Time: 0.71\n",
      "Epoch 4644/8192 --- L(Train): 0.3754; L(Val): 0.4425; Reg Param: 0.0477; Time: 0.70\n",
      "Epoch 4645/8192 --- L(Train): 0.3740; L(Val): 0.4426; Reg Param: 0.0477; Time: 0.71\n",
      "Epoch 4646/8192 --- L(Train): 0.3749; L(Val): 0.4425; Reg Param: 0.0477; Time: 0.71\n",
      "Epoch 4647/8192 --- L(Train): 0.3811; L(Val): 0.4422; Reg Param: 0.0477; Time: 0.73\n",
      "Epoch 4648/8192 --- L(Train): 0.3921; L(Val): 0.4412; Reg Param: 0.0477; Time: 0.85\n",
      "Epoch 4649/8192 --- L(Train): 0.3748; L(Val): 0.4405; Reg Param: 0.0477; Time: 0.72\n",
      "Epoch 4650/8192 --- L(Train): 0.3840; L(Val): 0.4405; Reg Param: 0.0469; Time: 7.91\n",
      "Epoch 4651/8192 --- L(Train): 0.3582; L(Val): 0.4412; Reg Param: 0.0469; Time: 0.72\n",
      "Epoch 4652/8192 --- L(Train): 0.3620; L(Val): 0.4422; Reg Param: 0.0469; Time: 0.76\n",
      "Epoch 4653/8192 --- L(Train): 0.3810; L(Val): 0.4423; Reg Param: 0.0469; Time: 0.77\n",
      "Epoch 4654/8192 --- L(Train): 0.3667; L(Val): 0.4424; Reg Param: 0.0469; Time: 0.77\n",
      "Epoch 4655/8192 --- L(Train): 0.3631; L(Val): 0.4427; Reg Param: 0.0469; Time: 0.75\n",
      "Epoch 4656/8192 --- L(Train): 0.3778; L(Val): 0.4423; Reg Param: 0.0469; Time: 0.76\n",
      "Epoch 4657/8192 --- L(Train): 0.3773; L(Val): 0.4418; Reg Param: 0.0469; Time: 0.78\n",
      "Epoch 4658/8192 --- L(Train): 0.3767; L(Val): 0.4411; Reg Param: 0.0469; Time: 0.75\n",
      "Epoch 4659/8192 --- L(Train): 0.3673; L(Val): 0.4408; Reg Param: 0.0469; Time: 0.73\n",
      "Epoch 4660/8192 --- L(Train): 0.3762; L(Val): 0.4404; Reg Param: 0.0469; Time: 0.75\n",
      "Epoch 4661/8192 --- L(Train): 0.3670; L(Val): 0.4403; Reg Param: 0.0469; Time: 0.74\n",
      "Epoch 4662/8192 --- L(Train): 0.3763; L(Val): 0.4405; Reg Param: 0.0469; Time: 0.72\n",
      "Epoch 4663/8192 --- L(Train): 0.3845; L(Val): 0.4402; Reg Param: 0.0469; Time: 0.73\n",
      "Epoch 4664/8192 --- L(Train): 0.3791; L(Val): 0.4403; Reg Param: 0.0469; Time: 0.75\n",
      "Epoch 4665/8192 --- L(Train): 0.3660; L(Val): 0.4415; Reg Param: 0.0469; Time: 0.78\n",
      "Epoch 4666/8192 --- L(Train): 0.3714; L(Val): 0.4422; Reg Param: 0.0469; Time: 0.73\n",
      "Epoch 4667/8192 --- L(Train): 0.3795; L(Val): 0.4432; Reg Param: 0.0469; Time: 0.75\n",
      "Epoch 4668/8192 --- L(Train): 0.3681; L(Val): 0.4438; Reg Param: 0.0469; Time: 0.96\n",
      "Epoch 4669/8192 --- L(Train): 0.3699; L(Val): 0.4439; Reg Param: 0.0469; Time: 0.71\n",
      "Epoch 4670/8192 --- L(Train): 0.3844; L(Val): 0.4437; Reg Param: 0.0469; Time: 0.73\n",
      "Epoch 4671/8192 --- L(Train): 0.3717; L(Val): 0.4437; Reg Param: 0.0469; Time: 0.70\n",
      "Epoch 4672/8192 --- L(Train): 0.3793; L(Val): 0.4436; Reg Param: 0.0469; Time: 0.70\n",
      "Epoch 4673/8192 --- L(Train): 0.3792; L(Val): 0.4434; Reg Param: 0.0469; Time: 0.70\n",
      "Epoch 4674/8192 --- L(Train): 0.3790; L(Val): 0.4433; Reg Param: 0.0469; Time: 0.70\n",
      "Epoch 4675/8192 --- L(Train): 0.3646; L(Val): 0.4441; Reg Param: 0.0469; Time: 0.70\n",
      "Epoch 4676/8192 --- L(Train): 0.3730; L(Val): 0.4435; Reg Param: 0.0469; Time: 0.69\n",
      "Epoch 4677/8192 --- L(Train): 0.3715; L(Val): 0.4434; Reg Param: 0.0469; Time: 0.71\n",
      "Epoch 4678/8192 --- L(Train): 0.3843; L(Val): 0.4432; Reg Param: 0.0469; Time: 0.71\n",
      "Epoch 4679/8192 --- L(Train): 0.3836; L(Val): 0.4430; Reg Param: 0.0469; Time: 0.71\n",
      "Epoch 4680/8192 --- L(Train): 0.3699; L(Val): 0.4429; Reg Param: 0.0469; Time: 0.71\n",
      "Epoch 4681/8192 --- L(Train): 0.3623; L(Val): 0.4435; Reg Param: 0.0469; Time: 0.72\n",
      "Epoch 4682/8192 --- L(Train): 0.3719; L(Val): 0.4440; Reg Param: 0.0469; Time: 0.73\n",
      "Epoch 4683/8192 --- L(Train): 0.3819; L(Val): 0.4436; Reg Param: 0.0469; Time: 0.71\n",
      "Epoch 4684/8192 --- L(Train): 0.3892; L(Val): 0.4439; Reg Param: 0.0469; Time: 0.71\n",
      "Epoch 4685/8192 --- L(Train): 0.3707; L(Val): 0.4446; Reg Param: 0.0469; Time: 0.70\n",
      "Epoch 4686/8192 --- L(Train): 0.3784; L(Val): 0.4452; Reg Param: 0.0469; Time: 0.71\n",
      "Epoch 4687/8192 --- L(Train): 0.3728; L(Val): 0.4451; Reg Param: 0.0469; Time: 0.71\n",
      "Epoch 4688/8192 --- L(Train): 0.3783; L(Val): 0.4443; Reg Param: 0.0469; Time: 0.78\n",
      "Epoch 4689/8192 --- L(Train): 0.3724; L(Val): 0.4424; Reg Param: 0.0469; Time: 0.73\n",
      "Epoch 4690/8192 --- L(Train): 0.3826; L(Val): 0.4408; Reg Param: 0.0469; Time: 0.71\n",
      "Epoch 4691/8192 --- L(Train): 0.3814; L(Val): 0.4395; Reg Param: 0.0469; Time: 0.72\n",
      "Epoch 4692/8192 --- L(Train): 0.3809; L(Val): 0.4389; Reg Param: 0.0469; Time: 0.77\n",
      "Epoch 4693/8192 --- L(Train): 0.3864; L(Val): 0.4388; Reg Param: 0.0469; Time: 0.75\n",
      "Epoch 4694/8192 --- L(Train): 0.3876; L(Val): 0.4385; Reg Param: 0.0469; Time: 0.75\n",
      "Epoch 4695/8192 --- L(Train): 0.3841; L(Val): 0.4383; Reg Param: 0.0469; Time: 0.74\n",
      "Epoch 4696/8192 --- L(Train): 0.3712; L(Val): 0.4380; Reg Param: 0.0469; Time: 0.87\n",
      "Epoch 4697/8192 --- L(Train): 0.3709; L(Val): 0.4385; Reg Param: 0.0469; Time: 0.72\n",
      "Epoch 4698/8192 --- L(Train): 0.3755; L(Val): 0.4391; Reg Param: 0.0469; Time: 0.72\n",
      "Epoch 4699/8192 --- L(Train): 0.3812; L(Val): 0.4395; Reg Param: 0.0469; Time: 0.72\n",
      "Epoch 4700/8192 --- L(Train): 0.3904; L(Val): 0.4395; Reg Param: 0.0462; Time: 7.75\n",
      "Epoch 4701/8192 --- L(Train): 0.3766; L(Val): 0.4398; Reg Param: 0.0462; Time: 0.71\n",
      "Epoch 4702/8192 --- L(Train): 0.3801; L(Val): 0.4395; Reg Param: 0.0462; Time: 0.74\n",
      "Epoch 4703/8192 --- L(Train): 0.3882; L(Val): 0.4390; Reg Param: 0.0462; Time: 0.74\n",
      "Epoch 4704/8192 --- L(Train): 0.3636; L(Val): 0.4385; Reg Param: 0.0462; Time: 0.76\n",
      "Epoch 4705/8192 --- L(Train): 0.3820; L(Val): 0.4383; Reg Param: 0.0462; Time: 0.75\n",
      "Epoch 4706/8192 --- L(Train): 0.3747; L(Val): 0.4382; Reg Param: 0.0462; Time: 0.75\n",
      "Epoch 4707/8192 --- L(Train): 0.3801; L(Val): 0.4374; Reg Param: 0.0462; Time: 0.76\n",
      "Epoch 4708/8192 --- L(Train): 0.3765; L(Val): 0.4370; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4709/8192 --- L(Train): 0.3841; L(Val): 0.4367; Reg Param: 0.0462; Time: 0.74\n",
      "Epoch 4710/8192 --- L(Train): 0.3775; L(Val): 0.4368; Reg Param: 0.0462; Time: 0.74\n",
      "Epoch 4711/8192 --- L(Train): 0.3649; L(Val): 0.4371; Reg Param: 0.0462; Time: 0.73\n",
      "Epoch 4712/8192 --- L(Train): 0.3703; L(Val): 0.4359; Reg Param: 0.0462; Time: 0.73\n",
      "Epoch 4713/8192 --- L(Train): 0.3754; L(Val): 0.4357; Reg Param: 0.0462; Time: 0.74\n",
      "Epoch 4714/8192 --- L(Train): 0.3851; L(Val): 0.4368; Reg Param: 0.0462; Time: 0.73\n",
      "Epoch 4715/8192 --- L(Train): 0.3764; L(Val): 0.4369; Reg Param: 0.0462; Time: 0.73\n",
      "Epoch 4716/8192 --- L(Train): 0.3811; L(Val): 0.4371; Reg Param: 0.0462; Time: 0.74\n",
      "Epoch 4717/8192 --- L(Train): 0.3717; L(Val): 0.4384; Reg Param: 0.0462; Time: 0.75\n",
      "Epoch 4718/8192 --- L(Train): 0.3760; L(Val): 0.4390; Reg Param: 0.0462; Time: 0.74\n",
      "Epoch 4719/8192 --- L(Train): 0.3926; L(Val): 0.4386; Reg Param: 0.0462; Time: 0.74\n",
      "Epoch 4720/8192 --- L(Train): 0.3664; L(Val): 0.4383; Reg Param: 0.0462; Time: 0.73\n",
      "Epoch 4721/8192 --- L(Train): 0.3724; L(Val): 0.4382; Reg Param: 0.0462; Time: 0.80\n",
      "Epoch 4722/8192 --- L(Train): 0.3673; L(Val): 0.4381; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4723/8192 --- L(Train): 0.3776; L(Val): 0.4388; Reg Param: 0.0462; Time: 0.73\n",
      "Epoch 4724/8192 --- L(Train): 0.3807; L(Val): 0.4395; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4725/8192 --- L(Train): 0.3853; L(Val): 0.4397; Reg Param: 0.0462; Time: 0.71\n",
      "Epoch 4726/8192 --- L(Train): 0.3839; L(Val): 0.4399; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4727/8192 --- L(Train): 0.3780; L(Val): 0.4397; Reg Param: 0.0462; Time: 0.83\n",
      "Epoch 4728/8192 --- L(Train): 0.3901; L(Val): 0.4392; Reg Param: 0.0462; Time: 0.74\n",
      "Epoch 4729/8192 --- L(Train): 0.3795; L(Val): 0.4388; Reg Param: 0.0462; Time: 0.83\n",
      "Epoch 4730/8192 --- L(Train): 0.3702; L(Val): 0.4388; Reg Param: 0.0462; Time: 0.71\n",
      "Epoch 4731/8192 --- L(Train): 0.3667; L(Val): 0.4384; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4732/8192 --- L(Train): 0.3735; L(Val): 0.4385; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4733/8192 --- L(Train): 0.3855; L(Val): 0.4391; Reg Param: 0.0462; Time: 0.71\n",
      "Epoch 4734/8192 --- L(Train): 0.3781; L(Val): 0.4398; Reg Param: 0.0462; Time: 0.70\n",
      "Epoch 4735/8192 --- L(Train): 0.3715; L(Val): 0.4404; Reg Param: 0.0462; Time: 0.71\n",
      "Epoch 4736/8192 --- L(Train): 0.3669; L(Val): 0.4410; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4737/8192 --- L(Train): 0.3712; L(Val): 0.4414; Reg Param: 0.0462; Time: 0.73\n",
      "Epoch 4738/8192 --- L(Train): 0.3759; L(Val): 0.4418; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4739/8192 --- L(Train): 0.3753; L(Val): 0.4418; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4740/8192 --- L(Train): 0.3767; L(Val): 0.4412; Reg Param: 0.0462; Time: 0.73\n",
      "Epoch 4741/8192 --- L(Train): 0.3813; L(Val): 0.4402; Reg Param: 0.0462; Time: 0.71\n",
      "Epoch 4742/8192 --- L(Train): 0.3648; L(Val): 0.4399; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4743/8192 --- L(Train): 0.3707; L(Val): 0.4395; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4744/8192 --- L(Train): 0.3773; L(Val): 0.4391; Reg Param: 0.0462; Time: 0.73\n",
      "Epoch 4745/8192 --- L(Train): 0.3828; L(Val): 0.4386; Reg Param: 0.0462; Time: 0.71\n",
      "Epoch 4746/8192 --- L(Train): 0.3746; L(Val): 0.4385; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4747/8192 --- L(Train): 0.3834; L(Val): 0.4383; Reg Param: 0.0462; Time: 0.71\n",
      "Epoch 4748/8192 --- L(Train): 0.3799; L(Val): 0.4381; Reg Param: 0.0462; Time: 0.74\n",
      "Epoch 4749/8192 --- L(Train): 0.3710; L(Val): 0.4383; Reg Param: 0.0462; Time: 0.72\n",
      "Epoch 4750/8192 --- L(Train): 0.3828; L(Val): 0.4383; Reg Param: 0.0455; Time: 7.54\n",
      "Epoch 4751/8192 --- L(Train): 0.3737; L(Val): 0.4389; Reg Param: 0.0455; Time: 0.69\n",
      "Epoch 4752/8192 --- L(Train): 0.3800; L(Val): 0.4393; Reg Param: 0.0455; Time: 0.71\n",
      "Epoch 4753/8192 --- L(Train): 0.3806; L(Val): 0.4398; Reg Param: 0.0455; Time: 0.69\n",
      "Epoch 4754/8192 --- L(Train): 0.3718; L(Val): 0.4402; Reg Param: 0.0455; Time: 0.69\n",
      "Epoch 4755/8192 --- L(Train): 0.3843; L(Val): 0.4404; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4756/8192 --- L(Train): 0.3732; L(Val): 0.4407; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4757/8192 --- L(Train): 0.3657; L(Val): 0.4409; Reg Param: 0.0455; Time: 0.71\n",
      "Epoch 4758/8192 --- L(Train): 0.3745; L(Val): 0.4408; Reg Param: 0.0455; Time: 0.69\n",
      "Epoch 4759/8192 --- L(Train): 0.3838; L(Val): 0.4403; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4760/8192 --- L(Train): 0.3757; L(Val): 0.4397; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4761/8192 --- L(Train): 0.3812; L(Val): 0.4396; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4762/8192 --- L(Train): 0.3697; L(Val): 0.4398; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4763/8192 --- L(Train): 0.3781; L(Val): 0.4400; Reg Param: 0.0455; Time: 0.71\n",
      "Epoch 4764/8192 --- L(Train): 0.3710; L(Val): 0.4397; Reg Param: 0.0455; Time: 0.71\n",
      "Epoch 4765/8192 --- L(Train): 0.3781; L(Val): 0.4398; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4766/8192 --- L(Train): 0.3771; L(Val): 0.4402; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4767/8192 --- L(Train): 0.3723; L(Val): 0.4400; Reg Param: 0.0455; Time: 0.69\n",
      "Epoch 4768/8192 --- L(Train): 0.3660; L(Val): 0.4397; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4769/8192 --- L(Train): 0.3774; L(Val): 0.4396; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4770/8192 --- L(Train): 0.3706; L(Val): 0.4397; Reg Param: 0.0455; Time: 0.71\n",
      "Epoch 4771/8192 --- L(Train): 0.3634; L(Val): 0.4401; Reg Param: 0.0455; Time: 0.71\n",
      "Epoch 4772/8192 --- L(Train): 0.3725; L(Val): 0.4405; Reg Param: 0.0455; Time: 0.69\n",
      "Epoch 4773/8192 --- L(Train): 0.3712; L(Val): 0.4405; Reg Param: 0.0455; Time: 0.86\n",
      "Epoch 4774/8192 --- L(Train): 0.3798; L(Val): 0.4409; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4775/8192 --- L(Train): 0.3717; L(Val): 0.4411; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4776/8192 --- L(Train): 0.3689; L(Val): 0.4415; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4777/8192 --- L(Train): 0.3814; L(Val): 0.4417; Reg Param: 0.0455; Time: 0.71\n",
      "Epoch 4778/8192 --- L(Train): 0.3841; L(Val): 0.4416; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4779/8192 --- L(Train): 0.3766; L(Val): 0.4410; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4780/8192 --- L(Train): 0.3872; L(Val): 0.4403; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4781/8192 --- L(Train): 0.3701; L(Val): 0.4400; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4782/8192 --- L(Train): 0.3776; L(Val): 0.4396; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4783/8192 --- L(Train): 0.3722; L(Val): 0.4394; Reg Param: 0.0455; Time: 0.71\n",
      "Epoch 4784/8192 --- L(Train): 0.3741; L(Val): 0.4392; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4785/8192 --- L(Train): 0.3747; L(Val): 0.4392; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4786/8192 --- L(Train): 0.3702; L(Val): 0.4397; Reg Param: 0.0455; Time: 0.71\n",
      "Epoch 4787/8192 --- L(Train): 0.3805; L(Val): 0.4403; Reg Param: 0.0455; Time: 0.71\n",
      "Epoch 4788/8192 --- L(Train): 0.3797; L(Val): 0.4411; Reg Param: 0.0455; Time: 0.94\n",
      "Epoch 4789/8192 --- L(Train): 0.3686; L(Val): 0.4422; Reg Param: 0.0455; Time: 0.80\n",
      "Epoch 4790/8192 --- L(Train): 0.3727; L(Val): 0.4432; Reg Param: 0.0455; Time: 0.76\n",
      "Epoch 4791/8192 --- L(Train): 0.3832; L(Val): 0.4439; Reg Param: 0.0455; Time: 0.72\n",
      "Epoch 4792/8192 --- L(Train): 0.3796; L(Val): 0.4436; Reg Param: 0.0455; Time: 0.71\n",
      "Epoch 4793/8192 --- L(Train): 0.3657; L(Val): 0.4432; Reg Param: 0.0455; Time: 0.87\n",
      "Epoch 4794/8192 --- L(Train): 0.3792; L(Val): 0.4428; Reg Param: 0.0455; Time: 0.96\n",
      "Epoch 4795/8192 --- L(Train): 0.3704; L(Val): 0.4427; Reg Param: 0.0455; Time: 0.80\n",
      "Epoch 4796/8192 --- L(Train): 0.3809; L(Val): 0.4426; Reg Param: 0.0455; Time: 0.73\n",
      "Epoch 4797/8192 --- L(Train): 0.3828; L(Val): 0.4423; Reg Param: 0.0455; Time: 0.72\n",
      "Epoch 4798/8192 --- L(Train): 0.3705; L(Val): 0.4422; Reg Param: 0.0455; Time: 0.72\n",
      "Epoch 4799/8192 --- L(Train): 0.3727; L(Val): 0.4412; Reg Param: 0.0455; Time: 0.70\n",
      "Epoch 4800/8192 --- L(Train): 0.3771; L(Val): 0.4412; Reg Param: 0.0448; Time: 7.76\n",
      "Epoch 4801/8192 --- L(Train): 0.3788; L(Val): 0.4405; Reg Param: 0.0448; Time: 0.73\n",
      "Epoch 4802/8192 --- L(Train): 0.3774; L(Val): 0.4416; Reg Param: 0.0448; Time: 0.70\n",
      "Epoch 4803/8192 --- L(Train): 0.3744; L(Val): 0.4424; Reg Param: 0.0448; Time: 0.70\n",
      "Epoch 4804/8192 --- L(Train): 0.3834; L(Val): 0.4430; Reg Param: 0.0448; Time: 0.75\n",
      "Epoch 4805/8192 --- L(Train): 0.3699; L(Val): 0.4429; Reg Param: 0.0448; Time: 0.69\n",
      "Epoch 4806/8192 --- L(Train): 0.3778; L(Val): 0.4431; Reg Param: 0.0448; Time: 0.71\n",
      "Epoch 4807/8192 --- L(Train): 0.3754; L(Val): 0.4434; Reg Param: 0.0448; Time: 0.71\n",
      "Epoch 4808/8192 --- L(Train): 0.3771; L(Val): 0.4435; Reg Param: 0.0448; Time: 0.92\n",
      "Epoch 4809/8192 --- L(Train): 0.3801; L(Val): 0.4434; Reg Param: 0.0448; Time: 0.76\n",
      "Epoch 4810/8192 --- L(Train): 0.3760; L(Val): 0.4433; Reg Param: 0.0448; Time: 0.79\n",
      "Epoch 4811/8192 --- L(Train): 0.3675; L(Val): 0.4427; Reg Param: 0.0448; Time: 0.80\n",
      "Epoch 4812/8192 --- L(Train): 0.3742; L(Val): 0.4428; Reg Param: 0.0448; Time: 0.72\n",
      "Epoch 4813/8192 --- L(Train): 0.3712; L(Val): 0.4433; Reg Param: 0.0448; Time: 0.72\n",
      "Epoch 4814/8192 --- L(Train): 0.3787; L(Val): 0.4437; Reg Param: 0.0448; Time: 0.73\n",
      "Epoch 4815/8192 --- L(Train): 0.3708; L(Val): 0.4441; Reg Param: 0.0448; Time: 0.73\n",
      "Epoch 4816/8192 --- L(Train): 0.3738; L(Val): 0.4444; Reg Param: 0.0448; Time: 0.71\n",
      "Epoch 4817/8192 --- L(Train): 0.3788; L(Val): 0.4445; Reg Param: 0.0448; Time: 0.73\n",
      "Epoch 4818/8192 --- L(Train): 0.3846; L(Val): 0.4437; Reg Param: 0.0448; Time: 0.71\n",
      "Epoch 4819/8192 --- L(Train): 0.3677; L(Val): 0.4431; Reg Param: 0.0448; Time: 0.71\n",
      "Epoch 4820/8192 --- L(Train): 0.3717; L(Val): 0.4425; Reg Param: 0.0448; Time: 0.71\n",
      "Epoch 4821/8192 --- L(Train): 0.3816; L(Val): 0.4419; Reg Param: 0.0448; Time: 0.70\n",
      "Epoch 4822/8192 --- L(Train): 0.3665; L(Val): 0.4410; Reg Param: 0.0448; Time: 0.72\n",
      "Epoch 4823/8192 --- L(Train): 0.3729; L(Val): 0.4400; Reg Param: 0.0448; Time: 0.76\n",
      "Epoch 4824/8192 --- L(Train): 0.3714; L(Val): 0.4397; Reg Param: 0.0448; Time: 0.74\n",
      "Epoch 4825/8192 --- L(Train): 0.3783; L(Val): 0.4396; Reg Param: 0.0448; Time: 0.81\n",
      "Epoch 4826/8192 --- L(Train): 0.3755; L(Val): 0.4400; Reg Param: 0.0448; Time: 1.04\n",
      "Epoch 4827/8192 --- L(Train): 0.3730; L(Val): 0.4405; Reg Param: 0.0448; Time: 0.75\n",
      "Epoch 4828/8192 --- L(Train): 0.3756; L(Val): 0.4406; Reg Param: 0.0448; Time: 0.74\n",
      "Epoch 4829/8192 --- L(Train): 0.3759; L(Val): 0.4408; Reg Param: 0.0448; Time: 0.76\n",
      "Epoch 4830/8192 --- L(Train): 0.3797; L(Val): 0.4409; Reg Param: 0.0448; Time: 0.75\n",
      "Epoch 4831/8192 --- L(Train): 0.3731; L(Val): 0.4405; Reg Param: 0.0448; Time: 0.75\n",
      "Epoch 4832/8192 --- L(Train): 0.3716; L(Val): 0.4399; Reg Param: 0.0448; Time: 0.75\n",
      "Epoch 4833/8192 --- L(Train): 0.3800; L(Val): 0.4406; Reg Param: 0.0448; Time: 0.75\n",
      "Epoch 4834/8192 --- L(Train): 0.3765; L(Val): 0.4413; Reg Param: 0.0448; Time: 0.88\n",
      "Epoch 4835/8192 --- L(Train): 0.3850; L(Val): 0.4419; Reg Param: 0.0448; Time: 0.75\n",
      "Epoch 4836/8192 --- L(Train): 0.3754; L(Val): 0.4424; Reg Param: 0.0448; Time: 0.78\n",
      "Epoch 4837/8192 --- L(Train): 0.3890; L(Val): 0.4418; Reg Param: 0.0448; Time: 0.75\n",
      "Epoch 4838/8192 --- L(Train): 0.3599; L(Val): 0.4417; Reg Param: 0.0448; Time: 0.78\n",
      "Epoch 4839/8192 --- L(Train): 0.3762; L(Val): 0.4419; Reg Param: 0.0448; Time: 0.78\n",
      "Epoch 4840/8192 --- L(Train): 0.3819; L(Val): 0.4422; Reg Param: 0.0448; Time: 0.75\n",
      "Epoch 4841/8192 --- L(Train): 0.3831; L(Val): 0.4426; Reg Param: 0.0448; Time: 0.80\n",
      "Epoch 4842/8192 --- L(Train): 0.3884; L(Val): 0.4430; Reg Param: 0.0448; Time: 0.79\n",
      "Epoch 4843/8192 --- L(Train): 0.3790; L(Val): 0.4432; Reg Param: 0.0448; Time: 0.77\n",
      "Epoch 4844/8192 --- L(Train): 0.3707; L(Val): 0.4436; Reg Param: 0.0448; Time: 0.76\n",
      "Epoch 4845/8192 --- L(Train): 0.3823; L(Val): 0.4438; Reg Param: 0.0448; Time: 0.75\n",
      "Epoch 4846/8192 --- L(Train): 0.3707; L(Val): 0.4433; Reg Param: 0.0448; Time: 0.76\n",
      "Epoch 4847/8192 --- L(Train): 0.3785; L(Val): 0.4429; Reg Param: 0.0448; Time: 0.72\n",
      "Epoch 4848/8192 --- L(Train): 0.3861; L(Val): 0.4427; Reg Param: 0.0448; Time: 0.72\n",
      "Epoch 4849/8192 --- L(Train): 0.3701; L(Val): 0.4430; Reg Param: 0.0448; Time: 0.75\n",
      "Epoch 4850/8192 --- L(Train): 0.3768; L(Val): 0.4430; Reg Param: 0.0442; Time: 8.00\n",
      "Epoch 4851/8192 --- L(Train): 0.3751; L(Val): 0.4438; Reg Param: 0.0442; Time: 0.73\n",
      "Epoch 4852/8192 --- L(Train): 0.3794; L(Val): 0.4440; Reg Param: 0.0442; Time: 0.73\n",
      "Epoch 4853/8192 --- L(Train): 0.3760; L(Val): 0.4435; Reg Param: 0.0442; Time: 0.72\n",
      "Epoch 4854/8192 --- L(Train): 0.3871; L(Val): 0.4430; Reg Param: 0.0442; Time: 0.80\n",
      "Epoch 4855/8192 --- L(Train): 0.3811; L(Val): 0.4425; Reg Param: 0.0442; Time: 0.83\n",
      "Epoch 4856/8192 --- L(Train): 0.3690; L(Val): 0.4417; Reg Param: 0.0442; Time: 0.84\n",
      "Epoch 4857/8192 --- L(Train): 0.3674; L(Val): 0.4407; Reg Param: 0.0442; Time: 0.85\n",
      "Epoch 4858/8192 --- L(Train): 0.3754; L(Val): 0.4404; Reg Param: 0.0442; Time: 0.80\n",
      "Epoch 4859/8192 --- L(Train): 0.3726; L(Val): 0.4400; Reg Param: 0.0442; Time: 0.78\n",
      "Epoch 4860/8192 --- L(Train): 0.3717; L(Val): 0.4400; Reg Param: 0.0442; Time: 0.86\n",
      "Epoch 4861/8192 --- L(Train): 0.3850; L(Val): 0.4402; Reg Param: 0.0442; Time: 0.84\n",
      "Epoch 4862/8192 --- L(Train): 0.3706; L(Val): 0.4404; Reg Param: 0.0442; Time: 0.77\n",
      "Epoch 4863/8192 --- L(Train): 0.3735; L(Val): 0.4408; Reg Param: 0.0442; Time: 0.86\n",
      "Epoch 4864/8192 --- L(Train): 0.3731; L(Val): 0.4408; Reg Param: 0.0442; Time: 0.80\n",
      "Epoch 4865/8192 --- L(Train): 0.3796; L(Val): 0.4401; Reg Param: 0.0442; Time: 0.81\n",
      "Epoch 4866/8192 --- L(Train): 0.3767; L(Val): 0.4399; Reg Param: 0.0442; Time: 0.80\n",
      "Epoch 4867/8192 --- L(Train): 0.3809; L(Val): 0.4392; Reg Param: 0.0442; Time: 0.79\n",
      "Epoch 4868/8192 --- L(Train): 0.3878; L(Val): 0.4381; Reg Param: 0.0442; Time: 0.78\n",
      "Epoch 4869/8192 --- L(Train): 0.3778; L(Val): 0.4377; Reg Param: 0.0442; Time: 0.84\n",
      "Epoch 4870/8192 --- L(Train): 0.3781; L(Val): 0.4378; Reg Param: 0.0442; Time: 0.81\n",
      "Epoch 4871/8192 --- L(Train): 0.3771; L(Val): 0.4383; Reg Param: 0.0442; Time: 0.81\n",
      "Epoch 4872/8192 --- L(Train): 0.3941; L(Val): 0.4394; Reg Param: 0.0442; Time: 0.82\n",
      "Epoch 4873/8192 --- L(Train): 0.3856; L(Val): 0.4397; Reg Param: 0.0442; Time: 0.83\n",
      "Epoch 4874/8192 --- L(Train): 0.3782; L(Val): 0.4398; Reg Param: 0.0442; Time: 0.77\n",
      "Epoch 4875/8192 --- L(Train): 0.3734; L(Val): 0.4402; Reg Param: 0.0442; Time: 1.21\n",
      "Epoch 4876/8192 --- L(Train): 0.3741; L(Val): 0.4408; Reg Param: 0.0442; Time: 0.93\n",
      "Epoch 4877/8192 --- L(Train): 0.3922; L(Val): 0.4411; Reg Param: 0.0442; Time: 0.83\n",
      "Epoch 4878/8192 --- L(Train): 0.3815; L(Val): 0.4418; Reg Param: 0.0442; Time: 0.81\n",
      "Epoch 4879/8192 --- L(Train): 0.3768; L(Val): 0.4424; Reg Param: 0.0442; Time: 0.76\n",
      "Epoch 4880/8192 --- L(Train): 0.3738; L(Val): 0.4424; Reg Param: 0.0442; Time: 0.77\n",
      "Epoch 4881/8192 --- L(Train): 0.3684; L(Val): 0.4424; Reg Param: 0.0442; Time: 0.83\n",
      "Epoch 4882/8192 --- L(Train): 0.3776; L(Val): 0.4427; Reg Param: 0.0442; Time: 0.75\n",
      "Epoch 4883/8192 --- L(Train): 0.3617; L(Val): 0.4426; Reg Param: 0.0442; Time: 0.74\n",
      "Epoch 4884/8192 --- L(Train): 0.3874; L(Val): 0.4422; Reg Param: 0.0442; Time: 0.79\n",
      "Epoch 4885/8192 --- L(Train): 0.3801; L(Val): 0.4419; Reg Param: 0.0442; Time: 0.75\n",
      "Epoch 4886/8192 --- L(Train): 0.3750; L(Val): 0.4419; Reg Param: 0.0442; Time: 0.76\n",
      "Epoch 4887/8192 --- L(Train): 0.3802; L(Val): 0.4419; Reg Param: 0.0442; Time: 0.74\n",
      "Epoch 4888/8192 --- L(Train): 0.3743; L(Val): 0.4419; Reg Param: 0.0442; Time: 0.70\n",
      "Epoch 4889/8192 --- L(Train): 0.3778; L(Val): 0.4422; Reg Param: 0.0442; Time: 0.76\n",
      "Epoch 4890/8192 --- L(Train): 0.3683; L(Val): 0.4425; Reg Param: 0.0442; Time: 0.75\n",
      "Epoch 4891/8192 --- L(Train): 0.3793; L(Val): 0.4426; Reg Param: 0.0442; Time: 0.73\n",
      "Epoch 4892/8192 --- L(Train): 0.3637; L(Val): 0.4427; Reg Param: 0.0442; Time: 0.75\n",
      "Epoch 4893/8192 --- L(Train): 0.3757; L(Val): 0.4430; Reg Param: 0.0442; Time: 0.75\n",
      "Epoch 4894/8192 --- L(Train): 0.3795; L(Val): 0.4430; Reg Param: 0.0442; Time: 0.77\n",
      "Epoch 4895/8192 --- L(Train): 0.3601; L(Val): 0.4433; Reg Param: 0.0442; Time: 0.77\n",
      "Epoch 4896/8192 --- L(Train): 0.3767; L(Val): 0.4435; Reg Param: 0.0442; Time: 0.75\n",
      "Epoch 4897/8192 --- L(Train): 0.3787; L(Val): 0.4436; Reg Param: 0.0442; Time: 0.71\n",
      "Epoch 4898/8192 --- L(Train): 0.3720; L(Val): 0.4437; Reg Param: 0.0442; Time: 0.74\n",
      "Epoch 4899/8192 --- L(Train): 0.3810; L(Val): 0.4439; Reg Param: 0.0442; Time: 0.72\n",
      "Epoch 4900/8192 --- L(Train): 0.3856; L(Val): 0.4439; Reg Param: 0.0434; Time: 8.40\n",
      "Epoch 4901/8192 --- L(Train): 0.3918; L(Val): 0.4434; Reg Param: 0.0434; Time: 0.77\n",
      "Epoch 4902/8192 --- L(Train): 0.3777; L(Val): 0.4433; Reg Param: 0.0434; Time: 0.77\n",
      "Epoch 4903/8192 --- L(Train): 0.3660; L(Val): 0.4429; Reg Param: 0.0434; Time: 0.79\n",
      "Epoch 4904/8192 --- L(Train): 0.3674; L(Val): 0.4429; Reg Param: 0.0434; Time: 0.79\n",
      "Epoch 4905/8192 --- L(Train): 0.3724; L(Val): 0.4430; Reg Param: 0.0434; Time: 0.76\n",
      "Epoch 4906/8192 --- L(Train): 0.3759; L(Val): 0.4426; Reg Param: 0.0434; Time: 0.79\n",
      "Epoch 4907/8192 --- L(Train): 0.3712; L(Val): 0.4427; Reg Param: 0.0434; Time: 0.78\n",
      "Epoch 4908/8192 --- L(Train): 0.3723; L(Val): 0.4429; Reg Param: 0.0434; Time: 0.78\n",
      "Epoch 4909/8192 --- L(Train): 0.3716; L(Val): 0.4430; Reg Param: 0.0434; Time: 0.77\n",
      "Epoch 4910/8192 --- L(Train): 0.3672; L(Val): 0.4437; Reg Param: 0.0434; Time: 0.77\n",
      "Epoch 4911/8192 --- L(Train): 0.3811; L(Val): 0.4444; Reg Param: 0.0434; Time: 0.77\n",
      "Epoch 4912/8192 --- L(Train): 0.3830; L(Val): 0.4446; Reg Param: 0.0434; Time: 0.75\n",
      "Epoch 4913/8192 --- L(Train): 0.3818; L(Val): 0.4450; Reg Param: 0.0434; Time: 0.74\n",
      "Epoch 4914/8192 --- L(Train): 0.3681; L(Val): 0.4449; Reg Param: 0.0434; Time: 0.75\n",
      "Epoch 4915/8192 --- L(Train): 0.3867; L(Val): 0.4448; Reg Param: 0.0434; Time: 0.75\n",
      "Epoch 4916/8192 --- L(Train): 0.3741; L(Val): 0.4448; Reg Param: 0.0434; Time: 0.75\n",
      "Epoch 4917/8192 --- L(Train): 0.3773; L(Val): 0.4448; Reg Param: 0.0434; Time: 0.78\n",
      "Epoch 4918/8192 --- L(Train): 0.3741; L(Val): 0.4446; Reg Param: 0.0434; Time: 0.78\n",
      "Epoch 4919/8192 --- L(Train): 0.3743; L(Val): 0.4442; Reg Param: 0.0434; Time: 1.06\n",
      "Epoch 4920/8192 --- L(Train): 0.3686; L(Val): 0.4440; Reg Param: 0.0434; Time: 0.75\n",
      "Epoch 4921/8192 --- L(Train): 0.3728; L(Val): 0.4438; Reg Param: 0.0434; Time: 0.75\n",
      "Epoch 4922/8192 --- L(Train): 0.3900; L(Val): 0.4433; Reg Param: 0.0434; Time: 0.86\n",
      "Epoch 4923/8192 --- L(Train): 0.3825; L(Val): 0.4424; Reg Param: 0.0434; Time: 0.90\n",
      "Epoch 4924/8192 --- L(Train): 0.3661; L(Val): 0.4422; Reg Param: 0.0434; Time: 0.80\n",
      "Epoch 4925/8192 --- L(Train): 0.3763; L(Val): 0.4422; Reg Param: 0.0434; Time: 0.78\n",
      "Epoch 4926/8192 --- L(Train): 0.3626; L(Val): 0.4418; Reg Param: 0.0434; Time: 0.81\n",
      "Epoch 4927/8192 --- L(Train): 0.3747; L(Val): 0.4417; Reg Param: 0.0434; Time: 0.78\n",
      "Epoch 4928/8192 --- L(Train): 0.3939; L(Val): 0.4418; Reg Param: 0.0434; Time: 0.77\n",
      "Epoch 4929/8192 --- L(Train): 0.3664; L(Val): 0.4419; Reg Param: 0.0434; Time: 0.78\n",
      "Epoch 4930/8192 --- L(Train): 0.3751; L(Val): 0.4420; Reg Param: 0.0434; Time: 0.78\n",
      "Epoch 4931/8192 --- L(Train): 0.3788; L(Val): 0.4426; Reg Param: 0.0434; Time: 0.79\n",
      "Epoch 4932/8192 --- L(Train): 0.3792; L(Val): 0.4426; Reg Param: 0.0434; Time: 0.79\n",
      "Epoch 4933/8192 --- L(Train): 0.3838; L(Val): 0.4428; Reg Param: 0.0434; Time: 0.77\n",
      "Epoch 4934/8192 --- L(Train): 0.3717; L(Val): 0.4433; Reg Param: 0.0434; Time: 0.80\n",
      "Epoch 4935/8192 --- L(Train): 0.3726; L(Val): 0.4435; Reg Param: 0.0434; Time: 0.80\n",
      "Epoch 4936/8192 --- L(Train): 0.3772; L(Val): 0.4434; Reg Param: 0.0434; Time: 0.79\n",
      "Epoch 4937/8192 --- L(Train): 0.3720; L(Val): 0.4431; Reg Param: 0.0434; Time: 0.81\n",
      "Epoch 4938/8192 --- L(Train): 0.3844; L(Val): 0.4427; Reg Param: 0.0434; Time: 0.80\n",
      "Epoch 4939/8192 --- L(Train): 0.3688; L(Val): 0.4425; Reg Param: 0.0434; Time: 0.79\n",
      "Epoch 4940/8192 --- L(Train): 0.3711; L(Val): 0.4420; Reg Param: 0.0434; Time: 0.88\n",
      "Epoch 4941/8192 --- L(Train): 0.3713; L(Val): 0.4418; Reg Param: 0.0434; Time: 0.79\n",
      "Epoch 4942/8192 --- L(Train): 0.3822; L(Val): 0.4413; Reg Param: 0.0434; Time: 0.80\n",
      "Epoch 4943/8192 --- L(Train): 0.3874; L(Val): 0.4409; Reg Param: 0.0434; Time: 0.78\n",
      "Epoch 4944/8192 --- L(Train): 0.3812; L(Val): 0.4407; Reg Param: 0.0434; Time: 0.80\n",
      "Epoch 4945/8192 --- L(Train): 0.3803; L(Val): 0.4405; Reg Param: 0.0434; Time: 1.10\n",
      "Epoch 4946/8192 --- L(Train): 0.3737; L(Val): 0.4404; Reg Param: 0.0434; Time: 0.76\n",
      "Epoch 4947/8192 --- L(Train): 0.3907; L(Val): 0.4408; Reg Param: 0.0434; Time: 0.78\n",
      "Epoch 4948/8192 --- L(Train): 0.3747; L(Val): 0.4412; Reg Param: 0.0434; Time: 0.73\n",
      "Epoch 4949/8192 --- L(Train): 0.3706; L(Val): 0.4415; Reg Param: 0.0434; Time: 0.74\n",
      "Epoch 4950/8192 --- L(Train): 0.3819; L(Val): 0.4415; Reg Param: 0.0427; Time: 8.10\n",
      "Epoch 4951/8192 --- L(Train): 0.3834; L(Val): 0.4414; Reg Param: 0.0427; Time: 0.73\n",
      "Epoch 4952/8192 --- L(Train): 0.3685; L(Val): 0.4416; Reg Param: 0.0427; Time: 0.84\n",
      "Epoch 4953/8192 --- L(Train): 0.3783; L(Val): 0.4418; Reg Param: 0.0427; Time: 0.75\n",
      "Epoch 4954/8192 --- L(Train): 0.3718; L(Val): 0.4421; Reg Param: 0.0427; Time: 0.76\n",
      "Epoch 4955/8192 --- L(Train): 0.3739; L(Val): 0.4420; Reg Param: 0.0427; Time: 0.75\n",
      "Epoch 4956/8192 --- L(Train): 0.3698; L(Val): 0.4420; Reg Param: 0.0427; Time: 0.77\n",
      "Epoch 4957/8192 --- L(Train): 0.3663; L(Val): 0.4425; Reg Param: 0.0427; Time: 0.77\n",
      "Epoch 4958/8192 --- L(Train): 0.3843; L(Val): 0.4427; Reg Param: 0.0427; Time: 0.78\n",
      "Epoch 4959/8192 --- L(Train): 0.3864; L(Val): 0.4423; Reg Param: 0.0427; Time: 0.77\n",
      "Epoch 4960/8192 --- L(Train): 0.3681; L(Val): 0.4420; Reg Param: 0.0427; Time: 0.73\n",
      "Epoch 4961/8192 --- L(Train): 0.3821; L(Val): 0.4417; Reg Param: 0.0427; Time: 0.74\n",
      "Epoch 4962/8192 --- L(Train): 0.3842; L(Val): 0.4414; Reg Param: 0.0427; Time: 0.74\n",
      "Epoch 4963/8192 --- L(Train): 0.3673; L(Val): 0.4410; Reg Param: 0.0427; Time: 0.76\n",
      "Epoch 4964/8192 --- L(Train): 0.3820; L(Val): 0.4408; Reg Param: 0.0427; Time: 0.76\n",
      "Epoch 4965/8192 --- L(Train): 0.3736; L(Val): 0.4406; Reg Param: 0.0427; Time: 0.87\n",
      "Epoch 4966/8192 --- L(Train): 0.3864; L(Val): 0.4400; Reg Param: 0.0427; Time: 1.04\n",
      "Epoch 4967/8192 --- L(Train): 0.3785; L(Val): 0.4394; Reg Param: 0.0427; Time: 0.77\n",
      "Epoch 4968/8192 --- L(Train): 0.3745; L(Val): 0.4391; Reg Param: 0.0427; Time: 0.77\n",
      "Epoch 4969/8192 --- L(Train): 0.3748; L(Val): 0.4393; Reg Param: 0.0427; Time: 0.79\n",
      "Epoch 4970/8192 --- L(Train): 0.3787; L(Val): 0.4394; Reg Param: 0.0427; Time: 0.77\n",
      "Epoch 4971/8192 --- L(Train): 0.3786; L(Val): 0.4400; Reg Param: 0.0427; Time: 0.78\n",
      "Epoch 4972/8192 --- L(Train): 0.3709; L(Val): 0.4406; Reg Param: 0.0427; Time: 0.76\n",
      "Epoch 4973/8192 --- L(Train): 0.3764; L(Val): 0.4414; Reg Param: 0.0427; Time: 0.79\n",
      "Epoch 4974/8192 --- L(Train): 0.3842; L(Val): 0.4423; Reg Param: 0.0427; Time: 0.76\n",
      "Epoch 4975/8192 --- L(Train): 0.3765; L(Val): 0.4431; Reg Param: 0.0427; Time: 0.77\n",
      "Epoch 4976/8192 --- L(Train): 0.3824; L(Val): 0.4440; Reg Param: 0.0427; Time: 0.76\n",
      "Epoch 4977/8192 --- L(Train): 0.3747; L(Val): 0.4447; Reg Param: 0.0427; Time: 0.76\n",
      "Epoch 4978/8192 --- L(Train): 0.3742; L(Val): 0.4452; Reg Param: 0.0427; Time: 0.78\n",
      "Epoch 4979/8192 --- L(Train): 0.3906; L(Val): 0.4458; Reg Param: 0.0427; Time: 0.74\n",
      "Epoch 4980/8192 --- L(Train): 0.3660; L(Val): 0.4457; Reg Param: 0.0427; Time: 0.73\n",
      "Epoch 4981/8192 --- L(Train): 0.3730; L(Val): 0.4451; Reg Param: 0.0427; Time: 1.07\n",
      "Epoch 4982/8192 --- L(Train): 0.3727; L(Val): 0.4440; Reg Param: 0.0427; Time: 0.81\n",
      "Epoch 4983/8192 --- L(Train): 0.3819; L(Val): 0.4431; Reg Param: 0.0427; Time: 1.07\n",
      "Epoch 4984/8192 --- L(Train): 0.3588; L(Val): 0.4427; Reg Param: 0.0427; Time: 0.77\n",
      "Epoch 4985/8192 --- L(Train): 0.3749; L(Val): 0.4422; Reg Param: 0.0427; Time: 0.82\n",
      "Epoch 4986/8192 --- L(Train): 0.3793; L(Val): 0.4418; Reg Param: 0.0427; Time: 0.97\n",
      "Epoch 4987/8192 --- L(Train): 0.3734; L(Val): 0.4418; Reg Param: 0.0427; Time: 0.76\n",
      "Epoch 4988/8192 --- L(Train): 0.3717; L(Val): 0.4420; Reg Param: 0.0427; Time: 0.76\n",
      "Epoch 4989/8192 --- L(Train): 0.3805; L(Val): 0.4420; Reg Param: 0.0427; Time: 0.75\n",
      "Epoch 4990/8192 --- L(Train): 0.3692; L(Val): 0.4418; Reg Param: 0.0427; Time: 0.83\n",
      "Epoch 4991/8192 --- L(Train): 0.3875; L(Val): 0.4410; Reg Param: 0.0427; Time: 0.74\n",
      "Epoch 4992/8192 --- L(Train): 0.3720; L(Val): 0.4402; Reg Param: 0.0427; Time: 0.78\n",
      "Epoch 4993/8192 --- L(Train): 0.3748; L(Val): 0.4399; Reg Param: 0.0427; Time: 0.80\n",
      "Epoch 4994/8192 --- L(Train): 0.3695; L(Val): 0.4397; Reg Param: 0.0427; Time: 0.78\n",
      "Epoch 4995/8192 --- L(Train): 0.3733; L(Val): 0.4398; Reg Param: 0.0427; Time: 0.79\n",
      "Epoch 4996/8192 --- L(Train): 0.3861; L(Val): 0.4402; Reg Param: 0.0427; Time: 0.77\n",
      "Epoch 4997/8192 --- L(Train): 0.3754; L(Val): 0.4404; Reg Param: 0.0427; Time: 0.84\n",
      "Epoch 4998/8192 --- L(Train): 0.3726; L(Val): 0.4410; Reg Param: 0.0427; Time: 0.76\n",
      "Epoch 4999/8192 --- L(Train): 0.3740; L(Val): 0.4415; Reg Param: 0.0427; Time: 0.75\n",
      "Epoch 5000/8192 --- L(Train): 0.3760; L(Val): 0.4415; Reg Param: 0.0419; Time: 8.07\n",
      "Epoch 5001/8192 --- L(Train): 0.3909; L(Val): 0.4411; Reg Param: 0.0419; Time: 0.75\n",
      "Epoch 5002/8192 --- L(Train): 0.3636; L(Val): 0.4415; Reg Param: 0.0419; Time: 0.79\n",
      "Epoch 5003/8192 --- L(Train): 0.3714; L(Val): 0.4414; Reg Param: 0.0419; Time: 1.00\n",
      "Epoch 5004/8192 --- L(Train): 0.3838; L(Val): 0.4415; Reg Param: 0.0419; Time: 0.75\n",
      "Epoch 5005/8192 --- L(Train): 0.3753; L(Val): 0.4414; Reg Param: 0.0419; Time: 0.78\n",
      "Epoch 5006/8192 --- L(Train): 0.3743; L(Val): 0.4415; Reg Param: 0.0419; Time: 0.79\n",
      "Epoch 5007/8192 --- L(Train): 0.3825; L(Val): 0.4421; Reg Param: 0.0419; Time: 0.80\n",
      "Epoch 5008/8192 --- L(Train): 0.3847; L(Val): 0.4425; Reg Param: 0.0419; Time: 0.78\n",
      "Epoch 5009/8192 --- L(Train): 0.3772; L(Val): 0.4428; Reg Param: 0.0419; Time: 0.75\n",
      "Epoch 5010/8192 --- L(Train): 0.3846; L(Val): 0.4434; Reg Param: 0.0419; Time: 0.79\n",
      "Epoch 5011/8192 --- L(Train): 0.3607; L(Val): 0.4437; Reg Param: 0.0419; Time: 0.75\n",
      "Epoch 5012/8192 --- L(Train): 0.3745; L(Val): 0.4439; Reg Param: 0.0419; Time: 0.79\n",
      "Epoch 5013/8192 --- L(Train): 0.3749; L(Val): 0.4441; Reg Param: 0.0419; Time: 1.06\n",
      "Epoch 5014/8192 --- L(Train): 0.3710; L(Val): 0.4445; Reg Param: 0.0419; Time: 0.72\n",
      "Epoch 5015/8192 --- L(Train): 0.3670; L(Val): 0.4447; Reg Param: 0.0419; Time: 0.79\n",
      "Epoch 5016/8192 --- L(Train): 0.3808; L(Val): 0.4448; Reg Param: 0.0419; Time: 0.77\n",
      "Epoch 5017/8192 --- L(Train): 0.3670; L(Val): 0.4448; Reg Param: 0.0419; Time: 0.77\n",
      "Epoch 5018/8192 --- L(Train): 0.3683; L(Val): 0.4447; Reg Param: 0.0419; Time: 0.77\n",
      "Epoch 5019/8192 --- L(Train): 0.3642; L(Val): 0.4443; Reg Param: 0.0419; Time: 0.78\n",
      "Epoch 5020/8192 --- L(Train): 0.3695; L(Val): 0.4452; Reg Param: 0.0419; Time: 0.76\n",
      "Epoch 5021/8192 --- L(Train): 0.3781; L(Val): 0.4442; Reg Param: 0.0419; Time: 0.76\n",
      "Epoch 5022/8192 --- L(Train): 0.3690; L(Val): 0.4431; Reg Param: 0.0419; Time: 0.79\n",
      "Epoch 5023/8192 --- L(Train): 0.3704; L(Val): 0.4431; Reg Param: 0.0419; Time: 0.78\n",
      "Epoch 5024/8192 --- L(Train): 0.3765; L(Val): 0.4434; Reg Param: 0.0419; Time: 0.79\n",
      "Epoch 5025/8192 --- L(Train): 0.3736; L(Val): 0.4437; Reg Param: 0.0419; Time: 1.01\n",
      "Epoch 5026/8192 --- L(Train): 0.3728; L(Val): 0.4440; Reg Param: 0.0419; Time: 0.77\n",
      "Epoch 5027/8192 --- L(Train): 0.3697; L(Val): 0.4445; Reg Param: 0.0419; Time: 0.77\n",
      "Epoch 5028/8192 --- L(Train): 0.3698; L(Val): 0.4453; Reg Param: 0.0419; Time: 0.77\n",
      "Epoch 5029/8192 --- L(Train): 0.3758; L(Val): 0.4455; Reg Param: 0.0419; Time: 0.78\n",
      "Epoch 5030/8192 --- L(Train): 0.3656; L(Val): 0.4462; Reg Param: 0.0419; Time: 0.79\n",
      "Epoch 5031/8192 --- L(Train): 0.3822; L(Val): 0.4464; Reg Param: 0.0419; Time: 0.77\n",
      "Epoch 5032/8192 --- L(Train): 0.3689; L(Val): 0.4465; Reg Param: 0.0419; Time: 0.80\n",
      "Epoch 5033/8192 --- L(Train): 0.3698; L(Val): 0.4464; Reg Param: 0.0419; Time: 0.78\n",
      "Epoch 5034/8192 --- L(Train): 0.3613; L(Val): 0.4464; Reg Param: 0.0419; Time: 0.79\n",
      "Epoch 5035/8192 --- L(Train): 0.3676; L(Val): 0.4464; Reg Param: 0.0419; Time: 0.77\n",
      "Epoch 5036/8192 --- L(Train): 0.3783; L(Val): 0.4464; Reg Param: 0.0419; Time: 0.77\n",
      "Epoch 5037/8192 --- L(Train): 0.3766; L(Val): 0.4463; Reg Param: 0.0419; Time: 1.05\n",
      "Epoch 5038/8192 --- L(Train): 0.3783; L(Val): 0.4461; Reg Param: 0.0419; Time: 0.76\n",
      "Epoch 5039/8192 --- L(Train): 0.3854; L(Val): 0.4457; Reg Param: 0.0419; Time: 0.78\n",
      "Epoch 5040/8192 --- L(Train): 0.3744; L(Val): 0.4449; Reg Param: 0.0419; Time: 0.77\n",
      "Epoch 5041/8192 --- L(Train): 0.3752; L(Val): 0.4441; Reg Param: 0.0419; Time: 0.78\n",
      "Epoch 5042/8192 --- L(Train): 0.3812; L(Val): 0.4433; Reg Param: 0.0419; Time: 0.78\n",
      "Epoch 5043/8192 --- L(Train): 0.3697; L(Val): 0.4430; Reg Param: 0.0419; Time: 0.80\n",
      "Epoch 5044/8192 --- L(Train): 0.3645; L(Val): 0.4424; Reg Param: 0.0419; Time: 0.74\n",
      "Epoch 5045/8192 --- L(Train): 0.3780; L(Val): 0.4422; Reg Param: 0.0419; Time: 0.78\n",
      "Epoch 5046/8192 --- L(Train): 0.3724; L(Val): 0.4422; Reg Param: 0.0419; Time: 0.74\n",
      "Epoch 5047/8192 --- L(Train): 0.3814; L(Val): 0.4423; Reg Param: 0.0419; Time: 0.75\n",
      "Epoch 5048/8192 --- L(Train): 0.3616; L(Val): 0.4426; Reg Param: 0.0419; Time: 0.73\n",
      "Epoch 5049/8192 --- L(Train): 0.3730; L(Val): 0.4433; Reg Param: 0.0419; Time: 0.78\n",
      "Epoch 5050/8192 --- L(Train): 0.3828; L(Val): 0.4433; Reg Param: 0.0411; Time: 8.00\n",
      "Epoch 5051/8192 --- L(Train): 0.3753; L(Val): 0.4441; Reg Param: 0.0411; Time: 0.77\n",
      "Epoch 5052/8192 --- L(Train): 0.3708; L(Val): 0.4443; Reg Param: 0.0411; Time: 0.96\n",
      "Epoch 5053/8192 --- L(Train): 0.3635; L(Val): 0.4443; Reg Param: 0.0411; Time: 0.76\n",
      "Epoch 5054/8192 --- L(Train): 0.3682; L(Val): 0.4435; Reg Param: 0.0411; Time: 0.73\n",
      "Epoch 5055/8192 --- L(Train): 0.3813; L(Val): 0.4431; Reg Param: 0.0411; Time: 0.71\n",
      "Epoch 5056/8192 --- L(Train): 0.3822; L(Val): 0.4434; Reg Param: 0.0411; Time: 0.74\n",
      "Epoch 5057/8192 --- L(Train): 0.3728; L(Val): 0.4433; Reg Param: 0.0411; Time: 0.72\n",
      "Epoch 5058/8192 --- L(Train): 0.3782; L(Val): 0.4435; Reg Param: 0.0411; Time: 0.80\n",
      "Epoch 5059/8192 --- L(Train): 0.3816; L(Val): 0.4437; Reg Param: 0.0411; Time: 0.72\n",
      "Epoch 5060/8192 --- L(Train): 0.3578; L(Val): 0.4432; Reg Param: 0.0411; Time: 0.73\n",
      "Epoch 5061/8192 --- L(Train): 0.3869; L(Val): 0.4428; Reg Param: 0.0411; Time: 0.76\n",
      "Epoch 5062/8192 --- L(Train): 0.3646; L(Val): 0.4422; Reg Param: 0.0411; Time: 0.78\n",
      "Epoch 5063/8192 --- L(Train): 0.3660; L(Val): 0.4415; Reg Param: 0.0411; Time: 0.74\n",
      "Epoch 5064/8192 --- L(Train): 0.3674; L(Val): 0.4409; Reg Param: 0.0411; Time: 0.72\n",
      "Epoch 5065/8192 --- L(Train): 0.3897; L(Val): 0.4404; Reg Param: 0.0411; Time: 0.73\n",
      "Epoch 5066/8192 --- L(Train): 0.3761; L(Val): 0.4400; Reg Param: 0.0411; Time: 0.73\n",
      "Epoch 5067/8192 --- L(Train): 0.3678; L(Val): 0.4401; Reg Param: 0.0411; Time: 0.74\n",
      "Epoch 5068/8192 --- L(Train): 0.3679; L(Val): 0.4404; Reg Param: 0.0411; Time: 0.74\n",
      "Epoch 5069/8192 --- L(Train): 0.3756; L(Val): 0.4411; Reg Param: 0.0411; Time: 0.72\n",
      "Epoch 5070/8192 --- L(Train): 0.3685; L(Val): 0.4416; Reg Param: 0.0411; Time: 0.72\n",
      "Epoch 5071/8192 --- L(Train): 0.3748; L(Val): 0.4426; Reg Param: 0.0411; Time: 0.76\n",
      "Epoch 5072/8192 --- L(Train): 0.3768; L(Val): 0.4428; Reg Param: 0.0411; Time: 0.71\n",
      "Epoch 5073/8192 --- L(Train): 0.3769; L(Val): 0.4428; Reg Param: 0.0411; Time: 0.72\n",
      "Epoch 5074/8192 --- L(Train): 0.3693; L(Val): 0.4424; Reg Param: 0.0411; Time: 0.72\n",
      "Epoch 5075/8192 --- L(Train): 0.3752; L(Val): 0.4422; Reg Param: 0.0411; Time: 0.72\n",
      "Epoch 5076/8192 --- L(Train): 0.3910; L(Val): 0.4425; Reg Param: 0.0411; Time: 0.72\n",
      "Epoch 5077/8192 --- L(Train): 0.3762; L(Val): 0.4428; Reg Param: 0.0411; Time: 0.72\n",
      "Epoch 5078/8192 --- L(Train): 0.3764; L(Val): 0.4430; Reg Param: 0.0411; Time: 0.71\n",
      "Epoch 5079/8192 --- L(Train): 0.3658; L(Val): 0.4434; Reg Param: 0.0411; Time: 0.72\n",
      "Epoch 5080/8192 --- L(Train): 0.3756; L(Val): 0.4440; Reg Param: 0.0411; Time: 0.72\n",
      "Epoch 5081/8192 --- L(Train): 0.3775; L(Val): 0.4447; Reg Param: 0.0411; Time: 0.71\n",
      "Epoch 5082/8192 --- L(Train): 0.3661; L(Val): 0.4450; Reg Param: 0.0411; Time: 0.71\n",
      "Epoch 5083/8192 --- L(Train): 0.3698; L(Val): 0.4451; Reg Param: 0.0411; Time: 0.75\n",
      "Epoch 5084/8192 --- L(Train): 0.3725; L(Val): 0.4450; Reg Param: 0.0411; Time: 0.76\n",
      "Epoch 5085/8192 --- L(Train): 0.3875; L(Val): 0.4442; Reg Param: 0.0411; Time: 0.76\n",
      "Epoch 5086/8192 --- L(Train): 0.3552; L(Val): 0.4434; Reg Param: 0.0411; Time: 0.76\n",
      "Epoch 5087/8192 --- L(Train): 0.3705; L(Val): 0.4430; Reg Param: 0.0411; Time: 0.79\n",
      "Epoch 5088/8192 --- L(Train): 0.3833; L(Val): 0.4423; Reg Param: 0.0411; Time: 0.76\n",
      "Epoch 5089/8192 --- L(Train): 0.3698; L(Val): 0.4423; Reg Param: 0.0411; Time: 0.76\n",
      "Epoch 5090/8192 --- L(Train): 0.3633; L(Val): 0.4425; Reg Param: 0.0411; Time: 0.77\n",
      "Epoch 5091/8192 --- L(Train): 0.3715; L(Val): 0.4428; Reg Param: 0.0411; Time: 0.77\n",
      "Epoch 5092/8192 --- L(Train): 0.3760; L(Val): 0.4425; Reg Param: 0.0411; Time: 0.78\n",
      "Epoch 5093/8192 --- L(Train): 0.3743; L(Val): 0.4421; Reg Param: 0.0411; Time: 0.76\n",
      "Epoch 5094/8192 --- L(Train): 0.3840; L(Val): 0.4419; Reg Param: 0.0411; Time: 0.75\n",
      "Epoch 5095/8192 --- L(Train): 0.3742; L(Val): 0.4421; Reg Param: 0.0411; Time: 0.77\n",
      "Epoch 5096/8192 --- L(Train): 0.3776; L(Val): 0.4424; Reg Param: 0.0411; Time: 0.77\n",
      "Epoch 5097/8192 --- L(Train): 0.3799; L(Val): 0.4426; Reg Param: 0.0411; Time: 0.76\n",
      "Epoch 5098/8192 --- L(Train): 0.3829; L(Val): 0.4428; Reg Param: 0.0411; Time: 0.74\n",
      "Epoch 5099/8192 --- L(Train): 0.3599; L(Val): 0.4427; Reg Param: 0.0411; Time: 0.78\n",
      "Epoch 5100/8192 --- L(Train): 0.3767; L(Val): 0.4427; Reg Param: 0.0402; Time: 8.38\n",
      "Epoch 5101/8192 --- L(Train): 0.3694; L(Val): 0.4434; Reg Param: 0.0402; Time: 1.00\n",
      "Epoch 5102/8192 --- L(Train): 0.3722; L(Val): 0.4437; Reg Param: 0.0402; Time: 0.66\n",
      "Epoch 5103/8192 --- L(Train): 0.3737; L(Val): 0.4439; Reg Param: 0.0402; Time: 0.61\n",
      "Epoch 5104/8192 --- L(Train): 0.3770; L(Val): 0.4439; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5105/8192 --- L(Train): 0.3664; L(Val): 0.4442; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5106/8192 --- L(Train): 0.3804; L(Val): 0.4442; Reg Param: 0.0402; Time: 0.61\n",
      "Epoch 5107/8192 --- L(Train): 0.3751; L(Val): 0.4442; Reg Param: 0.0402; Time: 0.61\n",
      "Epoch 5108/8192 --- L(Train): 0.3765; L(Val): 0.4442; Reg Param: 0.0402; Time: 0.59\n",
      "Epoch 5109/8192 --- L(Train): 0.3692; L(Val): 0.4441; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5110/8192 --- L(Train): 0.3660; L(Val): 0.4443; Reg Param: 0.0402; Time: 0.61\n",
      "Epoch 5111/8192 --- L(Train): 0.3706; L(Val): 0.4448; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5112/8192 --- L(Train): 0.3734; L(Val): 0.4449; Reg Param: 0.0402; Time: 0.64\n",
      "Epoch 5113/8192 --- L(Train): 0.3808; L(Val): 0.4454; Reg Param: 0.0402; Time: 0.62\n",
      "Epoch 5114/8192 --- L(Train): 0.3767; L(Val): 0.4457; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5115/8192 --- L(Train): 0.3823; L(Val): 0.4456; Reg Param: 0.0402; Time: 0.61\n",
      "Epoch 5116/8192 --- L(Train): 0.3779; L(Val): 0.4459; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5117/8192 --- L(Train): 0.3711; L(Val): 0.4461; Reg Param: 0.0402; Time: 0.63\n",
      "Epoch 5118/8192 --- L(Train): 0.3783; L(Val): 0.4459; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5119/8192 --- L(Train): 0.3656; L(Val): 0.4454; Reg Param: 0.0402; Time: 0.59\n",
      "Epoch 5120/8192 --- L(Train): 0.3811; L(Val): 0.4445; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5121/8192 --- L(Train): 0.3657; L(Val): 0.4436; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5122/8192 --- L(Train): 0.3855; L(Val): 0.4430; Reg Param: 0.0402; Time: 0.67\n",
      "Epoch 5123/8192 --- L(Train): 0.3806; L(Val): 0.4431; Reg Param: 0.0402; Time: 0.61\n",
      "Epoch 5124/8192 --- L(Train): 0.3802; L(Val): 0.4431; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5125/8192 --- L(Train): 0.3857; L(Val): 0.4433; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5126/8192 --- L(Train): 0.3783; L(Val): 0.4440; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5127/8192 --- L(Train): 0.3671; L(Val): 0.4447; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5128/8192 --- L(Train): 0.3834; L(Val): 0.4458; Reg Param: 0.0402; Time: 0.59\n",
      "Epoch 5129/8192 --- L(Train): 0.3749; L(Val): 0.4467; Reg Param: 0.0402; Time: 0.59\n",
      "Epoch 5130/8192 --- L(Train): 0.3718; L(Val): 0.4447; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5131/8192 --- L(Train): 0.3852; L(Val): 0.4445; Reg Param: 0.0402; Time: 0.61\n",
      "Epoch 5132/8192 --- L(Train): 0.3638; L(Val): 0.4435; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5133/8192 --- L(Train): 0.3844; L(Val): 0.4424; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5134/8192 --- L(Train): 0.3709; L(Val): 0.4422; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5135/8192 --- L(Train): 0.3662; L(Val): 0.4424; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5136/8192 --- L(Train): 0.3780; L(Val): 0.4426; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5137/8192 --- L(Train): 0.3742; L(Val): 0.4440; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5138/8192 --- L(Train): 0.3855; L(Val): 0.4454; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5139/8192 --- L(Train): 0.3730; L(Val): 0.4449; Reg Param: 0.0402; Time: 0.61\n",
      "Epoch 5140/8192 --- L(Train): 0.3704; L(Val): 0.4446; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5141/8192 --- L(Train): 0.3745; L(Val): 0.4443; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5142/8192 --- L(Train): 0.3773; L(Val): 0.4430; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5143/8192 --- L(Train): 0.3786; L(Val): 0.4419; Reg Param: 0.0402; Time: 0.61\n",
      "Epoch 5144/8192 --- L(Train): 0.3696; L(Val): 0.4416; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5145/8192 --- L(Train): 0.3727; L(Val): 0.4409; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5146/8192 --- L(Train): 0.3830; L(Val): 0.4399; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5147/8192 --- L(Train): 0.3811; L(Val): 0.4390; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5148/8192 --- L(Train): 0.3844; L(Val): 0.4390; Reg Param: 0.0402; Time: 0.61\n",
      "Epoch 5149/8192 --- L(Train): 0.3776; L(Val): 0.4391; Reg Param: 0.0402; Time: 0.60\n",
      "Epoch 5150/8192 --- L(Train): 0.3891; L(Val): 0.4391; Reg Param: 0.0394; Time: 6.31\n",
      "Epoch 5151/8192 --- L(Train): 0.3703; L(Val): 0.4398; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5152/8192 --- L(Train): 0.3830; L(Val): 0.4404; Reg Param: 0.0394; Time: 0.61\n",
      "Epoch 5153/8192 --- L(Train): 0.3725; L(Val): 0.4407; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5154/8192 --- L(Train): 0.3755; L(Val): 0.4410; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5155/8192 --- L(Train): 0.3785; L(Val): 0.4410; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5156/8192 --- L(Train): 0.3629; L(Val): 0.4415; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5157/8192 --- L(Train): 0.3776; L(Val): 0.4420; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5158/8192 --- L(Train): 0.3788; L(Val): 0.4415; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5159/8192 --- L(Train): 0.3772; L(Val): 0.4414; Reg Param: 0.0394; Time: 0.61\n",
      "Epoch 5160/8192 --- L(Train): 0.3842; L(Val): 0.4411; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5161/8192 --- L(Train): 0.3683; L(Val): 0.4410; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5162/8192 --- L(Train): 0.3755; L(Val): 0.4412; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5163/8192 --- L(Train): 0.3678; L(Val): 0.4414; Reg Param: 0.0394; Time: 0.64\n",
      "Epoch 5164/8192 --- L(Train): 0.3757; L(Val): 0.4415; Reg Param: 0.0394; Time: 0.77\n",
      "Epoch 5165/8192 --- L(Train): 0.3719; L(Val): 0.4415; Reg Param: 0.0394; Time: 0.66\n",
      "Epoch 5166/8192 --- L(Train): 0.3830; L(Val): 0.4413; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5167/8192 --- L(Train): 0.3755; L(Val): 0.4408; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5168/8192 --- L(Train): 0.3752; L(Val): 0.4406; Reg Param: 0.0394; Time: 0.63\n",
      "Epoch 5169/8192 --- L(Train): 0.3731; L(Val): 0.4401; Reg Param: 0.0394; Time: 0.59\n",
      "Epoch 5170/8192 --- L(Train): 0.3721; L(Val): 0.4399; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5171/8192 --- L(Train): 0.3649; L(Val): 0.4399; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5172/8192 --- L(Train): 0.3711; L(Val): 0.4402; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5173/8192 --- L(Train): 0.3781; L(Val): 0.4411; Reg Param: 0.0394; Time: 0.61\n",
      "Epoch 5174/8192 --- L(Train): 0.3756; L(Val): 0.4422; Reg Param: 0.0394; Time: 0.61\n",
      "Epoch 5175/8192 --- L(Train): 0.3680; L(Val): 0.4427; Reg Param: 0.0394; Time: 0.71\n",
      "Epoch 5176/8192 --- L(Train): 0.3732; L(Val): 0.4430; Reg Param: 0.0394; Time: 0.62\n",
      "Epoch 5177/8192 --- L(Train): 0.3741; L(Val): 0.4436; Reg Param: 0.0394; Time: 0.61\n",
      "Epoch 5178/8192 --- L(Train): 0.3673; L(Val): 0.4435; Reg Param: 0.0394; Time: 0.61\n",
      "Epoch 5179/8192 --- L(Train): 0.3863; L(Val): 0.4435; Reg Param: 0.0394; Time: 0.61\n",
      "Epoch 5180/8192 --- L(Train): 0.3692; L(Val): 0.4433; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5181/8192 --- L(Train): 0.3697; L(Val): 0.4433; Reg Param: 0.0394; Time: 0.61\n",
      "Epoch 5182/8192 --- L(Train): 0.3804; L(Val): 0.4438; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5183/8192 --- L(Train): 0.3878; L(Val): 0.4443; Reg Param: 0.0394; Time: 0.61\n",
      "Epoch 5184/8192 --- L(Train): 0.3801; L(Val): 0.4445; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5185/8192 --- L(Train): 0.3854; L(Val): 0.4448; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5186/8192 --- L(Train): 0.3754; L(Val): 0.4445; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5187/8192 --- L(Train): 0.3769; L(Val): 0.4442; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5188/8192 --- L(Train): 0.3708; L(Val): 0.4441; Reg Param: 0.0394; Time: 0.61\n",
      "Epoch 5189/8192 --- L(Train): 0.3759; L(Val): 0.4441; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5190/8192 --- L(Train): 0.3768; L(Val): 0.4439; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5191/8192 --- L(Train): 0.3672; L(Val): 0.4441; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5192/8192 --- L(Train): 0.3650; L(Val): 0.4442; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5193/8192 --- L(Train): 0.3705; L(Val): 0.4441; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5194/8192 --- L(Train): 0.3773; L(Val): 0.4442; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5195/8192 --- L(Train): 0.3765; L(Val): 0.4443; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5196/8192 --- L(Train): 0.3764; L(Val): 0.4442; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5197/8192 --- L(Train): 0.3622; L(Val): 0.4442; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5198/8192 --- L(Train): 0.3732; L(Val): 0.4444; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5199/8192 --- L(Train): 0.3725; L(Val): 0.4440; Reg Param: 0.0394; Time: 0.60\n",
      "Epoch 5200/8192 --- L(Train): 0.3752; L(Val): 0.4440; Reg Param: 0.0385; Time: 6.26\n",
      "Epoch 5201/8192 --- L(Train): 0.3775; L(Val): 0.4431; Reg Param: 0.0385; Time: 0.59\n",
      "Epoch 5202/8192 --- L(Train): 0.3753; L(Val): 0.4426; Reg Param: 0.0385; Time: 0.64\n",
      "Epoch 5203/8192 --- L(Train): 0.3776; L(Val): 0.4420; Reg Param: 0.0385; Time: 0.62\n",
      "Epoch 5204/8192 --- L(Train): 0.3779; L(Val): 0.4415; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5205/8192 --- L(Train): 0.3772; L(Val): 0.4417; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5206/8192 --- L(Train): 0.3672; L(Val): 0.4414; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5207/8192 --- L(Train): 0.3635; L(Val): 0.4417; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5208/8192 --- L(Train): 0.3708; L(Val): 0.4416; Reg Param: 0.0385; Time: 0.59\n",
      "Epoch 5209/8192 --- L(Train): 0.3854; L(Val): 0.4418; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5210/8192 --- L(Train): 0.3743; L(Val): 0.4417; Reg Param: 0.0385; Time: 0.59\n",
      "Epoch 5211/8192 --- L(Train): 0.3791; L(Val): 0.4422; Reg Param: 0.0385; Time: 0.59\n",
      "Epoch 5212/8192 --- L(Train): 0.3662; L(Val): 0.4423; Reg Param: 0.0385; Time: 0.59\n",
      "Epoch 5213/8192 --- L(Train): 0.3879; L(Val): 0.4426; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5214/8192 --- L(Train): 0.3835; L(Val): 0.4433; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5215/8192 --- L(Train): 0.3706; L(Val): 0.4437; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5216/8192 --- L(Train): 0.3760; L(Val): 0.4437; Reg Param: 0.0385; Time: 0.59\n",
      "Epoch 5217/8192 --- L(Train): 0.3727; L(Val): 0.4436; Reg Param: 0.0385; Time: 0.62\n",
      "Epoch 5218/8192 --- L(Train): 0.3667; L(Val): 0.4435; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5219/8192 --- L(Train): 0.3751; L(Val): 0.4430; Reg Param: 0.0385; Time: 0.61\n",
      "Epoch 5220/8192 --- L(Train): 0.3791; L(Val): 0.4424; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5221/8192 --- L(Train): 0.3701; L(Val): 0.4419; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5222/8192 --- L(Train): 0.3784; L(Val): 0.4416; Reg Param: 0.0385; Time: 0.64\n",
      "Epoch 5223/8192 --- L(Train): 0.3853; L(Val): 0.4416; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5224/8192 --- L(Train): 0.3798; L(Val): 0.4418; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5225/8192 --- L(Train): 0.3651; L(Val): 0.4421; Reg Param: 0.0385; Time: 0.64\n",
      "Epoch 5226/8192 --- L(Train): 0.3839; L(Val): 0.4422; Reg Param: 0.0385; Time: 0.66\n",
      "Epoch 5227/8192 --- L(Train): 0.3749; L(Val): 0.4419; Reg Param: 0.0385; Time: 0.63\n",
      "Epoch 5228/8192 --- L(Train): 0.3719; L(Val): 0.4416; Reg Param: 0.0385; Time: 0.64\n",
      "Epoch 5229/8192 --- L(Train): 0.3650; L(Val): 0.4416; Reg Param: 0.0385; Time: 0.61\n",
      "Epoch 5230/8192 --- L(Train): 0.3673; L(Val): 0.4417; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5231/8192 --- L(Train): 0.3656; L(Val): 0.4420; Reg Param: 0.0385; Time: 0.61\n",
      "Epoch 5232/8192 --- L(Train): 0.3637; L(Val): 0.4418; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5233/8192 --- L(Train): 0.3640; L(Val): 0.4413; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5234/8192 --- L(Train): 0.3736; L(Val): 0.4410; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5235/8192 --- L(Train): 0.3712; L(Val): 0.4406; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5236/8192 --- L(Train): 0.3782; L(Val): 0.4404; Reg Param: 0.0385; Time: 0.61\n",
      "Epoch 5237/8192 --- L(Train): 0.3810; L(Val): 0.4407; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5238/8192 --- L(Train): 0.3695; L(Val): 0.4409; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5239/8192 --- L(Train): 0.3789; L(Val): 0.4410; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5240/8192 --- L(Train): 0.3813; L(Val): 0.4412; Reg Param: 0.0385; Time: 0.59\n",
      "Epoch 5241/8192 --- L(Train): 0.3723; L(Val): 0.4415; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5242/8192 --- L(Train): 0.3824; L(Val): 0.4414; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5243/8192 --- L(Train): 0.3717; L(Val): 0.4414; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5244/8192 --- L(Train): 0.3708; L(Val): 0.4415; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5245/8192 --- L(Train): 0.3733; L(Val): 0.4419; Reg Param: 0.0385; Time: 0.59\n",
      "Epoch 5246/8192 --- L(Train): 0.3759; L(Val): 0.4423; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5247/8192 --- L(Train): 0.3726; L(Val): 0.4426; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5248/8192 --- L(Train): 0.3714; L(Val): 0.4428; Reg Param: 0.0385; Time: 0.59\n",
      "Epoch 5249/8192 --- L(Train): 0.3772; L(Val): 0.4424; Reg Param: 0.0385; Time: 0.60\n",
      "Epoch 5250/8192 --- L(Train): 0.3824; L(Val): 0.4424; Reg Param: 0.0376; Time: 6.36\n",
      "Epoch 5251/8192 --- L(Train): 0.3872; L(Val): 0.4422; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5252/8192 --- L(Train): 0.3764; L(Val): 0.4419; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5253/8192 --- L(Train): 0.3766; L(Val): 0.4417; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5254/8192 --- L(Train): 0.3773; L(Val): 0.4415; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5255/8192 --- L(Train): 0.3672; L(Val): 0.4414; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5256/8192 --- L(Train): 0.3758; L(Val): 0.4413; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5257/8192 --- L(Train): 0.3728; L(Val): 0.4411; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5258/8192 --- L(Train): 0.3846; L(Val): 0.4412; Reg Param: 0.0376; Time: 0.62\n",
      "Epoch 5259/8192 --- L(Train): 0.3618; L(Val): 0.4412; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5260/8192 --- L(Train): 0.3732; L(Val): 0.4410; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5261/8192 --- L(Train): 0.3552; L(Val): 0.4406; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5262/8192 --- L(Train): 0.3852; L(Val): 0.4402; Reg Param: 0.0376; Time: 0.63\n",
      "Epoch 5263/8192 --- L(Train): 0.3757; L(Val): 0.4401; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5264/8192 --- L(Train): 0.3760; L(Val): 0.4404; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5265/8192 --- L(Train): 0.3762; L(Val): 0.4406; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5266/8192 --- L(Train): 0.3742; L(Val): 0.4406; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5267/8192 --- L(Train): 0.3687; L(Val): 0.4412; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5268/8192 --- L(Train): 0.3769; L(Val): 0.4415; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5269/8192 --- L(Train): 0.3918; L(Val): 0.4415; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5270/8192 --- L(Train): 0.3661; L(Val): 0.4416; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5271/8192 --- L(Train): 0.3858; L(Val): 0.4420; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5272/8192 --- L(Train): 0.3822; L(Val): 0.4420; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5273/8192 --- L(Train): 0.3738; L(Val): 0.4423; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5274/8192 --- L(Train): 0.3692; L(Val): 0.4426; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5275/8192 --- L(Train): 0.3714; L(Val): 0.4428; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5276/8192 --- L(Train): 0.3791; L(Val): 0.4424; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5277/8192 --- L(Train): 0.3599; L(Val): 0.4419; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5278/8192 --- L(Train): 0.3587; L(Val): 0.4412; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5279/8192 --- L(Train): 0.3741; L(Val): 0.4409; Reg Param: 0.0376; Time: 0.63\n",
      "Epoch 5280/8192 --- L(Train): 0.3674; L(Val): 0.4409; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5281/8192 --- L(Train): 0.3661; L(Val): 0.4410; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5282/8192 --- L(Train): 0.3663; L(Val): 0.4409; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5283/8192 --- L(Train): 0.3827; L(Val): 0.4409; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5284/8192 --- L(Train): 0.3784; L(Val): 0.4412; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5285/8192 --- L(Train): 0.3577; L(Val): 0.4416; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5286/8192 --- L(Train): 0.3757; L(Val): 0.4419; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5287/8192 --- L(Train): 0.3876; L(Val): 0.4428; Reg Param: 0.0376; Time: 0.62\n",
      "Epoch 5288/8192 --- L(Train): 0.3791; L(Val): 0.4448; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5289/8192 --- L(Train): 0.3882; L(Val): 0.4464; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5290/8192 --- L(Train): 0.3897; L(Val): 0.4457; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5291/8192 --- L(Train): 0.3824; L(Val): 0.4449; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5292/8192 --- L(Train): 0.3856; L(Val): 0.4451; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5293/8192 --- L(Train): 0.3811; L(Val): 0.4449; Reg Param: 0.0376; Time: 0.66\n",
      "Epoch 5294/8192 --- L(Train): 0.3663; L(Val): 0.4440; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5295/8192 --- L(Train): 0.3713; L(Val): 0.4431; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5296/8192 --- L(Train): 0.3822; L(Val): 0.4427; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5297/8192 --- L(Train): 0.3765; L(Val): 0.4420; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5298/8192 --- L(Train): 0.3733; L(Val): 0.4422; Reg Param: 0.0376; Time: 0.60\n",
      "Epoch 5299/8192 --- L(Train): 0.3765; L(Val): 0.4423; Reg Param: 0.0376; Time: 0.61\n",
      "Epoch 5300/8192 --- L(Train): 0.3836; L(Val): 0.4423; Reg Param: 0.0364; Time: 6.28\n",
      "Epoch 5301/8192 --- L(Train): 0.3840; L(Val): 0.4412; Reg Param: 0.0364; Time: 0.59\n",
      "Epoch 5302/8192 --- L(Train): 0.3750; L(Val): 0.4407; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5303/8192 --- L(Train): 0.3790; L(Val): 0.4398; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5304/8192 --- L(Train): 0.3787; L(Val): 0.4390; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5305/8192 --- L(Train): 0.3715; L(Val): 0.4383; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5306/8192 --- L(Train): 0.3847; L(Val): 0.4376; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5307/8192 --- L(Train): 0.3891; L(Val): 0.4368; Reg Param: 0.0364; Time: 0.61\n",
      "Epoch 5308/8192 --- L(Train): 0.3746; L(Val): 0.4368; Reg Param: 0.0364; Time: 0.61\n",
      "Epoch 5309/8192 --- L(Train): 0.3777; L(Val): 0.4367; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5310/8192 --- L(Train): 0.3697; L(Val): 0.4371; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5311/8192 --- L(Train): 0.3790; L(Val): 0.4375; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5312/8192 --- L(Train): 0.3887; L(Val): 0.4385; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5313/8192 --- L(Train): 0.3796; L(Val): 0.4402; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5314/8192 --- L(Train): 0.3779; L(Val): 0.4413; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5315/8192 --- L(Train): 0.3808; L(Val): 0.4418; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5316/8192 --- L(Train): 0.3691; L(Val): 0.4421; Reg Param: 0.0364; Time: 0.63\n",
      "Epoch 5317/8192 --- L(Train): 0.3829; L(Val): 0.4412; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5318/8192 --- L(Train): 0.3890; L(Val): 0.4405; Reg Param: 0.0364; Time: 0.59\n",
      "Epoch 5319/8192 --- L(Train): 0.3692; L(Val): 0.4396; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5320/8192 --- L(Train): 0.3787; L(Val): 0.4387; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5321/8192 --- L(Train): 0.3679; L(Val): 0.4372; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5322/8192 --- L(Train): 0.3810; L(Val): 0.4364; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5323/8192 --- L(Train): 0.3850; L(Val): 0.4365; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5324/8192 --- L(Train): 0.3759; L(Val): 0.4371; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5325/8192 --- L(Train): 0.3883; L(Val): 0.4372; Reg Param: 0.0364; Time: 0.70\n",
      "Epoch 5326/8192 --- L(Train): 0.3733; L(Val): 0.4376; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5327/8192 --- L(Train): 0.3815; L(Val): 0.4378; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5328/8192 --- L(Train): 0.3779; L(Val): 0.4375; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5329/8192 --- L(Train): 0.3717; L(Val): 0.4374; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5330/8192 --- L(Train): 0.3740; L(Val): 0.4373; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5331/8192 --- L(Train): 0.3768; L(Val): 0.4368; Reg Param: 0.0364; Time: 0.61\n",
      "Epoch 5332/8192 --- L(Train): 0.3717; L(Val): 0.4363; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5333/8192 --- L(Train): 0.3704; L(Val): 0.4362; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5334/8192 --- L(Train): 0.3780; L(Val): 0.4365; Reg Param: 0.0364; Time: 0.61\n",
      "Epoch 5335/8192 --- L(Train): 0.3784; L(Val): 0.4368; Reg Param: 0.0364; Time: 0.61\n",
      "Epoch 5336/8192 --- L(Train): 0.3811; L(Val): 0.4370; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5337/8192 --- L(Train): 0.3791; L(Val): 0.4366; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5338/8192 --- L(Train): 0.3751; L(Val): 0.4365; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5339/8192 --- L(Train): 0.3793; L(Val): 0.4366; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5340/8192 --- L(Train): 0.3734; L(Val): 0.4367; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5341/8192 --- L(Train): 0.3732; L(Val): 0.4354; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5342/8192 --- L(Train): 0.3704; L(Val): 0.4347; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5343/8192 --- L(Train): 0.3668; L(Val): 0.4342; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5344/8192 --- L(Train): 0.3800; L(Val): 0.4341; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5345/8192 --- L(Train): 0.3730; L(Val): 0.4347; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5346/8192 --- L(Train): 0.3797; L(Val): 0.4350; Reg Param: 0.0364; Time: 0.61\n",
      "Epoch 5347/8192 --- L(Train): 0.3786; L(Val): 0.4358; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5348/8192 --- L(Train): 0.3820; L(Val): 0.4372; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5349/8192 --- L(Train): 0.3658; L(Val): 0.4385; Reg Param: 0.0364; Time: 0.60\n",
      "Epoch 5350/8192 --- L(Train): 0.3744; L(Val): 0.4385; Reg Param: 0.0353; Time: 6.44\n",
      "Epoch 5351/8192 --- L(Train): 0.3805; L(Val): 0.4409; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5352/8192 --- L(Train): 0.3686; L(Val): 0.4414; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5353/8192 --- L(Train): 0.3720; L(Val): 0.4412; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5354/8192 --- L(Train): 0.3734; L(Val): 0.4409; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5355/8192 --- L(Train): 0.3826; L(Val): 0.4400; Reg Param: 0.0353; Time: 0.61\n",
      "Epoch 5356/8192 --- L(Train): 0.3692; L(Val): 0.4392; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5357/8192 --- L(Train): 0.3819; L(Val): 0.4384; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5358/8192 --- L(Train): 0.3741; L(Val): 0.4378; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5359/8192 --- L(Train): 0.3828; L(Val): 0.4371; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5360/8192 --- L(Train): 0.3769; L(Val): 0.4365; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5361/8192 --- L(Train): 0.3803; L(Val): 0.4362; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5362/8192 --- L(Train): 0.3930; L(Val): 0.4364; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5363/8192 --- L(Train): 0.3650; L(Val): 0.4370; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5364/8192 --- L(Train): 0.3692; L(Val): 0.4379; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5365/8192 --- L(Train): 0.3793; L(Val): 0.4390; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5366/8192 --- L(Train): 0.3883; L(Val): 0.4398; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5367/8192 --- L(Train): 0.3783; L(Val): 0.4401; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5368/8192 --- L(Train): 0.3780; L(Val): 0.4404; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5369/8192 --- L(Train): 0.3781; L(Val): 0.4400; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5370/8192 --- L(Train): 0.3727; L(Val): 0.4402; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5371/8192 --- L(Train): 0.3775; L(Val): 0.4398; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5372/8192 --- L(Train): 0.3610; L(Val): 0.4396; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5373/8192 --- L(Train): 0.3594; L(Val): 0.4395; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5374/8192 --- L(Train): 0.3747; L(Val): 0.4395; Reg Param: 0.0353; Time: 0.65\n",
      "Epoch 5375/8192 --- L(Train): 0.3629; L(Val): 0.4396; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5376/8192 --- L(Train): 0.3803; L(Val): 0.4402; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5377/8192 --- L(Train): 0.3748; L(Val): 0.4406; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5378/8192 --- L(Train): 0.3751; L(Val): 0.4407; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5379/8192 --- L(Train): 0.3907; L(Val): 0.4413; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5380/8192 --- L(Train): 0.3842; L(Val): 0.4420; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5381/8192 --- L(Train): 0.3701; L(Val): 0.4423; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5382/8192 --- L(Train): 0.3918; L(Val): 0.4424; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5383/8192 --- L(Train): 0.3842; L(Val): 0.4424; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5384/8192 --- L(Train): 0.3627; L(Val): 0.4420; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5385/8192 --- L(Train): 0.3820; L(Val): 0.4412; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5386/8192 --- L(Train): 0.3832; L(Val): 0.4405; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5387/8192 --- L(Train): 0.3613; L(Val): 0.4402; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5388/8192 --- L(Train): 0.3739; L(Val): 0.4401; Reg Param: 0.0353; Time: 0.61\n",
      "Epoch 5389/8192 --- L(Train): 0.3685; L(Val): 0.4399; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5390/8192 --- L(Train): 0.3740; L(Val): 0.4395; Reg Param: 0.0353; Time: 0.61\n",
      "Epoch 5391/8192 --- L(Train): 0.3751; L(Val): 0.4393; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5392/8192 --- L(Train): 0.3815; L(Val): 0.4388; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5393/8192 --- L(Train): 0.3776; L(Val): 0.4382; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5394/8192 --- L(Train): 0.3872; L(Val): 0.4377; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5395/8192 --- L(Train): 0.3667; L(Val): 0.4371; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5396/8192 --- L(Train): 0.3724; L(Val): 0.4370; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5397/8192 --- L(Train): 0.3732; L(Val): 0.4374; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5398/8192 --- L(Train): 0.3600; L(Val): 0.4374; Reg Param: 0.0353; Time: 0.59\n",
      "Epoch 5399/8192 --- L(Train): 0.3726; L(Val): 0.4379; Reg Param: 0.0353; Time: 0.60\n",
      "Epoch 5400/8192 --- L(Train): 0.3856; L(Val): 0.4379; Reg Param: 0.0339; Time: 6.38\n",
      "Epoch 5401/8192 --- L(Train): 0.3695; L(Val): 0.4384; Reg Param: 0.0339; Time: 0.59\n",
      "Epoch 5402/8192 --- L(Train): 0.3762; L(Val): 0.4391; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5403/8192 --- L(Train): 0.3689; L(Val): 0.4396; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5404/8192 --- L(Train): 0.3751; L(Val): 0.4400; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5405/8192 --- L(Train): 0.3704; L(Val): 0.4401; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5406/8192 --- L(Train): 0.3702; L(Val): 0.4398; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5407/8192 --- L(Train): 0.3679; L(Val): 0.4397; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5408/8192 --- L(Train): 0.3733; L(Val): 0.4398; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5409/8192 --- L(Train): 0.3847; L(Val): 0.4396; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5410/8192 --- L(Train): 0.3881; L(Val): 0.4401; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5411/8192 --- L(Train): 0.3764; L(Val): 0.4393; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5412/8192 --- L(Train): 0.3801; L(Val): 0.4392; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5413/8192 --- L(Train): 0.3762; L(Val): 0.4394; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5414/8192 --- L(Train): 0.3865; L(Val): 0.4400; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5415/8192 --- L(Train): 0.3752; L(Val): 0.4402; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5416/8192 --- L(Train): 0.3756; L(Val): 0.4403; Reg Param: 0.0339; Time: 0.61\n",
      "Epoch 5417/8192 --- L(Train): 0.3754; L(Val): 0.4405; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5418/8192 --- L(Train): 0.3798; L(Val): 0.4401; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5419/8192 --- L(Train): 0.3684; L(Val): 0.4399; Reg Param: 0.0339; Time: 0.64\n",
      "Epoch 5420/8192 --- L(Train): 0.3812; L(Val): 0.4395; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5421/8192 --- L(Train): 0.3928; L(Val): 0.4395; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5422/8192 --- L(Train): 0.3658; L(Val): 0.4397; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5423/8192 --- L(Train): 0.3825; L(Val): 0.4400; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5424/8192 --- L(Train): 0.3752; L(Val): 0.4404; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5425/8192 --- L(Train): 0.3752; L(Val): 0.4409; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5426/8192 --- L(Train): 0.3650; L(Val): 0.4412; Reg Param: 0.0339; Time: 0.61\n",
      "Epoch 5427/8192 --- L(Train): 0.3740; L(Val): 0.4410; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5428/8192 --- L(Train): 0.3658; L(Val): 0.4409; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5429/8192 --- L(Train): 0.3582; L(Val): 0.4406; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5430/8192 --- L(Train): 0.3680; L(Val): 0.4402; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5431/8192 --- L(Train): 0.3701; L(Val): 0.4400; Reg Param: 0.0339; Time: 0.61\n",
      "Epoch 5432/8192 --- L(Train): 0.3798; L(Val): 0.4402; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5433/8192 --- L(Train): 0.3811; L(Val): 0.4399; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5434/8192 --- L(Train): 0.3649; L(Val): 0.4398; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5435/8192 --- L(Train): 0.3789; L(Val): 0.4406; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5436/8192 --- L(Train): 0.3713; L(Val): 0.4415; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5437/8192 --- L(Train): 0.3790; L(Val): 0.4416; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5438/8192 --- L(Train): 0.3671; L(Val): 0.4417; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5439/8192 --- L(Train): 0.3869; L(Val): 0.4414; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5440/8192 --- L(Train): 0.3735; L(Val): 0.4402; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5441/8192 --- L(Train): 0.3734; L(Val): 0.4396; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5442/8192 --- L(Train): 0.3763; L(Val): 0.4389; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5443/8192 --- L(Train): 0.3847; L(Val): 0.4386; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5444/8192 --- L(Train): 0.3785; L(Val): 0.4384; Reg Param: 0.0339; Time: 0.60\n",
      "Epoch 5445/8192 --- L(Train): 0.3773; L(Val): 0.4381; Reg Param: 0.0339; Time: 0.67\n",
      "Epoch 5446/8192 --- L(Train): 0.3691; L(Val): 0.4378; Reg Param: 0.0339; Time: 0.74\n",
      "Epoch 5447/8192 --- L(Train): 0.3690; L(Val): 0.4379; Reg Param: 0.0339; Time: 0.64\n",
      "Epoch 5448/8192 --- L(Train): 0.3855; L(Val): 0.4384; Reg Param: 0.0339; Time: 0.62\n",
      "Epoch 5449/8192 --- L(Train): 0.3646; L(Val): 0.4390; Reg Param: 0.0339; Time: 0.63\n",
      "Epoch 5450/8192 --- L(Train): 0.3774; L(Val): 0.4390; Reg Param: 0.0327; Time: 6.30\n",
      "Epoch 5451/8192 --- L(Train): 0.3766; L(Val): 0.4398; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5452/8192 --- L(Train): 0.3837; L(Val): 0.4401; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5453/8192 --- L(Train): 0.3844; L(Val): 0.4394; Reg Param: 0.0327; Time: 0.61\n",
      "Epoch 5454/8192 --- L(Train): 0.3807; L(Val): 0.4386; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5455/8192 --- L(Train): 0.3874; L(Val): 0.4380; Reg Param: 0.0327; Time: 0.66\n",
      "Epoch 5456/8192 --- L(Train): 0.3699; L(Val): 0.4375; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5457/8192 --- L(Train): 0.3815; L(Val): 0.4370; Reg Param: 0.0327; Time: 0.59\n",
      "Epoch 5458/8192 --- L(Train): 0.3800; L(Val): 0.4367; Reg Param: 0.0327; Time: 0.61\n",
      "Epoch 5459/8192 --- L(Train): 0.3694; L(Val): 0.4366; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5460/8192 --- L(Train): 0.3818; L(Val): 0.4367; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5461/8192 --- L(Train): 0.3939; L(Val): 0.4369; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5462/8192 --- L(Train): 0.3739; L(Val): 0.4377; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5463/8192 --- L(Train): 0.3726; L(Val): 0.4390; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5464/8192 --- L(Train): 0.3833; L(Val): 0.4400; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5465/8192 --- L(Train): 0.3766; L(Val): 0.4410; Reg Param: 0.0327; Time: 0.70\n",
      "Epoch 5466/8192 --- L(Train): 0.3685; L(Val): 0.4423; Reg Param: 0.0327; Time: 0.71\n",
      "Epoch 5467/8192 --- L(Train): 0.3662; L(Val): 0.4431; Reg Param: 0.0327; Time: 0.66\n",
      "Epoch 5468/8192 --- L(Train): 0.3739; L(Val): 0.4430; Reg Param: 0.0327; Time: 0.62\n",
      "Epoch 5469/8192 --- L(Train): 0.3665; L(Val): 0.4429; Reg Param: 0.0327; Time: 0.62\n",
      "Epoch 5470/8192 --- L(Train): 0.3669; L(Val): 0.4426; Reg Param: 0.0327; Time: 0.64\n",
      "Epoch 5471/8192 --- L(Train): 0.3780; L(Val): 0.4418; Reg Param: 0.0327; Time: 0.63\n",
      "Epoch 5472/8192 --- L(Train): 0.3784; L(Val): 0.4413; Reg Param: 0.0327; Time: 0.61\n",
      "Epoch 5473/8192 --- L(Train): 0.3775; L(Val): 0.4406; Reg Param: 0.0327; Time: 0.62\n",
      "Epoch 5474/8192 --- L(Train): 0.3778; L(Val): 0.4410; Reg Param: 0.0327; Time: 0.61\n",
      "Epoch 5475/8192 --- L(Train): 0.3746; L(Val): 0.4417; Reg Param: 0.0327; Time: 0.72\n",
      "Epoch 5476/8192 --- L(Train): 0.3809; L(Val): 0.4421; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5477/8192 --- L(Train): 0.3728; L(Val): 0.4426; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5478/8192 --- L(Train): 0.3761; L(Val): 0.4429; Reg Param: 0.0327; Time: 0.65\n",
      "Epoch 5479/8192 --- L(Train): 0.3720; L(Val): 0.4428; Reg Param: 0.0327; Time: 0.69\n",
      "Epoch 5480/8192 --- L(Train): 0.3685; L(Val): 0.4427; Reg Param: 0.0327; Time: 0.66\n",
      "Epoch 5481/8192 --- L(Train): 0.3741; L(Val): 0.4428; Reg Param: 0.0327; Time: 0.67\n",
      "Epoch 5482/8192 --- L(Train): 0.3784; L(Val): 0.4430; Reg Param: 0.0327; Time: 0.72\n",
      "Epoch 5483/8192 --- L(Train): 0.3775; L(Val): 0.4435; Reg Param: 0.0327; Time: 0.72\n",
      "Epoch 5484/8192 --- L(Train): 0.3748; L(Val): 0.4441; Reg Param: 0.0327; Time: 1.16\n",
      "Epoch 5485/8192 --- L(Train): 0.3821; L(Val): 0.4446; Reg Param: 0.0327; Time: 1.67\n",
      "Epoch 5486/8192 --- L(Train): 0.3753; L(Val): 0.4447; Reg Param: 0.0327; Time: 1.15\n",
      "Epoch 5487/8192 --- L(Train): 0.3696; L(Val): 0.4450; Reg Param: 0.0327; Time: 0.93\n",
      "Epoch 5488/8192 --- L(Train): 0.3737; L(Val): 0.4455; Reg Param: 0.0327; Time: 0.99\n",
      "Epoch 5489/8192 --- L(Train): 0.3852; L(Val): 0.4452; Reg Param: 0.0327; Time: 1.03\n",
      "Epoch 5490/8192 --- L(Train): 0.3780; L(Val): 0.4450; Reg Param: 0.0327; Time: 0.75\n",
      "Epoch 5491/8192 --- L(Train): 0.3808; L(Val): 0.4447; Reg Param: 0.0327; Time: 0.72\n",
      "Epoch 5492/8192 --- L(Train): 0.3876; L(Val): 0.4442; Reg Param: 0.0327; Time: 0.77\n",
      "Epoch 5493/8192 --- L(Train): 0.3789; L(Val): 0.4439; Reg Param: 0.0327; Time: 0.71\n",
      "Epoch 5494/8192 --- L(Train): 0.3696; L(Val): 0.4439; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5495/8192 --- L(Train): 0.3688; L(Val): 0.4436; Reg Param: 0.0327; Time: 0.61\n",
      "Epoch 5496/8192 --- L(Train): 0.3811; L(Val): 0.4430; Reg Param: 0.0327; Time: 0.61\n",
      "Epoch 5497/8192 --- L(Train): 0.3853; L(Val): 0.4424; Reg Param: 0.0327; Time: 0.61\n",
      "Epoch 5498/8192 --- L(Train): 0.3690; L(Val): 0.4419; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5499/8192 --- L(Train): 0.3862; L(Val): 0.4415; Reg Param: 0.0327; Time: 0.60\n",
      "Epoch 5500/8192 --- L(Train): 0.3618; L(Val): 0.4415; Reg Param: 0.0316; Time: 6.37\n",
      "Epoch 5501/8192 --- L(Train): 0.3714; L(Val): 0.4404; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5502/8192 --- L(Train): 0.3633; L(Val): 0.4402; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5503/8192 --- L(Train): 0.3705; L(Val): 0.4405; Reg Param: 0.0316; Time: 0.61\n",
      "Epoch 5504/8192 --- L(Train): 0.3730; L(Val): 0.4408; Reg Param: 0.0316; Time: 0.61\n",
      "Epoch 5505/8192 --- L(Train): 0.3750; L(Val): 0.4412; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5506/8192 --- L(Train): 0.3850; L(Val): 0.4417; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5507/8192 --- L(Train): 0.3778; L(Val): 0.4420; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5508/8192 --- L(Train): 0.3752; L(Val): 0.4422; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5509/8192 --- L(Train): 0.3801; L(Val): 0.4423; Reg Param: 0.0316; Time: 0.61\n",
      "Epoch 5510/8192 --- L(Train): 0.3746; L(Val): 0.4424; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5511/8192 --- L(Train): 0.3698; L(Val): 0.4422; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5512/8192 --- L(Train): 0.3728; L(Val): 0.4418; Reg Param: 0.0316; Time: 0.61\n",
      "Epoch 5513/8192 --- L(Train): 0.3780; L(Val): 0.4411; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5514/8192 --- L(Train): 0.3898; L(Val): 0.4394; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5515/8192 --- L(Train): 0.3802; L(Val): 0.4378; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5516/8192 --- L(Train): 0.3667; L(Val): 0.4368; Reg Param: 0.0316; Time: 0.72\n",
      "Epoch 5517/8192 --- L(Train): 0.3796; L(Val): 0.4362; Reg Param: 0.0316; Time: 0.62\n",
      "Epoch 5518/8192 --- L(Train): 0.3784; L(Val): 0.4361; Reg Param: 0.0316; Time: 0.65\n",
      "Epoch 5519/8192 --- L(Train): 0.3801; L(Val): 0.4366; Reg Param: 0.0316; Time: 0.78\n",
      "Epoch 5520/8192 --- L(Train): 0.3791; L(Val): 0.4370; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5521/8192 --- L(Train): 0.3700; L(Val): 0.4377; Reg Param: 0.0316; Time: 0.61\n",
      "Epoch 5522/8192 --- L(Train): 0.3701; L(Val): 0.4389; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5523/8192 --- L(Train): 0.3760; L(Val): 0.4398; Reg Param: 0.0316; Time: 0.65\n",
      "Epoch 5524/8192 --- L(Train): 0.3774; L(Val): 0.4408; Reg Param: 0.0316; Time: 0.61\n",
      "Epoch 5525/8192 --- L(Train): 0.3759; L(Val): 0.4417; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5526/8192 --- L(Train): 0.3753; L(Val): 0.4425; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5527/8192 --- L(Train): 0.3783; L(Val): 0.4427; Reg Param: 0.0316; Time: 0.61\n",
      "Epoch 5528/8192 --- L(Train): 0.3757; L(Val): 0.4423; Reg Param: 0.0316; Time: 0.64\n",
      "Epoch 5529/8192 --- L(Train): 0.3765; L(Val): 0.4416; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5530/8192 --- L(Train): 0.3806; L(Val): 0.4416; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5531/8192 --- L(Train): 0.3707; L(Val): 0.4413; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5532/8192 --- L(Train): 0.3713; L(Val): 0.4411; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5533/8192 --- L(Train): 0.3790; L(Val): 0.4411; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5534/8192 --- L(Train): 0.3765; L(Val): 0.4411; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5535/8192 --- L(Train): 0.3725; L(Val): 0.4411; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5536/8192 --- L(Train): 0.3700; L(Val): 0.4411; Reg Param: 0.0316; Time: 0.64\n",
      "Epoch 5537/8192 --- L(Train): 0.3752; L(Val): 0.4413; Reg Param: 0.0316; Time: 0.63\n",
      "Epoch 5538/8192 --- L(Train): 0.3817; L(Val): 0.4411; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5539/8192 --- L(Train): 0.3748; L(Val): 0.4410; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5540/8192 --- L(Train): 0.3822; L(Val): 0.4409; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5541/8192 --- L(Train): 0.3754; L(Val): 0.4410; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5542/8192 --- L(Train): 0.3703; L(Val): 0.4410; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5543/8192 --- L(Train): 0.3795; L(Val): 0.4412; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5544/8192 --- L(Train): 0.3774; L(Val): 0.4414; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5545/8192 --- L(Train): 0.3744; L(Val): 0.4415; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5546/8192 --- L(Train): 0.3815; L(Val): 0.4417; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5547/8192 --- L(Train): 0.3763; L(Val): 0.4416; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5548/8192 --- L(Train): 0.3701; L(Val): 0.4416; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5549/8192 --- L(Train): 0.3882; L(Val): 0.4410; Reg Param: 0.0316; Time: 0.60\n",
      "Epoch 5550/8192 --- L(Train): 0.3700; L(Val): 0.4410; Reg Param: 0.0304; Time: 6.40\n",
      "Epoch 5551/8192 --- L(Train): 0.3758; L(Val): 0.4398; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5552/8192 --- L(Train): 0.3813; L(Val): 0.4392; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5553/8192 --- L(Train): 0.3780; L(Val): 0.4385; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5554/8192 --- L(Train): 0.3750; L(Val): 0.4385; Reg Param: 0.0304; Time: 0.61\n",
      "Epoch 5555/8192 --- L(Train): 0.3915; L(Val): 0.4389; Reg Param: 0.0304; Time: 0.61\n",
      "Epoch 5556/8192 --- L(Train): 0.3746; L(Val): 0.4390; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5557/8192 --- L(Train): 0.3784; L(Val): 0.4398; Reg Param: 0.0304; Time: 0.61\n",
      "Epoch 5558/8192 --- L(Train): 0.3814; L(Val): 0.4405; Reg Param: 0.0304; Time: 0.61\n",
      "Epoch 5559/8192 --- L(Train): 0.3751; L(Val): 0.4412; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5560/8192 --- L(Train): 0.3737; L(Val): 0.4420; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5561/8192 --- L(Train): 0.3956; L(Val): 0.4423; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5562/8192 --- L(Train): 0.3733; L(Val): 0.4421; Reg Param: 0.0304; Time: 0.59\n",
      "Epoch 5563/8192 --- L(Train): 0.3665; L(Val): 0.4418; Reg Param: 0.0304; Time: 0.62\n",
      "Epoch 5564/8192 --- L(Train): 0.3805; L(Val): 0.4414; Reg Param: 0.0304; Time: 0.62\n",
      "Epoch 5565/8192 --- L(Train): 0.3756; L(Val): 0.4411; Reg Param: 0.0304; Time: 0.61\n",
      "Epoch 5566/8192 --- L(Train): 0.3707; L(Val): 0.4407; Reg Param: 0.0304; Time: 0.62\n",
      "Epoch 5567/8192 --- L(Train): 0.3740; L(Val): 0.4408; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5568/8192 --- L(Train): 0.3781; L(Val): 0.4409; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5569/8192 --- L(Train): 0.3735; L(Val): 0.4411; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5570/8192 --- L(Train): 0.3722; L(Val): 0.4411; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5571/8192 --- L(Train): 0.3764; L(Val): 0.4411; Reg Param: 0.0304; Time: 0.61\n",
      "Epoch 5572/8192 --- L(Train): 0.3697; L(Val): 0.4412; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5573/8192 --- L(Train): 0.3741; L(Val): 0.4412; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5574/8192 --- L(Train): 0.3574; L(Val): 0.4411; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5575/8192 --- L(Train): 0.3618; L(Val): 0.4413; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5576/8192 --- L(Train): 0.3702; L(Val): 0.4414; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5577/8192 --- L(Train): 0.3693; L(Val): 0.4417; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5578/8192 --- L(Train): 0.3786; L(Val): 0.4416; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5579/8192 --- L(Train): 0.3712; L(Val): 0.4411; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5580/8192 --- L(Train): 0.3739; L(Val): 0.4412; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5581/8192 --- L(Train): 0.3711; L(Val): 0.4413; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5582/8192 --- L(Train): 0.3654; L(Val): 0.4413; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5583/8192 --- L(Train): 0.3711; L(Val): 0.4413; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5584/8192 --- L(Train): 0.3674; L(Val): 0.4413; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5585/8192 --- L(Train): 0.3841; L(Val): 0.4416; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5586/8192 --- L(Train): 0.3772; L(Val): 0.4418; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5587/8192 --- L(Train): 0.3704; L(Val): 0.4420; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5588/8192 --- L(Train): 0.3708; L(Val): 0.4418; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5589/8192 --- L(Train): 0.3732; L(Val): 0.4416; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5590/8192 --- L(Train): 0.3752; L(Val): 0.4414; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5591/8192 --- L(Train): 0.3683; L(Val): 0.4413; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5592/8192 --- L(Train): 0.3841; L(Val): 0.4413; Reg Param: 0.0304; Time: 0.61\n",
      "Epoch 5593/8192 --- L(Train): 0.3593; L(Val): 0.4415; Reg Param: 0.0304; Time: 0.61\n",
      "Epoch 5594/8192 --- L(Train): 0.3751; L(Val): 0.4413; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5595/8192 --- L(Train): 0.3712; L(Val): 0.4409; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5596/8192 --- L(Train): 0.3753; L(Val): 0.4406; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5597/8192 --- L(Train): 0.3809; L(Val): 0.4407; Reg Param: 0.0304; Time: 0.61\n",
      "Epoch 5598/8192 --- L(Train): 0.3776; L(Val): 0.4406; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5599/8192 --- L(Train): 0.3707; L(Val): 0.4409; Reg Param: 0.0304; Time: 0.60\n",
      "Epoch 5600/8192 --- L(Train): 0.3702; L(Val): 0.4409; Reg Param: 0.0293; Time: 6.29\n",
      "Epoch 5601/8192 --- L(Train): 0.3900; L(Val): 0.4407; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5602/8192 --- L(Train): 0.3841; L(Val): 0.4410; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5603/8192 --- L(Train): 0.3699; L(Val): 0.4412; Reg Param: 0.0293; Time: 0.59\n",
      "Epoch 5604/8192 --- L(Train): 0.3710; L(Val): 0.4414; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5605/8192 --- L(Train): 0.3754; L(Val): 0.4416; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5606/8192 --- L(Train): 0.3726; L(Val): 0.4416; Reg Param: 0.0293; Time: 0.59\n",
      "Epoch 5607/8192 --- L(Train): 0.3727; L(Val): 0.4416; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5608/8192 --- L(Train): 0.3785; L(Val): 0.4415; Reg Param: 0.0293; Time: 0.65\n",
      "Epoch 5609/8192 --- L(Train): 0.3703; L(Val): 0.4415; Reg Param: 0.0293; Time: 0.61\n",
      "Epoch 5610/8192 --- L(Train): 0.3815; L(Val): 0.4413; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5611/8192 --- L(Train): 0.3689; L(Val): 0.4413; Reg Param: 0.0293; Time: 0.61\n",
      "Epoch 5612/8192 --- L(Train): 0.3818; L(Val): 0.4411; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5613/8192 --- L(Train): 0.3826; L(Val): 0.4409; Reg Param: 0.0293; Time: 0.61\n",
      "Epoch 5614/8192 --- L(Train): 0.3678; L(Val): 0.4411; Reg Param: 0.0293; Time: 0.59\n",
      "Epoch 5615/8192 --- L(Train): 0.3867; L(Val): 0.4416; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5616/8192 --- L(Train): 0.3731; L(Val): 0.4418; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5617/8192 --- L(Train): 0.3777; L(Val): 0.4418; Reg Param: 0.0293; Time: 0.64\n",
      "Epoch 5618/8192 --- L(Train): 0.3700; L(Val): 0.4418; Reg Param: 0.0293; Time: 0.63\n",
      "Epoch 5619/8192 --- L(Train): 0.3853; L(Val): 0.4418; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5620/8192 --- L(Train): 0.3679; L(Val): 0.4417; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5621/8192 --- L(Train): 0.3760; L(Val): 0.4418; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5622/8192 --- L(Train): 0.3797; L(Val): 0.4420; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5623/8192 --- L(Train): 0.3836; L(Val): 0.4420; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5624/8192 --- L(Train): 0.3844; L(Val): 0.4420; Reg Param: 0.0293; Time: 0.69\n",
      "Epoch 5625/8192 --- L(Train): 0.3709; L(Val): 0.4421; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5626/8192 --- L(Train): 0.3703; L(Val): 0.4423; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5627/8192 --- L(Train): 0.3724; L(Val): 0.4421; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5628/8192 --- L(Train): 0.3706; L(Val): 0.4417; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5629/8192 --- L(Train): 0.3826; L(Val): 0.4411; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5630/8192 --- L(Train): 0.3851; L(Val): 0.4410; Reg Param: 0.0293; Time: 0.61\n",
      "Epoch 5631/8192 --- L(Train): 0.3708; L(Val): 0.4409; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5632/8192 --- L(Train): 0.3873; L(Val): 0.4408; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5633/8192 --- L(Train): 0.3721; L(Val): 0.4407; Reg Param: 0.0293; Time: 0.65\n",
      "Epoch 5634/8192 --- L(Train): 0.3599; L(Val): 0.4406; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5635/8192 --- L(Train): 0.3765; L(Val): 0.4404; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5636/8192 --- L(Train): 0.3836; L(Val): 0.4402; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5637/8192 --- L(Train): 0.3685; L(Val): 0.4401; Reg Param: 0.0293; Time: 0.76\n",
      "Epoch 5638/8192 --- L(Train): 0.3634; L(Val): 0.4400; Reg Param: 0.0293; Time: 0.65\n",
      "Epoch 5639/8192 --- L(Train): 0.3886; L(Val): 0.4397; Reg Param: 0.0293; Time: 0.65\n",
      "Epoch 5640/8192 --- L(Train): 0.3754; L(Val): 0.4393; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5641/8192 --- L(Train): 0.3690; L(Val): 0.4390; Reg Param: 0.0293; Time: 0.61\n",
      "Epoch 5642/8192 --- L(Train): 0.3763; L(Val): 0.4386; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5643/8192 --- L(Train): 0.3685; L(Val): 0.4386; Reg Param: 0.0293; Time: 0.61\n",
      "Epoch 5644/8192 --- L(Train): 0.3725; L(Val): 0.4390; Reg Param: 0.0293; Time: 0.61\n",
      "Epoch 5645/8192 --- L(Train): 0.3713; L(Val): 0.4396; Reg Param: 0.0293; Time: 0.61\n",
      "Epoch 5646/8192 --- L(Train): 0.3673; L(Val): 0.4400; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5647/8192 --- L(Train): 0.3851; L(Val): 0.4403; Reg Param: 0.0293; Time: 0.62\n",
      "Epoch 5648/8192 --- L(Train): 0.3831; L(Val): 0.4403; Reg Param: 0.0293; Time: 0.60\n",
      "Epoch 5649/8192 --- L(Train): 0.3851; L(Val): 0.4403; Reg Param: 0.0293; Time: 0.61\n",
      "Epoch 5650/8192 --- L(Train): 0.3717; L(Val): 0.4403; Reg Param: 0.0282; Time: 6.27\n",
      "Epoch 5651/8192 --- L(Train): 0.3785; L(Val): 0.4405; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5652/8192 --- L(Train): 0.3831; L(Val): 0.4406; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5653/8192 --- L(Train): 0.3631; L(Val): 0.4406; Reg Param: 0.0282; Time: 0.61\n",
      "Epoch 5654/8192 --- L(Train): 0.3804; L(Val): 0.4407; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5655/8192 --- L(Train): 0.3666; L(Val): 0.4410; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5656/8192 --- L(Train): 0.3790; L(Val): 0.4412; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5657/8192 --- L(Train): 0.3730; L(Val): 0.4410; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5658/8192 --- L(Train): 0.3802; L(Val): 0.4407; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5659/8192 --- L(Train): 0.3695; L(Val): 0.4409; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5660/8192 --- L(Train): 0.3802; L(Val): 0.4408; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5661/8192 --- L(Train): 0.3826; L(Val): 0.4399; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5662/8192 --- L(Train): 0.3766; L(Val): 0.4395; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5663/8192 --- L(Train): 0.3850; L(Val): 0.4395; Reg Param: 0.0282; Time: 0.61\n",
      "Epoch 5664/8192 --- L(Train): 0.3821; L(Val): 0.4400; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5665/8192 --- L(Train): 0.3798; L(Val): 0.4402; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5666/8192 --- L(Train): 0.3723; L(Val): 0.4403; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5667/8192 --- L(Train): 0.3654; L(Val): 0.4405; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5668/8192 --- L(Train): 0.3795; L(Val): 0.4405; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5669/8192 --- L(Train): 0.3887; L(Val): 0.4404; Reg Param: 0.0282; Time: 0.61\n",
      "Epoch 5670/8192 --- L(Train): 0.3684; L(Val): 0.4405; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5671/8192 --- L(Train): 0.3696; L(Val): 0.4405; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5672/8192 --- L(Train): 0.3739; L(Val): 0.4407; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5673/8192 --- L(Train): 0.3662; L(Val): 0.4406; Reg Param: 0.0282; Time: 0.61\n",
      "Epoch 5674/8192 --- L(Train): 0.3681; L(Val): 0.4405; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5675/8192 --- L(Train): 0.3892; L(Val): 0.4404; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5676/8192 --- L(Train): 0.3703; L(Val): 0.4405; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5677/8192 --- L(Train): 0.3717; L(Val): 0.4407; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5678/8192 --- L(Train): 0.3816; L(Val): 0.4409; Reg Param: 0.0282; Time: 0.61\n",
      "Epoch 5679/8192 --- L(Train): 0.3750; L(Val): 0.4410; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5680/8192 --- L(Train): 0.3658; L(Val): 0.4409; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5681/8192 --- L(Train): 0.3709; L(Val): 0.4409; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5682/8192 --- L(Train): 0.3626; L(Val): 0.4408; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5683/8192 --- L(Train): 0.3788; L(Val): 0.4408; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5684/8192 --- L(Train): 0.3725; L(Val): 0.4410; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5685/8192 --- L(Train): 0.3712; L(Val): 0.4412; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5686/8192 --- L(Train): 0.3740; L(Val): 0.4415; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5687/8192 --- L(Train): 0.3721; L(Val): 0.4416; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5688/8192 --- L(Train): 0.3701; L(Val): 0.4413; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5689/8192 --- L(Train): 0.3685; L(Val): 0.4410; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5690/8192 --- L(Train): 0.3829; L(Val): 0.4411; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5691/8192 --- L(Train): 0.3886; L(Val): 0.4411; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5692/8192 --- L(Train): 0.3665; L(Val): 0.4411; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5693/8192 --- L(Train): 0.3756; L(Val): 0.4410; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5694/8192 --- L(Train): 0.3833; L(Val): 0.4411; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5695/8192 --- L(Train): 0.3817; L(Val): 0.4416; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5696/8192 --- L(Train): 0.3786; L(Val): 0.4421; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5697/8192 --- L(Train): 0.3652; L(Val): 0.4426; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5698/8192 --- L(Train): 0.3735; L(Val): 0.4429; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5699/8192 --- L(Train): 0.3749; L(Val): 0.4431; Reg Param: 0.0282; Time: 0.60\n",
      "Epoch 5700/8192 --- L(Train): 0.3702; L(Val): 0.4431; Reg Param: 0.0269; Time: 6.43\n",
      "Epoch 5701/8192 --- L(Train): 0.3765; L(Val): 0.4432; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5702/8192 --- L(Train): 0.3766; L(Val): 0.4423; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5703/8192 --- L(Train): 0.3732; L(Val): 0.4415; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5704/8192 --- L(Train): 0.3726; L(Val): 0.4410; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5705/8192 --- L(Train): 0.3720; L(Val): 0.4407; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5706/8192 --- L(Train): 0.3828; L(Val): 0.4407; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5707/8192 --- L(Train): 0.3750; L(Val): 0.4409; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5708/8192 --- L(Train): 0.3624; L(Val): 0.4413; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5709/8192 --- L(Train): 0.3820; L(Val): 0.4417; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5710/8192 --- L(Train): 0.3837; L(Val): 0.4428; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5711/8192 --- L(Train): 0.3821; L(Val): 0.4438; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5712/8192 --- L(Train): 0.3743; L(Val): 0.4444; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5713/8192 --- L(Train): 0.3791; L(Val): 0.4446; Reg Param: 0.0269; Time: 0.62\n",
      "Epoch 5714/8192 --- L(Train): 0.3725; L(Val): 0.4452; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5715/8192 --- L(Train): 0.3664; L(Val): 0.4457; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5716/8192 --- L(Train): 0.3895; L(Val): 0.4458; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5717/8192 --- L(Train): 0.3891; L(Val): 0.4453; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5718/8192 --- L(Train): 0.3920; L(Val): 0.4447; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5719/8192 --- L(Train): 0.3813; L(Val): 0.4443; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5720/8192 --- L(Train): 0.3649; L(Val): 0.4437; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5721/8192 --- L(Train): 0.3762; L(Val): 0.4429; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5722/8192 --- L(Train): 0.3813; L(Val): 0.4422; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5723/8192 --- L(Train): 0.3654; L(Val): 0.4420; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5724/8192 --- L(Train): 0.3832; L(Val): 0.4419; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5725/8192 --- L(Train): 0.3739; L(Val): 0.4420; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5726/8192 --- L(Train): 0.3732; L(Val): 0.4428; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5727/8192 --- L(Train): 0.3660; L(Val): 0.4437; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5728/8192 --- L(Train): 0.3586; L(Val): 0.4446; Reg Param: 0.0269; Time: 0.62\n",
      "Epoch 5729/8192 --- L(Train): 0.3789; L(Val): 0.4449; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5730/8192 --- L(Train): 0.3914; L(Val): 0.4451; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5731/8192 --- L(Train): 0.3796; L(Val): 0.4449; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5732/8192 --- L(Train): 0.3746; L(Val): 0.4446; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5733/8192 --- L(Train): 0.3806; L(Val): 0.4448; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5734/8192 --- L(Train): 0.3826; L(Val): 0.4452; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5735/8192 --- L(Train): 0.3576; L(Val): 0.4452; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5736/8192 --- L(Train): 0.3770; L(Val): 0.4445; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5737/8192 --- L(Train): 0.3820; L(Val): 0.4440; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5738/8192 --- L(Train): 0.3682; L(Val): 0.4443; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5739/8192 --- L(Train): 0.3807; L(Val): 0.4445; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5740/8192 --- L(Train): 0.3753; L(Val): 0.4448; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5741/8192 --- L(Train): 0.3774; L(Val): 0.4450; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5742/8192 --- L(Train): 0.3795; L(Val): 0.4452; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5743/8192 --- L(Train): 0.3852; L(Val): 0.4450; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5744/8192 --- L(Train): 0.3719; L(Val): 0.4448; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5745/8192 --- L(Train): 0.3850; L(Val): 0.4449; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5746/8192 --- L(Train): 0.3784; L(Val): 0.4447; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5747/8192 --- L(Train): 0.3749; L(Val): 0.4447; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5748/8192 --- L(Train): 0.3679; L(Val): 0.4446; Reg Param: 0.0269; Time: 0.60\n",
      "Epoch 5749/8192 --- L(Train): 0.3810; L(Val): 0.4445; Reg Param: 0.0269; Time: 0.61\n",
      "Epoch 5750/8192 --- L(Train): 0.3816; L(Val): 0.4445; Reg Param: 0.0256; Time: 6.27\n",
      "Epoch 5751/8192 --- L(Train): 0.3808; L(Val): 0.4445; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5752/8192 --- L(Train): 0.3638; L(Val): 0.4444; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5753/8192 --- L(Train): 0.3729; L(Val): 0.4441; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5754/8192 --- L(Train): 0.3823; L(Val): 0.4436; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5755/8192 --- L(Train): 0.3691; L(Val): 0.4433; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5756/8192 --- L(Train): 0.3844; L(Val): 0.4430; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5757/8192 --- L(Train): 0.3685; L(Val): 0.4430; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5758/8192 --- L(Train): 0.3829; L(Val): 0.4428; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5759/8192 --- L(Train): 0.3686; L(Val): 0.4425; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5760/8192 --- L(Train): 0.3781; L(Val): 0.4422; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5761/8192 --- L(Train): 0.3741; L(Val): 0.4421; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5762/8192 --- L(Train): 0.3795; L(Val): 0.4420; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5763/8192 --- L(Train): 0.3744; L(Val): 0.4422; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5764/8192 --- L(Train): 0.3692; L(Val): 0.4424; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5765/8192 --- L(Train): 0.3649; L(Val): 0.4426; Reg Param: 0.0256; Time: 0.62\n",
      "Epoch 5766/8192 --- L(Train): 0.3747; L(Val): 0.4428; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5767/8192 --- L(Train): 0.3789; L(Val): 0.4428; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5768/8192 --- L(Train): 0.3756; L(Val): 0.4432; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5769/8192 --- L(Train): 0.3763; L(Val): 0.4431; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5770/8192 --- L(Train): 0.3815; L(Val): 0.4430; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5771/8192 --- L(Train): 0.3802; L(Val): 0.4431; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5772/8192 --- L(Train): 0.3683; L(Val): 0.4430; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5773/8192 --- L(Train): 0.3809; L(Val): 0.4424; Reg Param: 0.0256; Time: 0.70\n",
      "Epoch 5774/8192 --- L(Train): 0.3735; L(Val): 0.4422; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5775/8192 --- L(Train): 0.3650; L(Val): 0.4422; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5776/8192 --- L(Train): 0.3871; L(Val): 0.4415; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5777/8192 --- L(Train): 0.3765; L(Val): 0.4411; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5778/8192 --- L(Train): 0.3746; L(Val): 0.4412; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5779/8192 --- L(Train): 0.3688; L(Val): 0.4415; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5780/8192 --- L(Train): 0.3682; L(Val): 0.4416; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5781/8192 --- L(Train): 0.3644; L(Val): 0.4421; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5782/8192 --- L(Train): 0.3594; L(Val): 0.4429; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5783/8192 --- L(Train): 0.3826; L(Val): 0.4437; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5784/8192 --- L(Train): 0.3796; L(Val): 0.4445; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5785/8192 --- L(Train): 0.3744; L(Val): 0.4451; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5786/8192 --- L(Train): 0.3659; L(Val): 0.4452; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5787/8192 --- L(Train): 0.3769; L(Val): 0.4452; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5788/8192 --- L(Train): 0.3771; L(Val): 0.4452; Reg Param: 0.0256; Time: 0.66\n",
      "Epoch 5789/8192 --- L(Train): 0.3653; L(Val): 0.4453; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5790/8192 --- L(Train): 0.3793; L(Val): 0.4453; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5791/8192 --- L(Train): 0.3672; L(Val): 0.4453; Reg Param: 0.0256; Time: 0.62\n",
      "Epoch 5792/8192 --- L(Train): 0.3671; L(Val): 0.4455; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5793/8192 --- L(Train): 0.3686; L(Val): 0.4459; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5794/8192 --- L(Train): 0.3780; L(Val): 0.4460; Reg Param: 0.0256; Time: 0.62\n",
      "Epoch 5795/8192 --- L(Train): 0.3735; L(Val): 0.4459; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5796/8192 --- L(Train): 0.3806; L(Val): 0.4456; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5797/8192 --- L(Train): 0.3669; L(Val): 0.4451; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5798/8192 --- L(Train): 0.3746; L(Val): 0.4445; Reg Param: 0.0256; Time: 0.61\n",
      "Epoch 5799/8192 --- L(Train): 0.3838; L(Val): 0.4437; Reg Param: 0.0256; Time: 0.60\n",
      "Epoch 5800/8192 --- L(Train): 0.3732; L(Val): 0.4437; Reg Param: 0.0241; Time: 6.30\n",
      "Epoch 5801/8192 --- L(Train): 0.3842; L(Val): 0.4421; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5802/8192 --- L(Train): 0.3755; L(Val): 0.4418; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5803/8192 --- L(Train): 0.3795; L(Val): 0.4422; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5804/8192 --- L(Train): 0.3740; L(Val): 0.4421; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5805/8192 --- L(Train): 0.3595; L(Val): 0.4419; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5806/8192 --- L(Train): 0.3734; L(Val): 0.4422; Reg Param: 0.0241; Time: 0.61\n",
      "Epoch 5807/8192 --- L(Train): 0.3616; L(Val): 0.4427; Reg Param: 0.0241; Time: 0.61\n",
      "Epoch 5808/8192 --- L(Train): 0.3685; L(Val): 0.4430; Reg Param: 0.0241; Time: 0.61\n",
      "Epoch 5809/8192 --- L(Train): 0.3696; L(Val): 0.4433; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5810/8192 --- L(Train): 0.3716; L(Val): 0.4433; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5811/8192 --- L(Train): 0.3801; L(Val): 0.4430; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5812/8192 --- L(Train): 0.3749; L(Val): 0.4431; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5813/8192 --- L(Train): 0.3784; L(Val): 0.4429; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5814/8192 --- L(Train): 0.3686; L(Val): 0.4429; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5815/8192 --- L(Train): 0.3719; L(Val): 0.4429; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5816/8192 --- L(Train): 0.3643; L(Val): 0.4427; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5817/8192 --- L(Train): 0.3712; L(Val): 0.4428; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5818/8192 --- L(Train): 0.3686; L(Val): 0.4428; Reg Param: 0.0241; Time: 0.61\n",
      "Epoch 5819/8192 --- L(Train): 0.3745; L(Val): 0.4427; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5820/8192 --- L(Train): 0.3787; L(Val): 0.4426; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5821/8192 --- L(Train): 0.3894; L(Val): 0.4427; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5822/8192 --- L(Train): 0.3818; L(Val): 0.4426; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5823/8192 --- L(Train): 0.3713; L(Val): 0.4425; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5824/8192 --- L(Train): 0.3748; L(Val): 0.4418; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5825/8192 --- L(Train): 0.3721; L(Val): 0.4416; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5826/8192 --- L(Train): 0.3751; L(Val): 0.4414; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5827/8192 --- L(Train): 0.3807; L(Val): 0.4413; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5828/8192 --- L(Train): 0.3744; L(Val): 0.4414; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5829/8192 --- L(Train): 0.3585; L(Val): 0.4417; Reg Param: 0.0241; Time: 0.61\n",
      "Epoch 5830/8192 --- L(Train): 0.3781; L(Val): 0.4420; Reg Param: 0.0241; Time: 0.63\n",
      "Epoch 5831/8192 --- L(Train): 0.3910; L(Val): 0.4421; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5832/8192 --- L(Train): 0.3788; L(Val): 0.4421; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5833/8192 --- L(Train): 0.3756; L(Val): 0.4423; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5834/8192 --- L(Train): 0.3661; L(Val): 0.4427; Reg Param: 0.0241; Time: 0.61\n",
      "Epoch 5835/8192 --- L(Train): 0.3792; L(Val): 0.4428; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5836/8192 --- L(Train): 0.3684; L(Val): 0.4428; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5837/8192 --- L(Train): 0.3729; L(Val): 0.4428; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5838/8192 --- L(Train): 0.3732; L(Val): 0.4432; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5839/8192 --- L(Train): 0.3803; L(Val): 0.4435; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5840/8192 --- L(Train): 0.3747; L(Val): 0.4433; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5841/8192 --- L(Train): 0.3889; L(Val): 0.4430; Reg Param: 0.0241; Time: 0.61\n",
      "Epoch 5842/8192 --- L(Train): 0.3722; L(Val): 0.4428; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5843/8192 --- L(Train): 0.3738; L(Val): 0.4427; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5844/8192 --- L(Train): 0.3696; L(Val): 0.4428; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5845/8192 --- L(Train): 0.3698; L(Val): 0.4432; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5846/8192 --- L(Train): 0.3691; L(Val): 0.4433; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5847/8192 --- L(Train): 0.3733; L(Val): 0.4435; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5848/8192 --- L(Train): 0.3660; L(Val): 0.4437; Reg Param: 0.0241; Time: 0.60\n",
      "Epoch 5849/8192 --- L(Train): 0.3686; L(Val): 0.4439; Reg Param: 0.0241; Time: 0.61\n",
      "Epoch 5850/8192 --- L(Train): 0.3811; L(Val): 0.4439; Reg Param: 0.0225; Time: 6.38\n",
      "Epoch 5851/8192 --- L(Train): 0.3688; L(Val): 0.4443; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5852/8192 --- L(Train): 0.3622; L(Val): 0.4444; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5853/8192 --- L(Train): 0.3820; L(Val): 0.4444; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5854/8192 --- L(Train): 0.3668; L(Val): 0.4446; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5855/8192 --- L(Train): 0.3898; L(Val): 0.4443; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5856/8192 --- L(Train): 0.3804; L(Val): 0.4438; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5857/8192 --- L(Train): 0.3816; L(Val): 0.4433; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5858/8192 --- L(Train): 0.3794; L(Val): 0.4430; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5859/8192 --- L(Train): 0.3777; L(Val): 0.4426; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5860/8192 --- L(Train): 0.3724; L(Val): 0.4423; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5861/8192 --- L(Train): 0.3804; L(Val): 0.4419; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5862/8192 --- L(Train): 0.3800; L(Val): 0.4414; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5863/8192 --- L(Train): 0.3759; L(Val): 0.4411; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5864/8192 --- L(Train): 0.3702; L(Val): 0.4410; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5865/8192 --- L(Train): 0.3721; L(Val): 0.4411; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5866/8192 --- L(Train): 0.3702; L(Val): 0.4414; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5867/8192 --- L(Train): 0.3715; L(Val): 0.4417; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5868/8192 --- L(Train): 0.3815; L(Val): 0.4422; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5869/8192 --- L(Train): 0.3839; L(Val): 0.4429; Reg Param: 0.0225; Time: 0.64\n",
      "Epoch 5870/8192 --- L(Train): 0.3791; L(Val): 0.4435; Reg Param: 0.0225; Time: 0.62\n",
      "Epoch 5871/8192 --- L(Train): 0.3771; L(Val): 0.4437; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5872/8192 --- L(Train): 0.3738; L(Val): 0.4438; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5873/8192 --- L(Train): 0.3744; L(Val): 0.4439; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5874/8192 --- L(Train): 0.3812; L(Val): 0.4435; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5875/8192 --- L(Train): 0.3701; L(Val): 0.4433; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5876/8192 --- L(Train): 0.3753; L(Val): 0.4428; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5877/8192 --- L(Train): 0.3616; L(Val): 0.4425; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5878/8192 --- L(Train): 0.3719; L(Val): 0.4422; Reg Param: 0.0225; Time: 0.61\n",
      "Epoch 5879/8192 --- L(Train): 0.3840; L(Val): 0.4422; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5880/8192 --- L(Train): 0.3807; L(Val): 0.4424; Reg Param: 0.0225; Time: 0.59\n",
      "Epoch 5881/8192 --- L(Train): 0.3750; L(Val): 0.4426; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5882/8192 --- L(Train): 0.3647; L(Val): 0.4431; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5883/8192 --- L(Train): 0.3776; L(Val): 0.4432; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5884/8192 --- L(Train): 0.3650; L(Val): 0.4435; Reg Param: 0.0225; Time: 0.61\n",
      "Epoch 5885/8192 --- L(Train): 0.3719; L(Val): 0.4439; Reg Param: 0.0225; Time: 0.61\n",
      "Epoch 5886/8192 --- L(Train): 0.3652; L(Val): 0.4438; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5887/8192 --- L(Train): 0.3784; L(Val): 0.4436; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5888/8192 --- L(Train): 0.3799; L(Val): 0.4434; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5889/8192 --- L(Train): 0.3778; L(Val): 0.4432; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5890/8192 --- L(Train): 0.3802; L(Val): 0.4429; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5891/8192 --- L(Train): 0.3842; L(Val): 0.4427; Reg Param: 0.0225; Time: 0.75\n",
      "Epoch 5892/8192 --- L(Train): 0.3632; L(Val): 0.4427; Reg Param: 0.0225; Time: 0.63\n",
      "Epoch 5893/8192 --- L(Train): 0.3890; L(Val): 0.4428; Reg Param: 0.0225; Time: 0.60\n",
      "Epoch 5894/8192 --- L(Train): 0.3685; L(Val): 0.4431; Reg Param: 0.0225; Time: 0.61\n",
      "Epoch 5895/8192 --- L(Train): 0.3611; L(Val): 0.4430; Reg Param: 0.0225; Time: 0.61\n",
      "Epoch 5896/8192 --- L(Train): 0.3770; L(Val): 0.4429; Reg Param: 0.0225; Time: 0.61\n",
      "Epoch 5897/8192 --- L(Train): 0.3671; L(Val): 0.4428; Reg Param: 0.0225; Time: 0.61\n",
      "Epoch 5898/8192 --- L(Train): 0.3790; L(Val): 0.4424; Reg Param: 0.0225; Time: 0.61\n",
      "Epoch 5899/8192 --- L(Train): 0.3777; L(Val): 0.4419; Reg Param: 0.0225; Time: 0.61\n",
      "Epoch 5900/8192 --- L(Train): 0.3846; L(Val): 0.4419; Reg Param: 0.0206; Time: 6.27\n",
      "Epoch 5901/8192 --- L(Train): 0.3725; L(Val): 0.4409; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5902/8192 --- L(Train): 0.3822; L(Val): 0.4406; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5903/8192 --- L(Train): 0.3827; L(Val): 0.4409; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5904/8192 --- L(Train): 0.3758; L(Val): 0.4412; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5905/8192 --- L(Train): 0.3754; L(Val): 0.4418; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5906/8192 --- L(Train): 0.3689; L(Val): 0.4422; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5907/8192 --- L(Train): 0.3771; L(Val): 0.4425; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5908/8192 --- L(Train): 0.3892; L(Val): 0.4427; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5909/8192 --- L(Train): 0.3671; L(Val): 0.4423; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5910/8192 --- L(Train): 0.3693; L(Val): 0.4419; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5911/8192 --- L(Train): 0.3623; L(Val): 0.4414; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5912/8192 --- L(Train): 0.3729; L(Val): 0.4410; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5913/8192 --- L(Train): 0.3676; L(Val): 0.4406; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5914/8192 --- L(Train): 0.3734; L(Val): 0.4404; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5915/8192 --- L(Train): 0.3823; L(Val): 0.4403; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5916/8192 --- L(Train): 0.3824; L(Val): 0.4402; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5917/8192 --- L(Train): 0.3645; L(Val): 0.4400; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5918/8192 --- L(Train): 0.3812; L(Val): 0.4403; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5919/8192 --- L(Train): 0.3725; L(Val): 0.4409; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5920/8192 --- L(Train): 0.3831; L(Val): 0.4413; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5921/8192 --- L(Train): 0.3856; L(Val): 0.4419; Reg Param: 0.0206; Time: 0.71\n",
      "Epoch 5922/8192 --- L(Train): 0.3712; L(Val): 0.4422; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5923/8192 --- L(Train): 0.3854; L(Val): 0.4423; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5924/8192 --- L(Train): 0.3723; L(Val): 0.4420; Reg Param: 0.0206; Time: 0.62\n",
      "Epoch 5925/8192 --- L(Train): 0.3654; L(Val): 0.4417; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5926/8192 --- L(Train): 0.3689; L(Val): 0.4415; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5927/8192 --- L(Train): 0.3782; L(Val): 0.4412; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5928/8192 --- L(Train): 0.3669; L(Val): 0.4408; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5929/8192 --- L(Train): 0.3663; L(Val): 0.4407; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5930/8192 --- L(Train): 0.3625; L(Val): 0.4404; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5931/8192 --- L(Train): 0.3715; L(Val): 0.4406; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5932/8192 --- L(Train): 0.3908; L(Val): 0.4410; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5933/8192 --- L(Train): 0.3725; L(Val): 0.4414; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5934/8192 --- L(Train): 0.3853; L(Val): 0.4417; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5935/8192 --- L(Train): 0.3589; L(Val): 0.4419; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5936/8192 --- L(Train): 0.3727; L(Val): 0.4419; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5937/8192 --- L(Train): 0.3712; L(Val): 0.4417; Reg Param: 0.0206; Time: 0.63\n",
      "Epoch 5938/8192 --- L(Train): 0.3757; L(Val): 0.4415; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5939/8192 --- L(Train): 0.3750; L(Val): 0.4416; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5940/8192 --- L(Train): 0.3615; L(Val): 0.4416; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5941/8192 --- L(Train): 0.3798; L(Val): 0.4417; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5942/8192 --- L(Train): 0.3663; L(Val): 0.4419; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5943/8192 --- L(Train): 0.3809; L(Val): 0.4419; Reg Param: 0.0206; Time: 0.65\n",
      "Epoch 5944/8192 --- L(Train): 0.3758; L(Val): 0.4420; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5945/8192 --- L(Train): 0.3816; L(Val): 0.4417; Reg Param: 0.0206; Time: 0.60\n",
      "Epoch 5946/8192 --- L(Train): 0.3707; L(Val): 0.4419; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5947/8192 --- L(Train): 0.3718; L(Val): 0.4421; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5948/8192 --- L(Train): 0.3595; L(Val): 0.4421; Reg Param: 0.0206; Time: 0.64\n",
      "Epoch 5949/8192 --- L(Train): 0.3745; L(Val): 0.4422; Reg Param: 0.0206; Time: 0.61\n",
      "Epoch 5950/8192 --- L(Train): 0.3665; L(Val): 0.4422; Reg Param: 0.0190; Time: 6.33\n",
      "Epoch 5951/8192 --- L(Train): 0.3850; L(Val): 0.4430; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5952/8192 --- L(Train): 0.3726; L(Val): 0.4434; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5953/8192 --- L(Train): 0.3663; L(Val): 0.4439; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5954/8192 --- L(Train): 0.3736; L(Val): 0.4438; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5955/8192 --- L(Train): 0.3636; L(Val): 0.4437; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5956/8192 --- L(Train): 0.3697; L(Val): 0.4435; Reg Param: 0.0190; Time: 0.62\n",
      "Epoch 5957/8192 --- L(Train): 0.3769; L(Val): 0.4434; Reg Param: 0.0190; Time: 0.61\n",
      "Epoch 5958/8192 --- L(Train): 0.3642; L(Val): 0.4432; Reg Param: 0.0190; Time: 0.62\n",
      "Epoch 5959/8192 --- L(Train): 0.3794; L(Val): 0.4432; Reg Param: 0.0190; Time: 0.74\n",
      "Epoch 5960/8192 --- L(Train): 0.3571; L(Val): 0.4431; Reg Param: 0.0190; Time: 0.70\n",
      "Epoch 5961/8192 --- L(Train): 0.3713; L(Val): 0.4431; Reg Param: 0.0190; Time: 0.69\n",
      "Epoch 5962/8192 --- L(Train): 0.3765; L(Val): 0.4431; Reg Param: 0.0190; Time: 0.68\n",
      "Epoch 5963/8192 --- L(Train): 0.3586; L(Val): 0.4432; Reg Param: 0.0190; Time: 0.69\n",
      "Epoch 5964/8192 --- L(Train): 0.3754; L(Val): 0.4430; Reg Param: 0.0190; Time: 0.68\n",
      "Epoch 5965/8192 --- L(Train): 0.3725; L(Val): 0.4431; Reg Param: 0.0190; Time: 0.68\n",
      "Epoch 5966/8192 --- L(Train): 0.3799; L(Val): 0.4431; Reg Param: 0.0190; Time: 0.68\n",
      "Epoch 5967/8192 --- L(Train): 0.3659; L(Val): 0.4431; Reg Param: 0.0190; Time: 0.69\n",
      "Epoch 5968/8192 --- L(Train): 0.3777; L(Val): 0.4430; Reg Param: 0.0190; Time: 0.67\n",
      "Epoch 5969/8192 --- L(Train): 0.3637; L(Val): 0.4429; Reg Param: 0.0190; Time: 0.66\n",
      "Epoch 5970/8192 --- L(Train): 0.3805; L(Val): 0.4429; Reg Param: 0.0190; Time: 0.65\n",
      "Epoch 5971/8192 --- L(Train): 0.3689; L(Val): 0.4430; Reg Param: 0.0190; Time: 0.67\n",
      "Epoch 5972/8192 --- L(Train): 0.3656; L(Val): 0.4431; Reg Param: 0.0190; Time: 0.67\n",
      "Epoch 5973/8192 --- L(Train): 0.3770; L(Val): 0.4432; Reg Param: 0.0190; Time: 0.66\n",
      "Epoch 5974/8192 --- L(Train): 0.3601; L(Val): 0.4432; Reg Param: 0.0190; Time: 0.66\n",
      "Epoch 5975/8192 --- L(Train): 0.3653; L(Val): 0.4433; Reg Param: 0.0190; Time: 0.63\n",
      "Epoch 5976/8192 --- L(Train): 0.3669; L(Val): 0.4439; Reg Param: 0.0190; Time: 0.61\n",
      "Epoch 5977/8192 --- L(Train): 0.3806; L(Val): 0.4443; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5978/8192 --- L(Train): 0.3689; L(Val): 0.4444; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5979/8192 --- L(Train): 0.3672; L(Val): 0.4449; Reg Param: 0.0190; Time: 0.61\n",
      "Epoch 5980/8192 --- L(Train): 0.3682; L(Val): 0.4452; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5981/8192 --- L(Train): 0.3868; L(Val): 0.4453; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5982/8192 --- L(Train): 0.3713; L(Val): 0.4452; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5983/8192 --- L(Train): 0.3732; L(Val): 0.4451; Reg Param: 0.0190; Time: 0.61\n",
      "Epoch 5984/8192 --- L(Train): 0.3677; L(Val): 0.4450; Reg Param: 0.0190; Time: 0.61\n",
      "Epoch 5985/8192 --- L(Train): 0.3583; L(Val): 0.4451; Reg Param: 0.0190; Time: 0.62\n",
      "Epoch 5986/8192 --- L(Train): 0.3643; L(Val): 0.4454; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5987/8192 --- L(Train): 0.3866; L(Val): 0.4454; Reg Param: 0.0190; Time: 0.61\n",
      "Epoch 5988/8192 --- L(Train): 0.3714; L(Val): 0.4454; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5989/8192 --- L(Train): 0.3760; L(Val): 0.4456; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5990/8192 --- L(Train): 0.3836; L(Val): 0.4460; Reg Param: 0.0190; Time: 0.61\n",
      "Epoch 5991/8192 --- L(Train): 0.3855; L(Val): 0.4462; Reg Param: 0.0190; Time: 0.59\n",
      "Epoch 5992/8192 --- L(Train): 0.3756; L(Val): 0.4462; Reg Param: 0.0190; Time: 0.63\n",
      "Epoch 5993/8192 --- L(Train): 0.3749; L(Val): 0.4460; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5994/8192 --- L(Train): 0.3807; L(Val): 0.4458; Reg Param: 0.0190; Time: 0.61\n",
      "Epoch 5995/8192 --- L(Train): 0.3646; L(Val): 0.4454; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5996/8192 --- L(Train): 0.3733; L(Val): 0.4449; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5997/8192 --- L(Train): 0.3795; L(Val): 0.4443; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5998/8192 --- L(Train): 0.3806; L(Val): 0.4441; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 5999/8192 --- L(Train): 0.3659; L(Val): 0.4438; Reg Param: 0.0190; Time: 0.60\n",
      "Epoch 6000/8192 --- L(Train): 0.3776; L(Val): 0.4438; Reg Param: 0.0176; Time: 6.37\n",
      "Epoch 6001/8192 --- L(Train): 0.3618; L(Val): 0.4439; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6002/8192 --- L(Train): 0.3743; L(Val): 0.4440; Reg Param: 0.0176; Time: 0.62\n",
      "Epoch 6003/8192 --- L(Train): 0.3692; L(Val): 0.4439; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6004/8192 --- L(Train): 0.3859; L(Val): 0.4438; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6005/8192 --- L(Train): 0.3756; L(Val): 0.4436; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6006/8192 --- L(Train): 0.3794; L(Val): 0.4434; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6007/8192 --- L(Train): 0.3712; L(Val): 0.4432; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6008/8192 --- L(Train): 0.3679; L(Val): 0.4430; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6009/8192 --- L(Train): 0.3745; L(Val): 0.4429; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6010/8192 --- L(Train): 0.3746; L(Val): 0.4429; Reg Param: 0.0176; Time: 0.64\n",
      "Epoch 6011/8192 --- L(Train): 0.3700; L(Val): 0.4432; Reg Param: 0.0176; Time: 0.62\n",
      "Epoch 6012/8192 --- L(Train): 0.3747; L(Val): 0.4435; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6013/8192 --- L(Train): 0.3839; L(Val): 0.4439; Reg Param: 0.0176; Time: 0.62\n",
      "Epoch 6014/8192 --- L(Train): 0.3680; L(Val): 0.4441; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6015/8192 --- L(Train): 0.3766; L(Val): 0.4443; Reg Param: 0.0176; Time: 0.62\n",
      "Epoch 6016/8192 --- L(Train): 0.3771; L(Val): 0.4447; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6017/8192 --- L(Train): 0.3874; L(Val): 0.4452; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6018/8192 --- L(Train): 0.3684; L(Val): 0.4458; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6019/8192 --- L(Train): 0.3735; L(Val): 0.4461; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6020/8192 --- L(Train): 0.3719; L(Val): 0.4463; Reg Param: 0.0176; Time: 0.73\n",
      "Epoch 6021/8192 --- L(Train): 0.3749; L(Val): 0.4464; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6022/8192 --- L(Train): 0.3687; L(Val): 0.4463; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6023/8192 --- L(Train): 0.3664; L(Val): 0.4462; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6024/8192 --- L(Train): 0.3772; L(Val): 0.4460; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6025/8192 --- L(Train): 0.3790; L(Val): 0.4455; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6026/8192 --- L(Train): 0.3761; L(Val): 0.4451; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6027/8192 --- L(Train): 0.3771; L(Val): 0.4447; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6028/8192 --- L(Train): 0.3805; L(Val): 0.4446; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6029/8192 --- L(Train): 0.3696; L(Val): 0.4446; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6030/8192 --- L(Train): 0.3728; L(Val): 0.4446; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6031/8192 --- L(Train): 0.3793; L(Val): 0.4446; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6032/8192 --- L(Train): 0.3760; L(Val): 0.4444; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6033/8192 --- L(Train): 0.3817; L(Val): 0.4441; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6034/8192 --- L(Train): 0.3734; L(Val): 0.4438; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6035/8192 --- L(Train): 0.3725; L(Val): 0.4436; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6036/8192 --- L(Train): 0.3715; L(Val): 0.4435; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6037/8192 --- L(Train): 0.3682; L(Val): 0.4432; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6038/8192 --- L(Train): 0.3667; L(Val): 0.4427; Reg Param: 0.0176; Time: 0.67\n",
      "Epoch 6039/8192 --- L(Train): 0.3688; L(Val): 0.4424; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6040/8192 --- L(Train): 0.3753; L(Val): 0.4420; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6041/8192 --- L(Train): 0.3691; L(Val): 0.4420; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6042/8192 --- L(Train): 0.3745; L(Val): 0.4423; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6043/8192 --- L(Train): 0.3725; L(Val): 0.4424; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6044/8192 --- L(Train): 0.3742; L(Val): 0.4425; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6045/8192 --- L(Train): 0.3899; L(Val): 0.4427; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6046/8192 --- L(Train): 0.3698; L(Val): 0.4430; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6047/8192 --- L(Train): 0.3700; L(Val): 0.4430; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6048/8192 --- L(Train): 0.3645; L(Val): 0.4434; Reg Param: 0.0176; Time: 0.60\n",
      "Epoch 6049/8192 --- L(Train): 0.3664; L(Val): 0.4436; Reg Param: 0.0176; Time: 0.61\n",
      "Epoch 6050/8192 --- L(Train): 0.3608; L(Val): 0.4436; Reg Param: 0.0160; Time: 6.66\n",
      "Epoch 6051/8192 --- L(Train): 0.3747; L(Val): 0.4436; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6052/8192 --- L(Train): 0.3759; L(Val): 0.4430; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6053/8192 --- L(Train): 0.3668; L(Val): 0.4426; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6054/8192 --- L(Train): 0.3828; L(Val): 0.4422; Reg Param: 0.0160; Time: 0.62\n",
      "Epoch 6055/8192 --- L(Train): 0.3829; L(Val): 0.4420; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6056/8192 --- L(Train): 0.3807; L(Val): 0.4419; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6057/8192 --- L(Train): 0.3618; L(Val): 0.4418; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6058/8192 --- L(Train): 0.3796; L(Val): 0.4421; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6059/8192 --- L(Train): 0.3664; L(Val): 0.4423; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6060/8192 --- L(Train): 0.3703; L(Val): 0.4425; Reg Param: 0.0160; Time: 0.69\n",
      "Epoch 6061/8192 --- L(Train): 0.3827; L(Val): 0.4427; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6062/8192 --- L(Train): 0.3779; L(Val): 0.4430; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6063/8192 --- L(Train): 0.3706; L(Val): 0.4430; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6064/8192 --- L(Train): 0.3799; L(Val): 0.4432; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6065/8192 --- L(Train): 0.3794; L(Val): 0.4431; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6066/8192 --- L(Train): 0.3745; L(Val): 0.4427; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6067/8192 --- L(Train): 0.3821; L(Val): 0.4422; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6068/8192 --- L(Train): 0.3770; L(Val): 0.4418; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6069/8192 --- L(Train): 0.3735; L(Val): 0.4415; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6070/8192 --- L(Train): 0.3774; L(Val): 0.4414; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6071/8192 --- L(Train): 0.3861; L(Val): 0.4416; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6072/8192 --- L(Train): 0.3770; L(Val): 0.4420; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6073/8192 --- L(Train): 0.3839; L(Val): 0.4424; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6074/8192 --- L(Train): 0.3769; L(Val): 0.4428; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6075/8192 --- L(Train): 0.3801; L(Val): 0.4433; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6076/8192 --- L(Train): 0.3786; L(Val): 0.4440; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6077/8192 --- L(Train): 0.3770; L(Val): 0.4445; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6078/8192 --- L(Train): 0.3713; L(Val): 0.4447; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6079/8192 --- L(Train): 0.3713; L(Val): 0.4446; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6080/8192 --- L(Train): 0.3667; L(Val): 0.4445; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6081/8192 --- L(Train): 0.3676; L(Val): 0.4445; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6082/8192 --- L(Train): 0.3601; L(Val): 0.4443; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6083/8192 --- L(Train): 0.3735; L(Val): 0.4441; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6084/8192 --- L(Train): 0.3743; L(Val): 0.4440; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6085/8192 --- L(Train): 0.3778; L(Val): 0.4440; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6086/8192 --- L(Train): 0.3772; L(Val): 0.4442; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6087/8192 --- L(Train): 0.3724; L(Val): 0.4440; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6088/8192 --- L(Train): 0.3740; L(Val): 0.4438; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6089/8192 --- L(Train): 0.3791; L(Val): 0.4437; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6090/8192 --- L(Train): 0.3826; L(Val): 0.4441; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6091/8192 --- L(Train): 0.3807; L(Val): 0.4440; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6092/8192 --- L(Train): 0.3684; L(Val): 0.4441; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6093/8192 --- L(Train): 0.3731; L(Val): 0.4443; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6094/8192 --- L(Train): 0.3667; L(Val): 0.4442; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6095/8192 --- L(Train): 0.3736; L(Val): 0.4439; Reg Param: 0.0160; Time: 0.61\n",
      "Epoch 6096/8192 --- L(Train): 0.3639; L(Val): 0.4436; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6097/8192 --- L(Train): 0.3766; L(Val): 0.4433; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6098/8192 --- L(Train): 0.3650; L(Val): 0.4429; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6099/8192 --- L(Train): 0.3795; L(Val): 0.4427; Reg Param: 0.0160; Time: 0.60\n",
      "Epoch 6100/8192 --- L(Train): 0.3662; L(Val): 0.4427; Reg Param: 0.0147; Time: 6.27\n",
      "Epoch 6101/8192 --- L(Train): 0.3707; L(Val): 0.4426; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6102/8192 --- L(Train): 0.3640; L(Val): 0.4429; Reg Param: 0.0147; Time: 0.61\n",
      "Epoch 6103/8192 --- L(Train): 0.3730; L(Val): 0.4433; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6104/8192 --- L(Train): 0.3832; L(Val): 0.4435; Reg Param: 0.0147; Time: 0.61\n",
      "Epoch 6105/8192 --- L(Train): 0.3659; L(Val): 0.4439; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6106/8192 --- L(Train): 0.3782; L(Val): 0.4441; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6107/8192 --- L(Train): 0.3605; L(Val): 0.4444; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6108/8192 --- L(Train): 0.3577; L(Val): 0.4448; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6109/8192 --- L(Train): 0.3816; L(Val): 0.4453; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6110/8192 --- L(Train): 0.3756; L(Val): 0.4455; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6111/8192 --- L(Train): 0.3715; L(Val): 0.4456; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6112/8192 --- L(Train): 0.3850; L(Val): 0.4453; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6113/8192 --- L(Train): 0.3790; L(Val): 0.4446; Reg Param: 0.0147; Time: 0.62\n",
      "Epoch 6114/8192 --- L(Train): 0.3855; L(Val): 0.4440; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6115/8192 --- L(Train): 0.3769; L(Val): 0.4434; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6116/8192 --- L(Train): 0.3695; L(Val): 0.4434; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6117/8192 --- L(Train): 0.3603; L(Val): 0.4434; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6118/8192 --- L(Train): 0.3658; L(Val): 0.4434; Reg Param: 0.0147; Time: 0.67\n",
      "Epoch 6119/8192 --- L(Train): 0.3715; L(Val): 0.4436; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6120/8192 --- L(Train): 0.3727; L(Val): 0.4436; Reg Param: 0.0147; Time: 0.61\n",
      "Epoch 6121/8192 --- L(Train): 0.3748; L(Val): 0.4438; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6122/8192 --- L(Train): 0.3804; L(Val): 0.4440; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6123/8192 --- L(Train): 0.3755; L(Val): 0.4443; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6124/8192 --- L(Train): 0.3738; L(Val): 0.4445; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6125/8192 --- L(Train): 0.3778; L(Val): 0.4447; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6126/8192 --- L(Train): 0.3772; L(Val): 0.4449; Reg Param: 0.0147; Time: 0.62\n",
      "Epoch 6127/8192 --- L(Train): 0.3749; L(Val): 0.4448; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6128/8192 --- L(Train): 0.3750; L(Val): 0.4444; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6129/8192 --- L(Train): 0.3673; L(Val): 0.4438; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6130/8192 --- L(Train): 0.3623; L(Val): 0.4437; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6131/8192 --- L(Train): 0.3829; L(Val): 0.4438; Reg Param: 0.0147; Time: 0.64\n",
      "Epoch 6132/8192 --- L(Train): 0.3763; L(Val): 0.4440; Reg Param: 0.0147; Time: 0.61\n",
      "Epoch 6133/8192 --- L(Train): 0.3679; L(Val): 0.4443; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6134/8192 --- L(Train): 0.3732; L(Val): 0.4446; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6135/8192 --- L(Train): 0.3676; L(Val): 0.4448; Reg Param: 0.0147; Time: 0.61\n",
      "Epoch 6136/8192 --- L(Train): 0.3765; L(Val): 0.4449; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6137/8192 --- L(Train): 0.3764; L(Val): 0.4454; Reg Param: 0.0147; Time: 0.61\n",
      "Epoch 6138/8192 --- L(Train): 0.3678; L(Val): 0.4460; Reg Param: 0.0147; Time: 0.61\n",
      "Epoch 6139/8192 --- L(Train): 0.3847; L(Val): 0.4464; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6140/8192 --- L(Train): 0.3750; L(Val): 0.4465; Reg Param: 0.0147; Time: 0.61\n",
      "Epoch 6141/8192 --- L(Train): 0.3822; L(Val): 0.4464; Reg Param: 0.0147; Time: 0.61\n",
      "Epoch 6142/8192 --- L(Train): 0.3751; L(Val): 0.4462; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6143/8192 --- L(Train): 0.3648; L(Val): 0.4457; Reg Param: 0.0147; Time: 0.70\n",
      "Epoch 6144/8192 --- L(Train): 0.3733; L(Val): 0.4450; Reg Param: 0.0147; Time: 0.62\n",
      "Epoch 6145/8192 --- L(Train): 0.3803; L(Val): 0.4445; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6146/8192 --- L(Train): 0.3699; L(Val): 0.4439; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6147/8192 --- L(Train): 0.3715; L(Val): 0.4432; Reg Param: 0.0147; Time: 0.61\n",
      "Epoch 6148/8192 --- L(Train): 0.3607; L(Val): 0.4427; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6149/8192 --- L(Train): 0.3805; L(Val): 0.4426; Reg Param: 0.0147; Time: 0.60\n",
      "Epoch 6150/8192 --- L(Train): 0.3792; L(Val): 0.4426; Reg Param: 0.0133; Time: 6.26\n",
      "Epoch 6151/8192 --- L(Train): 0.3701; L(Val): 0.4422; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6152/8192 --- L(Train): 0.3654; L(Val): 0.4425; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6153/8192 --- L(Train): 0.3771; L(Val): 0.4432; Reg Param: 0.0133; Time: 0.61\n",
      "Epoch 6154/8192 --- L(Train): 0.3744; L(Val): 0.4436; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6155/8192 --- L(Train): 0.3752; L(Val): 0.4443; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6156/8192 --- L(Train): 0.3702; L(Val): 0.4454; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6157/8192 --- L(Train): 0.3736; L(Val): 0.4461; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6158/8192 --- L(Train): 0.3716; L(Val): 0.4463; Reg Param: 0.0133; Time: 0.61\n",
      "Epoch 6159/8192 --- L(Train): 0.3724; L(Val): 0.4466; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6160/8192 --- L(Train): 0.3672; L(Val): 0.4468; Reg Param: 0.0133; Time: 0.62\n",
      "Epoch 6161/8192 --- L(Train): 0.3734; L(Val): 0.4464; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6162/8192 --- L(Train): 0.3828; L(Val): 0.4458; Reg Param: 0.0133; Time: 0.59\n",
      "Epoch 6163/8192 --- L(Train): 0.3676; L(Val): 0.4455; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6164/8192 --- L(Train): 0.3737; L(Val): 0.4454; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6165/8192 --- L(Train): 0.3743; L(Val): 0.4449; Reg Param: 0.0133; Time: 0.61\n",
      "Epoch 6166/8192 --- L(Train): 0.3833; L(Val): 0.4447; Reg Param: 0.0133; Time: 0.62\n",
      "Epoch 6167/8192 --- L(Train): 0.3703; L(Val): 0.4447; Reg Param: 0.0133; Time: 0.59\n",
      "Epoch 6168/8192 --- L(Train): 0.3571; L(Val): 0.4445; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6169/8192 --- L(Train): 0.3706; L(Val): 0.4443; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6170/8192 --- L(Train): 0.3693; L(Val): 0.4442; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6171/8192 --- L(Train): 0.3709; L(Val): 0.4443; Reg Param: 0.0133; Time: 0.61\n",
      "Epoch 6172/8192 --- L(Train): 0.3682; L(Val): 0.4440; Reg Param: 0.0133; Time: 0.61\n",
      "Epoch 6173/8192 --- L(Train): 0.3706; L(Val): 0.4438; Reg Param: 0.0133; Time: 0.61\n",
      "Epoch 6174/8192 --- L(Train): 0.3740; L(Val): 0.4440; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6175/8192 --- L(Train): 0.3855; L(Val): 0.4441; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6176/8192 --- L(Train): 0.3755; L(Val): 0.4440; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6177/8192 --- L(Train): 0.3735; L(Val): 0.4441; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6178/8192 --- L(Train): 0.3709; L(Val): 0.4443; Reg Param: 0.0133; Time: 0.61\n",
      "Epoch 6179/8192 --- L(Train): 0.3729; L(Val): 0.4443; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6180/8192 --- L(Train): 0.3832; L(Val): 0.4444; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6181/8192 --- L(Train): 0.3690; L(Val): 0.4444; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6182/8192 --- L(Train): 0.3627; L(Val): 0.4445; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6183/8192 --- L(Train): 0.3739; L(Val): 0.4445; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6184/8192 --- L(Train): 0.3641; L(Val): 0.4443; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6185/8192 --- L(Train): 0.3835; L(Val): 0.4443; Reg Param: 0.0133; Time: 0.61\n",
      "Epoch 6186/8192 --- L(Train): 0.3765; L(Val): 0.4445; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6187/8192 --- L(Train): 0.3695; L(Val): 0.4447; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6188/8192 --- L(Train): 0.3807; L(Val): 0.4448; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6189/8192 --- L(Train): 0.3710; L(Val): 0.4447; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6190/8192 --- L(Train): 0.3710; L(Val): 0.4448; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6191/8192 --- L(Train): 0.3776; L(Val): 0.4449; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6192/8192 --- L(Train): 0.3752; L(Val): 0.4449; Reg Param: 0.0133; Time: 0.59\n",
      "Epoch 6193/8192 --- L(Train): 0.3749; L(Val): 0.4449; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6194/8192 --- L(Train): 0.3754; L(Val): 0.4449; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6195/8192 --- L(Train): 0.3712; L(Val): 0.4449; Reg Param: 0.0133; Time: 0.61\n",
      "Epoch 6196/8192 --- L(Train): 0.3631; L(Val): 0.4449; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6197/8192 --- L(Train): 0.3729; L(Val): 0.4449; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6198/8192 --- L(Train): 0.3628; L(Val): 0.4448; Reg Param: 0.0133; Time: 0.61\n",
      "Epoch 6199/8192 --- L(Train): 0.3799; L(Val): 0.4447; Reg Param: 0.0133; Time: 0.60\n",
      "Epoch 6200/8192 --- L(Train): 0.3758; L(Val): 0.4447; Reg Param: 0.0118; Time: 6.33\n",
      "Epoch 6201/8192 --- L(Train): 0.3708; L(Val): 0.4446; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6202/8192 --- L(Train): 0.3730; L(Val): 0.4444; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6203/8192 --- L(Train): 0.3726; L(Val): 0.4444; Reg Param: 0.0118; Time: 0.69\n",
      "Epoch 6204/8192 --- L(Train): 0.3813; L(Val): 0.4443; Reg Param: 0.0118; Time: 0.59\n",
      "Epoch 6205/8192 --- L(Train): 0.3739; L(Val): 0.4442; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6206/8192 --- L(Train): 0.3774; L(Val): 0.4440; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6207/8192 --- L(Train): 0.3729; L(Val): 0.4438; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6208/8192 --- L(Train): 0.3698; L(Val): 0.4439; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6209/8192 --- L(Train): 0.3702; L(Val): 0.4440; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6210/8192 --- L(Train): 0.3633; L(Val): 0.4442; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6211/8192 --- L(Train): 0.3639; L(Val): 0.4444; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6212/8192 --- L(Train): 0.3867; L(Val): 0.4446; Reg Param: 0.0118; Time: 0.61\n",
      "Epoch 6213/8192 --- L(Train): 0.3644; L(Val): 0.4448; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6214/8192 --- L(Train): 0.3741; L(Val): 0.4449; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6215/8192 --- L(Train): 0.3682; L(Val): 0.4451; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6216/8192 --- L(Train): 0.3718; L(Val): 0.4454; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6217/8192 --- L(Train): 0.3698; L(Val): 0.4456; Reg Param: 0.0118; Time: 0.61\n",
      "Epoch 6218/8192 --- L(Train): 0.3759; L(Val): 0.4456; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6219/8192 --- L(Train): 0.3655; L(Val): 0.4455; Reg Param: 0.0118; Time: 0.61\n",
      "Epoch 6220/8192 --- L(Train): 0.3764; L(Val): 0.4453; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6221/8192 --- L(Train): 0.3647; L(Val): 0.4449; Reg Param: 0.0118; Time: 0.61\n",
      "Epoch 6222/8192 --- L(Train): 0.3813; L(Val): 0.4445; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6223/8192 --- L(Train): 0.3821; L(Val): 0.4442; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6224/8192 --- L(Train): 0.3772; L(Val): 0.4440; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6225/8192 --- L(Train): 0.3856; L(Val): 0.4440; Reg Param: 0.0118; Time: 0.62\n",
      "Epoch 6226/8192 --- L(Train): 0.3681; L(Val): 0.4443; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6227/8192 --- L(Train): 0.3745; L(Val): 0.4447; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6228/8192 --- L(Train): 0.3762; L(Val): 0.4451; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6229/8192 --- L(Train): 0.3659; L(Val): 0.4454; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6230/8192 --- L(Train): 0.3762; L(Val): 0.4454; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6231/8192 --- L(Train): 0.3692; L(Val): 0.4455; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6232/8192 --- L(Train): 0.3729; L(Val): 0.4457; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6233/8192 --- L(Train): 0.3724; L(Val): 0.4459; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6234/8192 --- L(Train): 0.3785; L(Val): 0.4462; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6235/8192 --- L(Train): 0.3779; L(Val): 0.4464; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6236/8192 --- L(Train): 0.3814; L(Val): 0.4463; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6237/8192 --- L(Train): 0.3706; L(Val): 0.4461; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6238/8192 --- L(Train): 0.3622; L(Val): 0.4456; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6239/8192 --- L(Train): 0.3859; L(Val): 0.4452; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6240/8192 --- L(Train): 0.3688; L(Val): 0.4458; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6241/8192 --- L(Train): 0.3714; L(Val): 0.4457; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6242/8192 --- L(Train): 0.3709; L(Val): 0.4454; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6243/8192 --- L(Train): 0.3636; L(Val): 0.4451; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6244/8192 --- L(Train): 0.3729; L(Val): 0.4450; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6245/8192 --- L(Train): 0.3675; L(Val): 0.4451; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6246/8192 --- L(Train): 0.3775; L(Val): 0.4454; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6247/8192 --- L(Train): 0.3806; L(Val): 0.4452; Reg Param: 0.0118; Time: 0.64\n",
      "Epoch 6248/8192 --- L(Train): 0.3736; L(Val): 0.4452; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6249/8192 --- L(Train): 0.3770; L(Val): 0.4451; Reg Param: 0.0118; Time: 0.60\n",
      "Epoch 6250/8192 --- L(Train): 0.3752; L(Val): 0.4451; Reg Param: 0.0098; Time: 6.27\n",
      "Epoch 6251/8192 --- L(Train): 0.3684; L(Val): 0.4448; Reg Param: 0.0098; Time: 0.59\n",
      "Epoch 6252/8192 --- L(Train): 0.3784; L(Val): 0.4448; Reg Param: 0.0098; Time: 0.64\n",
      "Epoch 6253/8192 --- L(Train): 0.3721; L(Val): 0.4448; Reg Param: 0.0098; Time: 0.61\n",
      "Epoch 6254/8192 --- L(Train): 0.3686; L(Val): 0.4448; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6255/8192 --- L(Train): 0.3748; L(Val): 0.4448; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6256/8192 --- L(Train): 0.3744; L(Val): 0.4450; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6257/8192 --- L(Train): 0.3801; L(Val): 0.4451; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6258/8192 --- L(Train): 0.3697; L(Val): 0.4451; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6259/8192 --- L(Train): 0.3652; L(Val): 0.4453; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6260/8192 --- L(Train): 0.3684; L(Val): 0.4453; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6261/8192 --- L(Train): 0.3766; L(Val): 0.4451; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6262/8192 --- L(Train): 0.3651; L(Val): 0.4452; Reg Param: 0.0098; Time: 0.71\n",
      "Epoch 6263/8192 --- L(Train): 0.3652; L(Val): 0.4453; Reg Param: 0.0098; Time: 0.63\n",
      "Epoch 6264/8192 --- L(Train): 0.3592; L(Val): 0.4452; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6265/8192 --- L(Train): 0.3790; L(Val): 0.4452; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6266/8192 --- L(Train): 0.3735; L(Val): 0.4449; Reg Param: 0.0098; Time: 0.61\n",
      "Epoch 6267/8192 --- L(Train): 0.3885; L(Val): 0.4445; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6268/8192 --- L(Train): 0.3744; L(Val): 0.4445; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6269/8192 --- L(Train): 0.3725; L(Val): 0.4446; Reg Param: 0.0098; Time: 0.59\n",
      "Epoch 6270/8192 --- L(Train): 0.3776; L(Val): 0.4449; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6271/8192 --- L(Train): 0.3695; L(Val): 0.4453; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6272/8192 --- L(Train): 0.3803; L(Val): 0.4456; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6273/8192 --- L(Train): 0.3806; L(Val): 0.4458; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6274/8192 --- L(Train): 0.3694; L(Val): 0.4460; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6275/8192 --- L(Train): 0.3797; L(Val): 0.4462; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6276/8192 --- L(Train): 0.3736; L(Val): 0.4466; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6277/8192 --- L(Train): 0.3598; L(Val): 0.4467; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6278/8192 --- L(Train): 0.3718; L(Val): 0.4466; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6279/8192 --- L(Train): 0.3754; L(Val): 0.4463; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6280/8192 --- L(Train): 0.3826; L(Val): 0.4462; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6281/8192 --- L(Train): 0.3684; L(Val): 0.4461; Reg Param: 0.0098; Time: 0.59\n",
      "Epoch 6282/8192 --- L(Train): 0.3801; L(Val): 0.4459; Reg Param: 0.0098; Time: 0.59\n",
      "Epoch 6283/8192 --- L(Train): 0.3736; L(Val): 0.4456; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6284/8192 --- L(Train): 0.3787; L(Val): 0.4452; Reg Param: 0.0098; Time: 0.61\n",
      "Epoch 6285/8192 --- L(Train): 0.3721; L(Val): 0.4446; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6286/8192 --- L(Train): 0.3874; L(Val): 0.4442; Reg Param: 0.0098; Time: 0.59\n",
      "Epoch 6287/8192 --- L(Train): 0.3791; L(Val): 0.4440; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6288/8192 --- L(Train): 0.3764; L(Val): 0.4439; Reg Param: 0.0098; Time: 0.70\n",
      "Epoch 6289/8192 --- L(Train): 0.3611; L(Val): 0.4442; Reg Param: 0.0098; Time: 0.63\n",
      "Epoch 6290/8192 --- L(Train): 0.3725; L(Val): 0.4444; Reg Param: 0.0098; Time: 0.64\n",
      "Epoch 6291/8192 --- L(Train): 0.3719; L(Val): 0.4447; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6292/8192 --- L(Train): 0.3698; L(Val): 0.4450; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6293/8192 --- L(Train): 0.3751; L(Val): 0.4454; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6294/8192 --- L(Train): 0.3828; L(Val): 0.4457; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6295/8192 --- L(Train): 0.3713; L(Val): 0.4459; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6296/8192 --- L(Train): 0.3677; L(Val): 0.4457; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6297/8192 --- L(Train): 0.3733; L(Val): 0.4453; Reg Param: 0.0098; Time: 0.61\n",
      "Epoch 6298/8192 --- L(Train): 0.3699; L(Val): 0.4449; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6299/8192 --- L(Train): 0.3771; L(Val): 0.4447; Reg Param: 0.0098; Time: 0.60\n",
      "Epoch 6300/8192 --- L(Train): 0.3581; L(Val): 0.4447; Reg Param: 0.0079; Time: 6.29\n",
      "Epoch 6301/8192 --- L(Train): 0.3752; L(Val): 0.4450; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6302/8192 --- L(Train): 0.3801; L(Val): 0.4453; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6303/8192 --- L(Train): 0.3660; L(Val): 0.4456; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6304/8192 --- L(Train): 0.3696; L(Val): 0.4459; Reg Param: 0.0079; Time: 0.61\n",
      "Epoch 6305/8192 --- L(Train): 0.3696; L(Val): 0.4462; Reg Param: 0.0079; Time: 0.62\n",
      "Epoch 6306/8192 --- L(Train): 0.3893; L(Val): 0.4462; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6307/8192 --- L(Train): 0.3835; L(Val): 0.4462; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6308/8192 --- L(Train): 0.3712; L(Val): 0.4462; Reg Param: 0.0079; Time: 0.59\n",
      "Epoch 6309/8192 --- L(Train): 0.3677; L(Val): 0.4458; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6310/8192 --- L(Train): 0.3677; L(Val): 0.4456; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6311/8192 --- L(Train): 0.3666; L(Val): 0.4458; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6312/8192 --- L(Train): 0.3726; L(Val): 0.4458; Reg Param: 0.0079; Time: 0.59\n",
      "Epoch 6313/8192 --- L(Train): 0.3629; L(Val): 0.4456; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6314/8192 --- L(Train): 0.3744; L(Val): 0.4453; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6315/8192 --- L(Train): 0.3599; L(Val): 0.4453; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6316/8192 --- L(Train): 0.3651; L(Val): 0.4455; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6317/8192 --- L(Train): 0.3695; L(Val): 0.4459; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6318/8192 --- L(Train): 0.3734; L(Val): 0.4463; Reg Param: 0.0079; Time: 0.59\n",
      "Epoch 6319/8192 --- L(Train): 0.3713; L(Val): 0.4467; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6320/8192 --- L(Train): 0.3769; L(Val): 0.4472; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6321/8192 --- L(Train): 0.3826; L(Val): 0.4474; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6322/8192 --- L(Train): 0.3701; L(Val): 0.4474; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6323/8192 --- L(Train): 0.3735; L(Val): 0.4470; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6324/8192 --- L(Train): 0.3718; L(Val): 0.4464; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6325/8192 --- L(Train): 0.3680; L(Val): 0.4461; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6326/8192 --- L(Train): 0.3824; L(Val): 0.4461; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6327/8192 --- L(Train): 0.3791; L(Val): 0.4461; Reg Param: 0.0079; Time: 0.61\n",
      "Epoch 6328/8192 --- L(Train): 0.3797; L(Val): 0.4461; Reg Param: 0.0079; Time: 0.62\n",
      "Epoch 6329/8192 --- L(Train): 0.3813; L(Val): 0.4459; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6330/8192 --- L(Train): 0.3753; L(Val): 0.4458; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6331/8192 --- L(Train): 0.3778; L(Val): 0.4461; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6332/8192 --- L(Train): 0.3723; L(Val): 0.4464; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6333/8192 --- L(Train): 0.3767; L(Val): 0.4466; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6334/8192 --- L(Train): 0.3744; L(Val): 0.4467; Reg Param: 0.0079; Time: 0.61\n",
      "Epoch 6335/8192 --- L(Train): 0.3731; L(Val): 0.4468; Reg Param: 0.0079; Time: 0.61\n",
      "Epoch 6336/8192 --- L(Train): 0.3652; L(Val): 0.4467; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6337/8192 --- L(Train): 0.3690; L(Val): 0.4465; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6338/8192 --- L(Train): 0.3714; L(Val): 0.4464; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6339/8192 --- L(Train): 0.3768; L(Val): 0.4463; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6340/8192 --- L(Train): 0.3699; L(Val): 0.4463; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6341/8192 --- L(Train): 0.3720; L(Val): 0.4464; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6342/8192 --- L(Train): 0.3703; L(Val): 0.4466; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6343/8192 --- L(Train): 0.3685; L(Val): 0.4467; Reg Param: 0.0079; Time: 0.61\n",
      "Epoch 6344/8192 --- L(Train): 0.3733; L(Val): 0.4468; Reg Param: 0.0079; Time: 0.61\n",
      "Epoch 6345/8192 --- L(Train): 0.3792; L(Val): 0.4469; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6346/8192 --- L(Train): 0.3713; L(Val): 0.4470; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6347/8192 --- L(Train): 0.3804; L(Val): 0.4469; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6348/8192 --- L(Train): 0.3841; L(Val): 0.4468; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6349/8192 --- L(Train): 0.3819; L(Val): 0.4467; Reg Param: 0.0079; Time: 0.60\n",
      "Epoch 6350/8192 --- L(Train): 0.3746; L(Val): 0.4467; Reg Param: 0.0062; Time: 6.28\n",
      "Epoch 6351/8192 --- L(Train): 0.3792; L(Val): 0.4464; Reg Param: 0.0062; Time: 0.70\n",
      "Epoch 6352/8192 --- L(Train): 0.3727; L(Val): 0.4461; Reg Param: 0.0062; Time: 0.64\n",
      "Epoch 6353/8192 --- L(Train): 0.3789; L(Val): 0.4459; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6354/8192 --- L(Train): 0.3697; L(Val): 0.4459; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6355/8192 --- L(Train): 0.3723; L(Val): 0.4458; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6356/8192 --- L(Train): 0.3705; L(Val): 0.4460; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6357/8192 --- L(Train): 0.3737; L(Val): 0.4460; Reg Param: 0.0062; Time: 0.64\n",
      "Epoch 6358/8192 --- L(Train): 0.3662; L(Val): 0.4460; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6359/8192 --- L(Train): 0.3704; L(Val): 0.4459; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6360/8192 --- L(Train): 0.3674; L(Val): 0.4460; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6361/8192 --- L(Train): 0.3699; L(Val): 0.4460; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6362/8192 --- L(Train): 0.3717; L(Val): 0.4460; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6363/8192 --- L(Train): 0.3634; L(Val): 0.4461; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6364/8192 --- L(Train): 0.3697; L(Val): 0.4462; Reg Param: 0.0062; Time: 0.59\n",
      "Epoch 6365/8192 --- L(Train): 0.3657; L(Val): 0.4463; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6366/8192 --- L(Train): 0.3749; L(Val): 0.4463; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6367/8192 --- L(Train): 0.3681; L(Val): 0.4464; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6368/8192 --- L(Train): 0.3689; L(Val): 0.4463; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6369/8192 --- L(Train): 0.3660; L(Val): 0.4462; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6370/8192 --- L(Train): 0.3751; L(Val): 0.4461; Reg Param: 0.0062; Time: 0.62\n",
      "Epoch 6371/8192 --- L(Train): 0.3729; L(Val): 0.4460; Reg Param: 0.0062; Time: 0.65\n",
      "Epoch 6372/8192 --- L(Train): 0.3662; L(Val): 0.4460; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6373/8192 --- L(Train): 0.3604; L(Val): 0.4460; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6374/8192 --- L(Train): 0.3650; L(Val): 0.4459; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6375/8192 --- L(Train): 0.3700; L(Val): 0.4458; Reg Param: 0.0062; Time: 0.59\n",
      "Epoch 6376/8192 --- L(Train): 0.3769; L(Val): 0.4457; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6377/8192 --- L(Train): 0.3716; L(Val): 0.4456; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6378/8192 --- L(Train): 0.3866; L(Val): 0.4457; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6379/8192 --- L(Train): 0.3729; L(Val): 0.4458; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6380/8192 --- L(Train): 0.3654; L(Val): 0.4459; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6381/8192 --- L(Train): 0.3691; L(Val): 0.4461; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6382/8192 --- L(Train): 0.3696; L(Val): 0.4463; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6383/8192 --- L(Train): 0.3843; L(Val): 0.4464; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6384/8192 --- L(Train): 0.3727; L(Val): 0.4463; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6385/8192 --- L(Train): 0.3670; L(Val): 0.4464; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6386/8192 --- L(Train): 0.3640; L(Val): 0.4466; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6387/8192 --- L(Train): 0.3792; L(Val): 0.4467; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6388/8192 --- L(Train): 0.3734; L(Val): 0.4469; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6389/8192 --- L(Train): 0.3806; L(Val): 0.4470; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6390/8192 --- L(Train): 0.3645; L(Val): 0.4470; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6391/8192 --- L(Train): 0.3755; L(Val): 0.4469; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6392/8192 --- L(Train): 0.3642; L(Val): 0.4466; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6393/8192 --- L(Train): 0.3654; L(Val): 0.4463; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6394/8192 --- L(Train): 0.3611; L(Val): 0.4460; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6395/8192 --- L(Train): 0.3639; L(Val): 0.4457; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6396/8192 --- L(Train): 0.3737; L(Val): 0.4455; Reg Param: 0.0062; Time: 0.61\n",
      "Epoch 6397/8192 --- L(Train): 0.3655; L(Val): 0.4453; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6398/8192 --- L(Train): 0.3716; L(Val): 0.4452; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6399/8192 --- L(Train): 0.3729; L(Val): 0.4453; Reg Param: 0.0062; Time: 0.60\n",
      "Epoch 6400/8192 --- L(Train): 0.3697; L(Val): 0.4453; Reg Param: 0.0041; Time: 6.28\n",
      "Epoch 6401/8192 --- L(Train): 0.3676; L(Val): 0.4456; Reg Param: 0.0041; Time: 0.61\n",
      "Epoch 6402/8192 --- L(Train): 0.3690; L(Val): 0.4457; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6403/8192 --- L(Train): 0.3941; L(Val): 0.4459; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6404/8192 --- L(Train): 0.3786; L(Val): 0.4462; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6405/8192 --- L(Train): 0.3658; L(Val): 0.4465; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6406/8192 --- L(Train): 0.3839; L(Val): 0.4466; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6407/8192 --- L(Train): 0.3829; L(Val): 0.4465; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6408/8192 --- L(Train): 0.3652; L(Val): 0.4463; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6409/8192 --- L(Train): 0.3712; L(Val): 0.4461; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6410/8192 --- L(Train): 0.3573; L(Val): 0.4458; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6411/8192 --- L(Train): 0.3731; L(Val): 0.4455; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6412/8192 --- L(Train): 0.3707; L(Val): 0.4453; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6413/8192 --- L(Train): 0.3652; L(Val): 0.4452; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6414/8192 --- L(Train): 0.3772; L(Val): 0.4452; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6415/8192 --- L(Train): 0.3647; L(Val): 0.4453; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6416/8192 --- L(Train): 0.3785; L(Val): 0.4454; Reg Param: 0.0041; Time: 0.59\n",
      "Epoch 6417/8192 --- L(Train): 0.3843; L(Val): 0.4456; Reg Param: 0.0041; Time: 0.61\n",
      "Epoch 6418/8192 --- L(Train): 0.3707; L(Val): 0.4457; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6419/8192 --- L(Train): 0.3665; L(Val): 0.4457; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6420/8192 --- L(Train): 0.3770; L(Val): 0.4457; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6421/8192 --- L(Train): 0.3839; L(Val): 0.4457; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6422/8192 --- L(Train): 0.3703; L(Val): 0.4457; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6423/8192 --- L(Train): 0.3693; L(Val): 0.4457; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6424/8192 --- L(Train): 0.3834; L(Val): 0.4455; Reg Param: 0.0041; Time: 0.61\n",
      "Epoch 6425/8192 --- L(Train): 0.3774; L(Val): 0.4452; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6426/8192 --- L(Train): 0.3621; L(Val): 0.4449; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6427/8192 --- L(Train): 0.3854; L(Val): 0.4447; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6428/8192 --- L(Train): 0.3709; L(Val): 0.4447; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6429/8192 --- L(Train): 0.3619; L(Val): 0.4447; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6430/8192 --- L(Train): 0.3713; L(Val): 0.4448; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6431/8192 --- L(Train): 0.3623; L(Val): 0.4450; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6432/8192 --- L(Train): 0.3717; L(Val): 0.4453; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6433/8192 --- L(Train): 0.3670; L(Val): 0.4454; Reg Param: 0.0041; Time: 0.69\n",
      "Epoch 6434/8192 --- L(Train): 0.3713; L(Val): 0.4456; Reg Param: 0.0041; Time: 0.61\n",
      "Epoch 6435/8192 --- L(Train): 0.3630; L(Val): 0.4458; Reg Param: 0.0041; Time: 0.61\n",
      "Epoch 6436/8192 --- L(Train): 0.3760; L(Val): 0.4460; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6437/8192 --- L(Train): 0.3676; L(Val): 0.4461; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6438/8192 --- L(Train): 0.3710; L(Val): 0.4462; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6439/8192 --- L(Train): 0.3786; L(Val): 0.4465; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6440/8192 --- L(Train): 0.3662; L(Val): 0.4467; Reg Param: 0.0041; Time: 0.59\n",
      "Epoch 6441/8192 --- L(Train): 0.3739; L(Val): 0.4459; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6442/8192 --- L(Train): 0.3685; L(Val): 0.4457; Reg Param: 0.0041; Time: 0.59\n",
      "Epoch 6443/8192 --- L(Train): 0.3756; L(Val): 0.4455; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6444/8192 --- L(Train): 0.3768; L(Val): 0.4454; Reg Param: 0.0041; Time: 0.59\n",
      "Epoch 6445/8192 --- L(Train): 0.3668; L(Val): 0.4455; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6446/8192 --- L(Train): 0.3889; L(Val): 0.4455; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6447/8192 --- L(Train): 0.3651; L(Val): 0.4455; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6448/8192 --- L(Train): 0.3639; L(Val): 0.4454; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6449/8192 --- L(Train): 0.3666; L(Val): 0.4453; Reg Param: 0.0041; Time: 0.60\n",
      "Epoch 6450/8192 --- L(Train): 0.3799; L(Val): 0.4453; Reg Param: 0.0024; Time: 6.63\n",
      "Epoch 6451/8192 --- L(Train): 0.3699; L(Val): 0.4455; Reg Param: 0.0024; Time: 0.66\n",
      "Epoch 6452/8192 --- L(Train): 0.3761; L(Val): 0.4460; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6453/8192 --- L(Train): 0.3730; L(Val): 0.4462; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6454/8192 --- L(Train): 0.3587; L(Val): 0.4462; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6455/8192 --- L(Train): 0.3684; L(Val): 0.4461; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6456/8192 --- L(Train): 0.3781; L(Val): 0.4458; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6457/8192 --- L(Train): 0.3711; L(Val): 0.4456; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6458/8192 --- L(Train): 0.3734; L(Val): 0.4455; Reg Param: 0.0024; Time: 0.62\n",
      "Epoch 6459/8192 --- L(Train): 0.3632; L(Val): 0.4455; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6460/8192 --- L(Train): 0.3715; L(Val): 0.4455; Reg Param: 0.0024; Time: 0.64\n",
      "Epoch 6461/8192 --- L(Train): 0.3784; L(Val): 0.4456; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6462/8192 --- L(Train): 0.3682; L(Val): 0.4455; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6463/8192 --- L(Train): 0.3571; L(Val): 0.4453; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6464/8192 --- L(Train): 0.3589; L(Val): 0.4452; Reg Param: 0.0024; Time: 0.65\n",
      "Epoch 6465/8192 --- L(Train): 0.3697; L(Val): 0.4452; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6466/8192 --- L(Train): 0.3721; L(Val): 0.4451; Reg Param: 0.0024; Time: 0.64\n",
      "Epoch 6467/8192 --- L(Train): 0.3727; L(Val): 0.4451; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6468/8192 --- L(Train): 0.3696; L(Val): 0.4452; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6469/8192 --- L(Train): 0.3760; L(Val): 0.4453; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6470/8192 --- L(Train): 0.3742; L(Val): 0.4452; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6471/8192 --- L(Train): 0.3773; L(Val): 0.4451; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6472/8192 --- L(Train): 0.3761; L(Val): 0.4451; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6473/8192 --- L(Train): 0.3683; L(Val): 0.4451; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6474/8192 --- L(Train): 0.3733; L(Val): 0.4451; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6475/8192 --- L(Train): 0.3767; L(Val): 0.4452; Reg Param: 0.0024; Time: 0.62\n",
      "Epoch 6476/8192 --- L(Train): 0.3804; L(Val): 0.4451; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6477/8192 --- L(Train): 0.3713; L(Val): 0.4449; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6478/8192 --- L(Train): 0.3690; L(Val): 0.4449; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6479/8192 --- L(Train): 0.3826; L(Val): 0.4448; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6480/8192 --- L(Train): 0.3723; L(Val): 0.4448; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6481/8192 --- L(Train): 0.3744; L(Val): 0.4448; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6482/8192 --- L(Train): 0.3814; L(Val): 0.4451; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6483/8192 --- L(Train): 0.3712; L(Val): 0.4456; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6484/8192 --- L(Train): 0.3743; L(Val): 0.4458; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6485/8192 --- L(Train): 0.3689; L(Val): 0.4460; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6486/8192 --- L(Train): 0.3808; L(Val): 0.4462; Reg Param: 0.0024; Time: 0.59\n",
      "Epoch 6487/8192 --- L(Train): 0.3709; L(Val): 0.4460; Reg Param: 0.0024; Time: 0.59\n",
      "Epoch 6488/8192 --- L(Train): 0.3743; L(Val): 0.4458; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6489/8192 --- L(Train): 0.3754; L(Val): 0.4457; Reg Param: 0.0024; Time: 0.67\n",
      "Epoch 6490/8192 --- L(Train): 0.3657; L(Val): 0.4456; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6491/8192 --- L(Train): 0.3735; L(Val): 0.4456; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6492/8192 --- L(Train): 0.3637; L(Val): 0.4457; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6493/8192 --- L(Train): 0.3807; L(Val): 0.4457; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6494/8192 --- L(Train): 0.3698; L(Val): 0.4459; Reg Param: 0.0024; Time: 0.61\n",
      "Epoch 6495/8192 --- L(Train): 0.3620; L(Val): 0.4461; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6496/8192 --- L(Train): 0.3638; L(Val): 0.4464; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6497/8192 --- L(Train): 0.3686; L(Val): 0.4467; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6498/8192 --- L(Train): 0.3733; L(Val): 0.4469; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6499/8192 --- L(Train): 0.3729; L(Val): 0.4472; Reg Param: 0.0024; Time: 0.60\n",
      "Epoch 6500/8192 --- L(Train): 0.3721; L(Val): 0.4472; Reg Param: 0.0001; Time: 6.37\n",
      "Epoch 6501/8192 --- L(Train): 0.3644; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6502/8192 --- L(Train): 0.3671; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6503/8192 --- L(Train): 0.3668; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6504/8192 --- L(Train): 0.3721; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6505/8192 --- L(Train): 0.3809; L(Val): 0.4470; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6506/8192 --- L(Train): 0.3677; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6507/8192 --- L(Train): 0.3702; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6508/8192 --- L(Train): 0.3716; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6509/8192 --- L(Train): 0.3736; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6510/8192 --- L(Train): 0.3665; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6511/8192 --- L(Train): 0.3689; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6512/8192 --- L(Train): 0.3616; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6513/8192 --- L(Train): 0.3791; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6514/8192 --- L(Train): 0.3643; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6515/8192 --- L(Train): 0.3613; L(Val): 0.4470; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6516/8192 --- L(Train): 0.3770; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6517/8192 --- L(Train): 0.3667; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6518/8192 --- L(Train): 0.3720; L(Val): 0.4477; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6519/8192 --- L(Train): 0.3638; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6520/8192 --- L(Train): 0.3591; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6521/8192 --- L(Train): 0.3707; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6522/8192 --- L(Train): 0.3583; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6523/8192 --- L(Train): 0.3648; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6524/8192 --- L(Train): 0.3641; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6525/8192 --- L(Train): 0.3758; L(Val): 0.4477; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6526/8192 --- L(Train): 0.3564; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6527/8192 --- L(Train): 0.3662; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6528/8192 --- L(Train): 0.3783; L(Val): 0.4470; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6529/8192 --- L(Train): 0.3683; L(Val): 0.4469; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6530/8192 --- L(Train): 0.3608; L(Val): 0.4470; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6531/8192 --- L(Train): 0.3653; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6532/8192 --- L(Train): 0.3738; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6533/8192 --- L(Train): 0.3698; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6534/8192 --- L(Train): 0.3720; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6535/8192 --- L(Train): 0.3724; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6536/8192 --- L(Train): 0.3644; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6537/8192 --- L(Train): 0.3620; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6538/8192 --- L(Train): 0.3703; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6539/8192 --- L(Train): 0.3765; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6540/8192 --- L(Train): 0.3763; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6541/8192 --- L(Train): 0.3733; L(Val): 0.4470; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 6542/8192 --- L(Train): 0.3737; L(Val): 0.4467; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 6543/8192 --- L(Train): 0.3856; L(Val): 0.4465; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6544/8192 --- L(Train): 0.3736; L(Val): 0.4463; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6545/8192 --- L(Train): 0.3699; L(Val): 0.4464; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6546/8192 --- L(Train): 0.3601; L(Val): 0.4465; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6547/8192 --- L(Train): 0.3727; L(Val): 0.4465; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6548/8192 --- L(Train): 0.3727; L(Val): 0.4465; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6549/8192 --- L(Train): 0.3711; L(Val): 0.4465; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6550/8192 --- L(Train): 0.3734; L(Val): 0.4465; Reg Param: 0.0001; Time: 6.31\n",
      "Epoch 6551/8192 --- L(Train): 0.3666; L(Val): 0.4469; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6552/8192 --- L(Train): 0.3729; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6553/8192 --- L(Train): 0.3729; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6554/8192 --- L(Train): 0.3837; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6555/8192 --- L(Train): 0.3832; L(Val): 0.4470; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 6556/8192 --- L(Train): 0.3664; L(Val): 0.4469; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6557/8192 --- L(Train): 0.3745; L(Val): 0.4467; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6558/8192 --- L(Train): 0.3674; L(Val): 0.4467; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6559/8192 --- L(Train): 0.3693; L(Val): 0.4467; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6560/8192 --- L(Train): 0.3670; L(Val): 0.4466; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6561/8192 --- L(Train): 0.3691; L(Val): 0.4467; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6562/8192 --- L(Train): 0.3686; L(Val): 0.4467; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6563/8192 --- L(Train): 0.3571; L(Val): 0.4464; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6564/8192 --- L(Train): 0.3808; L(Val): 0.4461; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6565/8192 --- L(Train): 0.3710; L(Val): 0.4461; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6566/8192 --- L(Train): 0.3791; L(Val): 0.4462; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6567/8192 --- L(Train): 0.3692; L(Val): 0.4464; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6568/8192 --- L(Train): 0.3614; L(Val): 0.4467; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6569/8192 --- L(Train): 0.3659; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6570/8192 --- L(Train): 0.3713; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6571/8192 --- L(Train): 0.3754; L(Val): 0.4477; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6572/8192 --- L(Train): 0.3757; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6573/8192 --- L(Train): 0.3575; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6574/8192 --- L(Train): 0.3614; L(Val): 0.4484; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 6575/8192 --- L(Train): 0.3708; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6576/8192 --- L(Train): 0.3676; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6577/8192 --- L(Train): 0.3863; L(Val): 0.4491; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 6578/8192 --- L(Train): 0.3806; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6579/8192 --- L(Train): 0.3778; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6580/8192 --- L(Train): 0.3799; L(Val): 0.4486; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6581/8192 --- L(Train): 0.3676; L(Val): 0.4484; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6582/8192 --- L(Train): 0.3827; L(Val): 0.4482; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6583/8192 --- L(Train): 0.3697; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6584/8192 --- L(Train): 0.3714; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6585/8192 --- L(Train): 0.3721; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6586/8192 --- L(Train): 0.3723; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6587/8192 --- L(Train): 0.3624; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6588/8192 --- L(Train): 0.3677; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6589/8192 --- L(Train): 0.3826; L(Val): 0.4478; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6590/8192 --- L(Train): 0.3629; L(Val): 0.4477; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6591/8192 --- L(Train): 0.3704; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6592/8192 --- L(Train): 0.3838; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6593/8192 --- L(Train): 0.3637; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6594/8192 --- L(Train): 0.3786; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6595/8192 --- L(Train): 0.3780; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6596/8192 --- L(Train): 0.3738; L(Val): 0.4468; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6597/8192 --- L(Train): 0.3680; L(Val): 0.4466; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6598/8192 --- L(Train): 0.3688; L(Val): 0.4465; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6599/8192 --- L(Train): 0.3645; L(Val): 0.4464; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6600/8192 --- L(Train): 0.3843; L(Val): 0.4464; Reg Param: 0.0001; Time: 6.28\n",
      "Epoch 6601/8192 --- L(Train): 0.3697; L(Val): 0.4465; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6602/8192 --- L(Train): 0.3676; L(Val): 0.4465; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6603/8192 --- L(Train): 0.3720; L(Val): 0.4467; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6604/8192 --- L(Train): 0.3766; L(Val): 0.4469; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6605/8192 --- L(Train): 0.3822; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6606/8192 --- L(Train): 0.3738; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6607/8192 --- L(Train): 0.3656; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6608/8192 --- L(Train): 0.3759; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6609/8192 --- L(Train): 0.3637; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6610/8192 --- L(Train): 0.3692; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6611/8192 --- L(Train): 0.3742; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6612/8192 --- L(Train): 0.3651; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6613/8192 --- L(Train): 0.3720; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6614/8192 --- L(Train): 0.3691; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6615/8192 --- L(Train): 0.3646; L(Val): 0.4470; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6616/8192 --- L(Train): 0.3862; L(Val): 0.4469; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6617/8192 --- L(Train): 0.3721; L(Val): 0.4467; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6618/8192 --- L(Train): 0.3841; L(Val): 0.4466; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6619/8192 --- L(Train): 0.3787; L(Val): 0.4466; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6620/8192 --- L(Train): 0.3657; L(Val): 0.4466; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6621/8192 --- L(Train): 0.3730; L(Val): 0.4464; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6622/8192 --- L(Train): 0.3750; L(Val): 0.4463; Reg Param: 0.0001; Time: 0.66\n",
      "Epoch 6623/8192 --- L(Train): 0.3779; L(Val): 0.4463; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6624/8192 --- L(Train): 0.3614; L(Val): 0.4460; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6625/8192 --- L(Train): 0.3797; L(Val): 0.4457; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6626/8192 --- L(Train): 0.3707; L(Val): 0.4456; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6627/8192 --- L(Train): 0.3738; L(Val): 0.4456; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6628/8192 --- L(Train): 0.3819; L(Val): 0.4457; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6629/8192 --- L(Train): 0.3733; L(Val): 0.4458; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6630/8192 --- L(Train): 0.3684; L(Val): 0.4460; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6631/8192 --- L(Train): 0.3821; L(Val): 0.4464; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6632/8192 --- L(Train): 0.3776; L(Val): 0.4468; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6633/8192 --- L(Train): 0.3610; L(Val): 0.4470; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6634/8192 --- L(Train): 0.3736; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6635/8192 --- L(Train): 0.3709; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6636/8192 --- L(Train): 0.3800; L(Val): 0.4476; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6637/8192 --- L(Train): 0.3649; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6638/8192 --- L(Train): 0.3678; L(Val): 0.4476; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6639/8192 --- L(Train): 0.3715; L(Val): 0.4478; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6640/8192 --- L(Train): 0.3740; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6641/8192 --- L(Train): 0.3655; L(Val): 0.4483; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6642/8192 --- L(Train): 0.3676; L(Val): 0.4486; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6643/8192 --- L(Train): 0.3644; L(Val): 0.4486; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6644/8192 --- L(Train): 0.3667; L(Val): 0.4486; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6645/8192 --- L(Train): 0.3659; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6646/8192 --- L(Train): 0.3876; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6647/8192 --- L(Train): 0.3773; L(Val): 0.4485; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6648/8192 --- L(Train): 0.3665; L(Val): 0.4482; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6649/8192 --- L(Train): 0.3681; L(Val): 0.4483; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6650/8192 --- L(Train): 0.3665; L(Val): 0.4483; Reg Param: 0.0001; Time: 6.56\n",
      "Epoch 6651/8192 --- L(Train): 0.3804; L(Val): 0.4485; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6652/8192 --- L(Train): 0.3702; L(Val): 0.4484; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6653/8192 --- L(Train): 0.3638; L(Val): 0.4482; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6654/8192 --- L(Train): 0.3704; L(Val): 0.4482; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6655/8192 --- L(Train): 0.3752; L(Val): 0.4484; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6656/8192 --- L(Train): 0.3723; L(Val): 0.4483; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6657/8192 --- L(Train): 0.3674; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6658/8192 --- L(Train): 0.3656; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6659/8192 --- L(Train): 0.3682; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6660/8192 --- L(Train): 0.3745; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6661/8192 --- L(Train): 0.3810; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6662/8192 --- L(Train): 0.3574; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6663/8192 --- L(Train): 0.3695; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6664/8192 --- L(Train): 0.3746; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6665/8192 --- L(Train): 0.3595; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6666/8192 --- L(Train): 0.3683; L(Val): 0.4482; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6667/8192 --- L(Train): 0.3622; L(Val): 0.4483; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6668/8192 --- L(Train): 0.3733; L(Val): 0.4485; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6669/8192 --- L(Train): 0.3766; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6670/8192 --- L(Train): 0.3703; L(Val): 0.4488; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6671/8192 --- L(Train): 0.3658; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6672/8192 --- L(Train): 0.3687; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6673/8192 --- L(Train): 0.3796; L(Val): 0.4488; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6674/8192 --- L(Train): 0.3762; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6675/8192 --- L(Train): 0.3759; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6676/8192 --- L(Train): 0.3696; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6677/8192 --- L(Train): 0.3811; L(Val): 0.4488; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6678/8192 --- L(Train): 0.3710; L(Val): 0.4485; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6679/8192 --- L(Train): 0.3609; L(Val): 0.4483; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6680/8192 --- L(Train): 0.3674; L(Val): 0.4477; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6681/8192 --- L(Train): 0.3734; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6682/8192 --- L(Train): 0.3717; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6683/8192 --- L(Train): 0.3629; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6684/8192 --- L(Train): 0.3718; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 6685/8192 --- L(Train): 0.3837; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6686/8192 --- L(Train): 0.3762; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6687/8192 --- L(Train): 0.3705; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6688/8192 --- L(Train): 0.3765; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6689/8192 --- L(Train): 0.3672; L(Val): 0.4477; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6690/8192 --- L(Train): 0.3787; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6691/8192 --- L(Train): 0.3680; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6692/8192 --- L(Train): 0.3675; L(Val): 0.4483; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6693/8192 --- L(Train): 0.3749; L(Val): 0.4486; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6694/8192 --- L(Train): 0.3787; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6695/8192 --- L(Train): 0.3710; L(Val): 0.4488; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6696/8192 --- L(Train): 0.3741; L(Val): 0.4488; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6697/8192 --- L(Train): 0.3687; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6698/8192 --- L(Train): 0.3676; L(Val): 0.4486; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6699/8192 --- L(Train): 0.3785; L(Val): 0.4485; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6700/8192 --- L(Train): 0.3810; L(Val): 0.4485; Reg Param: 0.0001; Time: 6.27\n",
      "Epoch 6701/8192 --- L(Train): 0.3773; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6702/8192 --- L(Train): 0.3732; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6703/8192 --- L(Train): 0.3670; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 6704/8192 --- L(Train): 0.3639; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6705/8192 --- L(Train): 0.3616; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6706/8192 --- L(Train): 0.3788; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6707/8192 --- L(Train): 0.3731; L(Val): 0.4478; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6708/8192 --- L(Train): 0.3600; L(Val): 0.4478; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6709/8192 --- L(Train): 0.3752; L(Val): 0.4477; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6710/8192 --- L(Train): 0.3752; L(Val): 0.4477; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6711/8192 --- L(Train): 0.3710; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6712/8192 --- L(Train): 0.3746; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6713/8192 --- L(Train): 0.3828; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6714/8192 --- L(Train): 0.3804; L(Val): 0.4483; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6715/8192 --- L(Train): 0.3796; L(Val): 0.4483; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6716/8192 --- L(Train): 0.3576; L(Val): 0.4482; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6717/8192 --- L(Train): 0.3643; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6718/8192 --- L(Train): 0.3688; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6719/8192 --- L(Train): 0.3653; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 6720/8192 --- L(Train): 0.3683; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6721/8192 --- L(Train): 0.3527; L(Val): 0.4482; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6722/8192 --- L(Train): 0.3828; L(Val): 0.4482; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6723/8192 --- L(Train): 0.3716; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6724/8192 --- L(Train): 0.3694; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6725/8192 --- L(Train): 0.3789; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6726/8192 --- L(Train): 0.3512; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6727/8192 --- L(Train): 0.3873; L(Val): 0.4484; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6728/8192 --- L(Train): 0.3848; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6729/8192 --- L(Train): 0.3697; L(Val): 0.4485; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6730/8192 --- L(Train): 0.3833; L(Val): 0.4485; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6731/8192 --- L(Train): 0.3771; L(Val): 0.4484; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6732/8192 --- L(Train): 0.3743; L(Val): 0.4482; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6733/8192 --- L(Train): 0.3705; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6734/8192 --- L(Train): 0.3702; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6735/8192 --- L(Train): 0.3706; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6736/8192 --- L(Train): 0.3759; L(Val): 0.4478; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6737/8192 --- L(Train): 0.3678; L(Val): 0.4476; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6738/8192 --- L(Train): 0.3679; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6739/8192 --- L(Train): 0.3712; L(Val): 0.4470; Reg Param: 0.0001; Time: 0.66\n",
      "Epoch 6740/8192 --- L(Train): 0.3729; L(Val): 0.4467; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 6741/8192 --- L(Train): 0.3710; L(Val): 0.4465; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6742/8192 --- L(Train): 0.3702; L(Val): 0.4465; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6743/8192 --- L(Train): 0.3727; L(Val): 0.4466; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6744/8192 --- L(Train): 0.3732; L(Val): 0.4467; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6745/8192 --- L(Train): 0.3565; L(Val): 0.4468; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6746/8192 --- L(Train): 0.3726; L(Val): 0.4470; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6747/8192 --- L(Train): 0.3746; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6748/8192 --- L(Train): 0.3749; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6749/8192 --- L(Train): 0.3721; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6750/8192 --- L(Train): 0.3688; L(Val): 0.4479; Reg Param: 0.0001; Time: 6.35\n",
      "Epoch 6751/8192 --- L(Train): 0.3775; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6752/8192 --- L(Train): 0.3719; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6753/8192 --- L(Train): 0.3703; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6754/8192 --- L(Train): 0.3786; L(Val): 0.4481; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6755/8192 --- L(Train): 0.3692; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 6756/8192 --- L(Train): 0.3672; L(Val): 0.4478; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 6757/8192 --- L(Train): 0.3673; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6758/8192 --- L(Train): 0.3708; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6759/8192 --- L(Train): 0.3736; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6760/8192 --- L(Train): 0.3668; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6761/8192 --- L(Train): 0.3714; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6762/8192 --- L(Train): 0.3663; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6763/8192 --- L(Train): 0.3762; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 6764/8192 --- L(Train): 0.3746; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 6765/8192 --- L(Train): 0.3666; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6766/8192 --- L(Train): 0.3700; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6767/8192 --- L(Train): 0.3613; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6768/8192 --- L(Train): 0.3645; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6769/8192 --- L(Train): 0.3773; L(Val): 0.4470; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6770/8192 --- L(Train): 0.3708; L(Val): 0.4469; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6771/8192 --- L(Train): 0.3730; L(Val): 0.4468; Reg Param: 0.0001; Time: 0.65\n",
      "Epoch 6772/8192 --- L(Train): 0.3685; L(Val): 0.4470; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6773/8192 --- L(Train): 0.3675; L(Val): 0.4474; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6774/8192 --- L(Train): 0.3636; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6775/8192 --- L(Train): 0.3638; L(Val): 0.4483; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6776/8192 --- L(Train): 0.3666; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 6777/8192 --- L(Train): 0.3802; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6778/8192 --- L(Train): 0.3675; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6779/8192 --- L(Train): 0.3592; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6780/8192 --- L(Train): 0.3592; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6781/8192 --- L(Train): 0.3686; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6782/8192 --- L(Train): 0.3723; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6783/8192 --- L(Train): 0.3689; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6784/8192 --- L(Train): 0.3790; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6785/8192 --- L(Train): 0.3588; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6786/8192 --- L(Train): 0.3586; L(Val): 0.4513; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6787/8192 --- L(Train): 0.3661; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6788/8192 --- L(Train): 0.3676; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6789/8192 --- L(Train): 0.3725; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6790/8192 --- L(Train): 0.3660; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6791/8192 --- L(Train): 0.3712; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6792/8192 --- L(Train): 0.3703; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6793/8192 --- L(Train): 0.3634; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 6794/8192 --- L(Train): 0.3789; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6795/8192 --- L(Train): 0.3776; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6796/8192 --- L(Train): 0.3813; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6797/8192 --- L(Train): 0.3653; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6798/8192 --- L(Train): 0.3840; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6799/8192 --- L(Train): 0.3703; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6800/8192 --- L(Train): 0.3746; L(Val): 0.4489; Reg Param: 0.0001; Time: 6.40\n",
      "Epoch 6801/8192 --- L(Train): 0.3727; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6802/8192 --- L(Train): 0.3592; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6803/8192 --- L(Train): 0.3590; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6804/8192 --- L(Train): 0.3793; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6805/8192 --- L(Train): 0.3693; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6806/8192 --- L(Train): 0.3672; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6807/8192 --- L(Train): 0.3718; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6808/8192 --- L(Train): 0.3671; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6809/8192 --- L(Train): 0.3749; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6810/8192 --- L(Train): 0.3639; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6811/8192 --- L(Train): 0.3604; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6812/8192 --- L(Train): 0.3657; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6813/8192 --- L(Train): 0.3740; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6814/8192 --- L(Train): 0.3792; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6815/8192 --- L(Train): 0.3743; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.66\n",
      "Epoch 6816/8192 --- L(Train): 0.3716; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.72\n",
      "Epoch 6817/8192 --- L(Train): 0.3731; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6818/8192 --- L(Train): 0.3694; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6819/8192 --- L(Train): 0.3636; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6820/8192 --- L(Train): 0.3746; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 6821/8192 --- L(Train): 0.3807; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6822/8192 --- L(Train): 0.3674; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6823/8192 --- L(Train): 0.3730; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6824/8192 --- L(Train): 0.3632; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6825/8192 --- L(Train): 0.3647; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6826/8192 --- L(Train): 0.3647; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6827/8192 --- L(Train): 0.3613; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6828/8192 --- L(Train): 0.3679; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6829/8192 --- L(Train): 0.3755; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6830/8192 --- L(Train): 0.3678; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6831/8192 --- L(Train): 0.3748; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6832/8192 --- L(Train): 0.3657; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6833/8192 --- L(Train): 0.3713; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6834/8192 --- L(Train): 0.3851; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6835/8192 --- L(Train): 0.3554; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6836/8192 --- L(Train): 0.3684; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6837/8192 --- L(Train): 0.3623; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6838/8192 --- L(Train): 0.3774; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6839/8192 --- L(Train): 0.3767; L(Val): 0.4513; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6840/8192 --- L(Train): 0.3744; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6841/8192 --- L(Train): 0.3636; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6842/8192 --- L(Train): 0.3609; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6843/8192 --- L(Train): 0.3682; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6844/8192 --- L(Train): 0.3710; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6845/8192 --- L(Train): 0.3710; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6846/8192 --- L(Train): 0.3760; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6847/8192 --- L(Train): 0.3653; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6848/8192 --- L(Train): 0.3673; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6849/8192 --- L(Train): 0.3616; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6850/8192 --- L(Train): 0.3779; L(Val): 0.4514; Reg Param: 0.0001; Time: 6.26\n",
      "Epoch 6851/8192 --- L(Train): 0.3665; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6852/8192 --- L(Train): 0.3731; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6853/8192 --- L(Train): 0.3618; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6854/8192 --- L(Train): 0.3626; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6855/8192 --- L(Train): 0.3682; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6856/8192 --- L(Train): 0.3690; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6857/8192 --- L(Train): 0.3635; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6858/8192 --- L(Train): 0.3617; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6859/8192 --- L(Train): 0.3651; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6860/8192 --- L(Train): 0.3740; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 6861/8192 --- L(Train): 0.3633; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6862/8192 --- L(Train): 0.3648; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6863/8192 --- L(Train): 0.3551; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6864/8192 --- L(Train): 0.3665; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6865/8192 --- L(Train): 0.3675; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6866/8192 --- L(Train): 0.3690; L(Val): 0.4485; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6867/8192 --- L(Train): 0.3673; L(Val): 0.4479; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6868/8192 --- L(Train): 0.3735; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6869/8192 --- L(Train): 0.3739; L(Val): 0.4472; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6870/8192 --- L(Train): 0.3726; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6871/8192 --- L(Train): 0.3667; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6872/8192 --- L(Train): 0.3705; L(Val): 0.4471; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6873/8192 --- L(Train): 0.3640; L(Val): 0.4473; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 6874/8192 --- L(Train): 0.3736; L(Val): 0.4475; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6875/8192 --- L(Train): 0.3675; L(Val): 0.4478; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6876/8192 --- L(Train): 0.3729; L(Val): 0.4480; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6877/8192 --- L(Train): 0.3656; L(Val): 0.4483; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6878/8192 --- L(Train): 0.3793; L(Val): 0.4485; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6879/8192 --- L(Train): 0.3614; L(Val): 0.4486; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 6880/8192 --- L(Train): 0.3705; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6881/8192 --- L(Train): 0.3800; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6882/8192 --- L(Train): 0.3667; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.72\n",
      "Epoch 6883/8192 --- L(Train): 0.3774; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 6884/8192 --- L(Train): 0.3693; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.65\n",
      "Epoch 6885/8192 --- L(Train): 0.3668; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 6886/8192 --- L(Train): 0.3639; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 6887/8192 --- L(Train): 0.3631; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6888/8192 --- L(Train): 0.3712; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6889/8192 --- L(Train): 0.3653; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6890/8192 --- L(Train): 0.3727; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.65\n",
      "Epoch 6891/8192 --- L(Train): 0.3812; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 6892/8192 --- L(Train): 0.3683; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 6893/8192 --- L(Train): 0.3699; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 6894/8192 --- L(Train): 0.3598; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 6895/8192 --- L(Train): 0.3700; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 6896/8192 --- L(Train): 0.3742; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.72\n",
      "Epoch 6897/8192 --- L(Train): 0.3646; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.66\n",
      "Epoch 6898/8192 --- L(Train): 0.3627; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6899/8192 --- L(Train): 0.3717; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6900/8192 --- L(Train): 0.3669; L(Val): 0.4493; Reg Param: 0.0001; Time: 6.30\n",
      "Epoch 6901/8192 --- L(Train): 0.3719; L(Val): 0.4491; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6902/8192 --- L(Train): 0.3812; L(Val): 0.4491; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6903/8192 --- L(Train): 0.3638; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6904/8192 --- L(Train): 0.3687; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6905/8192 --- L(Train): 0.3630; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6906/8192 --- L(Train): 0.3669; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6907/8192 --- L(Train): 0.3817; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6908/8192 --- L(Train): 0.3786; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6909/8192 --- L(Train): 0.3681; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6910/8192 --- L(Train): 0.3824; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6911/8192 --- L(Train): 0.3824; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6912/8192 --- L(Train): 0.3642; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6913/8192 --- L(Train): 0.3710; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.65\n",
      "Epoch 6914/8192 --- L(Train): 0.3675; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6915/8192 --- L(Train): 0.3812; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6916/8192 --- L(Train): 0.3591; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6917/8192 --- L(Train): 0.3764; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6918/8192 --- L(Train): 0.3688; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6919/8192 --- L(Train): 0.3686; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6920/8192 --- L(Train): 0.3650; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6921/8192 --- L(Train): 0.3692; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6922/8192 --- L(Train): 0.3699; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6923/8192 --- L(Train): 0.3738; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6924/8192 --- L(Train): 0.3627; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6925/8192 --- L(Train): 0.3712; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6926/8192 --- L(Train): 0.3640; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6927/8192 --- L(Train): 0.3667; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6928/8192 --- L(Train): 0.3643; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6929/8192 --- L(Train): 0.3715; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6930/8192 --- L(Train): 0.3684; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6931/8192 --- L(Train): 0.3727; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6932/8192 --- L(Train): 0.3728; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6933/8192 --- L(Train): 0.3774; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6934/8192 --- L(Train): 0.3764; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6935/8192 --- L(Train): 0.3810; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6936/8192 --- L(Train): 0.3709; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6937/8192 --- L(Train): 0.3676; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6938/8192 --- L(Train): 0.3626; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6939/8192 --- L(Train): 0.3758; L(Val): 0.4488; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6940/8192 --- L(Train): 0.3791; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6941/8192 --- L(Train): 0.3677; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6942/8192 --- L(Train): 0.3732; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6943/8192 --- L(Train): 0.3668; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 6944/8192 --- L(Train): 0.3725; L(Val): 0.4488; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6945/8192 --- L(Train): 0.3781; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6946/8192 --- L(Train): 0.3756; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6947/8192 --- L(Train): 0.3714; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6948/8192 --- L(Train): 0.3674; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6949/8192 --- L(Train): 0.3744; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6950/8192 --- L(Train): 0.3709; L(Val): 0.4498; Reg Param: 0.0001; Time: 6.27\n",
      "Epoch 6951/8192 --- L(Train): 0.3737; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6952/8192 --- L(Train): 0.3620; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 6953/8192 --- L(Train): 0.3679; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 6954/8192 --- L(Train): 0.3600; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6955/8192 --- L(Train): 0.3645; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6956/8192 --- L(Train): 0.3673; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6957/8192 --- L(Train): 0.3619; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6958/8192 --- L(Train): 0.3749; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6959/8192 --- L(Train): 0.3794; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6960/8192 --- L(Train): 0.3511; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6961/8192 --- L(Train): 0.3782; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6962/8192 --- L(Train): 0.3762; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 6963/8192 --- L(Train): 0.3740; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 6964/8192 --- L(Train): 0.3622; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6965/8192 --- L(Train): 0.3610; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6966/8192 --- L(Train): 0.3721; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6967/8192 --- L(Train): 0.3683; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6968/8192 --- L(Train): 0.3748; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6969/8192 --- L(Train): 0.3656; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6970/8192 --- L(Train): 0.3727; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6971/8192 --- L(Train): 0.3658; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6972/8192 --- L(Train): 0.3734; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6973/8192 --- L(Train): 0.3697; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 6974/8192 --- L(Train): 0.3741; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6975/8192 --- L(Train): 0.3746; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6976/8192 --- L(Train): 0.3826; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6977/8192 --- L(Train): 0.3686; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6978/8192 --- L(Train): 0.3793; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6979/8192 --- L(Train): 0.3858; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6980/8192 --- L(Train): 0.3694; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6981/8192 --- L(Train): 0.3732; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6982/8192 --- L(Train): 0.3671; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6983/8192 --- L(Train): 0.3607; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6984/8192 --- L(Train): 0.3757; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6985/8192 --- L(Train): 0.3809; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6986/8192 --- L(Train): 0.3750; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6987/8192 --- L(Train): 0.3675; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6988/8192 --- L(Train): 0.3741; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6989/8192 --- L(Train): 0.3785; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6990/8192 --- L(Train): 0.3908; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6991/8192 --- L(Train): 0.3719; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6992/8192 --- L(Train): 0.3699; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6993/8192 --- L(Train): 0.3783; L(Val): 0.4491; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6994/8192 --- L(Train): 0.3659; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6995/8192 --- L(Train): 0.3691; L(Val): 0.4491; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 6996/8192 --- L(Train): 0.3606; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6997/8192 --- L(Train): 0.3743; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6998/8192 --- L(Train): 0.3716; L(Val): 0.4488; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 6999/8192 --- L(Train): 0.3702; L(Val): 0.4486; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7000/8192 --- L(Train): 0.3880; L(Val): 0.4486; Reg Param: 0.0001; Time: 6.30\n",
      "Epoch 7001/8192 --- L(Train): 0.3685; L(Val): 0.4486; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7002/8192 --- L(Train): 0.3767; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7003/8192 --- L(Train): 0.3614; L(Val): 0.4488; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 7004/8192 --- L(Train): 0.3656; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7005/8192 --- L(Train): 0.3637; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7006/8192 --- L(Train): 0.3712; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7007/8192 --- L(Train): 0.3782; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7008/8192 --- L(Train): 0.3684; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7009/8192 --- L(Train): 0.3740; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7010/8192 --- L(Train): 0.3706; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7011/8192 --- L(Train): 0.3745; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7012/8192 --- L(Train): 0.3792; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7013/8192 --- L(Train): 0.3707; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7014/8192 --- L(Train): 0.3593; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7015/8192 --- L(Train): 0.3757; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7016/8192 --- L(Train): 0.3755; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7017/8192 --- L(Train): 0.3773; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7018/8192 --- L(Train): 0.3826; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7019/8192 --- L(Train): 0.3737; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7020/8192 --- L(Train): 0.3751; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7021/8192 --- L(Train): 0.3743; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7022/8192 --- L(Train): 0.3705; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7023/8192 --- L(Train): 0.3828; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7024/8192 --- L(Train): 0.3687; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7025/8192 --- L(Train): 0.3563; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7026/8192 --- L(Train): 0.3714; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7027/8192 --- L(Train): 0.3728; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7028/8192 --- L(Train): 0.3606; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7029/8192 --- L(Train): 0.3782; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7030/8192 --- L(Train): 0.3676; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7031/8192 --- L(Train): 0.3629; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7032/8192 --- L(Train): 0.3747; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7033/8192 --- L(Train): 0.3669; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7034/8192 --- L(Train): 0.3689; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7035/8192 --- L(Train): 0.3769; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7036/8192 --- L(Train): 0.3605; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7037/8192 --- L(Train): 0.3781; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7038/8192 --- L(Train): 0.3796; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7039/8192 --- L(Train): 0.3681; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7040/8192 --- L(Train): 0.3708; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7041/8192 --- L(Train): 0.3602; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7042/8192 --- L(Train): 0.3742; L(Val): 0.4491; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7043/8192 --- L(Train): 0.3598; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.66\n",
      "Epoch 7044/8192 --- L(Train): 0.3727; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7045/8192 --- L(Train): 0.3703; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7046/8192 --- L(Train): 0.3670; L(Val): 0.4491; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7047/8192 --- L(Train): 0.3783; L(Val): 0.4491; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7048/8192 --- L(Train): 0.3798; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7049/8192 --- L(Train): 0.3757; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7050/8192 --- L(Train): 0.3701; L(Val): 0.4492; Reg Param: 0.0001; Time: 6.31\n",
      "Epoch 7051/8192 --- L(Train): 0.3644; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7052/8192 --- L(Train): 0.3640; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7053/8192 --- L(Train): 0.3663; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7054/8192 --- L(Train): 0.3708; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7055/8192 --- L(Train): 0.3719; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7056/8192 --- L(Train): 0.3593; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7057/8192 --- L(Train): 0.3799; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7058/8192 --- L(Train): 0.3706; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7059/8192 --- L(Train): 0.3726; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7060/8192 --- L(Train): 0.3726; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7061/8192 --- L(Train): 0.3744; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7062/8192 --- L(Train): 0.3703; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7063/8192 --- L(Train): 0.3726; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7064/8192 --- L(Train): 0.3665; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7065/8192 --- L(Train): 0.3806; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7066/8192 --- L(Train): 0.3650; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7067/8192 --- L(Train): 0.3609; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7068/8192 --- L(Train): 0.3605; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7069/8192 --- L(Train): 0.3771; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7070/8192 --- L(Train): 0.3751; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7071/8192 --- L(Train): 0.3767; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7072/8192 --- L(Train): 0.3782; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7073/8192 --- L(Train): 0.3608; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7074/8192 --- L(Train): 0.3724; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7075/8192 --- L(Train): 0.3731; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7076/8192 --- L(Train): 0.3647; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7077/8192 --- L(Train): 0.3746; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7078/8192 --- L(Train): 0.3723; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7079/8192 --- L(Train): 0.3602; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7080/8192 --- L(Train): 0.3652; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7081/8192 --- L(Train): 0.3673; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7082/8192 --- L(Train): 0.3682; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7083/8192 --- L(Train): 0.3671; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7084/8192 --- L(Train): 0.3557; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7085/8192 --- L(Train): 0.3820; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7086/8192 --- L(Train): 0.3750; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 7087/8192 --- L(Train): 0.3658; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7088/8192 --- L(Train): 0.3766; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7089/8192 --- L(Train): 0.3747; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7090/8192 --- L(Train): 0.3637; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7091/8192 --- L(Train): 0.3755; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7092/8192 --- L(Train): 0.3760; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7093/8192 --- L(Train): 0.3699; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7094/8192 --- L(Train): 0.3640; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7095/8192 --- L(Train): 0.3838; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7096/8192 --- L(Train): 0.3659; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7097/8192 --- L(Train): 0.3855; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7098/8192 --- L(Train): 0.3751; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7099/8192 --- L(Train): 0.3649; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7100/8192 --- L(Train): 0.3746; L(Val): 0.4500; Reg Param: 0.0001; Time: 6.32\n",
      "Epoch 7101/8192 --- L(Train): 0.3737; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7102/8192 --- L(Train): 0.3643; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7103/8192 --- L(Train): 0.3581; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7104/8192 --- L(Train): 0.3793; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7105/8192 --- L(Train): 0.3790; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7106/8192 --- L(Train): 0.3543; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7107/8192 --- L(Train): 0.3711; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7108/8192 --- L(Train): 0.3719; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7109/8192 --- L(Train): 0.3677; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7110/8192 --- L(Train): 0.3726; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7111/8192 --- L(Train): 0.3816; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7112/8192 --- L(Train): 0.3708; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7113/8192 --- L(Train): 0.3669; L(Val): 0.4491; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7114/8192 --- L(Train): 0.3658; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7115/8192 --- L(Train): 0.3648; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7116/8192 --- L(Train): 0.3706; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7117/8192 --- L(Train): 0.3690; L(Val): 0.4491; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7118/8192 --- L(Train): 0.3745; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7119/8192 --- L(Train): 0.3658; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7120/8192 --- L(Train): 0.3830; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7121/8192 --- L(Train): 0.3717; L(Val): 0.4492; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7122/8192 --- L(Train): 0.3696; L(Val): 0.4490; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7123/8192 --- L(Train): 0.3673; L(Val): 0.4488; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7124/8192 --- L(Train): 0.3589; L(Val): 0.4487; Reg Param: 0.0001; Time: 0.68\n",
      "Epoch 7125/8192 --- L(Train): 0.3520; L(Val): 0.4485; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7126/8192 --- L(Train): 0.3612; L(Val): 0.4485; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7127/8192 --- L(Train): 0.3620; L(Val): 0.4484; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7128/8192 --- L(Train): 0.3673; L(Val): 0.4484; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7129/8192 --- L(Train): 0.3713; L(Val): 0.4485; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7130/8192 --- L(Train): 0.3713; L(Val): 0.4486; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7131/8192 --- L(Train): 0.3666; L(Val): 0.4488; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7132/8192 --- L(Train): 0.3725; L(Val): 0.4489; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7133/8192 --- L(Train): 0.3625; L(Val): 0.4491; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7134/8192 --- L(Train): 0.3694; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7135/8192 --- L(Train): 0.3715; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7136/8192 --- L(Train): 0.3670; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7137/8192 --- L(Train): 0.3722; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7138/8192 --- L(Train): 0.3757; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7139/8192 --- L(Train): 0.3679; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7140/8192 --- L(Train): 0.3701; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7141/8192 --- L(Train): 0.3815; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7142/8192 --- L(Train): 0.3644; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7143/8192 --- L(Train): 0.3721; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7144/8192 --- L(Train): 0.3682; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7145/8192 --- L(Train): 0.3748; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7146/8192 --- L(Train): 0.3591; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7147/8192 --- L(Train): 0.3628; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7148/8192 --- L(Train): 0.3559; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7149/8192 --- L(Train): 0.3759; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7150/8192 --- L(Train): 0.3724; L(Val): 0.4505; Reg Param: 0.0001; Time: 6.28\n",
      "Epoch 7151/8192 --- L(Train): 0.3713; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 7152/8192 --- L(Train): 0.3778; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7153/8192 --- L(Train): 0.3711; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7154/8192 --- L(Train): 0.3691; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7155/8192 --- L(Train): 0.3904; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7156/8192 --- L(Train): 0.3775; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7157/8192 --- L(Train): 0.3622; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7158/8192 --- L(Train): 0.3781; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7159/8192 --- L(Train): 0.3790; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7160/8192 --- L(Train): 0.3661; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7161/8192 --- L(Train): 0.3646; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7162/8192 --- L(Train): 0.3689; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7163/8192 --- L(Train): 0.3574; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7164/8192 --- L(Train): 0.3792; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7165/8192 --- L(Train): 0.3773; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7166/8192 --- L(Train): 0.3631; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7167/8192 --- L(Train): 0.3596; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7168/8192 --- L(Train): 0.3588; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7169/8192 --- L(Train): 0.3650; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7170/8192 --- L(Train): 0.3618; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7171/8192 --- L(Train): 0.3781; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7172/8192 --- L(Train): 0.3645; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7173/8192 --- L(Train): 0.3754; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7174/8192 --- L(Train): 0.3754; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7175/8192 --- L(Train): 0.3683; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7176/8192 --- L(Train): 0.3686; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7177/8192 --- L(Train): 0.3694; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7178/8192 --- L(Train): 0.3682; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7179/8192 --- L(Train): 0.3642; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7180/8192 --- L(Train): 0.3587; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7181/8192 --- L(Train): 0.3724; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7182/8192 --- L(Train): 0.3674; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7183/8192 --- L(Train): 0.3726; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7184/8192 --- L(Train): 0.3774; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7185/8192 --- L(Train): 0.3737; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7186/8192 --- L(Train): 0.3552; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7187/8192 --- L(Train): 0.3788; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7188/8192 --- L(Train): 0.3838; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7189/8192 --- L(Train): 0.3652; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.66\n",
      "Epoch 7190/8192 --- L(Train): 0.3737; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7191/8192 --- L(Train): 0.3692; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7192/8192 --- L(Train): 0.3675; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7193/8192 --- L(Train): 0.3640; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7194/8192 --- L(Train): 0.3813; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7195/8192 --- L(Train): 0.3592; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7196/8192 --- L(Train): 0.3706; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7197/8192 --- L(Train): 0.3807; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7198/8192 --- L(Train): 0.3629; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7199/8192 --- L(Train): 0.3738; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7200/8192 --- L(Train): 0.3602; L(Val): 0.4495; Reg Param: 0.0001; Time: 6.31\n",
      "Epoch 7201/8192 --- L(Train): 0.3631; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7202/8192 --- L(Train): 0.3755; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7203/8192 --- L(Train): 0.3532; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7204/8192 --- L(Train): 0.3669; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 7205/8192 --- L(Train): 0.3642; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7206/8192 --- L(Train): 0.3641; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7207/8192 --- L(Train): 0.3693; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.65\n",
      "Epoch 7208/8192 --- L(Train): 0.3716; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.73\n",
      "Epoch 7209/8192 --- L(Train): 0.3729; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7210/8192 --- L(Train): 0.3642; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7211/8192 --- L(Train): 0.3647; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7212/8192 --- L(Train): 0.3822; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7213/8192 --- L(Train): 0.3708; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7214/8192 --- L(Train): 0.3674; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7215/8192 --- L(Train): 0.3763; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7216/8192 --- L(Train): 0.3721; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7217/8192 --- L(Train): 0.3619; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7218/8192 --- L(Train): 0.3791; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7219/8192 --- L(Train): 0.3783; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7220/8192 --- L(Train): 0.3639; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7221/8192 --- L(Train): 0.3801; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7222/8192 --- L(Train): 0.3750; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7223/8192 --- L(Train): 0.3632; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7224/8192 --- L(Train): 0.3707; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7225/8192 --- L(Train): 0.3662; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7226/8192 --- L(Train): 0.3777; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7227/8192 --- L(Train): 0.3593; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7228/8192 --- L(Train): 0.3717; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7229/8192 --- L(Train): 0.3599; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7230/8192 --- L(Train): 0.3622; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7231/8192 --- L(Train): 0.3624; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7232/8192 --- L(Train): 0.3781; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7233/8192 --- L(Train): 0.3718; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7234/8192 --- L(Train): 0.3646; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 7235/8192 --- L(Train): 0.3760; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7236/8192 --- L(Train): 0.3664; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7237/8192 --- L(Train): 0.3634; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7238/8192 --- L(Train): 0.3787; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7239/8192 --- L(Train): 0.3763; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7240/8192 --- L(Train): 0.3631; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7241/8192 --- L(Train): 0.3739; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7242/8192 --- L(Train): 0.3771; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7243/8192 --- L(Train): 0.3838; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7244/8192 --- L(Train): 0.3766; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7245/8192 --- L(Train): 0.3630; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7246/8192 --- L(Train): 0.3683; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7247/8192 --- L(Train): 0.3780; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7248/8192 --- L(Train): 0.3690; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7249/8192 --- L(Train): 0.3696; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7250/8192 --- L(Train): 0.3657; L(Val): 0.4503; Reg Param: 0.0001; Time: 6.29\n",
      "Epoch 7251/8192 --- L(Train): 0.3760; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7252/8192 --- L(Train): 0.3833; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7253/8192 --- L(Train): 0.3693; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7254/8192 --- L(Train): 0.3629; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7255/8192 --- L(Train): 0.3634; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7256/8192 --- L(Train): 0.3580; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7257/8192 --- L(Train): 0.3620; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7258/8192 --- L(Train): 0.3732; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7259/8192 --- L(Train): 0.3804; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7260/8192 --- L(Train): 0.3696; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7261/8192 --- L(Train): 0.3671; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7262/8192 --- L(Train): 0.3830; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7263/8192 --- L(Train): 0.3587; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7264/8192 --- L(Train): 0.3634; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7265/8192 --- L(Train): 0.3826; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7266/8192 --- L(Train): 0.3646; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7267/8192 --- L(Train): 0.3862; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7268/8192 --- L(Train): 0.3667; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7269/8192 --- L(Train): 0.3672; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7270/8192 --- L(Train): 0.3640; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7271/8192 --- L(Train): 0.3838; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7272/8192 --- L(Train): 0.3707; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7273/8192 --- L(Train): 0.3817; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7274/8192 --- L(Train): 0.3650; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7275/8192 --- L(Train): 0.3795; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7276/8192 --- L(Train): 0.3683; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7277/8192 --- L(Train): 0.3759; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7278/8192 --- L(Train): 0.3613; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7279/8192 --- L(Train): 0.3691; L(Val): 0.4493; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7280/8192 --- L(Train): 0.3711; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7281/8192 --- L(Train): 0.3615; L(Val): 0.4494; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7282/8192 --- L(Train): 0.3666; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7283/8192 --- L(Train): 0.3737; L(Val): 0.4495; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7284/8192 --- L(Train): 0.3674; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7285/8192 --- L(Train): 0.3594; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7286/8192 --- L(Train): 0.3838; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7287/8192 --- L(Train): 0.3721; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7288/8192 --- L(Train): 0.3742; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7289/8192 --- L(Train): 0.3649; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7290/8192 --- L(Train): 0.3650; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7291/8192 --- L(Train): 0.3640; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7292/8192 --- L(Train): 0.3694; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7293/8192 --- L(Train): 0.3749; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7294/8192 --- L(Train): 0.3582; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7295/8192 --- L(Train): 0.3689; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.66\n",
      "Epoch 7296/8192 --- L(Train): 0.3876; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7297/8192 --- L(Train): 0.3596; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7298/8192 --- L(Train): 0.3702; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7299/8192 --- L(Train): 0.3604; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7300/8192 --- L(Train): 0.3682; L(Val): 0.4505; Reg Param: 0.0001; Time: 6.44\n",
      "Epoch 7301/8192 --- L(Train): 0.3591; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7302/8192 --- L(Train): 0.3616; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7303/8192 --- L(Train): 0.3777; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7304/8192 --- L(Train): 0.3732; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7305/8192 --- L(Train): 0.3639; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7306/8192 --- L(Train): 0.3701; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7307/8192 --- L(Train): 0.3652; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7308/8192 --- L(Train): 0.3773; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7309/8192 --- L(Train): 0.3803; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7310/8192 --- L(Train): 0.3738; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7311/8192 --- L(Train): 0.3738; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7312/8192 --- L(Train): 0.3724; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7313/8192 --- L(Train): 0.3611; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7314/8192 --- L(Train): 0.3686; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7315/8192 --- L(Train): 0.3670; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7316/8192 --- L(Train): 0.3659; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7317/8192 --- L(Train): 0.3718; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7318/8192 --- L(Train): 0.3746; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7319/8192 --- L(Train): 0.3591; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7320/8192 --- L(Train): 0.3742; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7321/8192 --- L(Train): 0.3785; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7322/8192 --- L(Train): 0.3635; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7323/8192 --- L(Train): 0.3694; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7324/8192 --- L(Train): 0.3719; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 7325/8192 --- L(Train): 0.3763; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7326/8192 --- L(Train): 0.3714; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7327/8192 --- L(Train): 0.3623; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7328/8192 --- L(Train): 0.3748; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.78\n",
      "Epoch 7329/8192 --- L(Train): 0.3669; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.66\n",
      "Epoch 7330/8192 --- L(Train): 0.3545; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7331/8192 --- L(Train): 0.3854; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7332/8192 --- L(Train): 0.3741; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7333/8192 --- L(Train): 0.3607; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7334/8192 --- L(Train): 0.3747; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7335/8192 --- L(Train): 0.3751; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7336/8192 --- L(Train): 0.3645; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7337/8192 --- L(Train): 0.3635; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7338/8192 --- L(Train): 0.3769; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7339/8192 --- L(Train): 0.3758; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7340/8192 --- L(Train): 0.3665; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7341/8192 --- L(Train): 0.3724; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7342/8192 --- L(Train): 0.3763; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7343/8192 --- L(Train): 0.3718; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7344/8192 --- L(Train): 0.3687; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7345/8192 --- L(Train): 0.3673; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7346/8192 --- L(Train): 0.3615; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7347/8192 --- L(Train): 0.3793; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7348/8192 --- L(Train): 0.3688; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7349/8192 --- L(Train): 0.3685; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7350/8192 --- L(Train): 0.3728; L(Val): 0.4507; Reg Param: 0.0001; Time: 6.28\n",
      "Epoch 7351/8192 --- L(Train): 0.3701; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7352/8192 --- L(Train): 0.3740; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7353/8192 --- L(Train): 0.3754; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7354/8192 --- L(Train): 0.3705; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7355/8192 --- L(Train): 0.3761; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7356/8192 --- L(Train): 0.3626; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7357/8192 --- L(Train): 0.3500; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7358/8192 --- L(Train): 0.3621; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7359/8192 --- L(Train): 0.3675; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7360/8192 --- L(Train): 0.3606; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7361/8192 --- L(Train): 0.3731; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7362/8192 --- L(Train): 0.3684; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7363/8192 --- L(Train): 0.3730; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7364/8192 --- L(Train): 0.3689; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7365/8192 --- L(Train): 0.3697; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7366/8192 --- L(Train): 0.3682; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7367/8192 --- L(Train): 0.3663; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7368/8192 --- L(Train): 0.3568; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7369/8192 --- L(Train): 0.3773; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7370/8192 --- L(Train): 0.3623; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7371/8192 --- L(Train): 0.3648; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7372/8192 --- L(Train): 0.3747; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7373/8192 --- L(Train): 0.3736; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7374/8192 --- L(Train): 0.3750; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 7375/8192 --- L(Train): 0.3787; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 7376/8192 --- L(Train): 0.3620; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7377/8192 --- L(Train): 0.3694; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7378/8192 --- L(Train): 0.3702; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7379/8192 --- L(Train): 0.3729; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7380/8192 --- L(Train): 0.3670; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7381/8192 --- L(Train): 0.3755; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7382/8192 --- L(Train): 0.3814; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7383/8192 --- L(Train): 0.3641; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7384/8192 --- L(Train): 0.3643; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7385/8192 --- L(Train): 0.3779; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7386/8192 --- L(Train): 0.3653; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7387/8192 --- L(Train): 0.3749; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7388/8192 --- L(Train): 0.3705; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7389/8192 --- L(Train): 0.3692; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7390/8192 --- L(Train): 0.3683; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7391/8192 --- L(Train): 0.3727; L(Val): 0.4496; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7392/8192 --- L(Train): 0.3788; L(Val): 0.4497; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7393/8192 --- L(Train): 0.3556; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7394/8192 --- L(Train): 0.3694; L(Val): 0.4498; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7395/8192 --- L(Train): 0.3615; L(Val): 0.4499; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7396/8192 --- L(Train): 0.3638; L(Val): 0.4500; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7397/8192 --- L(Train): 0.3713; L(Val): 0.4501; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7398/8192 --- L(Train): 0.3700; L(Val): 0.4502; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7399/8192 --- L(Train): 0.3867; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7400/8192 --- L(Train): 0.3772; L(Val): 0.4503; Reg Param: 0.0001; Time: 6.30\n",
      "Epoch 7401/8192 --- L(Train): 0.3668; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7402/8192 --- L(Train): 0.3791; L(Val): 0.4503; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7403/8192 --- L(Train): 0.3784; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7404/8192 --- L(Train): 0.3730; L(Val): 0.4504; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7405/8192 --- L(Train): 0.3527; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7406/8192 --- L(Train): 0.3742; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7407/8192 --- L(Train): 0.3652; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7408/8192 --- L(Train): 0.3565; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7409/8192 --- L(Train): 0.3624; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7410/8192 --- L(Train): 0.3676; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7411/8192 --- L(Train): 0.3715; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7412/8192 --- L(Train): 0.3615; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7413/8192 --- L(Train): 0.3723; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7414/8192 --- L(Train): 0.3586; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7415/8192 --- L(Train): 0.3669; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7416/8192 --- L(Train): 0.3721; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7417/8192 --- L(Train): 0.3672; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7418/8192 --- L(Train): 0.3698; L(Val): 0.4513; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7419/8192 --- L(Train): 0.3623; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7420/8192 --- L(Train): 0.3653; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7421/8192 --- L(Train): 0.3643; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7422/8192 --- L(Train): 0.3713; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7423/8192 --- L(Train): 0.3791; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7424/8192 --- L(Train): 0.3834; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7425/8192 --- L(Train): 0.3582; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7426/8192 --- L(Train): 0.3734; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7427/8192 --- L(Train): 0.3722; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7428/8192 --- L(Train): 0.3783; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7429/8192 --- L(Train): 0.3593; L(Val): 0.4513; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7430/8192 --- L(Train): 0.3693; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7431/8192 --- L(Train): 0.3716; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7432/8192 --- L(Train): 0.3717; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7433/8192 --- L(Train): 0.3696; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7434/8192 --- L(Train): 0.3606; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7435/8192 --- L(Train): 0.3735; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7436/8192 --- L(Train): 0.3851; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7437/8192 --- L(Train): 0.3670; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7438/8192 --- L(Train): 0.3731; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7439/8192 --- L(Train): 0.3760; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7440/8192 --- L(Train): 0.3667; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7441/8192 --- L(Train): 0.3660; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7442/8192 --- L(Train): 0.3754; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7443/8192 --- L(Train): 0.3636; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7444/8192 --- L(Train): 0.3618; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7445/8192 --- L(Train): 0.3737; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7446/8192 --- L(Train): 0.3581; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7447/8192 --- L(Train): 0.3776; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7448/8192 --- L(Train): 0.3793; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7449/8192 --- L(Train): 0.3685; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7450/8192 --- L(Train): 0.3787; L(Val): 0.4508; Reg Param: 0.0001; Time: 6.37\n",
      "Epoch 7451/8192 --- L(Train): 0.3646; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7452/8192 --- L(Train): 0.3700; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7453/8192 --- L(Train): 0.3629; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7454/8192 --- L(Train): 0.3788; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7455/8192 --- L(Train): 0.3729; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7456/8192 --- L(Train): 0.3742; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.65\n",
      "Epoch 7457/8192 --- L(Train): 0.3719; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7458/8192 --- L(Train): 0.3711; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7459/8192 --- L(Train): 0.3682; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7460/8192 --- L(Train): 0.3696; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7461/8192 --- L(Train): 0.3631; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7462/8192 --- L(Train): 0.3666; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7463/8192 --- L(Train): 0.3645; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7464/8192 --- L(Train): 0.3630; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7465/8192 --- L(Train): 0.3623; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7466/8192 --- L(Train): 0.3664; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7467/8192 --- L(Train): 0.3601; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7468/8192 --- L(Train): 0.3639; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7469/8192 --- L(Train): 0.3627; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7470/8192 --- L(Train): 0.3676; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7471/8192 --- L(Train): 0.3487; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7472/8192 --- L(Train): 0.3716; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7473/8192 --- L(Train): 0.3734; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7474/8192 --- L(Train): 0.3643; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7475/8192 --- L(Train): 0.3714; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7476/8192 --- L(Train): 0.3656; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7477/8192 --- L(Train): 0.3697; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7478/8192 --- L(Train): 0.3807; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7479/8192 --- L(Train): 0.3714; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7480/8192 --- L(Train): 0.3677; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7481/8192 --- L(Train): 0.3598; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7482/8192 --- L(Train): 0.3736; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7483/8192 --- L(Train): 0.3631; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7484/8192 --- L(Train): 0.3676; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7485/8192 --- L(Train): 0.3618; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7486/8192 --- L(Train): 0.3610; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7487/8192 --- L(Train): 0.3641; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7488/8192 --- L(Train): 0.3651; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7489/8192 --- L(Train): 0.3548; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7490/8192 --- L(Train): 0.3735; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7491/8192 --- L(Train): 0.3758; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7492/8192 --- L(Train): 0.3824; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7493/8192 --- L(Train): 0.3650; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7494/8192 --- L(Train): 0.3578; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7495/8192 --- L(Train): 0.3687; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7496/8192 --- L(Train): 0.3656; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7497/8192 --- L(Train): 0.3740; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7498/8192 --- L(Train): 0.3651; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7499/8192 --- L(Train): 0.3662; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7500/8192 --- L(Train): 0.3619; L(Val): 0.4508; Reg Param: 0.0001; Time: 6.52\n",
      "Epoch 7501/8192 --- L(Train): 0.3701; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7502/8192 --- L(Train): 0.3599; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7503/8192 --- L(Train): 0.3762; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7504/8192 --- L(Train): 0.3651; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7505/8192 --- L(Train): 0.3789; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7506/8192 --- L(Train): 0.3633; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7507/8192 --- L(Train): 0.3860; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7508/8192 --- L(Train): 0.3642; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7509/8192 --- L(Train): 0.3765; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7510/8192 --- L(Train): 0.3659; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7511/8192 --- L(Train): 0.3680; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7512/8192 --- L(Train): 0.3664; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7513/8192 --- L(Train): 0.3806; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7514/8192 --- L(Train): 0.3812; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7515/8192 --- L(Train): 0.3574; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7516/8192 --- L(Train): 0.3625; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7517/8192 --- L(Train): 0.3755; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7518/8192 --- L(Train): 0.3625; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7519/8192 --- L(Train): 0.3666; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7520/8192 --- L(Train): 0.3683; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 7521/8192 --- L(Train): 0.3590; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7522/8192 --- L(Train): 0.3631; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7523/8192 --- L(Train): 0.3630; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7524/8192 --- L(Train): 0.3727; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7525/8192 --- L(Train): 0.3784; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7526/8192 --- L(Train): 0.3661; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7527/8192 --- L(Train): 0.3798; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7528/8192 --- L(Train): 0.3682; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7529/8192 --- L(Train): 0.3730; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7530/8192 --- L(Train): 0.3662; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7531/8192 --- L(Train): 0.3661; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7532/8192 --- L(Train): 0.3676; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7533/8192 --- L(Train): 0.3752; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7534/8192 --- L(Train): 0.3646; L(Val): 0.4505; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7535/8192 --- L(Train): 0.3530; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7536/8192 --- L(Train): 0.3621; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7537/8192 --- L(Train): 0.3658; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7538/8192 --- L(Train): 0.3796; L(Val): 0.4506; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7539/8192 --- L(Train): 0.3715; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7540/8192 --- L(Train): 0.3738; L(Val): 0.4507; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7541/8192 --- L(Train): 0.3792; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7542/8192 --- L(Train): 0.3687; L(Val): 0.4508; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7543/8192 --- L(Train): 0.3751; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7544/8192 --- L(Train): 0.3672; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7545/8192 --- L(Train): 0.3637; L(Val): 0.4509; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7546/8192 --- L(Train): 0.3679; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7547/8192 --- L(Train): 0.3636; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 7548/8192 --- L(Train): 0.3605; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7549/8192 --- L(Train): 0.3694; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7550/8192 --- L(Train): 0.3728; L(Val): 0.4511; Reg Param: 0.0001; Time: 6.33\n",
      "Epoch 7551/8192 --- L(Train): 0.3730; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7552/8192 --- L(Train): 0.3695; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7553/8192 --- L(Train): 0.3741; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7554/8192 --- L(Train): 0.3797; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7555/8192 --- L(Train): 0.3674; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7556/8192 --- L(Train): 0.3761; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7557/8192 --- L(Train): 0.3543; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7558/8192 --- L(Train): 0.3640; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7559/8192 --- L(Train): 0.3851; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7560/8192 --- L(Train): 0.3682; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.73\n",
      "Epoch 7561/8192 --- L(Train): 0.3790; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7562/8192 --- L(Train): 0.3724; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7563/8192 --- L(Train): 0.3563; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7564/8192 --- L(Train): 0.3704; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7565/8192 --- L(Train): 0.3760; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7566/8192 --- L(Train): 0.3714; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7567/8192 --- L(Train): 0.3608; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7568/8192 --- L(Train): 0.3687; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7569/8192 --- L(Train): 0.3685; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7570/8192 --- L(Train): 0.3643; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7571/8192 --- L(Train): 0.3549; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7572/8192 --- L(Train): 0.3697; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7573/8192 --- L(Train): 0.3571; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7574/8192 --- L(Train): 0.3763; L(Val): 0.4510; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7575/8192 --- L(Train): 0.3764; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7576/8192 --- L(Train): 0.3578; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7577/8192 --- L(Train): 0.3654; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7578/8192 --- L(Train): 0.3714; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7579/8192 --- L(Train): 0.3748; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7580/8192 --- L(Train): 0.3598; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7581/8192 --- L(Train): 0.3755; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7582/8192 --- L(Train): 0.3689; L(Val): 0.4511; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7583/8192 --- L(Train): 0.3775; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7584/8192 --- L(Train): 0.3571; L(Val): 0.4512; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7585/8192 --- L(Train): 0.3767; L(Val): 0.4513; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7586/8192 --- L(Train): 0.3799; L(Val): 0.4513; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7587/8192 --- L(Train): 0.3736; L(Val): 0.4513; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7588/8192 --- L(Train): 0.3621; L(Val): 0.4513; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7589/8192 --- L(Train): 0.3687; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7590/8192 --- L(Train): 0.3685; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7591/8192 --- L(Train): 0.3735; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7592/8192 --- L(Train): 0.3731; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7593/8192 --- L(Train): 0.3757; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7594/8192 --- L(Train): 0.3647; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7595/8192 --- L(Train): 0.3611; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7596/8192 --- L(Train): 0.3691; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7597/8192 --- L(Train): 0.3598; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7598/8192 --- L(Train): 0.3635; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7599/8192 --- L(Train): 0.3647; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7600/8192 --- L(Train): 0.3728; L(Val): 0.4519; Reg Param: 0.0001; Time: 6.43\n",
      "Epoch 7601/8192 --- L(Train): 0.3614; L(Val): 0.4520; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7602/8192 --- L(Train): 0.3677; L(Val): 0.4520; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7603/8192 --- L(Train): 0.3625; L(Val): 0.4520; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7604/8192 --- L(Train): 0.3708; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 7605/8192 --- L(Train): 0.3693; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7606/8192 --- L(Train): 0.3813; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7607/8192 --- L(Train): 0.3713; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7608/8192 --- L(Train): 0.3855; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7609/8192 --- L(Train): 0.3651; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7610/8192 --- L(Train): 0.3732; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7611/8192 --- L(Train): 0.3792; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7612/8192 --- L(Train): 0.3675; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7613/8192 --- L(Train): 0.3668; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7614/8192 --- L(Train): 0.3758; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7615/8192 --- L(Train): 0.3641; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7616/8192 --- L(Train): 0.3698; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7617/8192 --- L(Train): 0.3668; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7618/8192 --- L(Train): 0.3598; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7619/8192 --- L(Train): 0.3626; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7620/8192 --- L(Train): 0.3666; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7621/8192 --- L(Train): 0.3635; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7622/8192 --- L(Train): 0.3593; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7623/8192 --- L(Train): 0.3735; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7624/8192 --- L(Train): 0.3641; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7625/8192 --- L(Train): 0.3733; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7626/8192 --- L(Train): 0.3818; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7627/8192 --- L(Train): 0.3708; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.65\n",
      "Epoch 7628/8192 --- L(Train): 0.3650; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7629/8192 --- L(Train): 0.3794; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7630/8192 --- L(Train): 0.3668; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7631/8192 --- L(Train): 0.3660; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7632/8192 --- L(Train): 0.3780; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7633/8192 --- L(Train): 0.3700; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7634/8192 --- L(Train): 0.3793; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7635/8192 --- L(Train): 0.3663; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7636/8192 --- L(Train): 0.3884; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7637/8192 --- L(Train): 0.3687; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7638/8192 --- L(Train): 0.3748; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7639/8192 --- L(Train): 0.3783; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7640/8192 --- L(Train): 0.3607; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7641/8192 --- L(Train): 0.3628; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7642/8192 --- L(Train): 0.3685; L(Val): 0.4514; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7643/8192 --- L(Train): 0.3666; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7644/8192 --- L(Train): 0.3514; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7645/8192 --- L(Train): 0.3639; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7646/8192 --- L(Train): 0.3780; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7647/8192 --- L(Train): 0.3789; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7648/8192 --- L(Train): 0.3628; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7649/8192 --- L(Train): 0.3705; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7650/8192 --- L(Train): 0.3680; L(Val): 0.4515; Reg Param: 0.0001; Time: 6.30\n",
      "Epoch 7651/8192 --- L(Train): 0.3662; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7652/8192 --- L(Train): 0.3467; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7653/8192 --- L(Train): 0.3748; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7654/8192 --- L(Train): 0.3786; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7655/8192 --- L(Train): 0.3754; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7656/8192 --- L(Train): 0.3664; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7657/8192 --- L(Train): 0.3742; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7658/8192 --- L(Train): 0.3616; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7659/8192 --- L(Train): 0.3794; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7660/8192 --- L(Train): 0.3719; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7661/8192 --- L(Train): 0.3679; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7662/8192 --- L(Train): 0.3610; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7663/8192 --- L(Train): 0.3684; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7664/8192 --- L(Train): 0.3783; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7665/8192 --- L(Train): 0.3826; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7666/8192 --- L(Train): 0.3685; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7667/8192 --- L(Train): 0.3633; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7668/8192 --- L(Train): 0.3650; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7669/8192 --- L(Train): 0.3683; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 7670/8192 --- L(Train): 0.3653; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7671/8192 --- L(Train): 0.3683; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7672/8192 --- L(Train): 0.3628; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7673/8192 --- L(Train): 0.3700; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7674/8192 --- L(Train): 0.3694; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7675/8192 --- L(Train): 0.3710; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7676/8192 --- L(Train): 0.3751; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7677/8192 --- L(Train): 0.3728; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7678/8192 --- L(Train): 0.3706; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7679/8192 --- L(Train): 0.3730; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7680/8192 --- L(Train): 0.3684; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7681/8192 --- L(Train): 0.3541; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7682/8192 --- L(Train): 0.3703; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7683/8192 --- L(Train): 0.3740; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7684/8192 --- L(Train): 0.3617; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7685/8192 --- L(Train): 0.3579; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7686/8192 --- L(Train): 0.3816; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7687/8192 --- L(Train): 0.3702; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7688/8192 --- L(Train): 0.3574; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7689/8192 --- L(Train): 0.3720; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7690/8192 --- L(Train): 0.3755; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7691/8192 --- L(Train): 0.3630; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7692/8192 --- L(Train): 0.3728; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7693/8192 --- L(Train): 0.3641; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7694/8192 --- L(Train): 0.3607; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7695/8192 --- L(Train): 0.3570; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7696/8192 --- L(Train): 0.3746; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7697/8192 --- L(Train): 0.3657; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7698/8192 --- L(Train): 0.3703; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7699/8192 --- L(Train): 0.3796; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7700/8192 --- L(Train): 0.3709; L(Val): 0.4518; Reg Param: 0.0001; Time: 6.28\n",
      "Epoch 7701/8192 --- L(Train): 0.3739; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7702/8192 --- L(Train): 0.3612; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7703/8192 --- L(Train): 0.3753; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7704/8192 --- L(Train): 0.3623; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7705/8192 --- L(Train): 0.3662; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7706/8192 --- L(Train): 0.3724; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7707/8192 --- L(Train): 0.3705; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7708/8192 --- L(Train): 0.3743; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7709/8192 --- L(Train): 0.3782; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.66\n",
      "Epoch 7710/8192 --- L(Train): 0.3705; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7711/8192 --- L(Train): 0.3719; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7712/8192 --- L(Train): 0.3766; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7713/8192 --- L(Train): 0.3644; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7714/8192 --- L(Train): 0.3638; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7715/8192 --- L(Train): 0.3637; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7716/8192 --- L(Train): 0.3694; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7717/8192 --- L(Train): 0.3705; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7718/8192 --- L(Train): 0.3731; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.65\n",
      "Epoch 7719/8192 --- L(Train): 0.3768; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7720/8192 --- L(Train): 0.3718; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7721/8192 --- L(Train): 0.3740; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7722/8192 --- L(Train): 0.3655; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.80\n",
      "Epoch 7723/8192 --- L(Train): 0.3714; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7724/8192 --- L(Train): 0.3675; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7725/8192 --- L(Train): 0.3610; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7726/8192 --- L(Train): 0.3713; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7727/8192 --- L(Train): 0.3679; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7728/8192 --- L(Train): 0.3844; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7729/8192 --- L(Train): 0.3628; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7730/8192 --- L(Train): 0.3622; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7731/8192 --- L(Train): 0.3689; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7732/8192 --- L(Train): 0.3808; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7733/8192 --- L(Train): 0.3557; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7734/8192 --- L(Train): 0.3694; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7735/8192 --- L(Train): 0.3627; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7736/8192 --- L(Train): 0.3845; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7737/8192 --- L(Train): 0.3714; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7738/8192 --- L(Train): 0.3704; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7739/8192 --- L(Train): 0.3765; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7740/8192 --- L(Train): 0.3600; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7741/8192 --- L(Train): 0.3637; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7742/8192 --- L(Train): 0.3748; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7743/8192 --- L(Train): 0.3748; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7744/8192 --- L(Train): 0.3696; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7745/8192 --- L(Train): 0.3762; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7746/8192 --- L(Train): 0.3706; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7747/8192 --- L(Train): 0.3759; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7748/8192 --- L(Train): 0.3694; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7749/8192 --- L(Train): 0.3818; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7750/8192 --- L(Train): 0.3731; L(Val): 0.4518; Reg Param: 0.0001; Time: 6.39\n",
      "Epoch 7751/8192 --- L(Train): 0.3782; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7752/8192 --- L(Train): 0.3740; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7753/8192 --- L(Train): 0.3751; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7754/8192 --- L(Train): 0.3644; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7755/8192 --- L(Train): 0.3521; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7756/8192 --- L(Train): 0.3754; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7757/8192 --- L(Train): 0.3855; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7758/8192 --- L(Train): 0.3793; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7759/8192 --- L(Train): 0.3686; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7760/8192 --- L(Train): 0.3545; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7761/8192 --- L(Train): 0.3747; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7762/8192 --- L(Train): 0.3754; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7763/8192 --- L(Train): 0.3607; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7764/8192 --- L(Train): 0.3693; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7765/8192 --- L(Train): 0.3729; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7766/8192 --- L(Train): 0.3848; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7767/8192 --- L(Train): 0.3683; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7768/8192 --- L(Train): 0.3640; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7769/8192 --- L(Train): 0.3735; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7770/8192 --- L(Train): 0.3594; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7771/8192 --- L(Train): 0.3680; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7772/8192 --- L(Train): 0.3682; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7773/8192 --- L(Train): 0.3715; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7774/8192 --- L(Train): 0.3762; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7775/8192 --- L(Train): 0.3699; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7776/8192 --- L(Train): 0.3745; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7777/8192 --- L(Train): 0.3743; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7778/8192 --- L(Train): 0.3734; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7779/8192 --- L(Train): 0.3697; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7780/8192 --- L(Train): 0.3591; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7781/8192 --- L(Train): 0.3704; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7782/8192 --- L(Train): 0.3787; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7783/8192 --- L(Train): 0.3670; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7784/8192 --- L(Train): 0.3800; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7785/8192 --- L(Train): 0.3725; L(Val): 0.4520; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7786/8192 --- L(Train): 0.3673; L(Val): 0.4521; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7787/8192 --- L(Train): 0.3577; L(Val): 0.4521; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7788/8192 --- L(Train): 0.3631; L(Val): 0.4522; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7789/8192 --- L(Train): 0.3635; L(Val): 0.4522; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7790/8192 --- L(Train): 0.3801; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7791/8192 --- L(Train): 0.3542; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7792/8192 --- L(Train): 0.3601; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7793/8192 --- L(Train): 0.3589; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7794/8192 --- L(Train): 0.3913; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7795/8192 --- L(Train): 0.3678; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7796/8192 --- L(Train): 0.3620; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7797/8192 --- L(Train): 0.3692; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7798/8192 --- L(Train): 0.3579; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7799/8192 --- L(Train): 0.3665; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 7800/8192 --- L(Train): 0.3653; L(Val): 0.4524; Reg Param: 0.0001; Time: 6.33\n",
      "Epoch 7801/8192 --- L(Train): 0.3721; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7802/8192 --- L(Train): 0.3808; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7803/8192 --- L(Train): 0.3666; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7804/8192 --- L(Train): 0.3633; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7805/8192 --- L(Train): 0.3714; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7806/8192 --- L(Train): 0.3683; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7807/8192 --- L(Train): 0.3694; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7808/8192 --- L(Train): 0.3695; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7809/8192 --- L(Train): 0.3765; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7810/8192 --- L(Train): 0.3627; L(Val): 0.4524; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7811/8192 --- L(Train): 0.3641; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7812/8192 --- L(Train): 0.3687; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 7813/8192 --- L(Train): 0.3612; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7814/8192 --- L(Train): 0.3613; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7815/8192 --- L(Train): 0.3724; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7816/8192 --- L(Train): 0.3607; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7817/8192 --- L(Train): 0.3681; L(Val): 0.4523; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7818/8192 --- L(Train): 0.3651; L(Val): 0.4522; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7819/8192 --- L(Train): 0.3705; L(Val): 0.4522; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7820/8192 --- L(Train): 0.3887; L(Val): 0.4522; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7821/8192 --- L(Train): 0.3536; L(Val): 0.4522; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7822/8192 --- L(Train): 0.3603; L(Val): 0.4521; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7823/8192 --- L(Train): 0.3592; L(Val): 0.4521; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7824/8192 --- L(Train): 0.3672; L(Val): 0.4521; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7825/8192 --- L(Train): 0.3703; L(Val): 0.4521; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7826/8192 --- L(Train): 0.3780; L(Val): 0.4520; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7827/8192 --- L(Train): 0.3676; L(Val): 0.4520; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7828/8192 --- L(Train): 0.3792; L(Val): 0.4520; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7829/8192 --- L(Train): 0.3673; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7830/8192 --- L(Train): 0.3632; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7831/8192 --- L(Train): 0.3583; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7832/8192 --- L(Train): 0.3744; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7833/8192 --- L(Train): 0.3589; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7834/8192 --- L(Train): 0.3620; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7835/8192 --- L(Train): 0.3516; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7836/8192 --- L(Train): 0.3771; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7837/8192 --- L(Train): 0.3755; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7838/8192 --- L(Train): 0.3603; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7839/8192 --- L(Train): 0.3562; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7840/8192 --- L(Train): 0.3692; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7841/8192 --- L(Train): 0.3726; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7842/8192 --- L(Train): 0.3747; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7843/8192 --- L(Train): 0.3821; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7844/8192 --- L(Train): 0.3754; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7845/8192 --- L(Train): 0.3706; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7846/8192 --- L(Train): 0.3712; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7847/8192 --- L(Train): 0.3662; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7848/8192 --- L(Train): 0.3618; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7849/8192 --- L(Train): 0.3671; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7850/8192 --- L(Train): 0.3654; L(Val): 0.4517; Reg Param: 0.0001; Time: 6.29\n",
      "Epoch 7851/8192 --- L(Train): 0.3710; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7852/8192 --- L(Train): 0.3755; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7853/8192 --- L(Train): 0.3712; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7854/8192 --- L(Train): 0.3673; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7855/8192 --- L(Train): 0.3722; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7856/8192 --- L(Train): 0.3736; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7857/8192 --- L(Train): 0.3593; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7858/8192 --- L(Train): 0.3769; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7859/8192 --- L(Train): 0.3692; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7860/8192 --- L(Train): 0.3724; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7861/8192 --- L(Train): 0.3790; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7862/8192 --- L(Train): 0.3780; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7863/8192 --- L(Train): 0.3584; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7864/8192 --- L(Train): 0.3677; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7865/8192 --- L(Train): 0.3651; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7866/8192 --- L(Train): 0.3668; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7867/8192 --- L(Train): 0.3731; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7868/8192 --- L(Train): 0.3702; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7869/8192 --- L(Train): 0.3779; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7870/8192 --- L(Train): 0.3750; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7871/8192 --- L(Train): 0.3643; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7872/8192 --- L(Train): 0.3636; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7873/8192 --- L(Train): 0.3734; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7874/8192 --- L(Train): 0.3556; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7875/8192 --- L(Train): 0.3527; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7876/8192 --- L(Train): 0.3601; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7877/8192 --- L(Train): 0.3727; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7878/8192 --- L(Train): 0.3841; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7879/8192 --- L(Train): 0.3789; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7880/8192 --- L(Train): 0.3617; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.66\n",
      "Epoch 7881/8192 --- L(Train): 0.3622; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7882/8192 --- L(Train): 0.3574; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7883/8192 --- L(Train): 0.3631; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7884/8192 --- L(Train): 0.3633; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7885/8192 --- L(Train): 0.3695; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7886/8192 --- L(Train): 0.3818; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7887/8192 --- L(Train): 0.3759; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7888/8192 --- L(Train): 0.3787; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7889/8192 --- L(Train): 0.3679; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7890/8192 --- L(Train): 0.3746; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7891/8192 --- L(Train): 0.3701; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7892/8192 --- L(Train): 0.3730; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7893/8192 --- L(Train): 0.3742; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 7894/8192 --- L(Train): 0.3724; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7895/8192 --- L(Train): 0.3804; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7896/8192 --- L(Train): 0.3666; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7897/8192 --- L(Train): 0.3523; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7898/8192 --- L(Train): 0.3814; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7899/8192 --- L(Train): 0.3573; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7900/8192 --- L(Train): 0.3740; L(Val): 0.4518; Reg Param: 0.0001; Time: 6.46\n",
      "Epoch 7901/8192 --- L(Train): 0.3607; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7902/8192 --- L(Train): 0.3678; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7903/8192 --- L(Train): 0.3733; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7904/8192 --- L(Train): 0.3649; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7905/8192 --- L(Train): 0.3711; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7906/8192 --- L(Train): 0.3829; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7907/8192 --- L(Train): 0.3742; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7908/8192 --- L(Train): 0.3795; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7909/8192 --- L(Train): 0.3797; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7910/8192 --- L(Train): 0.3673; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7911/8192 --- L(Train): 0.3776; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7912/8192 --- L(Train): 0.3648; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7913/8192 --- L(Train): 0.3759; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7914/8192 --- L(Train): 0.3710; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7915/8192 --- L(Train): 0.3631; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7916/8192 --- L(Train): 0.3715; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7917/8192 --- L(Train): 0.3728; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7918/8192 --- L(Train): 0.3617; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7919/8192 --- L(Train): 0.3666; L(Val): 0.4515; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7920/8192 --- L(Train): 0.3685; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7921/8192 --- L(Train): 0.3656; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7922/8192 --- L(Train): 0.3611; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7923/8192 --- L(Train): 0.3750; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7924/8192 --- L(Train): 0.3675; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7925/8192 --- L(Train): 0.3643; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7926/8192 --- L(Train): 0.3694; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7927/8192 --- L(Train): 0.3783; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7928/8192 --- L(Train): 0.3673; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7929/8192 --- L(Train): 0.3674; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7930/8192 --- L(Train): 0.3760; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7931/8192 --- L(Train): 0.3799; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7932/8192 --- L(Train): 0.3775; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7933/8192 --- L(Train): 0.3606; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7934/8192 --- L(Train): 0.3665; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7935/8192 --- L(Train): 0.3600; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7936/8192 --- L(Train): 0.3711; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7937/8192 --- L(Train): 0.3632; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7938/8192 --- L(Train): 0.3640; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7939/8192 --- L(Train): 0.3758; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7940/8192 --- L(Train): 0.3667; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7941/8192 --- L(Train): 0.3768; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7942/8192 --- L(Train): 0.3674; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 7943/8192 --- L(Train): 0.3564; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7944/8192 --- L(Train): 0.3637; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7945/8192 --- L(Train): 0.3730; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7946/8192 --- L(Train): 0.3727; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7947/8192 --- L(Train): 0.3700; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7948/8192 --- L(Train): 0.3674; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7949/8192 --- L(Train): 0.3703; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7950/8192 --- L(Train): 0.3677; L(Val): 0.4517; Reg Param: 0.0001; Time: 6.24\n",
      "Epoch 7951/8192 --- L(Train): 0.3618; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7952/8192 --- L(Train): 0.3701; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7953/8192 --- L(Train): 0.3624; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7954/8192 --- L(Train): 0.3694; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7955/8192 --- L(Train): 0.3639; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.70\n",
      "Epoch 7956/8192 --- L(Train): 0.3753; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7957/8192 --- L(Train): 0.3596; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7958/8192 --- L(Train): 0.3780; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7959/8192 --- L(Train): 0.3686; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7960/8192 --- L(Train): 0.3832; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7961/8192 --- L(Train): 0.3682; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 7962/8192 --- L(Train): 0.3721; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 7963/8192 --- L(Train): 0.3703; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7964/8192 --- L(Train): 0.3858; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7965/8192 --- L(Train): 0.3590; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7966/8192 --- L(Train): 0.3608; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7967/8192 --- L(Train): 0.3660; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7968/8192 --- L(Train): 0.3727; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7969/8192 --- L(Train): 0.3627; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7970/8192 --- L(Train): 0.3737; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7971/8192 --- L(Train): 0.3738; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7972/8192 --- L(Train): 0.3700; L(Val): 0.4516; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7973/8192 --- L(Train): 0.3708; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7974/8192 --- L(Train): 0.3644; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7975/8192 --- L(Train): 0.3692; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7976/8192 --- L(Train): 0.3587; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7977/8192 --- L(Train): 0.3700; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7978/8192 --- L(Train): 0.3644; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7979/8192 --- L(Train): 0.3693; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7980/8192 --- L(Train): 0.3654; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7981/8192 --- L(Train): 0.3711; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7982/8192 --- L(Train): 0.3684; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 7983/8192 --- L(Train): 0.3632; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7984/8192 --- L(Train): 0.3668; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7985/8192 --- L(Train): 0.3753; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7986/8192 --- L(Train): 0.3597; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7987/8192 --- L(Train): 0.3656; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7988/8192 --- L(Train): 0.3778; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7989/8192 --- L(Train): 0.3645; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7990/8192 --- L(Train): 0.3689; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 7991/8192 --- L(Train): 0.3657; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 7992/8192 --- L(Train): 0.3640; L(Val): 0.4517; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 7993/8192 --- L(Train): 0.3624; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7994/8192 --- L(Train): 0.3578; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7995/8192 --- L(Train): 0.3794; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7996/8192 --- L(Train): 0.3701; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7997/8192 --- L(Train): 0.3729; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7998/8192 --- L(Train): 0.3749; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 7999/8192 --- L(Train): 0.3705; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8000/8192 --- L(Train): 0.3808; L(Val): 0.4518; Reg Param: 0.0001; Time: 6.34\n",
      "Epoch 8001/8192 --- L(Train): 0.3624; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 8002/8192 --- L(Train): 0.3678; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8003/8192 --- L(Train): 0.3718; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8004/8192 --- L(Train): 0.3818; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8005/8192 --- L(Train): 0.3709; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8006/8192 --- L(Train): 0.3697; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8007/8192 --- L(Train): 0.3621; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8008/8192 --- L(Train): 0.3653; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8009/8192 --- L(Train): 0.3653; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 8010/8192 --- L(Train): 0.3763; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8011/8192 --- L(Train): 0.3676; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8012/8192 --- L(Train): 0.3594; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8013/8192 --- L(Train): 0.3712; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8014/8192 --- L(Train): 0.3753; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8015/8192 --- L(Train): 0.3787; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8016/8192 --- L(Train): 0.3680; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8017/8192 --- L(Train): 0.3681; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8018/8192 --- L(Train): 0.3652; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 8019/8192 --- L(Train): 0.3722; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8020/8192 --- L(Train): 0.3668; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 8021/8192 --- L(Train): 0.3761; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 8022/8192 --- L(Train): 0.3765; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8023/8192 --- L(Train): 0.3647; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8024/8192 --- L(Train): 0.3711; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8025/8192 --- L(Train): 0.3709; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8026/8192 --- L(Train): 0.3796; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 8027/8192 --- L(Train): 0.3777; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8028/8192 --- L(Train): 0.3628; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8029/8192 --- L(Train): 0.3728; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8030/8192 --- L(Train): 0.3841; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8031/8192 --- L(Train): 0.3656; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8032/8192 --- L(Train): 0.3726; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8033/8192 --- L(Train): 0.3661; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8034/8192 --- L(Train): 0.3572; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 8035/8192 --- L(Train): 0.3741; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 8036/8192 --- L(Train): 0.3692; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8037/8192 --- L(Train): 0.3685; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8038/8192 --- L(Train): 0.3730; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8039/8192 --- L(Train): 0.3778; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 8040/8192 --- L(Train): 0.3701; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 8041/8192 --- L(Train): 0.3726; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8042/8192 --- L(Train): 0.3610; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8043/8192 --- L(Train): 0.3681; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8044/8192 --- L(Train): 0.3717; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8045/8192 --- L(Train): 0.3710; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8046/8192 --- L(Train): 0.3643; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8047/8192 --- L(Train): 0.3660; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8048/8192 --- L(Train): 0.3637; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8049/8192 --- L(Train): 0.3711; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8050/8192 --- L(Train): 0.3738; L(Val): 0.4519; Reg Param: 0.0001; Time: 6.36\n",
      "Epoch 8051/8192 --- L(Train): 0.3695; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8052/8192 --- L(Train): 0.3683; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8053/8192 --- L(Train): 0.3683; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8054/8192 --- L(Train): 0.3699; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8055/8192 --- L(Train): 0.3796; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8056/8192 --- L(Train): 0.3709; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8057/8192 --- L(Train): 0.3767; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8058/8192 --- L(Train): 0.3615; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8059/8192 --- L(Train): 0.3715; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8060/8192 --- L(Train): 0.3637; L(Val): 0.4519; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8061/8192 --- L(Train): 0.3815; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8062/8192 --- L(Train): 0.3664; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8063/8192 --- L(Train): 0.3686; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8064/8192 --- L(Train): 0.3729; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8065/8192 --- L(Train): 0.3687; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8066/8192 --- L(Train): 0.3768; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8067/8192 --- L(Train): 0.3740; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8068/8192 --- L(Train): 0.3603; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8069/8192 --- L(Train): 0.3772; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8070/8192 --- L(Train): 0.3705; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8071/8192 --- L(Train): 0.3598; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8072/8192 --- L(Train): 0.3701; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8073/8192 --- L(Train): 0.3598; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8074/8192 --- L(Train): 0.3688; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8075/8192 --- L(Train): 0.3626; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8076/8192 --- L(Train): 0.3603; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8077/8192 --- L(Train): 0.3637; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8078/8192 --- L(Train): 0.3598; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8079/8192 --- L(Train): 0.3697; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8080/8192 --- L(Train): 0.3672; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8081/8192 --- L(Train): 0.3703; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8082/8192 --- L(Train): 0.3761; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8083/8192 --- L(Train): 0.3646; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8084/8192 --- L(Train): 0.3736; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 8085/8192 --- L(Train): 0.3655; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8086/8192 --- L(Train): 0.3695; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8087/8192 --- L(Train): 0.3598; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8088/8192 --- L(Train): 0.3623; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8089/8192 --- L(Train): 0.3764; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8090/8192 --- L(Train): 0.3643; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8091/8192 --- L(Train): 0.3643; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8092/8192 --- L(Train): 0.3565; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8093/8192 --- L(Train): 0.3639; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8094/8192 --- L(Train): 0.3614; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8095/8192 --- L(Train): 0.3775; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8096/8192 --- L(Train): 0.3736; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8097/8192 --- L(Train): 0.3619; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8098/8192 --- L(Train): 0.3606; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8099/8192 --- L(Train): 0.3641; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8100/8192 --- L(Train): 0.3693; L(Val): 0.4518; Reg Param: 0.0001; Time: 6.30\n",
      "Epoch 8101/8192 --- L(Train): 0.3701; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 8102/8192 --- L(Train): 0.3614; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8103/8192 --- L(Train): 0.3654; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8104/8192 --- L(Train): 0.3637; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8105/8192 --- L(Train): 0.3755; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8106/8192 --- L(Train): 0.3694; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8107/8192 --- L(Train): 0.3596; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 8108/8192 --- L(Train): 0.3696; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8109/8192 --- L(Train): 0.3763; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8110/8192 --- L(Train): 0.3728; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8111/8192 --- L(Train): 0.3617; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8112/8192 --- L(Train): 0.3627; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 8113/8192 --- L(Train): 0.3690; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8114/8192 --- L(Train): 0.3680; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 8115/8192 --- L(Train): 0.3731; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8116/8192 --- L(Train): 0.3767; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 8117/8192 --- L(Train): 0.3581; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8118/8192 --- L(Train): 0.3663; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8119/8192 --- L(Train): 0.3758; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8120/8192 --- L(Train): 0.3642; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8121/8192 --- L(Train): 0.3615; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8122/8192 --- L(Train): 0.3698; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8123/8192 --- L(Train): 0.3781; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8124/8192 --- L(Train): 0.3685; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8125/8192 --- L(Train): 0.3583; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8126/8192 --- L(Train): 0.3660; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8127/8192 --- L(Train): 0.3551; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8128/8192 --- L(Train): 0.3637; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8129/8192 --- L(Train): 0.3723; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8130/8192 --- L(Train): 0.3830; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8131/8192 --- L(Train): 0.3683; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8132/8192 --- L(Train): 0.3659; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8133/8192 --- L(Train): 0.3582; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.67\n",
      "Epoch 8134/8192 --- L(Train): 0.3636; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.63\n",
      "Epoch 8135/8192 --- L(Train): 0.3691; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8136/8192 --- L(Train): 0.3641; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 8137/8192 --- L(Train): 0.3690; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8138/8192 --- L(Train): 0.3765; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8139/8192 --- L(Train): 0.3789; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.65\n",
      "Epoch 8140/8192 --- L(Train): 0.3580; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 8141/8192 --- L(Train): 0.3737; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8142/8192 --- L(Train): 0.3589; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8143/8192 --- L(Train): 0.3591; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8144/8192 --- L(Train): 0.3624; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8145/8192 --- L(Train): 0.3630; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 8146/8192 --- L(Train): 0.3619; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8147/8192 --- L(Train): 0.3655; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8148/8192 --- L(Train): 0.3819; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8149/8192 --- L(Train): 0.3605; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8150/8192 --- L(Train): 0.3739; L(Val): 0.4518; Reg Param: 0.0001; Time: 6.27\n",
      "Epoch 8151/8192 --- L(Train): 0.3693; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8152/8192 --- L(Train): 0.3756; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8153/8192 --- L(Train): 0.3696; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8154/8192 --- L(Train): 0.3621; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8155/8192 --- L(Train): 0.3726; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8156/8192 --- L(Train): 0.3692; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 8157/8192 --- L(Train): 0.3684; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8158/8192 --- L(Train): 0.3619; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8159/8192 --- L(Train): 0.3685; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.64\n",
      "Epoch 8160/8192 --- L(Train): 0.3621; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.65\n",
      "Epoch 8161/8192 --- L(Train): 0.3657; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8162/8192 --- L(Train): 0.3734; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8163/8192 --- L(Train): 0.3686; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8164/8192 --- L(Train): 0.3748; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 8165/8192 --- L(Train): 0.3733; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8166/8192 --- L(Train): 0.3579; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8167/8192 --- L(Train): 0.3561; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8168/8192 --- L(Train): 0.3793; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 8169/8192 --- L(Train): 0.3663; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8170/8192 --- L(Train): 0.3853; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8171/8192 --- L(Train): 0.3763; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8172/8192 --- L(Train): 0.3662; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8173/8192 --- L(Train): 0.3683; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8174/8192 --- L(Train): 0.3690; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8175/8192 --- L(Train): 0.3730; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8176/8192 --- L(Train): 0.3624; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8177/8192 --- L(Train): 0.3586; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8178/8192 --- L(Train): 0.3636; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8179/8192 --- L(Train): 0.3730; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8180/8192 --- L(Train): 0.3756; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8181/8192 --- L(Train): 0.3602; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8182/8192 --- L(Train): 0.3740; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.69\n",
      "Epoch 8183/8192 --- L(Train): 0.3736; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Epoch 8184/8192 --- L(Train): 0.3614; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.62\n",
      "Epoch 8185/8192 --- L(Train): 0.3773; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8186/8192 --- L(Train): 0.3817; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8187/8192 --- L(Train): 0.3706; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8188/8192 --- L(Train): 0.3657; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.59\n",
      "Epoch 8189/8192 --- L(Train): 0.3722; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8190/8192 --- L(Train): 0.3771; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8191/8192 --- L(Train): 0.3830; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.60\n",
      "Epoch 8192/8192 --- L(Train): 0.3635; L(Val): 0.4518; Reg Param: 0.0001; Time: 0.61\n",
      "Training finished.\n",
      "Saved RNN parameters to file params/eckstein2022/iMAML_eckstein2022_ep8192_metalr-1e-02_in-1e-04_rnn.pkl.\n",
      "Training took 6843.01 seconds.\n",
      "\n",
      "Testing the trained RNN on the test dataset...\n",
      "Epoch 1/1 --- L(Train): 0.4263830; Time: 0.03s; Convergence: 7.87e-01\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\r"
     ]
    }
   ],
   "source": [
    "model, _, histories = pipeline_rnn_autoreg.main(\n",
    "    \n",
    "    dropout=0.25,\n",
    "    train_test_ratio=train_test_ratio,\n",
    "    \n",
    "    # general training parameters\n",
    "    checkpoint=False,\n",
    "    epochs=epochs, # <- 2^16\n",
    "    scheduler=True,\n",
    "    learning_rate=1e-2, # 1e-2\n",
    "\n",
    "    # Meta-optimization parameters\n",
    "    metaopt_type=metaopt_type,\n",
    "\n",
    "    lambda_awd=lambda_awd,\n",
    "\n",
    "    meta_update_interval=50,\n",
    "    inner_steps=3,\n",
    "    outer_lr=outer_lr,\n",
    "    hypergradient_steps=3,\n",
    "    initial_reg_param=initial_reg_param,\n",
    "\n",
    "    # hand-picked params\n",
    "    n_steps=-1,\n",
    "    embedding_size=32,\n",
    "    batch_size=-1,\n",
    "    sequence_length=-1,\n",
    "    bagging=True,\n",
    "    \n",
    "    class_rnn=class_rnn,\n",
    "    model=path_model,\n",
    "    data=path_data,\n",
    "    additional_inputs_data=additional_inputs,\n",
    "    \n",
    "    # synthetic dataset parameters\n",
    "    n_sessions=128,\n",
    "    n_trials=200,\n",
    "    sigma=0.2,\n",
    "    beta_reward=3.,\n",
    "    alpha_reward=0.25,\n",
    "    alpha_penalty=0.5,\n",
    "    forget_rate=0.,\n",
    "    confirmation_bias=0.,\n",
    "    beta_choice=0.,\n",
    "    alpha_choice=1.,\n",
    "    counterfactual=False,\n",
    "    alpha_counterfactual=0.,\n",
    "    \n",
    "    save_checkpoints=True,\n",
    "    analysis=False,\n",
    "    participant_id=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d6f107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_history, val_loss_history, reg_history = histories\n",
    "plot_epochs = np.arange(1, len(train_loss_history) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(plot_epochs, train_loss_history, label=\"Train Loss\", linewidth=2)\n",
    "plt.plot(plot_epochs, val_loss_history, label=\"Validation Loss\", linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "if metaopt_type == 'awd':\n",
    "    # Subplot 2: lambda weight decay\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(plot_epochs, reg_history, label='λ weight decay')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('λ weight decay')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "elif metaopt_type == 'imaml':\n",
    "    # Plot 2: Regularization parameter\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(plot_epochs, reg_history, label=\"λ (Regularization)\", linewidth=2, color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Regularization Parameter')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "941fa843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Configs\n",
    "from spice.resources import sindy_utils\n",
    "sindy_config = sindy_utils.SindyConfig_eckstein2022 if dataset == 'eckstein2022' else sindy_utils.SindyConfig_dezfouli2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c479892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library setup is valid. All keys and features appear in the provided list of features.\n",
      "[tensor(0.), tensor(1.)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/306 [00:00<?, ?it/s]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:12:16,969] A new study created in memory with name: no-name-f494d10c-bb91-4641-a09a-f7b0adf5d0f2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.77922; SPICE =  0.00664; Diff = 0.77258\n",
      "Using optuna to find a better set of pysindy parameters for participant 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \n",
      "  0%|          | 0/306 [00:07<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:12:20,374] Trial 0 finished with value: 0.048863064833393414 and parameters: {'optimizer_alpha': 0.025189539243336268, 'optimizer_threshold': 0.19566855582819848}. Best is trial 0 with value: 0.048863064833393414.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                       \n",
      "Best trial: 1. Best value: 0.00736196:   4%|▍         | 2/50 [00:06<02:44,  3.42s/it, 6.83/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:12:23,797] Trial 1 finished with value: 0.007361958063317657 and parameters: {'optimizer_alpha': 0.0804538158263467, 'optimizer_threshold': 0.049336296766759255}. Best is trial 1 with value: 0.007361958063317657.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  0%|          | 1/306 [00:13<1:10:37, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.77922; SPICE =  0.00664 -> 0.76791, Diff = 0.77258 -> 0.01131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  1%|          | 2/306 [00:17<40:35,  8.01s/it]  c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  1%|          | 3/306 [00:21<30:48,  6.10s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 5/306 [00:29<23:47,  4.74s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 6/306 [00:33<22:10,  4.44s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:12:50,399] A new study created in memory with name: no-name-43047ff5-4b0f-437b-b05f-735d8cc5681e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.76812; SPICE =  8e-05; Diff = 0.76804\n",
      "Using optuna to find a better set of pysindy parameters for participant 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [00:40<22:10,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:12:53,990] Trial 0 finished with value: 2.035845167017313 and parameters: {'optimizer_alpha': 0.015079100506233222, 'optimizer_threshold': 0.07007911018776107}. Best is trial 0 with value: 2.035845167017313.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [00:44<22:10,  4.44s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:12:57,621] Trial 1 finished with value: 0.6960262441711608 and parameters: {'optimizer_alpha': 0.07673734758100885, 'optimizer_threshold': 0.013263805677931942}. Best is trial 1 with value: 0.6960262441711608.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [00:47<22:10,  4.44s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:01,222] Trial 2 finished with value: 1.5652531858629817 and parameters: {'optimizer_alpha': 0.03636012165612374, 'optimizer_threshold': 0.030820582880643}. Best is trial 1 with value: 0.6960262441711608.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [00:51<22:10,  4.44s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:04,834] Trial 3 finished with value: 0.5897567733500266 and parameters: {'optimizer_alpha': 0.13178673810934272, 'optimizer_threshold': 0.09583541437996174}. Best is trial 3 with value: 0.5897567733500266.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [00:55<22:10,  4.44s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:08,543] Trial 4 finished with value: 0.03378759035712445 and parameters: {'optimizer_alpha': 0.3950515540538052, 'optimizer_threshold': 0.19457024584357974}. Best is trial 4 with value: 0.03378759035712445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [00:58<22:10,  4.44s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:12,149] Trial 5 finished with value: 21.565361377057705 and parameters: {'optimizer_alpha': 0.14615375926457494, 'optimizer_threshold': 0.039494026548203824}. Best is trial 4 with value: 0.03378759035712445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:02<22:10,  4.44s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:15,758] Trial 6 finished with value: 1.4926032425619689 and parameters: {'optimizer_alpha': 0.9308110404183011, 'optimizer_threshold': 0.03265183338119}. Best is trial 4 with value: 0.03378759035712445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:06<22:10,  4.44s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:19,363] Trial 7 finished with value: 1.9933252360196025 and parameters: {'optimizer_alpha': 0.030327288041148277, 'optimizer_threshold': 0.015213782859333867}. Best is trial 4 with value: 0.03378759035712445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:09<22:10,  4.44s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:22,972] Trial 8 finished with value: 0.0490571343004372 and parameters: {'optimizer_alpha': 0.10069711602915182, 'optimizer_threshold': 0.05073715197019729}. Best is trial 4 with value: 0.03378759035712445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:13<22:10,  4.44s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:26,590] Trial 9 finished with value: 1.5360821976325603 and parameters: {'optimizer_alpha': 0.07265740486224845, 'optimizer_threshold': 0.01368859334041275}. Best is trial 4 with value: 0.03378759035712445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:16<22:10,  4.44s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:30,220] Trial 10 finished with value: 1.5919756005050534 and parameters: {'optimizer_alpha': 0.5520414396059656, 'optimizer_threshold': 0.19823404954181548}. Best is trial 4 with value: 0.03378759035712445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:20<22:10,  4.44s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:33,871] Trial 11 finished with value: 0.27209173923170293 and parameters: {'optimizer_alpha': 0.3428578278524634, 'optimizer_threshold': 0.17958355236190718}. Best is trial 4 with value: 0.03378759035712445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:24<22:10,  4.44s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:37,506] Trial 12 finished with value: 0.014630736271494166 and parameters: {'optimizer_alpha': 0.2751074312654128, 'optimizer_threshold': 0.0920622741654194}. Best is trial 12 with value: 0.014630736271494166.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:27<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:41,210] Trial 13 finished with value: 2.5246590632053487 and parameters: {'optimizer_alpha': 0.2572096576480573, 'optimizer_threshold': 0.11731613976884823}. Best is trial 12 with value: 0.014630736271494166.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:31<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:44,810] Trial 14 finished with value: 0.12535804408863996 and parameters: {'optimizer_alpha': 0.29656117218537886, 'optimizer_threshold': 0.12526968238832847}. Best is trial 12 with value: 0.014630736271494166.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:35<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:48,420] Trial 15 finished with value: 0.9760792922554413 and parameters: {'optimizer_alpha': 0.7062770348103323, 'optimizer_threshold': 0.0746254823426102}. Best is trial 12 with value: 0.014630736271494166.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:38<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:52,056] Trial 16 finished with value: 0.013970398049108287 and parameters: {'optimizer_alpha': 0.44250400846182975, 'optimizer_threshold': 0.14815437192531677}. Best is trial 16 with value: 0.013970398049108287.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:42<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:55,790] Trial 17 finished with value: 10.212322555254332 and parameters: {'optimizer_alpha': 0.20526756391527354, 'optimizer_threshold': 0.12932581171476312}. Best is trial 16 with value: 0.013970398049108287.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:46<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:13:59,443] Trial 18 finished with value: 1.947071370246824 and parameters: {'optimizer_alpha': 0.6084002242518808, 'optimizer_threshold': 0.06386794416602211}. Best is trial 16 with value: 0.013970398049108287.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:49<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:03,070] Trial 19 finished with value: 0.9906726701240773 and parameters: {'optimizer_alpha': 0.19271842345218668, 'optimizer_threshold': 0.09731386948948563}. Best is trial 16 with value: 0.013970398049108287.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:53<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:06,709] Trial 20 finished with value: 1.503123029025419 and parameters: {'optimizer_alpha': 0.4133033009795356, 'optimizer_threshold': 0.1523603580272385}. Best is trial 16 with value: 0.013970398049108287.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [01:57<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:10,333] Trial 21 finished with value: 2.5027903655566335 and parameters: {'optimizer_alpha': 0.47063147522026344, 'optimizer_threshold': 0.19652513892195822}. Best is trial 16 with value: 0.013970398049108287.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:00<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:14,048] Trial 22 finished with value: 2.242689846907126 and parameters: {'optimizer_alpha': 0.9469262411780556, 'optimizer_threshold': 0.09668623175815007}. Best is trial 16 with value: 0.013970398049108287.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:04<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:17,676] Trial 23 finished with value: 0.010851874190367752 and parameters: {'optimizer_alpha': 0.3235915081357131, 'optimizer_threshold': 0.14940858661849765}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:07<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:21,306] Trial 24 finished with value: 2.1073902173715306 and parameters: {'optimizer_alpha': 0.23224713076320075, 'optimizer_threshold': 0.14113078591971878}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:11<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:24,923] Trial 25 finished with value: 0.013861509651637634 and parameters: {'optimizer_alpha': 0.1479891001264405, 'optimizer_threshold': 0.07968227185073018}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:15<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:28,551] Trial 26 finished with value: 0.3570083914017418 and parameters: {'optimizer_alpha': 0.043798108676616716, 'optimizer_threshold': 0.04897332792278228}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:18<22:10,  4.44s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:32,226] Trial 27 finished with value: 0.4150622582602568 and parameters: {'optimizer_alpha': 0.15068116437737542, 'optimizer_threshold': 0.021069069759887926}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:22<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:35,862] Trial 28 finished with value: 3.700050355115361 and parameters: {'optimizer_alpha': 0.010125396456408323, 'optimizer_threshold': 0.14965764093394637}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:26<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:39,515] Trial 29 finished with value: 0.021500070794233023 and parameters: {'optimizer_alpha': 0.06781758457841666, 'optimizer_threshold': 0.06777839473146435}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:29<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:43,243] Trial 30 finished with value: 6.668026816313167 and parameters: {'optimizer_alpha': 0.10233334409715053, 'optimizer_threshold': 0.08055414166939011}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:33<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:46,875] Trial 31 finished with value: 0.012306646964187188 and parameters: {'optimizer_alpha': 0.3134928919385464, 'optimizer_threshold': 0.11111169108409244}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:37<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:50,525] Trial 32 finished with value: 8.859098607095596 and parameters: {'optimizer_alpha': 0.16713223047270645, 'optimizer_threshold': 0.1190935358564942}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:40<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:54,157] Trial 33 finished with value: 1.3672626103133585 and parameters: {'optimizer_alpha': 0.3430934336967171, 'optimizer_threshold': 0.059423878131347196}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:44<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:14:57,891] Trial 34 finished with value: 1.871780348204673 and parameters: {'optimizer_alpha': 0.4661231607529604, 'optimizer_threshold': 0.16281815400583297}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:48<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:01,523] Trial 35 finished with value: 2.204862952529205 and parameters: {'optimizer_alpha': 0.7316356984974135, 'optimizer_threshold': 0.10172646642443986}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:51<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:05,151] Trial 36 finished with value: 19.91545672873138 and parameters: {'optimizer_alpha': 0.1302056779764424, 'optimizer_threshold': 0.08010925746558807}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:55<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:08,792] Trial 37 finished with value: 0.012688238110828287 and parameters: {'optimizer_alpha': 0.33833885723347695, 'optimizer_threshold': 0.15946954198760302}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [02:59<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:12,482] Trial 38 finished with value: 7.426099812303505 and parameters: {'optimizer_alpha': 0.05227009128819198, 'optimizer_threshold': 0.010095886094837843}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [03:02<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:16,231] Trial 39 finished with value: 2.3859965006114248 and parameters: {'optimizer_alpha': 0.027515950948078804, 'optimizer_threshold': 0.11091125291952965}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [03:06<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:19,849] Trial 40 finished with value: 6.992818504507741 and parameters: {'optimizer_alpha': 0.11179437613094015, 'optimizer_threshold': 0.17328370434354354}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [03:10<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:23,527] Trial 41 finished with value: 1.2871904348774132 and parameters: {'optimizer_alpha': 0.33166358620038944, 'optimizer_threshold': 0.14287345158275375}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [03:14<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:27,602] Trial 42 finished with value: 0.7060844007667724 and parameters: {'optimizer_alpha': 0.20516916500244292, 'optimizer_threshold': 0.13617930270410422}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [03:17<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:31,230] Trial 43 finished with value: 2.854592892081207 and parameters: {'optimizer_alpha': 0.5080581418342636, 'optimizer_threshold': 0.08532785805895932}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [03:21<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:34,909] Trial 44 finished with value: 3.9513119599965316 and parameters: {'optimizer_alpha': 0.3710623440777088, 'optimizer_threshold': 0.0369717372877945}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [03:25<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:38,555] Trial 45 finished with value: 0.9089855802626682 and parameters: {'optimizer_alpha': 0.6543171752341113, 'optimizer_threshold': 0.1676483188603882}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [03:28<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:42,253] Trial 46 finished with value: 23.016270821050785 and parameters: {'optimizer_alpha': 0.25309041035223734, 'optimizer_threshold': 0.10807138299272248}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [03:32<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:45,890] Trial 47 finished with value: 1.4436365706253462 and parameters: {'optimizer_alpha': 0.17213195171210402, 'optimizer_threshold': 0.05590867946285119}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "  2%|▏         | 6/306 [03:36<22:10,  4.44s/it]                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:49,643] Trial 48 finished with value: 1.0308480802153825 and parameters: {'optimizer_alpha': 0.4325425174394122, 'optimizer_threshold': 0.025566935661283523}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                               \n",
      "Best trial: 23. Best value: 0.0108519: 100%|██████████| 50/50 [03:02<00:00,  3.66s/it, 182.89/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:15:53,283] Trial 49 finished with value: 0.8998029595948016 and parameters: {'optimizer_alpha': 0.08046616863013173, 'optimizer_threshold': 0.12266883804089311}. Best is trial 23 with value: 0.010851874190367752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 7/306 [03:43<5:24:59, 65.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.76812; SPICE =  8e-05 -> 0.06541, Diff = 0.76804 -> 0.70271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 11/306 [03:59<1:26:46, 17.65s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  4%|▍         | 12/306 [04:02<1:05:53, 13.45s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  5%|▍         | 14/306 [04:10<41:34,  8.54s/it]  c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  5%|▍         | 15/306 [04:14<34:35,  7.13s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  5%|▌         | 16/306 [04:18<29:56,  6.19s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  6%|▌         | 17/306 [04:22<26:32,  5.51s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  6%|▌         | 19/306 [04:30<22:17,  4.66s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:16:47,286] A new study created in memory with name: no-name-8f334006-7b16-4b2e-a8cb-b3b061fa23f8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.74593; SPICE =  0.66455; Diff = 0.08138\n",
      "Using optuna to find a better set of pysindy parameters for participant 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "  6%|▌         | 19/306 [04:37<22:17,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:16:50,972] Trial 0 finished with value: 0.43856525435131377 and parameters: {'optimizer_alpha': 0.17538422236205378, 'optimizer_threshold': 0.018697665672913658}. Best is trial 0 with value: 0.43856525435131377.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      "  6%|▌         | 19/306 [04:41<22:17,  4.66s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:16:54,562] Trial 1 finished with value: 0.020487415155735628 and parameters: {'optimizer_alpha': 0.028883136584883874, 'optimizer_threshold': 0.11193564826080998}. Best is trial 1 with value: 0.020487415155735628.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "Best trial: 2. Best value: 0.00292231:   6%|▌         | 3/50 [00:10<02:50,  3.64s/it, 10.91/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:16:58,197] Trial 2 finished with value: 0.0029223126438255977 and parameters: {'optimizer_alpha': 0.6156769180453786, 'optimizer_threshold': 0.03918067421314951}. Best is trial 2 with value: 0.0029223126438255977.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 20/306 [04:48<41:53,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.74593; SPICE =  0.66455 -> 0.7399, Diff = 0.08138 -> 0.00603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 22/306 [04:56<29:40,  6.27s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  8%|▊         | 23/306 [05:00<26:09,  5.55s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:17:17,260] A new study created in memory with name: no-name-bb734720-218e-4cc9-92fb-aa15a9526c6c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.82724; SPICE =  0.79684; Diff = 0.03039\n",
      "Using optuna to find a better set of pysindy parameters for participant 23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "Best trial: 0. Best value: 0.00249511:   2%|▏         | 1/50 [00:03<03:00,  3.69s/it, 3.69/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:17:20,944] Trial 0 finished with value: 0.0024951090668683443 and parameters: {'optimizer_alpha': 0.17477614671282762, 'optimizer_threshold': 0.10779738628331433}. Best is trial 0 with value: 0.0024951090668683443.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  8%|▊         | 24/306 [05:11<33:56,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.82724; SPICE =  0.79684 -> 0.83908, Diff = 0.03039 -> -0.01185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  8%|▊         | 25/306 [05:15<29:05,  6.21s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  8%|▊         | 26/306 [05:18<25:42,  5.51s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  9%|▉         | 27/306 [05:22<23:24,  5.03s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "  9%|▉         | 29/306 [05:30<20:37,  4.47s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:17:47,910] A new study created in memory with name: no-name-9d76564e-9374-4b7a-9fe8-84456a09cf21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.89943; SPICE =  0.01177; Diff = 0.88766\n",
      "Using optuna to find a better set of pysindy parameters for participant 29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                \n",
      "  9%|▉         | 29/306 [05:38<20:37,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:17:51,624] Trial 0 failed with parameters: {'optimizer_alpha': 0.05042532256405286, 'optimizer_threshold': 0.051519092356700116} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:17:51,629] Trial 0 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                \n",
      "  9%|▉         | 29/306 [05:41<20:37,  4.47s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:17:55,246] Trial 1 failed with parameters: {'optimizer_alpha': 0.19771364040073652, 'optimizer_threshold': 0.03128102093498792} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:17:55,250] Trial 1 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                \n",
      "  9%|▉         | 29/306 [05:45<20:37,  4.47s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:17:58,866] Trial 2 failed with parameters: {'optimizer_alpha': 0.06223477989633186, 'optimizer_threshold': 0.0694049976415246} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:17:58,870] Trial 2 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "  9%|▉         | 29/306 [05:49<20:37,  4.47s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:18:02,476] Trial 3 finished with value: 0.08529973669490472 and parameters: {'optimizer_alpha': 0.35753493732236385, 'optimizer_threshold': 0.10764011461189645}. Best is trial 3 with value: 0.08529973669490472.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      "  9%|▉         | 29/306 [05:52<20:37,  4.47s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:18:06,102] Trial 4 failed with parameters: {'optimizer_alpha': 0.393083676012153, 'optimizer_threshold': 0.046066073961747354} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:18:06,107] Trial 4 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      "  9%|▉         | 29/306 [05:56<20:37,  4.47s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:18:09,738] Trial 5 failed with parameters: {'optimizer_alpha': 0.6625167603667299, 'optimizer_threshold': 0.012875253122804707} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:18:09,742] Trial 5 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      "  9%|▉         | 29/306 [06:00<20:37,  4.47s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:18:13,354] Trial 6 failed with parameters: {'optimizer_alpha': 0.7860476363143482, 'optimizer_threshold': 0.023174585507066643} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:18:13,359] Trial 6 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      "  9%|▉         | 29/306 [06:03<20:37,  4.47s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:18:16,964] Trial 7 failed with parameters: {'optimizer_alpha': 0.9913295753301148, 'optimizer_threshold': 0.05478027938714993} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:18:16,968] Trial 7 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      "  9%|▉         | 29/306 [06:07<20:37,  4.47s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:18:20,687] Trial 8 failed with parameters: {'optimizer_alpha': 0.06640712827920552, 'optimizer_threshold': 0.05635481558090204} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:18:20,692] Trial 8 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      "  9%|▉         | 29/306 [06:11<20:37,  4.47s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:18:24,318] Trial 9 failed with parameters: {'optimizer_alpha': 0.06984804053957333, 'optimizer_threshold': 0.0568588318853633} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:18:24,322] Trial 9 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      "  9%|▉         | 29/306 [06:14<20:37,  4.47s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:18:27,945] Trial 10 failed with parameters: {'optimizer_alpha': 0.018471464612181816, 'optimizer_threshold': 0.07633658430784146} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:18:27,949] Trial 10 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      "  9%|▉         | 29/306 [06:18<20:37,  4.47s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:18:31,572] Trial 11 failed with parameters: {'optimizer_alpha': 0.07186468901363498, 'optimizer_threshold': 0.012839125255079966} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:18:31,577] Trial 11 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      "  9%|▉         | 29/306 [06:21<20:37,  4.47s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:18:35,261] Trial 12 failed with parameters: {'optimizer_alpha': 0.18557656865715744, 'optimizer_threshold': 0.0890544776631117} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:18:35,265] Trial 12 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      "  9%|▉         | 29/306 [06:25<20:37,  4.47s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:18:38,918] Trial 13 failed with parameters: {'optimizer_alpha': 0.01159605428267306, 'optimizer_threshold': 0.11630794260773136} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:18:38,922] Trial 13 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      "  9%|▉         | 29/306 [06:29<20:37,  4.47s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:18:42,529] Trial 14 finished with value: 0.15710338492154519 and parameters: {'optimizer_alpha': 0.39605103374171663, 'optimizer_threshold': 0.18761124584673675}. Best is trial 3 with value: 0.08529973669490472.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "Best trial: 15. Best value: 0.00813956:  32%|███▏      | 16/50 [00:58<02:03,  3.64s/it, 58.24/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:18:46,146] Trial 15 finished with value: 0.008139559835906807 and parameters: {'optimizer_alpha': 0.09565322977706302, 'optimizer_threshold': 0.05550340219597438}. Best is trial 15 with value: 0.008139559835906807.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 10%|▉         | 30/306 [06:36<1:45:03, 22.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.89943; SPICE =  0.01177 -> nan, Diff = 0.88766 -> nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 32/306 [06:44<1:00:13, 13.19s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 11%|█         | 33/306 [06:48<47:17, 10.39s/it]  c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 11%|█         | 34/306 [06:51<38:12,  8.43s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 11%|█▏        | 35/306 [06:55<32:00,  7.09s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:19:13,104] A new study created in memory with name: no-name-450cc0ea-c7d2-4f42-b940-63135126ef66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.74657; SPICE =  0.57091; Diff = 0.17566\n",
      "Using optuna to find a better set of pysindy parameters for participant 35...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 11%|█▏        | 35/306 [07:03<32:00,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:19:16,736] Trial 0 finished with value: 0.011886794802125566 and parameters: {'optimizer_alpha': 0.21535783195239283, 'optimizer_threshold': 0.027389014142139335}. Best is trial 0 with value: 0.011886794802125566.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 11%|█▏        | 35/306 [07:07<32:00,  7.09s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:19:20,345] Trial 1 finished with value: 0.1102768419445583 and parameters: {'optimizer_alpha': 0.16049996856476179, 'optimizer_threshold': 0.05647172874563867}. Best is trial 0 with value: 0.011886794802125566.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 11%|█▏        | 35/306 [07:10<32:00,  7.09s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:19:23,933] Trial 2 finished with value: 0.06363630005965751 and parameters: {'optimizer_alpha': 0.26227394233778495, 'optimizer_threshold': 0.11939963872833353}. Best is trial 0 with value: 0.011886794802125566.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 11%|█▏        | 35/306 [07:14<32:00,  7.09s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:19:27,528] Trial 3 finished with value: 0.11969223967752644 and parameters: {'optimizer_alpha': 0.1677434859693534, 'optimizer_threshold': 0.10737789673037695}. Best is trial 0 with value: 0.011886794802125566.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 11%|█▏        | 35/306 [07:17<32:00,  7.09s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:19:31,244] Trial 4 finished with value: 0.15309748248697205 and parameters: {'optimizer_alpha': 0.0864819507086012, 'optimizer_threshold': 0.04689482394608572}. Best is trial 0 with value: 0.011886794802125566.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      "Best trial: 5. Best value: 0.00879752:  12%|█▏        | 6/50 [00:21<02:39,  3.64s/it, 21.81/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:19:34,912] Trial 5 finished with value: 0.008797523701306457 and parameters: {'optimizer_alpha': 0.023044565887293495, 'optimizer_threshold': 0.07551811650392676}. Best is trial 5 with value: 0.008797523701306457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 36/306 [07:25<1:01:51, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.74657; SPICE =  0.57091 -> 0.68904, Diff = 0.17566 -> 0.05753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 38/306 [07:32<38:50,  8.70s/it]  c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 40/306 [07:40<27:35,  6.22s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 41/306 [07:44<24:29,  5.55s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 14%|█▎        | 42/306 [07:48<22:09,  5.04s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 14%|█▍        | 43/306 [07:52<20:37,  4.71s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 14%|█▍        | 44/306 [07:56<19:25,  4.45s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:20:13,418] A new study created in memory with name: no-name-6df4552b-3ab5-49c2-8cc7-94ccc7588b0d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.82799; SPICE =  nan; Diff = nan\n",
      "Using optuna to find a better set of pysindy parameters for participant 44...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                \n",
      " 14%|█▍        | 44/306 [08:03<19:25,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:20:17,095] Trial 0 failed with parameters: {'optimizer_alpha': 0.34383351461182865, 'optimizer_threshold': 0.01883938382086806} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:20:17,099] Trial 0 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [08:07<19:25,  4.45s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:20:20,732] Trial 1 finished with value: 0.014932784748914515 and parameters: {'optimizer_alpha': 0.9918741142424178, 'optimizer_threshold': 0.012518161260335224}. Best is trial 1 with value: 0.014932784748914515.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                      \n",
      " 14%|█▍        | 44/306 [08:11<19:25,  4.45s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:20:24,351] Trial 2 failed with parameters: {'optimizer_alpha': 0.26334437353170503, 'optimizer_threshold': 0.08780625683283326} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:20:24,355] Trial 2 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [08:14<19:25,  4.45s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:20:27,983] Trial 3 finished with value: 1.0395496125162815 and parameters: {'optimizer_alpha': 0.025511697135136707, 'optimizer_threshold': 0.050669957346510384}. Best is trial 1 with value: 0.014932784748914515.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [08:18<19:25,  4.45s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:20:31,718] Trial 4 finished with value: 0.03846369265201185 and parameters: {'optimizer_alpha': 0.20115678213282331, 'optimizer_threshold': 0.14540669555110458}. Best is trial 1 with value: 0.014932784748914515.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 14%|█▍        | 44/306 [08:22<19:25,  4.45s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:20:35,414] Trial 5 failed with parameters: {'optimizer_alpha': 0.09662562325765374, 'optimizer_threshold': 0.06577995001764253} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:20:35,418] Trial 5 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 14%|█▍        | 44/306 [08:25<19:25,  4.45s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:20:39,058] Trial 6 failed with parameters: {'optimizer_alpha': 0.2724364853998231, 'optimizer_threshold': 0.014736201319744804} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:20:39,062] Trial 6 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [08:29<19:25,  4.45s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:20:42,747] Trial 7 finished with value: 0.01261709818315428 and parameters: {'optimizer_alpha': 0.026409316600216337, 'optimizer_threshold': 0.02093381649401331}. Best is trial 7 with value: 0.01261709818315428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 14%|█▍        | 44/306 [08:33<19:25,  4.45s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:20:46,592] Trial 8 failed with parameters: {'optimizer_alpha': 0.017473240394762658, 'optimizer_threshold': 0.011576781270167114} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:20:46,596] Trial 8 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 14%|█▍        | 44/306 [08:36<19:25,  4.45s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:20:50,312] Trial 9 failed with parameters: {'optimizer_alpha': 0.04900111841411406, 'optimizer_threshold': 0.013188176344010644} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:20:50,316] Trial 9 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      " 14%|█▍        | 44/306 [08:40<19:25,  4.45s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:20:53,976] Trial 10 failed with parameters: {'optimizer_alpha': 0.08247205530557473, 'optimizer_threshold': 0.024733302876992807} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:20:53,980] Trial 10 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      " 14%|█▍        | 44/306 [08:44<19:25,  4.45s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:20:57,711] Trial 11 failed with parameters: {'optimizer_alpha': 0.058585154663637534, 'optimizer_threshold': 0.01469654773861335} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:20:57,715] Trial 11 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [08:48<19:25,  4.45s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:21:01,336] Trial 12 finished with value: 0.25650697568604625 and parameters: {'optimizer_alpha': 0.596857337963586, 'optimizer_threshold': 0.022423568966481903}. Best is trial 7 with value: 0.01261709818315428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [08:51<19:25,  4.45s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:21:05,054] Trial 13 finished with value: 0.015182945287688954 and parameters: {'optimizer_alpha': 0.059379647749524964, 'optimizer_threshold': 0.011861661339911771}. Best is trial 7 with value: 0.01261709818315428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [08:55<19:25,  4.45s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:21:08,743] Trial 14 finished with value: 0.7815978273480986 and parameters: {'optimizer_alpha': 0.019619653113277378, 'optimizer_threshold': 0.017054375496324152}. Best is trial 7 with value: 0.01261709818315428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [08:59<19:25,  4.45s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:21:12,455] Trial 15 finished with value: 0.0170876295960892 and parameters: {'optimizer_alpha': 0.03481349376892331, 'optimizer_threshold': 0.15187330321755255}. Best is trial 7 with value: 0.01261709818315428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [09:02<19:25,  4.45s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:21:16,194] Trial 16 finished with value: 0.01509691932678559 and parameters: {'optimizer_alpha': 0.24905610322556745, 'optimizer_threshold': 0.06321648304161374}. Best is trial 7 with value: 0.01261709818315428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [09:06<19:25,  4.45s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:21:20,163] Trial 17 finished with value: 3.665387334333801 and parameters: {'optimizer_alpha': 0.3426138459918711, 'optimizer_threshold': 0.11274317164523492}. Best is trial 7 with value: 0.01261709818315428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [09:10<19:25,  4.45s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:21:24,032] Trial 18 finished with value: 0.010881340193084071 and parameters: {'optimizer_alpha': 0.010535940349846394, 'optimizer_threshold': 0.028142410800531516}. Best is trial 18 with value: 0.010881340193084071.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                         \n",
      " 14%|█▍        | 44/306 [09:14<19:25,  4.45s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:21:27,937] Trial 19 failed with parameters: {'optimizer_alpha': 0.011188908954045878, 'optimizer_threshold': 0.024315459118091055} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:21:27,941] Trial 19 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                         \n",
      " 14%|█▍        | 44/306 [09:18<19:25,  4.45s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:21:31,740] Trial 20 failed with parameters: {'optimizer_alpha': 0.010852903894471572, 'optimizer_threshold': 0.028492743800328176} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:21:31,746] Trial 20 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [09:22<19:25,  4.45s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:21:35,469] Trial 21 finished with value: 0.011767973579538961 and parameters: {'optimizer_alpha': 0.010237923789889419, 'optimizer_threshold': 0.03104501478255824}. Best is trial 18 with value: 0.010881340193084071.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [09:25<19:25,  4.45s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:21:39,282] Trial 22 finished with value: 0.011834807924001834 and parameters: {'optimizer_alpha': 0.010356134809179967, 'optimizer_threshold': 0.03379237522910697}. Best is trial 18 with value: 0.010881340193084071.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [09:29<19:25,  4.45s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:21:42,918] Trial 23 finished with value: 0.015463987567067556 and parameters: {'optimizer_alpha': 0.010207074616972771, 'optimizer_threshold': 0.033422233018078625}. Best is trial 18 with value: 0.010881340193084071.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                         \n",
      " 14%|█▍        | 44/306 [09:33<19:25,  4.45s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:21:46,558] Trial 24 failed with parameters: {'optimizer_alpha': 0.08789631549812929, 'optimizer_threshold': 0.07203561152308752} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:21:46,562] Trial 24 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [09:36<19:25,  4.45s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:21:50,187] Trial 25 finished with value: 0.035619419476272694 and parameters: {'optimizer_alpha': 0.08654150058195222, 'optimizer_threshold': 0.07169257059138946}. Best is trial 18 with value: 0.010881340193084071.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                         \n",
      " 14%|█▍        | 44/306 [09:40<19:25,  4.45s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:21:53,828] Trial 26 failed with parameters: {'optimizer_alpha': 0.01622876602120012, 'optimizer_threshold': 0.032189352206825755} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:21:53,832] Trial 26 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [09:44<19:25,  4.45s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:21:57,491] Trial 27 finished with value: 0.9034339861799306 and parameters: {'optimizer_alpha': 0.0160203830576743, 'optimizer_threshold': 0.03360408417816194}. Best is trial 18 with value: 0.010881340193084071.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [09:47<19:25,  4.45s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:22:01,117] Trial 28 finished with value: 4.888165720515818 and parameters: {'optimizer_alpha': 0.03899544344915303, 'optimizer_threshold': 0.026950972764725613}. Best is trial 18 with value: 0.010881340193084071.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 14%|█▍        | 44/306 [09:51<19:25,  4.45s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:22:04,758] Trial 29 finished with value: 0.03154445834838713 and parameters: {'optimizer_alpha': 0.10159965864867916, 'optimizer_threshold': 0.07238534573387784}. Best is trial 18 with value: 0.010881340193084071.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "Best trial: 30. Best value: 0.00978681:  62%|██████▏   | 31/50 [01:54<01:10,  3.71s/it, 114.98/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:22:08,397] Trial 30 finished with value: 0.009786810159600416 and parameters: {'optimizer_alpha': 0.013843465146044244, 'optimizer_threshold': 0.046390427442439264}. Best is trial 30 with value: 0.009786810159600416.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 15%|█▍        | 45/306 [09:58<2:53:32, 39.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.82799; SPICE =  nan -> nan, Diff = nan -> nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 16%|█▌        | 49/306 [10:14<53:30, 12.49s/it]  c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:22:31,228] A new study created in memory with name: no-name-873f2463-959c-44c1-b988-394a94bc72fb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.61662; SPICE =  0.58534; Diff = 0.03128\n",
      "Using optuna to find a better set of pysindy parameters for participant 49...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 16%|█▌        | 49/306 [10:21<53:30, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:22:34,827] Trial 0 finished with value: 0.039884920614082005 and parameters: {'optimizer_alpha': 0.1922910343894837, 'optimizer_threshold': 0.09827521798946441}. Best is trial 0 with value: 0.039884920614082005.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [10:25<53:30, 12.49s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:22:38,556] Trial 1 finished with value: 0.03266765144629053 and parameters: {'optimizer_alpha': 0.013788141115891446, 'optimizer_threshold': 0.028706533410122016}. Best is trial 1 with value: 0.03266765144629053.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 16%|█▌        | 49/306 [10:28<53:30, 12.49s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:22:42,128] Trial 2 finished with value: 0.03988989928961822 and parameters: {'optimizer_alpha': 0.0550491317533797, 'optimizer_threshold': 0.1924322466438892}. Best is trial 1 with value: 0.03266765144629053.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 16%|█▌        | 49/306 [10:32<53:30, 12.49s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:22:45,701] Trial 3 finished with value: 0.037966572091813665 and parameters: {'optimizer_alpha': 0.11475794571468066, 'optimizer_threshold': 0.19765685114367684}. Best is trial 1 with value: 0.03266765144629053.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 16%|█▌        | 49/306 [10:35<53:30, 12.49s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:22:49,267] Trial 4 finished with value: 0.028398662939702793 and parameters: {'optimizer_alpha': 0.22950518259488517, 'optimizer_threshold': 0.09445255101918819}. Best is trial 4 with value: 0.028398662939702793.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [10:39<53:30, 12.49s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:22:52,864] Trial 5 finished with value: 0.012902143926413642 and parameters: {'optimizer_alpha': 0.9484970824915271, 'optimizer_threshold': 0.04325103180750021}. Best is trial 5 with value: 0.012902143926413642.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 16%|█▌        | 49/306 [10:43<53:30, 12.49s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:22:56,438] Trial 6 finished with value: 0.03246018122358031 and parameters: {'optimizer_alpha': 0.16228885801918233, 'optimizer_threshold': 0.06837797028713255}. Best is trial 5 with value: 0.012902143926413642.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [10:46<53:30, 12.49s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:00,026] Trial 7 finished with value: 0.03555479807312908 and parameters: {'optimizer_alpha': 0.05328219395340977, 'optimizer_threshold': 0.0529796897946397}. Best is trial 5 with value: 0.012902143926413642.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [10:50<53:30, 12.49s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:03,609] Trial 8 finished with value: 0.026219609558565515 and parameters: {'optimizer_alpha': 0.01231071025612219, 'optimizer_threshold': 0.02927462586025275}. Best is trial 5 with value: 0.012902143926413642.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 16%|█▌        | 49/306 [10:53<53:30, 12.49s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:07,185] Trial 9 finished with value: 0.03858098332148792 and parameters: {'optimizer_alpha': 0.06827638117355268, 'optimizer_threshold': 0.07669860577994055}. Best is trial 5 with value: 0.012902143926413642.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [10:57<53:30, 12.49s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:10,981] Trial 10 finished with value: 0.015185945463540611 and parameters: {'optimizer_alpha': 0.9297598502569571, 'optimizer_threshold': 0.010247830697453717}. Best is trial 5 with value: 0.012902143926413642.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:01<53:30, 12.49s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:14,586] Trial 11 finished with value: 0.15698913138773327 and parameters: {'optimizer_alpha': 0.9710044434370639, 'optimizer_threshold': 0.011419545047773217}. Best is trial 5 with value: 0.012902143926413642.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:04<53:30, 12.49s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:18,193] Trial 12 finished with value: 0.37363195175969877 and parameters: {'optimizer_alpha': 0.9188145526451368, 'optimizer_threshold': 0.011904126781355657}. Best is trial 5 with value: 0.012902143926413642.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:08<53:30, 12.49s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:21,812] Trial 13 finished with value: 0.012034878929308216 and parameters: {'optimizer_alpha': 0.4449459726096925, 'optimizer_threshold': 0.01903628399121618}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:12<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:25,409] Trial 14 finished with value: 0.014347896405764298 and parameters: {'optimizer_alpha': 0.32494694452271944, 'optimizer_threshold': 0.020873981260445842}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:15<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:29,011] Trial 15 finished with value: 0.018772595373025905 and parameters: {'optimizer_alpha': 0.44727079997077934, 'optimizer_threshold': 0.022015308087988756}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:19<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:32,597] Trial 16 finished with value: 0.019269399142914125 and parameters: {'optimizer_alpha': 0.4373672297306478, 'optimizer_threshold': 0.03530649471133683}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:22<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:36,201] Trial 17 finished with value: 0.332975707165334 and parameters: {'optimizer_alpha': 0.5403249328515354, 'optimizer_threshold': 0.016371584710763932}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:26<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:39,803] Trial 18 finished with value: 0.025369932630947147 and parameters: {'optimizer_alpha': 0.5404737232677423, 'optimizer_threshold': 0.044341002291320736}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:30<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:43,480] Trial 19 finished with value: 0.017854463520754156 and parameters: {'optimizer_alpha': 0.0296107133806527, 'optimizer_threshold': 0.016432466328839716}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 16%|█▌        | 49/306 [11:33<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:47,043] Trial 20 finished with value: 0.035122170818870445 and parameters: {'optimizer_alpha': 0.27331493364758846, 'optimizer_threshold': 0.13070565886221192}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:37<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:50,654] Trial 21 finished with value: 0.013036445617427016 and parameters: {'optimizer_alpha': 0.3537378400002454, 'optimizer_threshold': 0.02182542387713436}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:40<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:54,245] Trial 22 finished with value: 0.013243886430926577 and parameters: {'optimizer_alpha': 0.7422379518298542, 'optimizer_threshold': 0.038429246944618876}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:44<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:23:57,854] Trial 23 finished with value: 0.012293346378556608 and parameters: {'optimizer_alpha': 0.3859602744192802, 'optimizer_threshold': 0.02355385759339669}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:48<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:01,446] Trial 24 finished with value: 0.3157068815559272 and parameters: {'optimizer_alpha': 0.5930574123676567, 'optimizer_threshold': 0.015636506675588236}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:51<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:05,035] Trial 25 finished with value: 0.013521652950950739 and parameters: {'optimizer_alpha': 0.12454209599226665, 'optimizer_threshold': 0.028226745780935078}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 16%|█▌        | 49/306 [11:55<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:08,632] Trial 26 finished with value: 0.018719331549986038 and parameters: {'optimizer_alpha': 0.677183639216778, 'optimizer_threshold': 0.05448699072590329}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [11:59<53:30, 12.49s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:12,331] Trial 27 finished with value: 0.01399643963340441 and parameters: {'optimizer_alpha': 0.3625883633052848, 'optimizer_threshold': 0.03407862213644231}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:02<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:15,924] Trial 28 finished with value: 0.3380886352346804 and parameters: {'optimizer_alpha': 0.24777017349885477, 'optimizer_threshold': 0.02403429995090613}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:06<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:19,519] Trial 29 finished with value: 0.3083276414591296 and parameters: {'optimizer_alpha': 0.16754878776723803, 'optimizer_threshold': 0.013677844371977735}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:09<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:23,119] Trial 30 finished with value: 0.014545814154034945 and parameters: {'optimizer_alpha': 0.08439573983954893, 'optimizer_threshold': 0.01863774124990034}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:13<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:26,715] Trial 31 finished with value: 0.014160178975672696 and parameters: {'optimizer_alpha': 0.4002639792011059, 'optimizer_threshold': 0.02511999824542671}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:17<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:30,319] Trial 32 finished with value: 0.013417438349809059 and parameters: {'optimizer_alpha': 0.31838302709594385, 'optimizer_threshold': 0.019339037531666898}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:20<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:33,907] Trial 33 finished with value: 0.015517062808492266 and parameters: {'optimizer_alpha': 0.681341409241708, 'optimizer_threshold': 0.04447949343404559}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:24<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:37,561] Trial 34 finished with value: 0.30046969075845004 and parameters: {'optimizer_alpha': 0.21133574137356106, 'optimizer_threshold': 0.030620432164131225}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:27<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:41,165] Trial 35 finished with value: 0.021168297140898722 and parameters: {'optimizer_alpha': 0.021782090388301256, 'optimizer_threshold': 0.02538698756083274}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:31<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:44,857] Trial 36 finished with value: 0.2591749172857134 and parameters: {'optimizer_alpha': 0.4708662304921766, 'optimizer_threshold': 0.013945421349510587}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:35<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:48,438] Trial 37 finished with value: 0.16671600018195984 and parameters: {'optimizer_alpha': 0.7143957156003182, 'optimizer_threshold': 0.062317033552237854}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 16%|█▌        | 49/306 [12:38<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:52,080] Trial 38 finished with value: 0.036842303804588215 and parameters: {'optimizer_alpha': 0.14335160279538608, 'optimizer_threshold': 0.14783569089069445}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:42<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:55,757] Trial 39 finished with value: 0.3616677939630163 and parameters: {'optimizer_alpha': 0.19231847724053588, 'optimizer_threshold': 0.038426772552475716}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 16%|█▌        | 49/306 [12:46<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:24:59,373] Trial 40 finished with value: 0.029365426378305654 and parameters: {'optimizer_alpha': 0.2850113341317119, 'optimizer_threshold': 0.09414841950110935}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:49<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:25:02,969] Trial 41 finished with value: 0.02199395294365391 and parameters: {'optimizer_alpha': 0.8607776460748785, 'optimizer_threshold': 0.03929130323161914}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:53<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:25:06,683] Trial 42 finished with value: 0.013077623800418586 and parameters: {'optimizer_alpha': 0.7826533007887618, 'optimizer_threshold': 0.031357292480911195}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [12:56<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:25:10,282] Trial 43 finished with value: 0.012591688268285 and parameters: {'optimizer_alpha': 0.5043339145222718, 'optimizer_threshold': 0.02868978805647807}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [13:00<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:25:13,883] Trial 44 finished with value: 0.2851383316359345 and parameters: {'optimizer_alpha': 0.5698975328060872, 'optimizer_threshold': 0.027158579545722938}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [13:04<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:25:17,718] Trial 45 finished with value: 0.012748086493983696 and parameters: {'optimizer_alpha': 0.34621024594782557, 'optimizer_threshold': 0.021847676518187014}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [13:08<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:25:21,671] Trial 46 finished with value: 0.012633175911849216 and parameters: {'optimizer_alpha': 0.4730190923632289, 'optimizer_threshold': 0.01833664396938623}. Best is trial 13 with value: 0.012034878929308216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [13:11<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:25:25,272] Trial 47 finished with value: 0.01077815903946913 and parameters: {'optimizer_alpha': 0.4456034960768503, 'optimizer_threshold': 0.018182071342257886}. Best is trial 47 with value: 0.01077815903946913.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 49/306 [13:15<53:30, 12.49s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:25:28,907] Trial 48 finished with value: 0.2878165879210622 and parameters: {'optimizer_alpha': 0.490795749229572, 'optimizer_threshold': 0.018293442207838143}. Best is trial 47 with value: 0.01077815903946913.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "Best trial: 47. Best value: 0.0107782: 100%|██████████| 50/50 [03:01<00:00,  3.63s/it, 181.29/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:25:32,512] Trial 49 finished with value: 0.3090290747709199 and parameters: {'optimizer_alpha': 0.24520737499977702, 'optimizer_threshold': 0.013904138568660341}. Best is trial 47 with value: 0.01077815903946913.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 16%|█▋        | 50/306 [13:22<4:38:56, 65.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.61662; SPICE =  0.58534 -> 2e-05, Diff = 0.03128 -> 0.6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 51/306 [13:26<3:19:24, 46.92s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 52/306 [13:30<2:23:56, 34.00s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:25:47,918] A new study created in memory with name: no-name-34753c81-1f2f-42d6-953b-858c48e1f91e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.88493; SPICE =  nan; Diff = nan\n",
      "Using optuna to find a better set of pysindy parameters for participant 52...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 17%|█▋        | 52/306 [13:38<2:23:56, 34.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:25:51,541] Trial 0 finished with value: 1.9922063169040416 and parameters: {'optimizer_alpha': 0.03006494904948207, 'optimizer_threshold': 0.06709637783951529}. Best is trial 0 with value: 1.9922063169040416.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                                                                    \n",
      " 17%|█▋        | 52/306 [13:41<2:23:56, 34.00s/it]                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:25:55,149] Trial 1 failed with parameters: {'optimizer_alpha': 0.05085213950596929, 'optimizer_threshold': 0.11816824288527751} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:25:55,153] Trial 1 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 17%|█▋        | 52/306 [13:45<2:23:56, 34.00s/it]                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:25:58,875] Trial 2 finished with value: 0.023520093358515064 and parameters: {'optimizer_alpha': 0.032494931197063696, 'optimizer_threshold': 0.0974959103798178}. Best is trial 2 with value: 0.023520093358515064.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                                                                       \n",
      " 17%|█▋        | 52/306 [13:49<2:23:56, 34.00s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:26:02,494] Trial 3 failed with parameters: {'optimizer_alpha': 0.015500468159127196, 'optimizer_threshold': 0.1266833623105928} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:26:02,498] Trial 3 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "Best trial: 4. Best value: 0.00068652:  10%|█         | 5/50 [00:18<02:43,  3.64s/it, 18.21/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:26:06,127] Trial 4 finished with value: 0.0006865199497911311 and parameters: {'optimizer_alpha': 0.14709551644030797, 'optimizer_threshold': 0.013143787814482766}. Best is trial 4 with value: 0.0006865199497911311.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 53/306 [13:56<2:13:01, 31.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.88493; SPICE =  nan -> 0.16294, Diff = nan -> 0.72199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 54/306 [14:00<1:37:36, 23.24s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 56/306 [14:08<55:45, 13.38s/it]  c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 58/306 [14:15<35:15,  8.53s/it][I 2025-09-26 12:26:32,978] A new study created in memory with name: no-name-d70a881a-298a-49df-8c4b-36434a4b0296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.62747; SPICE =  0.56401; Diff = 0.06346\n",
      "Using optuna to find a better set of pysindy parameters for participant 58...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 19%|█▉        | 58/306 [14:23<35:15,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:26:36,558] Trial 0 finished with value: 0.045116831737186135 and parameters: {'optimizer_alpha': 0.36627932788352624, 'optimizer_threshold': 0.16588211916003945}. Best is trial 0 with value: 0.045116831737186135.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 19%|█▉        | 58/306 [14:26<35:15,  8.53s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:26:40,148] Trial 1 finished with value: 0.011006270444502983 and parameters: {'optimizer_alpha': 0.16851418900424758, 'optimizer_threshold': 0.05929907121140444}. Best is trial 1 with value: 0.011006270444502983.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 19%|█▉        | 58/306 [14:30<35:15,  8.53s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:26:43,714] Trial 2 finished with value: 0.031394376959666535 and parameters: {'optimizer_alpha': 0.08882537404086212, 'optimizer_threshold': 0.06725434066598798}. Best is trial 1 with value: 0.011006270444502983.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 19%|█▉        | 58/306 [14:33<35:15,  8.53s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:26:47,272] Trial 3 finished with value: 0.06231799375543483 and parameters: {'optimizer_alpha': 0.5471254089603037, 'optimizer_threshold': 0.16844926463795654}. Best is trial 1 with value: 0.011006270444502983.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 19%|█▉        | 58/306 [14:37<35:15,  8.53s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:26:50,855] Trial 4 finished with value: 0.08599216985553557 and parameters: {'optimizer_alpha': 0.7953760072788378, 'optimizer_threshold': 0.11006035304153634}. Best is trial 1 with value: 0.011006270444502983.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 19%|█▉        | 58/306 [14:41<35:15,  8.53s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:26:54,751] Trial 5 finished with value: 0.03719082756539134 and parameters: {'optimizer_alpha': 0.02148249448151311, 'optimizer_threshold': 0.020015792417762062}. Best is trial 1 with value: 0.011006270444502983.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 19%|█▉        | 58/306 [14:45<35:15,  8.53s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:26:58,388] Trial 6 finished with value: 0.03419462955836353 and parameters: {'optimizer_alpha': 0.02012677223537418, 'optimizer_threshold': 0.18981044953242138}. Best is trial 1 with value: 0.011006270444502983.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 19%|█▉        | 58/306 [14:48<35:15,  8.53s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:27:01,942] Trial 7 finished with value: 0.046490722922647484 and parameters: {'optimizer_alpha': 0.06803441932519967, 'optimizer_threshold': 0.11879901661077027}. Best is trial 1 with value: 0.011006270444502983.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 19%|█▉        | 58/306 [14:52<35:15,  8.53s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:27:05,508] Trial 8 finished with value: 0.031881137926671604 and parameters: {'optimizer_alpha': 0.010879099369416848, 'optimizer_threshold': 0.1366360124292575}. Best is trial 1 with value: 0.011006270444502983.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "Best trial: 9. Best value: 0.00375513:  20%|██        | 10/50 [00:36<02:24,  3.61s/it, 36.11/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:27:09,090] Trial 9 finished with value: 0.003755130910520145 and parameters: {'optimizer_alpha': 0.7272473793344364, 'optimizer_threshold': 0.012036844902576158}. Best is trial 9 with value: 0.003755130910520145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 59/306 [14:59<1:18:20, 19.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.62747; SPICE =  0.56401 -> 0.606, Diff = 0.06346 -> 0.02147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 60/306 [15:03<59:21, 14.48s/it]  c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 21%|██        | 63/306 [15:14<30:28,  7.52s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 21%|██        | 64/306 [15:18<25:53,  6.42s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 21%|██        | 65/306 [15:22<22:56,  5.71s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 67/306 [15:30<19:00,  4.77s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 68/306 [15:34<17:51,  4.50s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 23%|██▎       | 69/306 [15:38<17:01,  4.31s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 23%|██▎       | 70/306 [15:42<16:27,  4.18s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 23%|██▎       | 71/306 [15:46<16:08,  4.12s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 73/306 [15:53<15:30,  3.99s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 74/306 [15:57<15:17,  3.95s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 25%|██▍       | 76/306 [16:05<14:51,  3.87s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:28:22,559] A new study created in memory with name: no-name-7c107f77-393f-46b0-ba2d-8e58662a6bc7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.82554; SPICE =  nan; Diff = nan\n",
      "Using optuna to find a better set of pysindy parameters for participant 76...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                \n",
      " 25%|██▍       | 76/306 [16:12<14:51,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:28:26,177] Trial 0 failed with parameters: {'optimizer_alpha': 0.10381374080549023, 'optimizer_threshold': 0.019773193682959742} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:28:26,182] Trial 0 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                \n",
      " 25%|██▍       | 76/306 [16:16<14:51,  3.87s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:28:29,797] Trial 1 failed with parameters: {'optimizer_alpha': 0.024763590715286606, 'optimizer_threshold': 0.01271779354699342} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:28:29,802] Trial 1 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                \n",
      " 25%|██▍       | 76/306 [16:20<14:51,  3.87s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:28:33,513] Trial 2 failed with parameters: {'optimizer_alpha': 0.7944604958700235, 'optimizer_threshold': 0.02772983682833876} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:28:33,518] Trial 2 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                 \n",
      " 25%|██▍       | 76/306 [16:23<14:51,  3.87s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:28:37,183] Trial 3 failed with parameters: {'optimizer_alpha': 0.20410229882274922, 'optimizer_threshold': 0.11841359219120053} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:28:37,188] Trial 3 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 25%|██▍       | 76/306 [16:27<14:51,  3.87s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:28:40,839] Trial 4 finished with value: 0.16231594204213184 and parameters: {'optimizer_alpha': 0.2000200640813032, 'optimizer_threshold': 0.1710796494914592}. Best is trial 4 with value: 0.16231594204213184.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                      \n",
      " 25%|██▍       | 76/306 [16:31<14:51,  3.87s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:28:44,470] Trial 5 failed with parameters: {'optimizer_alpha': 0.0445986613502225, 'optimizer_threshold': 0.011204626120435183} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:28:44,474] Trial 5 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                      \n",
      " 25%|██▍       | 76/306 [16:34<14:51,  3.87s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:28:48,108] Trial 6 failed with parameters: {'optimizer_alpha': 0.12569283873836987, 'optimizer_threshold': 0.025456121277181335} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:28:48,112] Trial 6 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                      \n",
      " 25%|██▍       | 76/306 [16:38<14:51,  3.87s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:28:51,740] Trial 7 failed with parameters: {'optimizer_alpha': 0.1024333808681078, 'optimizer_threshold': 0.047589296103889} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:28:51,744] Trial 7 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                      \n",
      " 25%|██▍       | 76/306 [16:42<14:51,  3.87s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:28:55,374] Trial 8 failed with parameters: {'optimizer_alpha': 0.020596563100410307, 'optimizer_threshold': 0.013832908126402846} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:28:55,380] Trial 8 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                      \n",
      " 25%|██▍       | 76/306 [16:45<14:51,  3.87s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:28:59,021] Trial 9 failed with parameters: {'optimizer_alpha': 0.34122019216410593, 'optimizer_threshold': 0.015444193435327127} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:28:59,025] Trial 9 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 25%|██▍       | 76/306 [16:49<14:51,  3.87s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:29:02,744] Trial 10 failed with parameters: {'optimizer_alpha': 0.23714341027831592, 'optimizer_threshold': 0.011861413177179994} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:29:02,749] Trial 10 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 25%|██▍       | 76/306 [16:53<14:51,  3.87s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:29:06,383] Trial 11 failed with parameters: {'optimizer_alpha': 0.031149758664136305, 'optimizer_threshold': 0.02586950078269399} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:29:06,387] Trial 11 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 25%|██▍       | 76/306 [16:56<14:51,  3.87s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:29:10,021] Trial 12 failed with parameters: {'optimizer_alpha': 0.13155320516562338, 'optimizer_threshold': 0.011984358155342375} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:29:10,025] Trial 12 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 25%|██▍       | 76/306 [17:00<14:51,  3.87s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:29:13,726] Trial 13 failed with parameters: {'optimizer_alpha': 0.4181166567731751, 'optimizer_threshold': 0.03376164428189947} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:29:13,730] Trial 13 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 25%|██▍       | 76/306 [17:04<14:51,  3.87s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:29:17,372] Trial 14 finished with value: 8.526509261477326 and parameters: {'optimizer_alpha': 0.01253123262068023, 'optimizer_threshold': 0.07277726869355769}. Best is trial 4 with value: 0.16231594204213184.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 25%|██▍       | 76/306 [17:07<14:51,  3.87s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:29:21,016] Trial 15 failed with parameters: {'optimizer_alpha': 0.2226373670496549, 'optimizer_threshold': 0.09871306139885148} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:29:21,020] Trial 15 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 25%|██▍       | 76/306 [17:11<14:51,  3.87s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:29:24,649] Trial 16 failed with parameters: {'optimizer_alpha': 0.08041687265726627, 'optimizer_threshold': 0.034541620086651105} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:29:24,653] Trial 16 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 25%|██▍       | 76/306 [17:14<14:51,  3.87s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:29:28,295] Trial 17 failed with parameters: {'optimizer_alpha': 0.01111627579291347, 'optimizer_threshold': 0.01434501406847711} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:29:28,299] Trial 17 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 25%|██▍       | 76/306 [17:18<14:51,  3.87s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:29:31,919] Trial 18 failed with parameters: {'optimizer_alpha': 0.36848063877920995, 'optimizer_threshold': 0.010870922266221836} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:29:31,923] Trial 18 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 25%|██▍       | 76/306 [17:22<14:51,  3.87s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:29:35,551] Trial 19 finished with value: 0.07183827353318517 and parameters: {'optimizer_alpha': 0.029980185476163948, 'optimizer_threshold': 0.17332532114399393}. Best is trial 19 with value: 0.07183827353318517.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                         \n",
      " 25%|██▍       | 76/306 [17:26<14:51,  3.87s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:29:39,339] Trial 20 failed with parameters: {'optimizer_alpha': 0.2611552683673152, 'optimizer_threshold': 0.014596541702379765} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:29:39,343] Trial 20 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                         \n",
      " 25%|██▍       | 76/306 [17:29<14:51,  3.87s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:29:42,979] Trial 21 failed with parameters: {'optimizer_alpha': 0.4847965468707989, 'optimizer_threshold': 0.052107926425507406} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:29:42,983] Trial 21 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 25%|██▍       | 76/306 [17:33<14:51,  3.87s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:29:46,610] Trial 22 finished with value: 0.49855417599180724 and parameters: {'optimizer_alpha': 0.900606944427271, 'optimizer_threshold': 0.031371619750549536}. Best is trial 19 with value: 0.07183827353318517.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 25%|██▍       | 76/306 [17:36<14:51,  3.87s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:29:50,239] Trial 23 finished with value: 1.1756371874073428 and parameters: {'optimizer_alpha': 0.32299428105344535, 'optimizer_threshold': 0.020048383033323673}. Best is trial 19 with value: 0.07183827353318517.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 25%|██▍       | 76/306 [17:40<14:51,  3.87s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:29:53,884] Trial 24 finished with value: 1.3596641427623979 and parameters: {'optimizer_alpha': 0.16572878593551632, 'optimizer_threshold': 0.029000020636953245}. Best is trial 19 with value: 0.07183827353318517.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                         \n",
      " 25%|██▍       | 76/306 [17:44<14:51,  3.87s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:29:57,523] Trial 25 failed with parameters: {'optimizer_alpha': 0.2527004799815035, 'optimizer_threshold': 0.026420261086095784} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:29:57,527] Trial 25 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 25%|██▍       | 76/306 [17:47<14:51,  3.87s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:30:01,155] Trial 26 finished with value: 0.01771845115138439 and parameters: {'optimizer_alpha': 0.34467653725663616, 'optimizer_threshold': 0.017619191212224335}. Best is trial 26 with value: 0.01771845115138439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                         \n",
      " 25%|██▍       | 76/306 [17:51<14:51,  3.87s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:04,774] Trial 27 failed with parameters: {'optimizer_alpha': 0.3691924495753978, 'optimizer_threshold': 0.04611371290422195} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:04,778] Trial 27 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [17:55<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:08,409] Trial 28 failed with parameters: {'optimizer_alpha': 0.08762776338387195, 'optimizer_threshold': 0.020050197387073317} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:08,414] Trial 28 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [17:58<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:12,133] Trial 29 failed with parameters: {'optimizer_alpha': 0.010856751415706729, 'optimizer_threshold': 0.029702779683643758} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:12,137] Trial 29 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:02<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:15,871] Trial 30 failed with parameters: {'optimizer_alpha': 0.08186492738799443, 'optimizer_threshold': 0.015411953576544716} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:15,876] Trial 30 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:06<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:19,558] Trial 31 failed with parameters: {'optimizer_alpha': 0.016621441920717423, 'optimizer_threshold': 0.05011199079016262} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:19,563] Trial 31 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:09<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:23,199] Trial 32 failed with parameters: {'optimizer_alpha': 0.127105446363964, 'optimizer_threshold': 0.02529845682547839} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:23,204] Trial 32 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:13<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:26,863] Trial 33 failed with parameters: {'optimizer_alpha': 0.713718516756325, 'optimizer_threshold': 0.021305813691086445} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:26,868] Trial 33 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:17<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:30,495] Trial 34 failed with parameters: {'optimizer_alpha': 0.011728624354808606, 'optimizer_threshold': 0.16978634179991411} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:30,500] Trial 34 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:20<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:34,123] Trial 35 failed with parameters: {'optimizer_alpha': 0.016954499291540512, 'optimizer_threshold': 0.08195271580851869} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:34,128] Trial 35 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:24<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:37,770] Trial 36 failed with parameters: {'optimizer_alpha': 0.06601500080948139, 'optimizer_threshold': 0.042693359003539155} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:37,775] Trial 36 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:28<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:41,422] Trial 37 failed with parameters: {'optimizer_alpha': 0.8913863669469433, 'optimizer_threshold': 0.011459926997671722} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:41,425] Trial 37 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:31<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:45,131] Trial 38 failed with parameters: {'optimizer_alpha': 0.1207265034338085, 'optimizer_threshold': 0.013022356801174783} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:45,135] Trial 38 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 25%|██▍       | 76/306 [18:35<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:30:48,765] Trial 39 finished with value: 1.1915859848428627 and parameters: {'optimizer_alpha': 0.5070367213315813, 'optimizer_threshold': 0.01667553008854406}. Best is trial 26 with value: 0.01771845115138439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 25%|██▍       | 76/306 [18:39<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:30:52,383] Trial 40 finished with value: 0.04695174521112826 and parameters: {'optimizer_alpha': 0.02081320130039302, 'optimizer_threshold': 0.06528135988096534}. Best is trial 26 with value: 0.01771845115138439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:42<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:56,114] Trial 41 failed with parameters: {'optimizer_alpha': 0.09004523140118106, 'optimizer_threshold': 0.010047112596123016} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:56,118] Trial 41 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:46<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:30:59,737] Trial 42 failed with parameters: {'optimizer_alpha': 0.13160160901933204, 'optimizer_threshold': 0.08389649018787217} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:30:59,741] Trial 42 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:50<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:31:03,358] Trial 43 failed with parameters: {'optimizer_alpha': 0.26262257287501717, 'optimizer_threshold': 0.12921895942442252} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:31:03,362] Trial 43 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:53<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:31:06,991] Trial 44 failed with parameters: {'optimizer_alpha': 0.10321856136478301, 'optimizer_threshold': 0.08319826614673219} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:31:06,995] Trial 44 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [18:57<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:31:10,620] Trial 45 failed with parameters: {'optimizer_alpha': 0.09972150137583435, 'optimizer_threshold': 0.06620726183695315} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:31:10,624] Trial 45 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [19:01<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:31:14,338] Trial 46 failed with parameters: {'optimizer_alpha': 0.03866691263431543, 'optimizer_threshold': 0.08029775402429695} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:31:14,342] Trial 46 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [19:04<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:31:17,977] Trial 47 failed with parameters: {'optimizer_alpha': 0.8277199300645658, 'optimizer_threshold': 0.028795984423450417} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:31:17,982] Trial 47 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 25%|██▍       | 76/306 [19:08<14:51,  3.87s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:31:21,611] Trial 48 failed with parameters: {'optimizer_alpha': 0.08121932928113432, 'optimizer_threshold': 0.07423745619087946} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:31:21,615] Trial 48 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      "Best trial: 26. Best value: 0.0177185: 100%|██████████| 50/50 [03:02<00:00,  3.65s/it, 182.69/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:31:25,244] Trial 49 failed with parameters: {'optimizer_alpha': 0.06177750210855219, 'optimizer_threshold': 0.06117911515753547} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:31:25,248] Trial 49 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 77/306 [19:15<3:48:08, 59.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.82554; SPICE =  nan -> nan, Diff = nan -> nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 78/306 [19:19<2:43:25, 43.01s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:31:36,623] A new study created in memory with name: no-name-add560d3-4781-4f66-8995-6f30b9344466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.96136; SPICE =  nan; Diff = nan\n",
      "Using optuna to find a better set of pysindy parameters for participant 78...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                  \n",
      " 25%|██▌       | 78/306 [19:26<2:43:25, 43.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:31:40,275] Trial 0 failed with parameters: {'optimizer_alpha': 0.01409726446232668, 'optimizer_threshold': 0.11964571382105138} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:31:40,279] Trial 0 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                                \n",
      " 25%|██▌       | 78/306 [19:30<2:43:25, 43.01s/it]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:31:44,002] Trial 1 failed with parameters: {'optimizer_alpha': 0.1387943882996201, 'optimizer_threshold': 0.1612373968349138} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:31:44,005] Trial 1 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                                \n",
      " 25%|██▌       | 78/306 [19:34<2:43:25, 43.01s/it]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:31:47,640] Trial 2 failed with parameters: {'optimizer_alpha': 0.013354995118848112, 'optimizer_threshold': 0.01175861176134039} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:31:47,645] Trial 2 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 25%|██▌       | 78/306 [19:37<2:43:25, 43.01s/it]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:31:51,289] Trial 3 finished with value: 0.033273469354973516 and parameters: {'optimizer_alpha': 0.019705916627054322, 'optimizer_threshold': 0.020432278035810554}. Best is trial 3 with value: 0.033273469354973516.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                                                                       \n",
      " 25%|██▌       | 78/306 [19:41<2:43:25, 43.01s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:31:55,082] Trial 4 failed with parameters: {'optimizer_alpha': 0.43923851916491935, 'optimizer_threshold': 0.013334448953892965} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:31:55,086] Trial 4 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                                                                       \n",
      " 25%|██▌       | 78/306 [19:45<2:43:25, 43.01s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:31:58,722] Trial 5 failed with parameters: {'optimizer_alpha': 0.09645444606552239, 'optimizer_threshold': 0.04040688413084459} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:31:58,727] Trial 5 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                                                                       \n",
      " 25%|██▌       | 78/306 [19:49<2:43:25, 43.01s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:32:02,362] Trial 6 failed with parameters: {'optimizer_alpha': 0.011621749004616235, 'optimizer_threshold': 0.05471880121576006} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:32:02,366] Trial 6 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                                                                       \n",
      " 25%|██▌       | 78/306 [19:52<2:43:25, 43.01s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:32:05,999] Trial 7 failed with parameters: {'optimizer_alpha': 0.9915558665818606, 'optimizer_threshold': 0.010895366559942056} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:32:06,003] Trial 7 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                                                                       \n",
      " 25%|██▌       | 78/306 [19:56<2:43:25, 43.01s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:32:09,630] Trial 8 failed with parameters: {'optimizer_alpha': 0.09303028393390697, 'optimizer_threshold': 0.10814587200335496} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:32:09,635] Trial 8 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                                                                       \n",
      " 25%|██▌       | 78/306 [19:59<2:43:25, 43.01s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:32:13,251] Trial 9 failed with parameters: {'optimizer_alpha': 0.010189949724348299, 'optimizer_threshold': 0.0795210092368824} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:32:13,255] Trial 9 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                                                                        \n",
      " 25%|██▌       | 78/306 [20:03<2:43:25, 43.01s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:32:16,870] Trial 10 failed with parameters: {'optimizer_alpha': 0.32450946037772055, 'optimizer_threshold': 0.012768719679886941} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:32:16,874] Trial 10 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "                                                                                                        \n",
      " 25%|██▌       | 78/306 [20:07<2:43:25, 43.01s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:32:20,592] Trial 11 failed with parameters: {'optimizer_alpha': 0.08251452572108804, 'optimizer_threshold': 0.06490447659235872} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:32:20,596] Trial 11 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "Best trial: 12. Best value: 0.00585192:  26%|██▌       | 13/50 [00:47<02:15,  3.66s/it, 47.60/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:32:24,217] Trial 12 finished with value: 0.0058519211791438555 and parameters: {'optimizer_alpha': 0.5003663436659171, 'optimizer_threshold': 0.04461431104111599}. Best is trial 12 with value: 0.0058519211791438555.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 26%|██▌       | 79/306 [20:14<2:56:25, 46.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.96136; SPICE =  nan -> nan, Diff = nan -> nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 26%|██▋       | 81/306 [20:22<1:33:04, 24.82s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 82/306 [20:26<1:09:13, 18.54s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 83/306 [20:29<52:31, 14.13s/it]  c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 84/306 [20:33<40:52, 11.05s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 87/306 [20:45<23:08,  6.34s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:33:02,679] A new study created in memory with name: no-name-41d916bb-c737-4dac-85f4-cb860cfe7e1e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.77331; SPICE =  nan; Diff = nan\n",
      "Using optuna to find a better set of pysindy parameters for participant 87...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                \n",
      " 28%|██▊       | 87/306 [20:52<23:08,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:33:06,283] Trial 0 failed with parameters: {'optimizer_alpha': 0.4319498354683752, 'optimizer_threshold': 0.07522865577428396} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:33:06,288] Trial 0 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                \n",
      " 28%|██▊       | 87/306 [20:56<23:08,  6.34s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:33:09,918] Trial 1 failed with parameters: {'optimizer_alpha': 0.011532724147294818, 'optimizer_threshold': 0.03342844987227219} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:33:09,923] Trial 1 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                \n",
      " 28%|██▊       | 87/306 [21:00<23:08,  6.34s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:33:13,539] Trial 2 failed with parameters: {'optimizer_alpha': 0.13929537884884516, 'optimizer_threshold': 0.01949906717845645} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:33:13,544] Trial 2 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                 \n",
      " 28%|██▊       | 87/306 [21:03<23:08,  6.34s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:33:17,148] Trial 3 failed with parameters: {'optimizer_alpha': 0.18041276073158474, 'optimizer_threshold': 0.06888317908046702} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:33:17,152] Trial 3 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 28%|██▊       | 87/306 [21:07<23:08,  6.34s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:33:20,868] Trial 4 finished with value: 0.1456053916481244 and parameters: {'optimizer_alpha': 0.016910105603853344, 'optimizer_threshold': 0.08235547656954609}. Best is trial 4 with value: 0.1456053916481244.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 28%|██▊       | 87/306 [21:11<23:08,  6.34s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:33:24,487] Trial 5 finished with value: 0.49718995484424644 and parameters: {'optimizer_alpha': 0.03696015191243686, 'optimizer_threshold': 0.13250628701158634}. Best is trial 4 with value: 0.1456053916481244.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                      \n",
      " 28%|██▊       | 87/306 [21:14<23:08,  6.34s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:33:28,110] Trial 6 failed with parameters: {'optimizer_alpha': 0.013751977655213224, 'optimizer_threshold': 0.07526225229925483} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:33:28,114] Trial 6 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                      \n",
      " 28%|██▊       | 87/306 [21:18<23:08,  6.34s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:33:31,747] Trial 7 failed with parameters: {'optimizer_alpha': 0.027217115028060606, 'optimizer_threshold': 0.05986423938543065} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:33:31,751] Trial 7 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                      \n",
      " 28%|██▊       | 87/306 [21:22<23:08,  6.34s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:33:35,368] Trial 8 failed with parameters: {'optimizer_alpha': 0.14512359628806998, 'optimizer_threshold': 0.06880222385266266} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:33:35,373] Trial 8 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                      \n",
      " 28%|██▊       | 87/306 [21:25<23:08,  6.34s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:33:39,014] Trial 9 failed with parameters: {'optimizer_alpha': 0.5730448727745336, 'optimizer_threshold': 0.03424989437682908} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:33:39,018] Trial 9 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 28%|██▊       | 87/306 [21:29<23:08,  6.34s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:33:42,696] Trial 10 failed with parameters: {'optimizer_alpha': 0.43830435857775485, 'optimizer_threshold': 0.022616626944919747} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:33:42,700] Trial 10 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 28%|██▊       | 87/306 [21:33<23:08,  6.34s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:33:46,436] Trial 11 failed with parameters: {'optimizer_alpha': 0.6094802647566047, 'optimizer_threshold': 0.02403824473650061} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:33:46,440] Trial 11 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                       \n",
      " 28%|██▊       | 87/306 [21:36<23:08,  6.34s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:33:50,087] Trial 12 failed with parameters: {'optimizer_alpha': 0.20907286277116244, 'optimizer_threshold': 0.039721914322764305} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:33:50,091] Trial 12 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 28%|██▊       | 87/306 [21:40<23:08,  6.34s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:33:53,816] Trial 13 finished with value: 0.03264207759439899 and parameters: {'optimizer_alpha': 0.3973615877505697, 'optimizer_threshold': 0.14653483175092058}. Best is trial 13 with value: 0.03264207759439899.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 28%|██▊       | 87/306 [21:44<23:08,  6.34s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:33:57,452] Trial 14 finished with value: 0.01510903579365797 and parameters: {'optimizer_alpha': 0.15459723025552752, 'optimizer_threshold': 0.09488592749203247}. Best is trial 14 with value: 0.01510903579365797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      " 28%|██▊       | 87/306 [21:47<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:34:01,078] Trial 15 failed with parameters: {'optimizer_alpha': 0.01889731198211587, 'optimizer_threshold': 0.09275326021505993} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:34:01,082] Trial 15 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 28%|██▊       | 87/306 [21:51<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:34:04,704] Trial 16 finished with value: 0.353437001174811 and parameters: {'optimizer_alpha': 0.04048738238783502, 'optimizer_threshold': 0.06402112297771557}. Best is trial 14 with value: 0.01510903579365797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      " 28%|██▊       | 87/306 [21:55<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:34:08,332] Trial 17 failed with parameters: {'optimizer_alpha': 0.480336555301475, 'optimizer_threshold': 0.010830794193134128} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:34:08,336] Trial 17 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      " 28%|██▊       | 87/306 [21:58<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:34:11,957] Trial 18 failed with parameters: {'optimizer_alpha': 0.10226183726020233, 'optimizer_threshold': 0.104466855481585} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:34:11,961] Trial 18 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 28%|██▊       | 87/306 [22:02<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:34:15,587] Trial 19 finished with value: 0.4038829173315184 and parameters: {'optimizer_alpha': 0.08517409400118445, 'optimizer_threshold': 0.11963506904020904}. Best is trial 14 with value: 0.01510903579365797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      " 28%|██▊       | 87/306 [22:05<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:34:19,196] Trial 20 failed with parameters: {'optimizer_alpha': 0.12117133918991463, 'optimizer_threshold': 0.026260343149149722} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:34:19,200] Trial 20 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      " 28%|██▊       | 87/306 [22:09<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:34:22,814] Trial 21 failed with parameters: {'optimizer_alpha': 0.6389314646768907, 'optimizer_threshold': 0.04099543464289227} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:34:22,817] Trial 21 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 28%|██▊       | 87/306 [22:13<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:34:26,531] Trial 22 finished with value: 0.043605327753856594 and parameters: {'optimizer_alpha': 0.03145888199891161, 'optimizer_threshold': 0.11178570969158806}. Best is trial 14 with value: 0.01510903579365797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      " 28%|██▊       | 87/306 [22:16<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:34:30,149] Trial 23 failed with parameters: {'optimizer_alpha': 0.4124138070281754, 'optimizer_threshold': 0.023044854025918236} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:34:30,154] Trial 23 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      " 28%|██▊       | 87/306 [22:20<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:34:33,772] Trial 24 failed with parameters: {'optimizer_alpha': 0.9149424972722923, 'optimizer_threshold': 0.027916810300053413} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:34:33,777] Trial 24 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      " 28%|██▊       | 87/306 [22:24<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:34:37,419] Trial 25 failed with parameters: {'optimizer_alpha': 0.751415174230007, 'optimizer_threshold': 0.028796968085077826} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:34:37,423] Trial 25 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 28%|██▊       | 87/306 [22:27<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:34:41,082] Trial 26 finished with value: 0.03725243176572572 and parameters: {'optimizer_alpha': 0.47266040974031537, 'optimizer_threshold': 0.16697703054258087}. Best is trial 14 with value: 0.01510903579365797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                        \n",
      " 28%|██▊       | 87/306 [22:31<23:08,  6.34s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:34:44,696] Trial 27 failed with parameters: {'optimizer_alpha': 0.06423006034427368, 'optimizer_threshold': 0.06625247700798104} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:34:44,701] Trial 27 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                         \n",
      " 28%|██▊       | 87/306 [22:35<23:08,  6.34s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:34:48,326] Trial 28 failed with parameters: {'optimizer_alpha': 0.04233680550938478, 'optimizer_threshold': 0.010346832661412723} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:34:48,330] Trial 28 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 28%|██▊       | 87/306 [22:38<23:08,  6.34s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:34:51,996] Trial 29 finished with value: 0.015600914675524989 and parameters: {'optimizer_alpha': 0.013352869788752981, 'optimizer_threshold': 0.06877879992293834}. Best is trial 14 with value: 0.01510903579365797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                         \n",
      " 28%|██▊       | 87/306 [22:42<23:08,  6.34s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:34:55,730] Trial 30 failed with parameters: {'optimizer_alpha': 0.12930593793314443, 'optimizer_threshold': 0.08429651005444677} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:34:55,733] Trial 30 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 28%|██▊       | 87/306 [22:46<23:08,  6.34s/it]                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:34:59,349] Trial 31 finished with value: 0.014751438872682576 and parameters: {'optimizer_alpha': 0.20841563147152234, 'optimizer_threshold': 0.1575951682016209}. Best is trial 31 with value: 0.014751438872682576.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [22:49<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:02,976] Trial 32 failed with parameters: {'optimizer_alpha': 0.19081473111747474, 'optimizer_threshold': 0.021236224053408168} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:02,980] Trial 32 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [22:53<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:06,628] Trial 33 failed with parameters: {'optimizer_alpha': 0.9391008177501644, 'optimizer_threshold': 0.01766009041473726} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:06,632] Trial 33 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [22:56<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:10,270] Trial 34 failed with parameters: {'optimizer_alpha': 0.9620910218323695, 'optimizer_threshold': 0.023515714671053126} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:10,274] Trial 34 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:00<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:13,911] Trial 35 failed with parameters: {'optimizer_alpha': 0.981821676197008, 'optimizer_threshold': 0.02259938687180795} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:13,915] Trial 35 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:04<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:17,644] Trial 36 failed with parameters: {'optimizer_alpha': 0.970870282519914, 'optimizer_threshold': 0.02258295082415998} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:17,648] Trial 36 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:08<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:21,323] Trial 37 failed with parameters: {'optimizer_alpha': 0.1795879039394746, 'optimizer_threshold': 0.020612477592928804} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:21,328] Trial 37 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:11<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:24,964] Trial 38 failed with parameters: {'optimizer_alpha': 0.1913696192116238, 'optimizer_threshold': 0.021714600566918978} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:24,969] Trial 38 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:15<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:28,716] Trial 39 failed with parameters: {'optimizer_alpha': 0.9414263679889875, 'optimizer_threshold': 0.025631986853939515} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:28,720] Trial 39 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:19<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:32,358] Trial 40 failed with parameters: {'optimizer_alpha': 0.9799423518391847, 'optimizer_threshold': 0.023872898270516533} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:32,362] Trial 40 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:22<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:36,017] Trial 41 failed with parameters: {'optimizer_alpha': 0.1975026286124933, 'optimizer_threshold': 0.021272544715712167} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:36,022] Trial 41 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:26<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:39,656] Trial 42 failed with parameters: {'optimizer_alpha': 0.7990656037882479, 'optimizer_threshold': 0.02248884073288475} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:39,660] Trial 42 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:30<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:43,346] Trial 43 failed with parameters: {'optimizer_alpha': 0.9567597367930892, 'optimizer_threshold': 0.020381020604495138} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:43,351] Trial 43 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:33<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:47,021] Trial 44 failed with parameters: {'optimizer_alpha': 0.9978267635054878, 'optimizer_threshold': 0.018770987428498566} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:47,025] Trial 44 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:37<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:50,682] Trial 45 failed with parameters: {'optimizer_alpha': 0.8348801244909596, 'optimizer_threshold': 0.023202851178477125} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:50,686] Trial 45 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:41<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:54,444] Trial 46 failed with parameters: {'optimizer_alpha': 0.9185073867843064, 'optimizer_threshold': 0.022663955119382267} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:54,449] Trial 46 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:44<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:35:58,106] Trial 47 failed with parameters: {'optimizer_alpha': 0.9524719013062705, 'optimizer_threshold': 0.018726741288710037} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:35:58,110] Trial 47 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      " 28%|██▊       | 87/306 [23:48<23:08,  6.34s/it]                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:36:01,850] Trial 48 failed with parameters: {'optimizer_alpha': 0.8584566536541302, 'optimizer_threshold': 0.022346396391812746} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:36:01,854] Trial 48 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                                                          \n",
      "Best trial: 31. Best value: 0.0147514: 100%|██████████| 50/50 [03:02<00:00,  3.66s/it, 182.83/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:36:05,507] Trial 49 failed with parameters: {'optimizer_alpha': 0.98775722408902, 'optimizer_threshold': 0.021160350541851575} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:36:05,512] Trial 49 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 29%|██▉       | 88/306 [23:55<3:43:33, 61.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.77331; SPICE =  nan -> 0.00362, Diff = nan -> 0.76969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 90/306 [24:03<1:55:38, 32.12s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:36:20,733] A new study created in memory with name: no-name-36d697c3-2e08-4fe3-b619-d1726ad58bcb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.5379; SPICE =  2e-05; Diff = 0.53788\n",
      "Using optuna to find a better set of pysindy parameters for participant 90...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:11<1:55:38, 32.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:36:24,349] Trial 0 finished with value: 0.3623910144223915 and parameters: {'optimizer_alpha': 0.7470852948675496, 'optimizer_threshold': 0.030181005055906505}. Best is trial 0 with value: 0.3623910144223915.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:14<1:55:38, 32.12s/it]                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:36:27,972] Trial 1 finished with value: 0.04743889581924782 and parameters: {'optimizer_alpha': 0.01948408191663768, 'optimizer_threshold': 0.01897070394831399}. Best is trial 1 with value: 0.04743889581924782.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:18<1:55:38, 32.12s/it]                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:36:31,591] Trial 2 finished with value: 0.09239890359531495 and parameters: {'optimizer_alpha': 0.04358788409045673, 'optimizer_threshold': 0.08463400175621669}. Best is trial 1 with value: 0.04743889581924782.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:21<1:55:38, 32.12s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:36:35,305] Trial 3 finished with value: 0.41237828484676986 and parameters: {'optimizer_alpha': 0.1718887866117563, 'optimizer_threshold': 0.01948274768112097}. Best is trial 1 with value: 0.04743889581924782.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:25<1:55:38, 32.12s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:36:38,916] Trial 4 finished with value: 0.33364327016014206 and parameters: {'optimizer_alpha': 0.426516695433853, 'optimizer_threshold': 0.11539610125771081}. Best is trial 1 with value: 0.04743889581924782.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:29<1:55:38, 32.12s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:36:42,558] Trial 5 finished with value: 0.5562987129741122 and parameters: {'optimizer_alpha': 0.30581949825794813, 'optimizer_threshold': 0.08480414052398562}. Best is trial 1 with value: 0.04743889581924782.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:32<1:55:38, 32.12s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:36:46,293] Trial 6 finished with value: 0.539981985174374 and parameters: {'optimizer_alpha': 0.040459026298013415, 'optimizer_threshold': 0.04060927040817307}. Best is trial 1 with value: 0.04743889581924782.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:36<1:55:38, 32.12s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:36:49,898] Trial 7 finished with value: 0.10999801187436346 and parameters: {'optimizer_alpha': 0.22614687751326518, 'optimizer_threshold': 0.08996984654896183}. Best is trial 1 with value: 0.04743889581924782.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:40<1:55:38, 32.12s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:36:53,578] Trial 8 finished with value: 0.3276930532256497 and parameters: {'optimizer_alpha': 0.8861946348926041, 'optimizer_threshold': 0.07811147938189804}. Best is trial 1 with value: 0.04743889581924782.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:43<1:55:38, 32.12s/it]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:36:57,219] Trial 9 finished with value: 0.16824158219711718 and parameters: {'optimizer_alpha': 0.24519445522402014, 'optimizer_threshold': 0.07163271518474675}. Best is trial 1 with value: 0.04743889581924782.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:47<1:55:38, 32.12s/it]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:00,850] Trial 10 finished with value: 0.013549089841952498 and parameters: {'optimizer_alpha': 0.011576021473532094, 'optimizer_threshold': 0.01194963630714312}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:51<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:04,499] Trial 11 finished with value: 0.4111743362919442 and parameters: {'optimizer_alpha': 0.010400180199444019, 'optimizer_threshold': 0.011263130244893128}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:54<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:08,217] Trial 12 finished with value: 0.2970961620882952 and parameters: {'optimizer_alpha': 0.010238444115665706, 'optimizer_threshold': 0.010294600210012324}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [24:58<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:11,828] Trial 13 finished with value: 0.03214884438616095 and parameters: {'optimizer_alpha': 0.023003530610723386, 'optimizer_threshold': 0.018888112291216994}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:02<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:15,462] Trial 14 finished with value: 0.03423377245938717 and parameters: {'optimizer_alpha': 0.033928382407179665, 'optimizer_threshold': 0.017326842018006813}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:05<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:19,070] Trial 15 finished with value: 0.04391534748398826 and parameters: {'optimizer_alpha': 0.07960538951429973, 'optimizer_threshold': 0.1909704736874292}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:09<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:22,701] Trial 16 finished with value: 0.022251078289158482 and parameters: {'optimizer_alpha': 0.01957719576810156, 'optimizer_threshold': 0.02576852423324089}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:13<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:26,325] Trial 17 finished with value: 0.12037575612758543 and parameters: {'optimizer_alpha': 0.09050739505913748, 'optimizer_threshold': 0.027820071878110933}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:16<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:29,973] Trial 18 finished with value: 0.019363673940381926 and parameters: {'optimizer_alpha': 0.017342231059956626, 'optimizer_threshold': 0.013608087454630902}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:20<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:33,602] Trial 19 finished with value: 0.025375571639907557 and parameters: {'optimizer_alpha': 0.014363364544906405, 'optimizer_threshold': 0.013383435874732081}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:24<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:37,327] Trial 20 finished with value: 0.4433213207143724 and parameters: {'optimizer_alpha': 0.0646206698397806, 'optimizer_threshold': 0.04445577417941319}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:27<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:41,029] Trial 21 finished with value: 0.4465240083941382 and parameters: {'optimizer_alpha': 0.0233421528761163, 'optimizer_threshold': 0.030528468815534636}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:31<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:44,651] Trial 22 finished with value: 0.02266655870183707 and parameters: {'optimizer_alpha': 0.01602009214134645, 'optimizer_threshold': 0.014107950445688602}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:35<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:48,393] Trial 23 finished with value: 0.34599513357328665 and parameters: {'optimizer_alpha': 0.02907760954865429, 'optimizer_threshold': 0.02440519651462019}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:38<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:52,026] Trial 24 finished with value: 0.19685746164492923 and parameters: {'optimizer_alpha': 0.014565182098128633, 'optimizer_threshold': 0.013327098330992201}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:42<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:55,685] Trial 25 finished with value: 0.11940925002410206 and parameters: {'optimizer_alpha': 0.05089776755611782, 'optimizer_threshold': 0.023458987866304418}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:45<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:37:59,314] Trial 26 finished with value: 0.028874130549060923 and parameters: {'optimizer_alpha': 0.010522166742289226, 'optimizer_threshold': 0.01488519687040596}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:49<1:55:38, 32.12s/it]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:38:02,938] Trial 27 finished with value: 0.4791426894393117 and parameters: {'optimizer_alpha': 0.026594415769460124, 'optimizer_threshold': 0.055925536984831416}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:53<1:55:38, 32.12s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:38:06,564] Trial 28 finished with value: 0.45611844689958586 and parameters: {'optimizer_alpha': 0.01664008417757125, 'optimizer_threshold': 0.03565721405640355}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [25:56<1:55:38, 32.12s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:38:10,294] Trial 29 finished with value: 0.2809648517394152 and parameters: {'optimizer_alpha': 0.1300216037745052, 'optimizer_threshold': 0.010099394625008426}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [26:00<1:55:38, 32.12s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:38:13,928] Trial 30 finished with value: 0.38796051723584557 and parameters: {'optimizer_alpha': 0.055496006351101156, 'optimizer_threshold': 0.022366055559392424}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [26:04<1:55:38, 32.12s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:38:17,560] Trial 31 finished with value: 0.3385528213651142 and parameters: {'optimizer_alpha': 0.014159799116175107, 'optimizer_threshold': 0.015725887320167064}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [26:07<1:55:38, 32.12s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:38:21,191] Trial 32 finished with value: 0.025621660290305243 and parameters: {'optimizer_alpha': 0.01825986190874794, 'optimizer_threshold': 0.012660114052211624}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 29%|██▉       | 90/306 [26:11<1:55:38, 32.12s/it]                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:38:24,831] Trial 33 finished with value: 0.014286233802074281 and parameters: {'optimizer_alpha': 0.019045729475645246, 'optimizer_threshold': 0.012305995718398223}. Best is trial 10 with value: 0.013549089841952498.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "Best trial: 34. Best value: 0.00699018:  70%|███████   | 35/50 [02:07<00:54,  3.65s/it, 127.74/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:38:28,475] Trial 34 finished with value: 0.0069901844896572566 and parameters: {'optimizer_alpha': 0.03709633270563145, 'optimizer_threshold': 0.017325332423781734}. Best is trial 34 with value: 0.0069901844896572566.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 30%|██▉       | 91/306 [26:18<3:45:56, 63.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.5379; SPICE =  2e-05 -> 0.53481, Diff = 0.53788 -> 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 31%|███       | 94/306 [26:30<1:25:32, 24.21s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 31%|███       | 95/306 [26:34<1:03:39, 18.10s/it]c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 31%|███▏      | 96/306 [26:38<48:23, 13.83s/it]  c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:38:55,454] A new study created in memory with name: no-name-250b8134-8632-4669-b0c2-c581c99d4400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.91666; SPICE =  nan; Diff = nan\n",
      "Using optuna to find a better set of pysindy parameters for participant 96...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "Best trial: 0. Best value: 0.00943054:   2%|▏         | 1/50 [00:03<03:00,  3.69s/it, 3.69/600 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 12:38:59,139] Trial 0 finished with value: 0.009430543057774991 and parameters: {'optimizer_alpha': 0.3281924132512473, 'optimizer_threshold': 0.056813067518307635}. Best is trial 0 with value: 0.009430543057774991.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 97/306 [26:49<45:25, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods after optuna fitting: RNN = 0.91666; SPICE =  nan -> 0.88258, Diff = nan -> 0.03408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-09-26 12:39:06,669] A new study created in memory with name: no-name-3efad238-e7c6-4a6e-b05e-1dd7e81db05e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods before optuna fitting: RNN = 0.78949; SPICE =  nan; Diff = nan\n",
      "Using optuna to find a better set of pysindy parameters for participant 97...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                \n",
      " 32%|███▏      | 97/306 [26:57<45:25, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:10,323] Trial 0 failed with parameters: {'optimizer_alpha': 0.4032211712972366, 'optimizer_threshold': 0.027000203995220438} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:10,327] Trial 0 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                \n",
      " 32%|███▏      | 97/306 [27:00<45:25, 13.04s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:13,930] Trial 1 failed with parameters: {'optimizer_alpha': 0.20567800331213165, 'optimizer_threshold': 0.015591444713189503} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:13,934] Trial 1 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                \n",
      " 32%|███▏      | 97/306 [27:04<45:25, 13.04s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:17,656] Trial 2 failed with parameters: {'optimizer_alpha': 0.022637845627706613, 'optimizer_threshold': 0.034301725609011485} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:17,660] Trial 2 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                 \n",
      " 32%|███▏      | 97/306 [27:07<45:25, 13.04s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:21,289] Trial 3 failed with parameters: {'optimizer_alpha': 0.13827241795320935, 'optimizer_threshold': 0.030526623266372885} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:21,293] Trial 3 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                 \n",
      " 32%|███▏      | 97/306 [27:11<45:25, 13.04s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:24,890] Trial 4 failed with parameters: {'optimizer_alpha': 0.5096831776682148, 'optimizer_threshold': 0.04273942024066476} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:24,894] Trial 4 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                 \n",
      " 32%|███▏      | 97/306 [27:15<45:25, 13.04s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:28,526] Trial 5 failed with parameters: {'optimizer_alpha': 0.01813641401352228, 'optimizer_threshold': 0.028494579935039798} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:28,530] Trial 5 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                 \n",
      " 32%|███▏      | 97/306 [27:18<45:25, 13.04s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:32,149] Trial 6 failed with parameters: {'optimizer_alpha': 0.21897504651436414, 'optimizer_threshold': 0.011317042560012144} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:32,153] Trial 6 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                 \n",
      " 32%|███▏      | 97/306 [27:22<45:25, 13.04s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:35,779] Trial 7 failed with parameters: {'optimizer_alpha': 0.32237946856516764, 'optimizer_threshold': 0.031113162935903625} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:35,783] Trial 7 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                 \n",
      " 32%|███▏      | 97/306 [27:26<45:25, 13.04s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:39,417] Trial 8 failed with parameters: {'optimizer_alpha': 0.015164698118899301, 'optimizer_threshold': 0.03375145792870981} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:39,421] Trial 8 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                 \n",
      " 32%|███▏      | 97/306 [27:29<45:25, 13.04s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:43,072] Trial 9 failed with parameters: {'optimizer_alpha': 0.42645049398024, 'optimizer_threshold': 0.015665101394615903} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:43,077] Trial 9 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [27:33<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:46,689] Trial 10 failed with parameters: {'optimizer_alpha': 0.21640073821151498, 'optimizer_threshold': 0.1043948789217968} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:46,694] Trial 10 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [27:37<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:50,528] Trial 11 failed with parameters: {'optimizer_alpha': 0.016243533381375155, 'optimizer_threshold': 0.01507794538774376} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:50,532] Trial 11 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [27:40<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:54,168] Trial 12 failed with parameters: {'optimizer_alpha': 0.019233484647122132, 'optimizer_threshold': 0.09230966722998177} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:54,172] Trial 12 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [27:44<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:39:57,802] Trial 13 failed with parameters: {'optimizer_alpha': 0.016803781357218755, 'optimizer_threshold': 0.05843996546389073} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:39:57,807] Trial 13 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [27:48<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:01,421] Trial 14 failed with parameters: {'optimizer_alpha': 0.9968769807916111, 'optimizer_threshold': 0.12761154557066842} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:01,426] Trial 14 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [27:51<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:05,043] Trial 15 failed with parameters: {'optimizer_alpha': 0.06440583067910198, 'optimizer_threshold': 0.0139705282995302} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:05,047] Trial 15 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [27:55<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:08,666] Trial 16 failed with parameters: {'optimizer_alpha': 0.7160554169340647, 'optimizer_threshold': 0.07476442231065651} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:08,670] Trial 16 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [27:58<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:12,288] Trial 17 failed with parameters: {'optimizer_alpha': 0.5227070654688906, 'optimizer_threshold': 0.03971695142318655} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:12,292] Trial 17 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [28:02<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:15,908] Trial 18 failed with parameters: {'optimizer_alpha': 0.8010831759368316, 'optimizer_threshold': 0.04613017633314848} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:15,912] Trial 18 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [28:06<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:19,593] Trial 19 failed with parameters: {'optimizer_alpha': 0.021538610398980727, 'optimizer_threshold': 0.0763708975296135} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:19,597] Trial 19 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [28:10<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:23,324] Trial 20 failed with parameters: {'optimizer_alpha': 0.2061060306818749, 'optimizer_threshold': 0.08060558827996354} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:23,329] Trial 20 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [28:13<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:26,943] Trial 21 failed with parameters: {'optimizer_alpha': 0.013205788528027337, 'optimizer_threshold': 0.17771685005809482} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:26,947] Trial 21 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [28:17<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:30,557] Trial 22 failed with parameters: {'optimizer_alpha': 0.39182319111229436, 'optimizer_threshold': 0.15197485771556016} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:30,560] Trial 22 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [28:20<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:34,183] Trial 23 failed with parameters: {'optimizer_alpha': 0.4287383320834361, 'optimizer_threshold': 0.04475222026208342} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:34,188] Trial 23 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [28:24<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:37,806] Trial 24 failed with parameters: {'optimizer_alpha': 0.6980758979170665, 'optimizer_threshold': 0.1005933336606708} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:37,810] Trial 24 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [28:28<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:41,463] Trial 25 failed with parameters: {'optimizer_alpha': 0.013252216909387814, 'optimizer_threshold': 0.016862231277671093} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:41,467] Trial 25 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [28:31<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:45,130] Trial 26 failed with parameters: {'optimizer_alpha': 0.6062678892166189, 'optimizer_threshold': 0.034043905686217256} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:45,135] Trial 26 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                  \n",
      " 32%|███▏      | 97/306 [28:35<45:25, 13.04s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:48,746] Trial 27 failed with parameters: {'optimizer_alpha': 0.015995351326823677, 'optimizer_threshold': 0.19956042984563932} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:48,750] Trial 27 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [28:39<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:52,368] Trial 28 failed with parameters: {'optimizer_alpha': 0.06504014397110619, 'optimizer_threshold': 0.17826216537130843} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:52,371] Trial 28 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [28:42<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:56,098] Trial 29 failed with parameters: {'optimizer_alpha': 0.2662058169530091, 'optimizer_threshold': 0.17900342114076118} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:56,102] Trial 29 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [28:46<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:40:59,721] Trial 30 failed with parameters: {'optimizer_alpha': 0.11975383608824107, 'optimizer_threshold': 0.04213536804402861} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:40:59,725] Trial 30 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [28:50<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:03,345] Trial 31 failed with parameters: {'optimizer_alpha': 0.15199453635996016, 'optimizer_threshold': 0.04783774458397794} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:03,349] Trial 31 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [28:53<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:06,968] Trial 32 failed with parameters: {'optimizer_alpha': 0.013193707040778515, 'optimizer_threshold': 0.01718460955316201} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:06,973] Trial 32 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [28:57<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:10,588] Trial 33 failed with parameters: {'optimizer_alpha': 0.030857949215266488, 'optimizer_threshold': 0.15898384140236688} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:10,591] Trial 33 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:00<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:14,241] Trial 34 failed with parameters: {'optimizer_alpha': 0.0300574831672728, 'optimizer_threshold': 0.10957123120925932} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:14,246] Trial 34 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:04<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:17,862] Trial 35 failed with parameters: {'optimizer_alpha': 0.15341018552990807, 'optimizer_threshold': 0.04749713221967776} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:17,867] Trial 35 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:08<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:21,593] Trial 36 failed with parameters: {'optimizer_alpha': 0.40914281532958885, 'optimizer_threshold': 0.12969414928192888} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:21,597] Trial 36 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:11<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:25,306] Trial 37 failed with parameters: {'optimizer_alpha': 0.032871884678299726, 'optimizer_threshold': 0.06582363746113999} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:25,310] Trial 37 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:15<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:28,934] Trial 38 failed with parameters: {'optimizer_alpha': 0.03434637951359162, 'optimizer_threshold': 0.018714234335248495} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:28,938] Trial 38 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:19<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:32,571] Trial 39 failed with parameters: {'optimizer_alpha': 0.01908685100875511, 'optimizer_threshold': 0.07755418050342044} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:32,575] Trial 39 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:22<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:36,200] Trial 40 failed with parameters: {'optimizer_alpha': 0.02187931424579007, 'optimizer_threshold': 0.021424794650743536} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:36,204] Trial 40 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:26<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:39,857] Trial 41 failed with parameters: {'optimizer_alpha': 0.044594321962357054, 'optimizer_threshold': 0.042778165061811646} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:39,861] Trial 41 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:30<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:43,554] Trial 42 failed with parameters: {'optimizer_alpha': 0.24470530716124636, 'optimizer_threshold': 0.08480004252780193} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:43,557] Trial 42 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:33<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:47,192] Trial 43 failed with parameters: {'optimizer_alpha': 0.059890307008482524, 'optimizer_threshold': 0.18022829549583666} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:47,197] Trial 43 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:37<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:50,811] Trial 44 failed with parameters: {'optimizer_alpha': 0.020070064173806756, 'optimizer_threshold': 0.023535981479348905} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:50,816] Trial 44 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:41<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:54,432] Trial 45 failed with parameters: {'optimizer_alpha': 0.028755053211133367, 'optimizer_threshold': 0.13161815948188496} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:54,435] Trial 45 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:44<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:41:58,161] Trial 46 failed with parameters: {'optimizer_alpha': 0.25568288233219066, 'optimizer_threshold': 0.028548458034382194} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:41:58,166] Trial 46 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:48<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:42:01,779] Trial 47 failed with parameters: {'optimizer_alpha': 0.36107533488075505, 'optimizer_threshold': 0.058257403861057315} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:42:01,783] Trial 47 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      " 32%|███▏      | 97/306 [29:52<45:25, 13.04s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:42:05,420] Trial 48 failed with parameters: {'optimizer_alpha': 0.2971961213499357, 'optimizer_threshold': 0.04018679282151744} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:42:05,424] Trial 48 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\pysindy\\optimizers\\sr3.py:390: ConvergenceWarning: SR3._reduce did not converge after 100 iterations.\n",
      "  warnings.warn(\n",
      "                                                \n",
      "                                                                   \n",
      "100%|██████████| 50/50 [03:02<00:00,  3.65s/it, 182.40/600 seconds]\n",
      " 32%|███▏      | 97/306 [29:55<1:04:29, 18.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-26 12:42:09,064] Trial 49 failed with parameters: {'optimizer_alpha': 0.3705361455778813, 'optimizer_threshold': 0.013258295582052798} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-09-26 12:42:09,069] Trial 49 failed with value nan.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m### SINDy\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline_sindy\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m agent_spice, features, sindy_loss = \u001b[43mpipeline_sindy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_rnn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_rnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_inputs_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# general recovery parameters\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparticipant_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_bad_participants\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_optuna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpruning\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# sindy parameters\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_test_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_test_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 0.05\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials_off_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_sessions_off_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials_same_action_off_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptuna_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 0.1\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptuna_n_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 50\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSR3_weighted_l1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 'STLSQ',  'SR3_weighted_l1'\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# optimizer_type='SR3_L1',\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# generated training dataset parameters\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta_reward\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforget_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfirmation_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcounterfactual\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha_counterfactual\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43manalysis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msindy_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SPICE\\spice\\pipeline_sindy.py:190\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(class_rnn, model, data, save, additional_inputs_data, participant_id, rnn_modules, control_parameters, library_setup, filter_setup, dataprocessing_setup, optimizer_type, optimizer_threshold, optimizer_alpha, polynomial_degree, n_trials_off_policy, n_sessions_off_policy, n_trials_same_action_off_policy, verbose, use_optuna, filter_bad_participants, pruning, train_test_ratio, optuna_threshold, optuna_n_trials, beta_reward, alpha, alpha_penalty, forget_rate, confirmation_bias, beta_choice, alpha_choice, alpha_counterfactual, parameter_variance, n_actions, sigma, counterfactual, get_loss, analysis)\u001b[39m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUsing Optuna to find optimal optimizer configuration for each participant\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# setup the SINDy-agent\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m agent_spice, loss_spice = \u001b[43mfit_spice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrnn_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrnn_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrol_signals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrol_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent_rnn\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent_rnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_sessions_off_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_sessions_off_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials_off_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials_off_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials_same_action_off_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials_same_action_off_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_setup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_setup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_setup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_setup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataprocessing_setup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparticipant_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparticipant_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_optuna\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_optuna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_bad_participants\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_bad_participants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpruning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpruning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_test_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_test_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptuna_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptuna_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptuna_n_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptuna_n_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;66;03m# If agent_spice is None, we couldn't fit the model, so return early\u001b[39;00m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(participant_ids) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SPICE\\spice\\resources\\sindy_training.py:206\u001b[39m, in \u001b[36mfit_spice\u001b[39m\u001b[34m(rnn_modules, control_signals, agent_rnn, data, polynomial_degree, library_setup, filter_setup, optimizer_type, optimizer_threshold, optimizer_alpha, participant_id, shuffle, dataprocessing, n_trials_off_policy, n_sessions_off_policy, n_trials_same_action_off_policy, train_test_ratio, deterministic, get_loss, verbose, use_optuna, filter_bad_participants, pruning, optuna_threshold, optuna_n_trials)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing optuna to find a better set of pysindy parameters for participant \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Find optimal optimizer and parameters for this participant\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m sindy_config = \u001b[43moptimize_for_participant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparticipant_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent_rnn\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent_rnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_pid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_rnn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprobs_rnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrnn_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrnn_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrol_signals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrol_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_setup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_setup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_setup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_setup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_sessions_off_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_sessions_off_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials_off_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials_off_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials_same_action_off_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials_same_action_off_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptuna_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials_optuna\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptuna_n_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust as needed\u001b[39;49;00m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# optuna_optimizer_type = sindy_config[\"optimizer_type\"]\u001b[39;00m\n\u001b[32m    226\u001b[39m optuna_optimizer_alpha = sindy_config[\u001b[33m\"\u001b[39m\u001b[33moptimizer_alpha\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SPICE\\spice\\resources\\optimizer_selection.py:164\u001b[39m, in \u001b[36moptimize_for_participant\u001b[39m\u001b[34m(participant_id, agent_rnn, data, metric_rnn, rnn_modules, control_signals, library_setup, filter_setup, polynomial_degree, optimizer_type, n_sessions_off_policy, n_trials_off_policy, n_trials_same_action_off_policy, n_trials_optuna, timeout, threshold, verbose)\u001b[39m\n\u001b[32m    150\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    151\u001b[39m study.optimize(\n\u001b[32m    152\u001b[39m     objective, \n\u001b[32m    153\u001b[39m     n_trials=n_trials_optuna, \n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m         ],\n\u001b[32m    160\u001b[39m )\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# \"optimizer_type\": study.best_params[\"optimizer_type\"],\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moptimizer_alpha\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbest_params\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33moptimizer_alpha\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    165\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moptimizer_threshold\u001b[39m\u001b[33m\"\u001b[39m: study.best_params[\u001b[33m\"\u001b[39m\u001b[33moptimizer_threshold\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# \"n_sessions_off_policy\": study.best_params[\"n_sessions_off_policy\"],\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# \"n_trials_off_policy\": study.best_params[\"n_trials_off_policy\"],\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# \"n_trials_same_action_off_policy\": study.best_params[\"n_trials_same_action_off_policy\"],\u001b[39;00m\n\u001b[32m    169\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\optuna\\study\\study.py:119\u001b[39m, in \u001b[36mStudy.best_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbest_params\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    109\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return parameters of the best trial in the study.\u001b[39;00m\n\u001b[32m    110\u001b[39m \n\u001b[32m    111\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbest_trial\u001b[49m.params\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\optuna\\study\\study.py:162\u001b[39m, in \u001b[36mStudy.best_trial\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_multi_objective():\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    158\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    159\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    160\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m best_trial = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_storage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_best_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_study_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# If the trial with the best value is infeasible, select the best trial from all feasible\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# trials. Note that the behavior is undefined when constrained optimization without the\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# violation value in the best-valued trial.\u001b[39;00m\n\u001b[32m    167\u001b[39m constraints = best_trial.system_attrs.get(_CONSTRAINTS_KEY)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Malte\\Desktop\\SPICE\\venv\\Lib\\site-packages\\optuna\\storages\\_in_memory.py:252\u001b[39m, in \u001b[36mInMemoryStorage.get_best_trial\u001b[39m\u001b[34m(self, study_id)\u001b[39m\n\u001b[32m    249\u001b[39m best_trial_id = \u001b[38;5;28mself\u001b[39m._studies[study_id].best_trial_id\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_trial_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo trials are completed yet.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._studies[study_id].directions) > \u001b[32m1\u001b[39m:\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    255\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBest trial can be obtained only for single-objective optimization.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    256\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "### SINDy\n",
    "from spice import pipeline_sindy\n",
    "\n",
    "agent_spice, features, sindy_loss = pipeline_sindy.main(\n",
    "    class_rnn=class_rnn,\n",
    "    model = path_model,\n",
    "    data = path_data,\n",
    "    additional_inputs_data=additional_inputs,\n",
    "    save = True,\n",
    "    \n",
    "    # general recovery parameters\n",
    "    participant_id=None,\n",
    "    filter_bad_participants=False,\n",
    "    use_optuna=True,\n",
    "    pruning=False,\n",
    "    \n",
    "    # sindy parameters\n",
    "    train_test_ratio=train_test_ratio,\n",
    "    polynomial_degree=3,\n",
    "    optimizer_alpha=0.1,\n",
    "    optimizer_threshold=0.05, # 0.05\n",
    "    n_trials_off_policy=1000,\n",
    "    n_sessions_off_policy=1,\n",
    "    n_trials_same_action_off_policy=5,\n",
    "    optuna_threshold=0.1, # 0.1\n",
    "    optuna_n_trials=50, # 50\n",
    "    optimizer_type='SR3_weighted_l1',  # 'STLSQ',  'SR3_weighted_l1'\n",
    "    # optimizer_type='SR3_L1',\n",
    "    verbose=False,\n",
    "    \n",
    "    # generated training dataset parameters\n",
    "    n_actions=2,\n",
    "    sigma=0.2,\n",
    "    beta_reward=1.,\n",
    "    alpha=0.25,\n",
    "    alpha_penalty=0.25,\n",
    "    forget_rate=0.,\n",
    "    confirmation_bias=0.,\n",
    "    beta_choice=1.,\n",
    "    alpha_choice=1.,\n",
    "    counterfactual=False,\n",
    "    alpha_counterfactual=0.,\n",
    "    \n",
    "    analysis=True,\n",
    "    get_loss=False,\n",
    "    \n",
    "    **sindy_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d75eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model analysis\n",
    "# if dataset == 'eckstein2022':\n",
    "#     # ------------------- CONFIGURATION ECKSTEIN2022 w/o AGE --------------------\n",
    "#     study = 'eckstein2022'\n",
    "#     models_benchmark = ['ApAnBrBcfBch']#['ApBr', 'ApBrAcfpBcf', 'ApBrAcfpBcfBch', 'ApAnBrBch', 'ApAnBrAcfpAcfnBcfBch', 'ApAnBrBcfBch']\n",
    "#     train_test_ratio = 0.8\n",
    "#     sindy_config = sindy_utils.SindyConfig_eckstein2022\n",
    "#     rnn_class = rnn.RLRNN_eckstein2022\n",
    "#     additional_inputs = None\n",
    "#     setup_agent_benchmark = benchmarking_eckstein2022.setup_agent_benchmark\n",
    "#     rl_model = benchmarking_eckstein2022.rl_model\n",
    "#     benchmark_file = f'mcmc_{study}_MODEL.nc'\n",
    "#     model_config_baseline = 'ApBr'\n",
    "#     baseline_file = f'mcmc_{study}_ApBr.nc'\n",
    "\n",
    "# elif dataset == 'dezfouli2019':\n",
    "#     # ------------------------ CONFIGURATION DEZFOULI2019 -----------------------\n",
    "#     study = 'dezfouli2019'\n",
    "#     train_test_ratio = [3, 6, 9]\n",
    "#     models_benchmark = ['PhiChiBetaKappaC']\n",
    "#     sindy_config = sindy_utils.SindyConfig_dezfouli2019\n",
    "#     rnn_class = rnn.RLRNN_dezfouli2019\n",
    "#     additional_inputs = []\n",
    "#     # setup_agent_benchmark = benchmarking_dezfouli2019.setup_agent_benchmark\n",
    "#     # gql_model = benchmarking_dezfouli2019.gql_model\n",
    "#     setup_agent_benchmark = benchmarking_dezfouli2019.setup_agent_gql\n",
    "#     gql_model = benchmarking_dezfouli2019.Dezfouli2019GQL\n",
    "#     benchmark_file = f'gql_{study}_MODEL.pkl'\n",
    "#     model_config_baseline = 'PhiBeta'\n",
    "#     baseline_file = f'gql_{study}_PhiBeta.pkl'\n",
    "\n",
    "# # ------------------------- CONFIGURATION FILE PATHS ------------------------\n",
    "# use_test = True\n",
    "\n",
    "# path_data = f'data/{study}/{study}.csv'\n",
    "# path_model_rnn = path_model\n",
    "# path_model_spice = path_model.replace('_rnn.pkl', '_spice.pkl')\n",
    "# path_model_baseline = None\n",
    "# path_model_benchmark = None\n",
    "\n",
    "# dataset = convert_dataset(path_data, additional_inputs=additional_inputs)[0]\n",
    "# # use these participant_ids if not defined later\n",
    "# participant_ids = dataset.xs[:, 0, -1].unique().cpu().numpy()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
