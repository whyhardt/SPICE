{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446bf2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from spice import SpiceEstimator, SpiceDataset, SpiceConfig, BaseRNN, Agent\n",
    "from weinhardt2025.utils.bandits import BanditsGeneral, create_dataset\n",
    "from weinhardt2025.benchmarking.benchmarking_qlearning import QLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327c52b",
   "metadata": {},
   "source": [
    "## General Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 2\n",
    "n_participants = 25\n",
    "n_trials = 100\n",
    "n_sessions = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d4207",
   "metadata": {},
   "source": [
    "### Specific experiment conditions per experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = []\n",
    "envs.append(BanditsGeneral(\n",
    "    n_arms=n_actions,\n",
    "    init_reward_prob=(0.2, 0.8),    # -> easy learning task; no special effects; basic q-learning\n",
    "    drift_rate=0,\n",
    "    hazard_rate=0.1,\n",
    "    reward_prob_correlation=0,\n",
    "))\n",
    "\n",
    "envs.append(BanditsGeneral(\n",
    "    n_arms=n_actions,\n",
    "    init_reward_prob=(0.2, 0.5),    # -> lower reward prob for better arm; enables choice perseverance\n",
    "    drift_rate=0.,\n",
    "    hazard_rate=0.1,\n",
    "    reward_prob_correlation=0,\n",
    "))\n",
    "\n",
    "envs.append(BanditsGeneral(\n",
    "    n_arms=n_actions,\n",
    "    init_reward_prob=(0.4, 0.6),\n",
    "    drift_rate=0.2,                 # -> drifting reward probabilities for both arms; enables gradual forgetting of unchosen arm  \n",
    "    hazard_rate=0,\n",
    "    reward_prob_correlation=0,\n",
    "))\n",
    "\n",
    "envs.append(BanditsGeneral(\n",
    "    n_arms=n_actions,\n",
    "    init_reward_prob=(0.4, 0.6),\n",
    "    drift_rate=0.2,\n",
    "    hazard_rate=0.,\n",
    "    reward_prob_correlation=-0.5,  # -> as previous environment + correlated reward probabilities; enables counterfactual updates of unchosen arm\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2369d9",
   "metadata": {},
   "source": [
    "### Plotting differences between experiments in terms of reward probabilities over trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ea51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4)\n",
    "for i_env in range(len(envs)):\n",
    "    for i_trial in range(n_trials):\n",
    "        envs[i_env].step(0)\n",
    "    reward_probs = np.stack(envs[i_env].history['reward_probs'])\n",
    "    axs[i_env].plot(reward_probs)\n",
    "    envs[i_env].new_sess()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37710c4",
   "metadata": {},
   "source": [
    "## Setting up agents with specific sets of cognitive mechanisms for each experiment condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30434798",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = []\n",
    "\n",
    "agents.append(Agent(\n",
    "    QLearning(\n",
    "        n_actions=n_actions,\n",
    "        n_participants=n_participants,\n",
    "        beta_reward=3.0,\n",
    "        alpha_reward=0.7,  # -> higher learning rate because of more certain environment\n",
    "        alpha_penalty=0.7,\n",
    "        forget_rate=0.,\n",
    "        beta_choice=0.,\n",
    "    ), use_sindy=True, deterministic=False\n",
    "))\n",
    "\n",
    "agents.append(Agent(\n",
    "    QLearning(\n",
    "        n_actions=n_actions,\n",
    "        n_participants=n_participants,\n",
    "        beta_reward=3.0,\n",
    "        alpha_reward=0.5,  # -> lower learning rate because of less certain environment\n",
    "        alpha_penalty=0.5,\n",
    "        forget_rate=0.,\n",
    "        beta_choice=1.,  # -> choice perseverance because of lower reward probability for better arm\n",
    "    ), use_sindy=True, deterministic=False\n",
    "))\n",
    "\n",
    "agents.append(Agent(\n",
    "    QLearning(\n",
    "        n_actions=n_actions,\n",
    "        n_participants=n_participants,\n",
    "        beta_reward=3.0,\n",
    "        alpha_reward=0.5,\n",
    "        alpha_penalty=0.5,\n",
    "        forget_rate=0.3,  # -> forgetting of unchosen value because of unknown random drift of both arms  \n",
    "        beta_choice=0.,\n",
    "    ), use_sindy=True, deterministic=False\n",
    "))\n",
    "\n",
    "agents.append(Agent(\n",
    "    QLearning(\n",
    "        n_actions=n_actions,\n",
    "        n_participants=n_participants,\n",
    "        beta_reward=3.0,\n",
    "        alpha_reward=0.5,\n",
    "        alpha_penalty=0.5,\n",
    "        forget_rate=0.3,  # -> same as previous agent + counterfactual update because of negative reward correlation \n",
    "        beta_choice=0.,\n",
    "        counterfactual_learning=True,\n",
    "    ), use_sindy=True, deterministic=False\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc1a5c",
   "metadata": {},
   "source": [
    "### Showing agent handling and agent differences in terms of SINDy coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Available modules: {agents[0].model.get_modules()}\")\n",
    "print(\"SINDy candidate terms per module:\")\n",
    "for module in agents[0].model.get_modules():\n",
    "    print(f\"\\t{module}: {agents[0].model.sindy_candidate_terms[module]}\")    \n",
    "print(\"SINDy coefficients for basic Q-Learning:\")\n",
    "for module in agents[0].model.get_modules():\n",
    "    print(f\"\\t{module}: {agents[0].model.sindy_coefficients[module][0, 0, 0]}\")\n",
    "print(\"SINDy coefficients for Q-Learning+Choice Perseverance:\")\n",
    "for module in agents[0].model.get_modules():\n",
    "    print(f\"\\t{module}: {agents[1].model.sindy_coefficients[module][0, 0, 0]}\")\n",
    "    print(\"SINDy coefficients for Q-Learning+Forgetting:\")\n",
    "for module in agents[0].model.get_modules():\n",
    "    print(f\"\\t{module}: {agents[2].model.sindy_coefficients[module][0, 0, 0]}\")\n",
    "print(\"SINDy coefficients for Q-Learning+Forgetting+Counterfactual Updating:\")\n",
    "for module in agents[0].model.get_modules():\n",
    "    print(f\"\\t{module}: {agents[3].model.sindy_coefficients[module][0, 0, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f966a61",
   "metadata": {},
   "source": [
    "## Data generation with each agent-experiment-pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "xs = torch.zeros((len(agents)*n_participants, n_trials, n_actions*2+3))\n",
    "ys = torch.zeros((len(agents)*n_participants, n_trials, n_actions))\n",
    "\n",
    "for i_agent in range(len(agents)):\n",
    "    dataset_agent = create_dataset(\n",
    "        agent=agents[i_agent],\n",
    "        environment=envs[i_agent],\n",
    "        n_trials=n_trials,\n",
    "        n_sessions=n_participants,\n",
    "        verbose=False,\n",
    "    )[0]\n",
    "    \n",
    "    # adjust experiment_id\n",
    "    dataset_agent.xs[..., -2] = i_agent\n",
    "    \n",
    "    xs[n_participants*i_agent:n_participants*(i_agent+1)] = dataset_agent.xs\n",
    "    ys[n_participants*i_agent:n_participants*(i_agent+1)] = dataset_agent.ys\n",
    "\n",
    "dataset = SpiceDataset(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a966a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.xs.shape)  # -> shape: (n_participants*n_blocks_per_participant*n_experiments_per_participant, n_trials, features); features.shape -> (action_0, action_1, ..., reward_0, reward_1, ..., block_id, experiment_id, participant_id)\n",
    "print(dataset.ys.shape) # -> shape: (n_participants*n_blocks_per_participant*n_experiments_per_participant, n_trials, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b9db94",
   "metadata": {},
   "source": [
    "## SPICE Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spice_config = SpiceConfig(\n",
    "    library_setup = {\n",
    "        'value_reward_chosen': ['reward'],                                              # --> n_terms = 6\n",
    "        'value_reward_not_chosen': ['reward_chosen_success', 'reward_chosen_fail'],     # --> n_terms = 10\n",
    "        'value_choice': ['choice'],                                                     # --> n_terms = 6\n",
    "        },                                                                              # --> n_terms_total = 22\n",
    "    memory_state={\n",
    "            'value_reward': 0.,\n",
    "            'value_choice': 0.,\n",
    "            },\n",
    ")\n",
    "\n",
    "\n",
    "class SpiceModel(BaseRNN):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        dropout = 0.1\n",
    "        embedding_size_experiment = 4\n",
    "        \n",
    "        # set up the participant-embedding layer\n",
    "        self.participant_embedding = self.setup_embedding(self.n_participants, self.embedding_size, dropout=dropout)\n",
    "        self.experiment_embedding = self.setup_embedding(self.n_experiments, embedding_size_experiment)\n",
    "\n",
    "        # set up the submodules\n",
    "        self.setup_module(key_module='value_reward_chosen', input_size=1+self.embedding_size+embedding_size_experiment, dropout=dropout)\n",
    "        self.setup_module(key_module='value_reward_not_chosen', input_size=2+self.embedding_size+embedding_size_experiment, dropout=dropout)\n",
    "        self.setup_module(key_module='value_choice', input_size=1+self.embedding_size+embedding_size_experiment, dropout=dropout)\n",
    "        \n",
    "    def forward(self, inputs, prev_state=None, batch_first=False):\n",
    "        \"\"\"Forward pass of the RNN\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): includes all necessary inputs (action, reward, participant id) to the RNN to let it compute the next action\n",
    "            prev_state (Tuple[torch.Tensor], optional): That's the previous memory state of the RNN containing the reward-based value. Defaults to None.\n",
    "            batch_first (bool, optional): Indicates whether the first dimension of inputs is batch (True) or timesteps (False). Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, we have to initialize all the inputs and outputs (i.e. logits)\n",
    "        spice_signals = self.init_forward_pass(inputs, prev_state, batch_first)\n",
    "        \n",
    "        # manual feature extraction\n",
    "        reward_chosen_success = spice_signals.rewards.sum(dim=-1, keepdim=True).repeat(1, 1, self.n_actions) == 1\n",
    "        reward_chosen_fail = spice_signals.rewards.sum(dim=-1, keepdim=True).repeat(1, 1, self.n_actions) != 1\n",
    "        \n",
    "        # We compute now the participant embeddings and inverse noise temperatures before the for-loop because they are anyways time-invariant\n",
    "        participant_embedding = self.participant_embedding(spice_signals.participant_ids)\n",
    "        experiment_embedding = self.experiment_embedding(spice_signals.experiment_ids)\n",
    "        \n",
    "        for timestep in spice_signals.timesteps:\n",
    "            \n",
    "            # updates for value_reward\n",
    "            self.call_module(\n",
    "                key_module='value_reward_chosen',\n",
    "                key_state='value_reward',\n",
    "                action_mask=spice_signals.actions[timestep],\n",
    "                inputs=(spice_signals.rewards[timestep]),\n",
    "                participant_index=spice_signals.participant_ids,\n",
    "                participant_embedding=participant_embedding,\n",
    "                experiment_index=spice_signals.experiment_ids,\n",
    "                experiment_embedding=experiment_embedding,\n",
    "                )\n",
    "            \n",
    "            self.call_module(\n",
    "                key_module='value_reward_not_chosen',\n",
    "                key_state='value_reward',\n",
    "                action_mask=1-spice_signals.actions[timestep],\n",
    "                inputs=(\n",
    "                    reward_chosen_success[timestep], \n",
    "                    reward_chosen_fail[timestep],\n",
    "                    ),\n",
    "                participant_index=spice_signals.participant_ids,\n",
    "                participant_embedding=participant_embedding,\n",
    "                experiment_index=spice_signals.experiment_ids,\n",
    "                experiment_embedding=experiment_embedding,\n",
    "                )\n",
    "            \n",
    "            # updates for value_choice\n",
    "            self.call_module(\n",
    "                key_module='value_choice',\n",
    "                key_state='value_choice',\n",
    "                action_mask=None,\n",
    "                inputs=(spice_signals.actions[timestep]),\n",
    "                participant_index=spice_signals.participant_ids,\n",
    "                participant_embedding=participant_embedding,\n",
    "                experiment_index=spice_signals.experiment_ids,\n",
    "                experiment_embedding=experiment_embedding,\n",
    "                )\n",
    "            \n",
    "            # Now keep track of the logit in the output array\n",
    "            # spice_signals.logits[timestep] = self.state['value_reward'] * beta_reward + self.state['value_choice'] * beta_choice\n",
    "            spice_signals.logits[timestep] = self.state['value_reward'] + self.state['value_choice']\n",
    "            \n",
    "        # post-process the forward pass; give here as inputs the logits, batch_first and all values from the memory state\n",
    "        spice_signals = self.post_forward_pass(spice_signals, batch_first)\n",
    "        \n",
    "        return spice_signals.logits, self.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be03a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_spice_model = 'spice.pkl'\n",
    "\n",
    "estimator = SpiceEstimator(\n",
    "    # meta parameters\n",
    "    rnn_class=SpiceModel,\n",
    "    spice_config=spice_config,\n",
    "    n_actions=n_actions,\n",
    "    n_participants=n_participants,\n",
    "    n_experiments=len(envs),\n",
    "    \n",
    "    # training parameters\n",
    "    epochs=1000,\n",
    "    warmup_steps=1000//4,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    \n",
    "    # sindy parameters\n",
    "    sindy_weight=0.1,\n",
    "    sindy_threshold=0.05,\n",
    "    sindy_alpha=0.0001,\n",
    "    sindy_cutoff_patience=100,\n",
    "    sindy_epochs=2000,\n",
    "    sindy_library_polynomial_degree=2,\n",
    "    \n",
    "    # additional generalization parameters\n",
    "    bagging=True,\n",
    "    scheduler=True,\n",
    "    \n",
    "    save_path_spice=path_spice_model,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(data=dataset.xs, targets=dataset.ys, data_test=dataset.xs, target_test=dataset.ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55644aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
