{"0": {
    "doc": "API Reference",
    "title": "API Reference",
    "content": "This page documents the main classes and functions in SPICE. ",
    "url": "/SPICE/api.html",
    
    "relUrl": "/api.html"
  },"1": {
    "doc": "API Reference",
    "title": "Core Classes",
    "content": "SpiceEstimator . The main class for training and using SPICE models. Implements scikit-learn’s estimator interface. from spice.estimator import SpiceEstimator . Parameters . | rnn_class (BaseRNN): RNN class to use (can be precoded or custom implementation) | spice_config (SpiceConfig): Configuration for SPICE features and library | hidden_size (int, default=8): Size of RNN hidden layer | dropout (float, default=0.25): Dropout rate for RNN | n_actions (int, default=2): Number of possible actions | n_participants (int, default=0): Number of participants | n_experiments (int, default=0): Number of experiments | epochs (int, default=128): Number of training epochs | learning_rate (float, default=5e-3): Learning rate for training | spice_optim_threshold (float, default=0.03): Threshold for SPICE optimization | spice_participant_id (int, optional): ID of specific participant to analyze | verbose (bool, default=False): Whether to print progress information | . Methods . fit(conditions, targets) . Trains both RNN and SPICE models on given data. def fit(conditions: np.ndarray, targets: np.ndarray) \"\"\" Args: conditions: Array of shape (n_participants, n_trials, n_features) targets: Array of shape (n_participants, n_trials, n_actions) \"\"\" . predict(conditions) . Makes predictions using both RNN and SPICE models. def predict(conditions: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray] \"\"\" Args: conditions: Array of shape (n_participants, n_trials, n_features) Returns: Tuple containing: - RNN predictions - SPICE predictions \"\"\" . get_spice_features() . Returns learned SPICE features and equations. def get_spice_features() -&gt; Dict \"\"\" Returns: Dictionary mapping participant IDs to their learned features and equations \"\"\" . SpiceConfig . Configuration class for setting up SPICE models. from spice.estimator import SpiceConfig . Parameters . | library_setup (Dict[str, List[str]]): Maps features to library components | filter_setup (Dict[str, List]): Maps features to filter conditions | control_parameters (List[str]): List of control parameter names | rnn_modules (List[str]): List of RNN module names | . ",
    "url": "/SPICE/api.html#core-classes",
    
    "relUrl": "/api.html#core-classes"
  },"2": {
    "doc": "API Reference",
    "title": "Precoded Models",
    "content": "SPICE comes with several precoded RNN models for common cognitive mechanisms: . RescorlaWagnerRNN . Implementation of the Rescorla-Wagner learning model. from spice.precoded import RescorlaWagnerRNN, RESCOLA_WAGNER_CONFIG . ForgettingRNN . Model incorporating forgetting mechanisms. from spice.precoded import ForgettingRNN . LearningRateRNN . Model with adaptive learning rates. from spice.precoded import LearningRateRNN . ParticipantEmbeddingRNN . Model that learns participant-specific embeddings. from spice.precoded import ParticipantEmbeddingRNN . ",
    "url": "/SPICE/api.html#precoded-models",
    
    "relUrl": "/api.html#precoded-models"
  },"3": {
    "doc": "API Reference",
    "title": "Agents",
    "content": "AgentSpice . SPICE agent that combines RNN and SINDy equations. from spice.resources.bandits import AgentSpice . Methods . get_spice_features() . Extracts features and coefficients for each module and participant. def get_spice_features(mapping_modules_values: dict = None) -&gt; Dict[int, Dict] \"\"\" Args: mapping_modules_values: Optional mapping of modules to memory state values Returns: Dictionary mapping participant IDs to their features and coefficients \"\"\" . count_parameters() . Counts non-zero parameters for each participant. def count_parameters(mapping_modules_values: dict = None) -&gt; Dict[int, int] \"\"\" Args: mapping_modules_values: Optional mapping of modules to memory state values Returns: Dictionary mapping participant IDs to parameter counts \"\"\" . ",
    "url": "/SPICE/api.html#agents",
    
    "relUrl": "/api.html#agents"
  },"4": {
    "doc": "API Reference",
    "title": "Utility Functions",
    "content": "fit_spice() . Fits SPICE by replacing RNN modules with SINDy equations. from spice.resources.sindy_training import fit_spice def fit_spice( rnn_modules: List[str], control_signals: List[str], agent_rnn: AgentNetwork, data: DatasetRNN = None, polynomial_degree: int = 2, optimizer_threshold: float = 0.05, optimizer_alpha: float = 0.1, participant_id: int = None, verbose: bool = False ) -&gt; Tuple[AgentSpice, float] \"\"\" Args: rnn_modules: List of RNN module names to replace control_signals: List of control signal names agent_rnn: Trained RNN agent data: Training dataset polynomial_degree: Degree for polynomial features optimizer_threshold: Threshold for optimization optimizer_alpha: Alpha parameter for optimization participant_id: Specific participant to process verbose: Whether to print progress Returns: Tuple of (SPICE agent, loss value) \"\"\" . optimize_for_participant() . Optimizes SPICE parameters for a specific participant. from spice.resources.optimizer_selection import optimize_for_participant def optimize_for_participant( participant_id: int, agent_rnn: AgentNetwork, data: DatasetRNN, metric_rnn: float, rnn_modules: list, control_signals: list, library_setup: dict, filter_setup: dict, polynomial_degree: int, n_sessions_off_policy: int, n_trials_optuna: int = 50, verbose: bool = False ) \"\"\" Args: participant_id: ID of participant to optimize for agent_rnn: Trained RNN agent data: Training data metric_rnn: RNN performance metric rnn_modules: List of RNN modules control_signals: List of control signals library_setup: Library configuration filter_setup: Filter configuration polynomial_degree: Degree for polynomial features n_sessions_off_policy: Number of off-policy sessions n_trials_optuna: Number of optimization trials verbose: Whether to print progress \"\"\" . ",
    "url": "/SPICE/api.html#utility-functions",
    
    "relUrl": "/api.html#utility-functions"
  },"5": {
    "doc": "Working with Hardcoded Equations",
    "title": "Working with Hardcoded Equations",
    "content": "This tutorial explains how to use predefined equations in SPICE models. You’ll learn how to: . | Incorporate known cognitive mechanisms as hardcoded equations | Combine hardcoded equations with learned mechanisms | Optimize model performance using domain knowledge | . ",
    "url": "/SPICE/tutorials/hardcoded_equations.html",
    
    "relUrl": "/tutorials/hardcoded_equations.html"
  },"6": {
    "doc": "Working with Hardcoded Equations",
    "title": "Prerequisites",
    "content": "Before starting this tutorial, make sure you have: . | Completed the previous tutorials | Understanding of basic cognitive modeling equations | Familiarity with the SPICE architecture | . ",
    "url": "/SPICE/tutorials/hardcoded_equations.html#prerequisites",
    
    "relUrl": "/tutorials/hardcoded_equations.html#prerequisites"
  },"7": {
    "doc": "Working with Hardcoded Equations",
    "title": "Why Use Hardcoded Equations?",
    "content": "Sometimes we have strong theoretical knowledge about certain cognitive mechanisms. For example: . | The reward prediction error in reinforcement learning | Memory decay functions in forgetting models | Attention mechanisms in decision making | . Using hardcoded equations allows us to: . | Incorporate established theoretical knowledge | Reduce the search space for model discovery | Focus learning on unknown mechanisms | . ",
    "url": "/SPICE/tutorials/hardcoded_equations.html#why-use-hardcoded-equations",
    
    "relUrl": "/tutorials/hardcoded_equations.html#why-use-hardcoded-equations"
  },"8": {
    "doc": "Working with Hardcoded Equations",
    "title": "Tutorial Contents",
    "content": ". | Understanding when to use hardcoded equations | Implementing reward prediction error as a hardcoded module | Combining hardcoded and learned mechanisms | Analyzing model performance | Best practices for equation design | . ",
    "url": "/SPICE/tutorials/hardcoded_equations.html#tutorial-contents",
    
    "relUrl": "/tutorials/hardcoded_equations.html#tutorial-contents"
  },"9": {
    "doc": "Working with Hardcoded Equations",
    "title": "Interactive Version",
    "content": "This is the static web version of the tutorial. For an interactive version: . | Go to the SPICE repository | Navigate to tutorials/3_hardcoded_equations.ipynb | Run the notebook in Jupyter | . ",
    "url": "/SPICE/tutorials/hardcoded_equations.html#interactive-version",
    
    "relUrl": "/tutorials/hardcoded_equations.html#interactive-version"
  },"10": {
    "doc": "Working with Hardcoded Equations",
    "title": "Full Tutorial",
    "content": "View or download the complete notebook . ",
    "url": "/SPICE/tutorials/hardcoded_equations.html#full-tutorial",
    
    "relUrl": "/tutorials/hardcoded_equations.html#full-tutorial"
  },"11": {
    "doc": "Working with Hardcoded Equations",
    "title": "Step-by-Step Guide",
    "content": "1. Setup and Imports . import numpy as np import torch from spice.resources.bandits import BanditsDrift, AgentQ, create_dataset # Set random seeds for reproducibility np.random.seed(42) torch.manual_seed(42) . 2. Create Environment and Agent . # Set up the environment n_actions = 2 sigma = 0.2 environment = BanditsDrift(sigma=sigma, n_actions=n_actions) # Set up the agent agent = AgentQ( n_actions=n_actions, alpha_reward=0.6, # Learning rate for positive rewards alpha_penalty=0.6, # Learning rate for negative rewards forget_rate=0.3, ) # Generate dataset dataset, _, _ = create_dataset( agent=agent, environment=environment, n_trials=200, n_sessions=256, ) . 3. Using Precoded Models with Hardcoded Equations . SPICE provides models with built-in hardcoded equations: . from spice.precoded import LearningRateRNN, LEARNING_RATE_CONFIG from spice.estimator import SpiceEstimator # Create and train SPICE model spice_estimator = SpiceEstimator( rnn_class=LearningRateRNN, spice_config=LEARNING_RATE_CONFIG, hidden_size=8, learning_rate=5e-3, epochs=16, verbose=True ) spice_estimator.fit(dataset.xs, dataset.ys) . 4. Creating Custom Hardcoded Equations . You can create your own hardcoded equations: . from spice.resources.rnn import BaseRNN import torch.nn as nn class CustomHardcodedRNN(BaseRNN): def __init__(self, n_actions, **kwargs): super().__init__(n_actions=n_actions, **kwargs) # Define hardcoded equation parameters self.alpha = nn.Parameter(torch.tensor(0.3)) def forward_hardcoded(self, x_t, r_t): # Implement reward prediction error delta = r_t - x_t return self.alpha * delta . 5. Combining Hardcoded and Learned Components . Create a configuration that uses both: . from spice.estimator import SpiceConfig MIXED_CONFIG = SpiceConfig( library_setup={ 'x_learning_rate': ['x_value', 'c_reward'], }, filter_setup={ 'x_learning_rate': ['c_action', 1, True], }, control_parameters=['c_action', 'c_reward'], rnn_modules=['x_learning_rate'] ) . ",
    "url": "/SPICE/tutorials/hardcoded_equations.html#step-by-step-guide",
    
    "relUrl": "/tutorials/hardcoded_equations.html#step-by-step-guide"
  },"12": {
    "doc": "Working with Hardcoded Equations",
    "title": "Understanding the Results",
    "content": "When analyzing models with hardcoded equations, look for: . | Interaction Effects: How hardcoded and learned mechanisms interact | Parameter Adaptation: How learned parameters modify hardcoded equations | Model Performance: Comparison with fully learned models | . ",
    "url": "/SPICE/tutorials/hardcoded_equations.html#understanding-the-results",
    
    "relUrl": "/tutorials/hardcoded_equations.html#understanding-the-results"
  },"13": {
    "doc": "Working with Hardcoded Equations",
    "title": "Best Practices",
    "content": "When implementing hardcoded equations: . | Validate Assumptions . | Test the equations independently | Verify theoretical foundations | Compare with empirical data | . | Balance Flexibility . | Allow some parameters to be learned | Don’t over-constrain the model | Consider multiple theoretical accounts | . | Document Clearly . | Explain equation choices | Reference theoretical sources | Document parameter meanings | . | . ",
    "url": "/SPICE/tutorials/hardcoded_equations.html#best-practices",
    
    "relUrl": "/tutorials/hardcoded_equations.html#best-practices"
  },"14": {
    "doc": "Working with Hardcoded Equations",
    "title": "Next Steps",
    "content": "After completing this tutorial, you can: . | Implement your own hardcoded equations | Combine multiple theoretical mechanisms | Move on to Modeling Individual Differences | . ",
    "url": "/SPICE/tutorials/hardcoded_equations.html#next-steps",
    
    "relUrl": "/tutorials/hardcoded_equations.html#next-steps"
  },"15": {
    "doc": "Working with Hardcoded Equations",
    "title": "Common Issues and Solutions",
    "content": ". | Overly Rigid Models: Allow some parameters to be learned | Poor Integration: Ensure proper interaction between components | Numerical Instability: Add bounds to hardcoded parameters | . ",
    "url": "/SPICE/tutorials/hardcoded_equations.html#common-issues-and-solutions",
    
    "relUrl": "/tutorials/hardcoded_equations.html#common-issues-and-solutions"
  },"16": {
    "doc": "Working with Hardcoded Equations",
    "title": "Additional Resources",
    "content": ". | Reward Prediction Error Theory | SPICE API Documentation | GitHub Repository | . ",
    "url": "/SPICE/tutorials/hardcoded_equations.html#additional-resources",
    
    "relUrl": "/tutorials/hardcoded_equations.html#additional-resources"
  },"17": {
    "doc": "Home",
    "title": "SPICE Documentation",
    "content": "Welcome to the documentation for SPICE (Sparse and Interpretable Cognitive Equations), a framework for automating scientific practice in cognitive science. ",
    "url": "/SPICE/#spice-documentation",
    
    "relUrl": "/#spice-documentation"
  },"18": {
    "doc": "Home",
    "title": "Overview",
    "content": "SPICE is built on two fundamental principles: . | Task-specific RNN Training: A neural network is trained to predict human behavior and implicitly learn latent cognitive mechanisms. | Equation Discovery: Using Sparse Identification of nonlinear Dynamics (SINDy) to derive interpretable mathematical equations from the learned mechanisms. | . ",
    "url": "/SPICE/#overview",
    
    "relUrl": "/#overview"
  },"19": {
    "doc": "Home",
    "title": "Quick Links",
    "content": ". | Installation Guide | Quick Start Tutorial | API Reference | Tutorials | . ",
    "url": "/SPICE/#quick-links",
    
    "relUrl": "/#quick-links"
  },"20": {
    "doc": "Home",
    "title": "Key Features",
    "content": ". | Scikit-learn compatible estimator interface | Customizable network architecture for identifying complex cognitive mechanisms | Participant embeddings for identifying individual differences | Precoded models for: . | Simple Rescorla-Wagner | Forgetting mechanism | Choice perseveration | Participant embeddings | . | . ",
    "url": "/SPICE/#key-features",
    
    "relUrl": "/#key-features"
  },"21": {
    "doc": "Home",
    "title": "Getting Help",
    "content": "If you need help using SPICE, you can: . | Check the tutorials section | Visit our GitHub repository | Open an issue on GitHub | . ",
    "url": "/SPICE/#getting-help",
    
    "relUrl": "/#getting-help"
  },"22": {
    "doc": "Home",
    "title": "License",
    "content": "SPICE is released under the MIT License. See the LICENSE file for more details. ",
    "url": "/SPICE/#license",
    
    "relUrl": "/#license"
  },"23": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/SPICE/",
    
    "relUrl": "/"
  },"24": {
    "doc": "Modeling Individual Differences",
    "title": "Modeling Individual Differences in SPICE",
    "content": "This tutorial explains how to model and analyze individual differences in cognitive mechanisms using SPICE. You’ll learn how to: . | Use participant embeddings to capture individual differences | Train models with participant-specific parameters | Analyze and interpret individual differences in cognitive mechanisms | . ",
    "url": "/SPICE/tutorials/individual_differences.html#modeling-individual-differences-in-spice",
    
    "relUrl": "/tutorials/individual_differences.html#modeling-individual-differences-in-spice"
  },"25": {
    "doc": "Modeling Individual Differences",
    "title": "Prerequisites",
    "content": "Before starting this tutorial, make sure you have: . | Completed the previous tutorials | Understanding of basic reinforcement learning concepts | Familiarity with embedding spaces and individual differences | . ",
    "url": "/SPICE/tutorials/individual_differences.html#prerequisites",
    
    "relUrl": "/tutorials/individual_differences.html#prerequisites"
  },"26": {
    "doc": "Modeling Individual Differences",
    "title": "Why Model Individual Differences?",
    "content": "People differ in how they learn and make decisions. These differences can manifest in: . | Learning rates | Decision noise (inverse temperature) | Forgetting rates | Choice perseveration | Strategy preferences | . SPICE can capture these differences through participant embeddings, allowing us to: . | Model participant-specific cognitive mechanisms | Discover patterns in individual differences | Make personalized predictions | . ",
    "url": "/SPICE/tutorials/individual_differences.html#why-model-individual-differences",
    
    "relUrl": "/tutorials/individual_differences.html#why-model-individual-differences"
  },"27": {
    "doc": "Modeling Individual Differences",
    "title": "Tutorial Contents",
    "content": ". | Setting up participant embeddings | Training models with individual differences | Analyzing participant-specific parameters | Visualizing individual differences | Best practices for individual difference modeling | . ",
    "url": "/SPICE/tutorials/individual_differences.html#tutorial-contents",
    
    "relUrl": "/tutorials/individual_differences.html#tutorial-contents"
  },"28": {
    "doc": "Modeling Individual Differences",
    "title": "Interactive Version",
    "content": "This is the static web version of the tutorial. For an interactive version: . | Go to the SPICE repository | Navigate to tutorials/4_individual_differences.ipynb | Run the notebook in Jupyter | . ",
    "url": "/SPICE/tutorials/individual_differences.html#interactive-version",
    
    "relUrl": "/tutorials/individual_differences.html#interactive-version"
  },"29": {
    "doc": "Modeling Individual Differences",
    "title": "Full Tutorial",
    "content": "View or download the complete notebook . ",
    "url": "/SPICE/tutorials/individual_differences.html#full-tutorial",
    
    "relUrl": "/tutorials/individual_differences.html#full-tutorial"
  },"30": {
    "doc": "Modeling Individual Differences",
    "title": "Step-by-Step Guide",
    "content": "1. Setup and Imports . import numpy as np import torch from spice.resources.bandits import BanditsDrift, AgentQ, create_dataset from spice.precoded import ParticipantEmbeddingRNN, PARTICIPANT_EMBEDDING_RNN_CONFIG from spice.estimator import SpiceEstimator # Set random seeds for reproducibility np.random.seed(42) torch.manual_seed(42) . 2. Create Environment and Multiple Agents . We’ll simulate data from multiple agents with different parameters: . # Set up the environment n_actions = 2 sigma = 0.2 environment = BanditsDrift(sigma=sigma, n_actions=n_actions) # Create multiple agents with different parameters n_participants = 50 agents = [] for _ in range(n_participants): agent = AgentQ( n_actions=n_actions, alpha_reward=np.random.uniform(0.2, 0.8), # Random learning rate alpha_penalty=np.random.uniform(0.2, 0.8), # Random penalty learning rate forget_rate=np.random.uniform(0.1, 0.5), # Random forgetting rate ) agents.append(agent) # Generate dataset for each agent datasets = [] for agent in agents: dataset, _, _ = create_dataset( agent=agent, environment=environment, n_trials=200, n_sessions=1, ) datasets.append(dataset) # Combine datasets combined_dataset = { 'xs': torch.cat([d.xs for d in datasets], dim=1), # Combine across participants 'ys': torch.cat([d.ys for d in datasets], dim=1) } . 3. Using the Participant Embedding RNN . SPICE provides a precoded RNN that includes participant embeddings: . # Create and train SPICE model with participant embeddings spice_estimator = SpiceEstimator( rnn_class=ParticipantEmbeddingRNN, spice_config=PARTICIPANT_EMBEDDING_RNN_CONFIG, hidden_size=8, learning_rate=5e-3, epochs=16, n_participants=n_participants, # Specify number of participants verbose=True ) spice_estimator.fit(combined_dataset.xs, combined_dataset.ys) . 4. Analyzing Individual Differences . Extract and examine the participant embeddings: . # Get participant embeddings embeddings = spice_estimator.get_participant_embeddings() for participant_id, embedding in embeddings.items(): print(f\"Participant {participant_id} embedding:\", embedding) # Get learned features for each participant features = spice_estimator.get_spice_features() for id, feat in features.items(): print(f\"\\nParticipant {id}:\") for model_name, (feat_names, coeffs) in feat.items(): print(f\" {model_name}:\") for name, coeff in zip(feat_names, coeffs): print(f\" {name}: {coeff}\") . 5. Visualizing Individual Differences . import matplotlib.pyplot as plt import seaborn as sns # Convert embeddings to numpy array for visualization embedding_matrix = np.stack([emb.detach().numpy() for emb in embeddings.values()]) # PCA visualization of participant embeddings from sklearn.decomposition import PCA pca = PCA(n_components=2) embedding_2d = pca.fit_transform(embedding_matrix) plt.figure(figsize=(10, 8)) plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1]) plt.title('Participant Embeddings (PCA)') plt.xlabel('PC1') plt.ylabel('PC2') plt.show() . ",
    "url": "/SPICE/tutorials/individual_differences.html#step-by-step-guide",
    
    "relUrl": "/tutorials/individual_differences.html#step-by-step-guide"
  },"31": {
    "doc": "Modeling Individual Differences",
    "title": "Understanding the Results",
    "content": "When analyzing individual differences, look for: . | Clustering: Groups of participants with similar cognitive mechanisms | Parameter Distributions: How cognitive parameters vary across participants | Strategy Differences: Different approaches to the same task | Learning Trajectories: How learning rates and strategies evolve | . ",
    "url": "/SPICE/tutorials/individual_differences.html#understanding-the-results",
    
    "relUrl": "/tutorials/individual_differences.html#understanding-the-results"
  },"32": {
    "doc": "Modeling Individual Differences",
    "title": "Best Practices",
    "content": "When modeling individual differences: . | Data Collection . | Ensure sufficient trials per participant | Balance participant characteristics | Consider task complexity | . | Model Design . | Choose appropriate embedding dimensions | Consider regularization for embeddings | Balance model complexity with data size | . | Analysis . | Validate individual predictions | Look for meaningful patterns | Consider real-world implications | . | . ",
    "url": "/SPICE/tutorials/individual_differences.html#best-practices",
    
    "relUrl": "/tutorials/individual_differences.html#best-practices"
  },"33": {
    "doc": "Modeling Individual Differences",
    "title": "Next Steps",
    "content": "After completing this tutorial, you can: . | Apply individual difference modeling to your own data | Explore more complex embedding architectures | Move on to Weinhardt et al. 2024 Case Study | . ",
    "url": "/SPICE/tutorials/individual_differences.html#next-steps",
    
    "relUrl": "/tutorials/individual_differences.html#next-steps"
  },"34": {
    "doc": "Modeling Individual Differences",
    "title": "Common Issues and Solutions",
    "content": ". | Overfitting: Use dropout and regularization for embeddings | High Variance: Increase trials per participant or reduce embedding dimension | Poor Generalization: Balance model complexity with data size | . ",
    "url": "/SPICE/tutorials/individual_differences.html#common-issues-and-solutions",
    
    "relUrl": "/tutorials/individual_differences.html#common-issues-and-solutions"
  },"35": {
    "doc": "Modeling Individual Differences",
    "title": "Additional Resources",
    "content": ". | Individual Differences in RL | SPICE API Documentation | GitHub Repository | . ",
    "url": "/SPICE/tutorials/individual_differences.html#additional-resources",
    
    "relUrl": "/tutorials/individual_differences.html#additional-resources"
  },"36": {
    "doc": "Modeling Individual Differences",
    "title": "Modeling Individual Differences",
    "content": " ",
    "url": "/SPICE/tutorials/individual_differences.html",
    
    "relUrl": "/tutorials/individual_differences.html"
  },"37": {
    "doc": "Installation",
    "title": "Installation Guide",
    "content": "SPICE can be installed using pip or by building from source. Choose the method that best suits your needs. ",
    "url": "/SPICE/installation.html#installation-guide",
    
    "relUrl": "/installation.html#installation-guide"
  },"38": {
    "doc": "Installation",
    "title": "Quick Installation",
    "content": "The fastest way to get started with SPICE is to install it via pip: . pip install autospice . ",
    "url": "/SPICE/installation.html#quick-installation",
    
    "relUrl": "/installation.html#quick-installation"
  },"39": {
    "doc": "Installation",
    "title": "Installing from Source",
    "content": "For the latest development version or if you want to contribute to SPICE, you can install from source: . | Clone the repository: git clone https://github.com/whyhardt/SPICE.git cd SPICE . | Install in development mode: pip install -e . | . ",
    "url": "/SPICE/installation.html#installing-from-source",
    
    "relUrl": "/installation.html#installing-from-source"
  },"40": {
    "doc": "Installation",
    "title": "Requirements",
    "content": "SPICE requires Python 3.8 or later. The main dependencies are: . | PyTorch | NumPy | SciPy | Scikit-learn | PySINDy | . For a complete list of dependencies, see the requirements.txt file in the repository. ",
    "url": "/SPICE/installation.html#requirements",
    
    "relUrl": "/installation.html#requirements"
  },"41": {
    "doc": "Installation",
    "title": "Verifying Installation",
    "content": "To verify that SPICE is installed correctly, you can run: . from spice.estimator import SpiceEstimator print(\"SPICE installed successfully!\") . ",
    "url": "/SPICE/installation.html#verifying-installation",
    
    "relUrl": "/installation.html#verifying-installation"
  },"42": {
    "doc": "Installation",
    "title": "Development Installation",
    "content": "If you plan to contribute to SPICE, you should install the development dependencies: . pip install -r requirements-dev.txt . This will install additional packages needed for testing and development. ",
    "url": "/SPICE/installation.html#development-installation",
    
    "relUrl": "/installation.html#development-installation"
  },"43": {
    "doc": "Installation",
    "title": "Troubleshooting",
    "content": "If you encounter any issues during installation: . | Make sure you have Python 3.8 or later installed | Verify that pip is up to date: pip install --upgrade pip | Check that all dependencies are properly installed | If problems persist, please open an issue on our GitHub repository | . ",
    "url": "/SPICE/installation.html#troubleshooting",
    
    "relUrl": "/installation.html#troubleshooting"
  },"44": {
    "doc": "Installation",
    "title": "Installation",
    "content": " ",
    "url": "/SPICE/installation.html",
    
    "relUrl": "/installation.html"
  },"45": {
    "doc": "Quick Start",
    "title": "Quick Start Guide",
    "content": "This guide will help you get started with SPICE by walking through a complete example using a two-armed bandit task. ",
    "url": "/SPICE/quickstart.html#quick-start-guide",
    
    "relUrl": "/quickstart.html#quick-start-guide"
  },"46": {
    "doc": "Quick Start",
    "title": "Basic Usage",
    "content": "Here’s a complete example that demonstrates the core functionality of SPICE: . from spice.estimator import SpiceEstimator from spice.precoded import RescorlaWagnerRNN, RESCOLA_WAGNER_CONFIG from spice.resources.bandits import BanditsDrift, AgentQ, create_dataset # Step 1: Create a simulated environment environment = BanditsDrift( sigma=0.2, # Noise level in the environment n_actions=2 # Number of possible actions (arms) ) # Step 2: Create a Q-learning agent agent = AgentQ( n_actions=2, alpha_reward=0.6, # Learning rate for positive rewards alpha_penalty=0.6, # Learning rate for negative rewards forget_rate=0.3, # Rate at which the agent forgets previous learning ) # Step 3: Generate synthetic data dataset, _, _ = create_dataset( agent=agent, environment=environment, n_trials=200, # Number of trials per session n_sessions=256, # Number of sessions to simulate ) # Step 4: Create and configure SPICE estimator spice_estimator = SpiceEstimator( rnn_class=RescorlaWagnerRNN, # Type of RNN to use spice_config=RESCOLA_WAGNER_CONFIG, # Configuration for SPICE hidden_size=8, # Size of hidden layer learning_rate=5e-3, # Learning rate for training epochs=16, # Number of training epochs n_steps_per_call=16, # Steps per training iteration spice_participant_id=0, # Participant ID for analysis verbose=True, # Enable progress output ) # Step 5: Train the model spice_estimator.fit(dataset.xs, dataset.ys) # Step 6: Extract learned features features = spice_estimator.spice_agent.get_spice_features() for id, feat in features.items(): print(f\"\\nAgent {id}:\") for model_name, (feat_names, coeffs) in feat.items(): print(f\" {model_name}:\") for name, coeff in zip(feat_names, coeffs): print(f\" {name}: {coeff}\") # Step 7: Make predictions pred_rnn, pred_spice = spice_estimator.predict(dataset.xs) . ",
    "url": "/SPICE/quickstart.html#basic-usage",
    
    "relUrl": "/quickstart.html#basic-usage"
  },"47": {
    "doc": "Quick Start",
    "title": "Understanding the Components",
    "content": "Environment Setup . The BanditsDrift class creates a two-armed bandit environment where: . | Each arm has a reward probability that drifts over time | sigma controls the noise level in the environment | n_actions specifies the number of possible actions (arms) | . Agent Configuration . The AgentQ class implements a Q-learning agent with: . | Separate learning rates for rewards and penalties | A forgetting mechanism that gradually decays learned values | Support for multiple actions | . SPICE Estimator . The SpiceEstimator class is the main interface to SPICE, combining: . | An RNN for learning behavioral patterns | SINDy for discovering interpretable equations | Scikit-learn compatible interface | . ",
    "url": "/SPICE/quickstart.html#understanding-the-components",
    
    "relUrl": "/quickstart.html#understanding-the-components"
  },"48": {
    "doc": "Quick Start",
    "title": "Working with Real Data",
    "content": "When working with real data instead of simulated data, your input should be structured as: . # X: Input features (trials × features) # y: Target variables (trials × targets) spice_estimator.fit(X, y) . The exact structure of X and y will depend on your specific task. ",
    "url": "/SPICE/quickstart.html#working-with-real-data",
    
    "relUrl": "/quickstart.html#working-with-real-data"
  },"49": {
    "doc": "Quick Start",
    "title": "Next Steps",
    "content": "Now that you’ve seen the basics, you might want to: . | Check out the tutorials for more complex examples | Read the API documentation for detailed information | Learn about customizing SPICE for your specific needs | . ",
    "url": "/SPICE/quickstart.html#next-steps",
    
    "relUrl": "/quickstart.html#next-steps"
  },"50": {
    "doc": "Quick Start",
    "title": "Common Patterns",
    "content": "Here are some common patterns you might find useful: . Custom RNN Architecture . from spice.estimator import SpiceEstimator from your_module import CustomRNN, CUSTOM_CONFIG estimator = SpiceEstimator( rnn_class=CustomRNN, spice_config=CUSTOM_CONFIG, # ... other parameters ) . Multiple Participants . # Train on multiple participants for participant_id in range(n_participants): estimator = SpiceEstimator( spice_participant_id=participant_id, # ... other parameters ) estimator.fit(X[participant_id], y[participant_id]) . Hyperparameter Tuning . from sklearn.model_selection import GridSearchCV param_grid = { 'hidden_size': [4, 8, 16], 'learning_rate': [1e-3, 5e-3, 1e-2], } grid_search = GridSearchCV( estimator=SpiceEstimator(), param_grid=param_grid, cv=5 ) . ",
    "url": "/SPICE/quickstart.html#common-patterns",
    
    "relUrl": "/quickstart.html#common-patterns"
  },"51": {
    "doc": "Quick Start",
    "title": "Quick Start",
    "content": " ",
    "url": "/SPICE/quickstart.html",
    
    "relUrl": "/quickstart.html"
  },"52": {
    "doc": "Basic Rescorla-Wagner Model",
    "title": "Basic Rescorla-Wagner Model Tutorial",
    "content": "This tutorial introduces SPICE using a simple Rescorla-Wagner learning model. You’ll learn how to: . | Set up a basic SPICE model | Train it on simulated data | Extract and interpret the discovered equations | . ",
    "url": "/SPICE/tutorials/rescorla_wagner.html#basic-rescorla-wagner-model-tutorial",
    
    "relUrl": "/tutorials/rescorla_wagner.html#basic-rescorla-wagner-model-tutorial"
  },"53": {
    "doc": "Basic Rescorla-Wagner Model",
    "title": "Prerequisites",
    "content": "Before starting this tutorial, make sure you have: . | SPICE installed (pip install autospice) | Basic understanding of reinforcement learning | Familiarity with Python and NumPy | . ",
    "url": "/SPICE/tutorials/rescorla_wagner.html#prerequisites",
    
    "relUrl": "/tutorials/rescorla_wagner.html#prerequisites"
  },"54": {
    "doc": "Basic Rescorla-Wagner Model",
    "title": "The Rescorla-Wagner Model",
    "content": "The Rescorla-Wagner model is a fundamental model of associative learning that describes how associations between stimuli and outcomes are learned through experience. The basic equation is: . ΔV = α(λ - V) . where: . | V is the associative strength | α is the learning rate | λ is the maximum possible associative strength | ΔV is the change in associative strength | . ",
    "url": "/SPICE/tutorials/rescorla_wagner.html#the-rescorla-wagner-model",
    
    "relUrl": "/tutorials/rescorla_wagner.html#the-rescorla-wagner-model"
  },"55": {
    "doc": "Basic Rescorla-Wagner Model",
    "title": "Tutorial Contents",
    "content": ". | Setting up the environment | Creating simulated data | Training the SPICE model | Analyzing the results | Interpreting the equations | . ",
    "url": "/SPICE/tutorials/rescorla_wagner.html#tutorial-contents",
    
    "relUrl": "/tutorials/rescorla_wagner.html#tutorial-contents"
  },"56": {
    "doc": "Basic Rescorla-Wagner Model",
    "title": "Interactive Version",
    "content": "This is the static web version of the tutorial. For an interactive version: . | Go to the SPICE repository | Navigate to tutorials/1_rescorla_wagner.ipynb | Run the notebook in Jupyter | . ",
    "url": "/SPICE/tutorials/rescorla_wagner.html#interactive-version",
    
    "relUrl": "/tutorials/rescorla_wagner.html#interactive-version"
  },"57": {
    "doc": "Basic Rescorla-Wagner Model",
    "title": "Full Tutorial",
    "content": "View or download the complete notebook . ",
    "url": "/SPICE/tutorials/rescorla_wagner.html#full-tutorial",
    
    "relUrl": "/tutorials/rescorla_wagner.html#full-tutorial"
  },"58": {
    "doc": "Basic Rescorla-Wagner Model",
    "title": "Step-by-Step Guide",
    "content": "1. Setup . First, let’s import the necessary modules: . from spice.estimator import SpiceEstimator from spice.precoded import RescorlaWagnerRNN, RESCOLA_WAGNER_CONFIG from spice.resources.bandits import BanditsDrift, AgentQ, create_dataset import numpy as np . 2. Create the Environment . We’ll create a two-armed bandit environment: . environment = BanditsDrift( sigma=0.2, # Noise level n_actions=2 # Number of arms ) . 3. Create the Agent . Set up a Q-learning agent with specific learning parameters: . agent = AgentQ( n_actions=2, alpha_reward=0.6, # Learning rate for rewards alpha_penalty=0.6, # Learning rate for penalties forget_rate=0.3, # Rate of forgetting ) . 4. Generate Data . Create a synthetic dataset for training: . dataset, _, _ = create_dataset( agent=agent, environment=environment, n_trials=200, # Trials per session n_sessions=256, # Number of sessions ) . 5. Create and Train SPICE Model . Set up and train the SPICE model: . spice_estimator = SpiceEstimator( rnn_class=RescorlaWagnerRNN, spice_config=RESCOLA_WAGNER_CONFIG, hidden_size=8, learning_rate=5e-3, epochs=16, verbose=True ) spice_estimator.fit(dataset.xs, dataset.ys) . 6. Extract Learned Features . Examine what SPICE has learned: . features = spice_estimator.spice_agent.get_spice_features() for id, feat in features.items(): print(f\"\\nAgent {id}:\") for model_name, (feat_names, coeffs) in feat.items(): print(f\" {model_name}:\") for name, coeff in zip(feat_names, coeffs): print(f\" {name}: {coeff}\") . 7. Make Predictions . Use the trained model to make predictions: . pred_rnn, pred_spice = spice_estimator.predict(dataset.xs) . ",
    "url": "/SPICE/tutorials/rescorla_wagner.html#step-by-step-guide",
    
    "relUrl": "/tutorials/rescorla_wagner.html#step-by-step-guide"
  },"59": {
    "doc": "Basic Rescorla-Wagner Model",
    "title": "Understanding the Results",
    "content": "The SPICE model should discover equations similar to the Rescorla-Wagner update rule. Key things to look for: . | The relationship between reward prediction error and value updates | The learning rate parameter | How well the discovered equations match the original agent’s parameters | . ",
    "url": "/SPICE/tutorials/rescorla_wagner.html#understanding-the-results",
    
    "relUrl": "/tutorials/rescorla_wagner.html#understanding-the-results"
  },"60": {
    "doc": "Basic Rescorla-Wagner Model",
    "title": "Next Steps",
    "content": "After completing this tutorial, you can: . | Experiment with different parameter values | Try more complex environments | Move on to the Rescorla-Wagner with Forgetting tutorial | . ",
    "url": "/SPICE/tutorials/rescorla_wagner.html#next-steps",
    
    "relUrl": "/tutorials/rescorla_wagner.html#next-steps"
  },"61": {
    "doc": "Basic Rescorla-Wagner Model",
    "title": "Common Issues and Solutions",
    "content": ". | Poor Convergence: Try increasing the number of epochs or adjusting the learning rate | Overfitting: Reduce the hidden size or increase the dataset size | Unstable Training: Adjust the optimizer parameters or reduce the learning rate | . ",
    "url": "/SPICE/tutorials/rescorla_wagner.html#common-issues-and-solutions",
    
    "relUrl": "/tutorials/rescorla_wagner.html#common-issues-and-solutions"
  },"62": {
    "doc": "Basic Rescorla-Wagner Model",
    "title": "Additional Resources",
    "content": ". | Original Rescorla-Wagner Paper | SPICE API Documentation | GitHub Repository | . ",
    "url": "/SPICE/tutorials/rescorla_wagner.html#additional-resources",
    
    "relUrl": "/tutorials/rescorla_wagner.html#additional-resources"
  },"63": {
    "doc": "Basic Rescorla-Wagner Model",
    "title": "Basic Rescorla-Wagner Model",
    "content": " ",
    "url": "/SPICE/tutorials/rescorla_wagner.html",
    
    "relUrl": "/tutorials/rescorla_wagner.html"
  },"64": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "Rescorla-Wagner with Forgetting Tutorial",
    "content": "This tutorial extends the basic Rescorla-Wagner model by adding a forgetting mechanism for not-chosen actions. You’ll learn how to: . | Implement forgetting mechanisms in SPICE | Work with multiple cognitive mechanisms simultaneously | Understand how SPICE discovers interaction effects | . ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html#rescorla-wagner-with-forgetting-tutorial",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html#rescorla-wagner-with-forgetting-tutorial"
  },"65": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "Prerequisites",
    "content": "Before starting this tutorial, make sure you have: . | Completed the Basic Rescorla-Wagner Tutorial | SPICE installed with all dependencies | Understanding of basic reinforcement learning concepts | . ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html#prerequisites",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html#prerequisites"
  },"66": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "The Forgetting Mechanism",
    "content": "In real-world learning scenarios, humans tend to forget information about options they haven’t chosen recently. The forgetting mechanism models this by: . | Gradually decreasing the value of non-chosen actions | Maintaining separate learning rates for chosen and non-chosen actions | Allowing for dynamic adjustment of forgetting rates | . ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html#the-forgetting-mechanism",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html#the-forgetting-mechanism"
  },"67": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "Tutorial Contents",
    "content": ". | Setting up the environment with forgetting | Creating a Q-learning agent with forgetting | Training SPICE with multiple mechanisms | Analyzing the discovered equations | Implementing custom forgetting mechanisms | . ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html#tutorial-contents",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html#tutorial-contents"
  },"68": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "Interactive Version",
    "content": "This is the static web version of the tutorial. For an interactive version: . | Go to the SPICE repository | Navigate to tutorials/2_rescorla-wagner_forgetting.ipynb | Run the notebook in Jupyter | . ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html#interactive-version",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html#interactive-version"
  },"69": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "Full Tutorial",
    "content": "View or download the complete notebook . ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html#full-tutorial",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html#full-tutorial"
  },"70": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "Step-by-Step Guide",
    "content": "1. Setup and Imports . import numpy as np import torch from spice.resources.bandits import BanditsDrift, AgentQ, create_dataset # Set random seeds for reproducibility np.random.seed(42) torch.manual_seed(42) . 2. Create Environment and Agent . Now we’ll create an agent with forgetting: . # Set up the environment n_actions = 2 sigma = 0.2 environment = BanditsDrift(sigma=sigma, n_actions=n_actions) # Set up the agent with forgetting agent = AgentQ( n_actions=n_actions, alpha_reward=0.3, # Learning rate for rewards forget_rate=0.2, # Rate of forgetting for non-chosen actions ) # Generate dataset n_trials = 200 n_sessions = 256 dataset, _, _ = create_dataset( agent=agent, environment=environment, n_trials=n_trials, n_sessions=n_sessions, ) . 3. Using the Precoded Forgetting RNN . SPICE provides a precoded RNN that includes forgetting mechanisms: . from spice.precoded import ForgettingRNN, FORGETTING_CONFIG from spice.estimator import SpiceEstimator # Create and train SPICE model spice_estimator = SpiceEstimator( rnn_class=ForgettingRNN, spice_config=FORGETTING_CONFIG, hidden_size=8, learning_rate=5e-3, epochs=16, verbose=True ) spice_estimator.fit(dataset.xs, dataset.ys) . 4. Analyzing the Results . Extract and examine the learned features: . features = spice_estimator.spice_agent.get_spice_features() for id, feat in features.items(): print(f\"\\nAgent {id}:\") for model_name, (feat_names, coeffs) in feat.items(): print(f\" {model_name}:\") for name, coeff in zip(feat_names, coeffs): print(f\" {name}: {coeff}\") . 5. Custom Forgetting Mechanisms . You can also implement your own forgetting mechanism: . from spice.estimator import SpiceConfig CUSTOM_FORGETTING_CONFIG = SpiceConfig( library_setup={ 'x_value_reward': ['c_reward'], 'x_value_forget': ['c_action'], }, filter_setup={ 'x_value_reward': ['c_action', 1, True], 'x_value_forget': ['c_action', 0, True], }, control_parameters=['c_action', 'c_reward'], rnn_modules=['x_value_reward', 'x_value_forget'] ) . ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html#step-by-step-guide",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html#step-by-step-guide"
  },"71": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "Understanding the Results",
    "content": "When analyzing the results, look for: . | Forgetting Rate: The coefficient that determines how quickly non-chosen values decay | Interaction Effects: How forgetting interacts with reward learning | Value Updates: Different update rules for chosen vs non-chosen actions | . ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html#understanding-the-results",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html#understanding-the-results"
  },"72": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "Common Patterns",
    "content": "The model typically discovers: . | Faster learning rates for chosen actions | Gradual decay for non-chosen actions | Balance between exploration and exploitation | . ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html#common-patterns",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html#common-patterns"
  },"73": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "Next Steps",
    "content": "After completing this tutorial, you can: . | Experiment with different forgetting rates | Implement more complex forgetting mechanisms | Move on to Working with Hardcoded Equations | . ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html#next-steps",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html#next-steps"
  },"74": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "Common Issues and Solutions",
    "content": ". | Unstable Learning: Try reducing the learning rate or increasing batch size | Poor Forgetting: Adjust the forgetting rate or increase training data | Convergence Issues: Increase the number of epochs or adjust optimizer parameters | . ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html#common-issues-and-solutions",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html#common-issues-and-solutions"
  },"75": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "Additional Resources",
    "content": ". | Memory and Forgetting in RL | SPICE API Documentation | GitHub Repository | . ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html#additional-resources",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html#additional-resources"
  },"76": {
    "doc": "Rescorla-Wagner with Forgetting",
    "title": "Rescorla-Wagner with Forgetting",
    "content": " ",
    "url": "/SPICE/tutorials/rescorla_wagner_forgetting.html",
    
    "relUrl": "/tutorials/rescorla_wagner_forgetting.html"
  },"77": {
    "doc": "Tutorials",
    "title": "Tutorials",
    "content": "Welcome to the SPICE tutorials section. These tutorials will guide you through various aspects of using SPICE for cognitive modeling. ",
    "url": "/SPICE/tutorials.html",
    
    "relUrl": "/tutorials.html"
  },"78": {
    "doc": "Tutorials",
    "title": "Available Tutorials",
    "content": ". | Basic Rescorla-Wagner Model . | Introduction to SPICE using a simple Rescorla-Wagner learning model | Learn how to set up and train your first SPICE model | Understand the basics of combining RNNs with equation discovery | . | Rescorla-Wagner with Forgetting . | Extend the basic model with forgetting mechanisms | Learn how to work with multiple cognitive mechanisms | Understand how SPICE discovers interaction effects | . | Working with Hardcoded Equations . | Learn how to use predefined equations in SPICE | Understand when and why to use hardcoded equations | Compare performance with discovered equations | . | Modeling Individual Differences . | Learn how to capture individual differences in cognitive models | Work with participant-specific parameters | Analyze and interpret individual variations | . | Case Study: Weinhardt et al. 2024 . | Complete case study from recent research | Advanced modeling techniques | Real-world application example | . | . ",
    "url": "/SPICE/tutorials.html#available-tutorials",
    
    "relUrl": "/tutorials.html#available-tutorials"
  },"79": {
    "doc": "Tutorials",
    "title": "Running the Tutorials",
    "content": "Each tutorial is available in two formats: . | As an interactive Jupyter notebook in the tutorials/ directory of the repository | As a web page in this documentation | . To run the interactive notebooks: . | Clone the SPICE repository: git clone https://github.com/whyhardt/SPICE.git cd SPICE . | Install SPICE and its dependencies: pip install -e . pip install -r requirements-dev.txt . | Launch Jupyter: jupyter notebook tutorials/ . | . ",
    "url": "/SPICE/tutorials.html#running-the-tutorials",
    
    "relUrl": "/tutorials.html#running-the-tutorials"
  },"80": {
    "doc": "Tutorials",
    "title": "Prerequisites",
    "content": ". | Basic understanding of Python programming | Familiarity with machine learning concepts | Basic knowledge of cognitive modeling principles | . ",
    "url": "/SPICE/tutorials.html#prerequisites",
    
    "relUrl": "/tutorials.html#prerequisites"
  },"81": {
    "doc": "Tutorials",
    "title": "Getting Help",
    "content": "If you encounter any issues while following the tutorials: . | Check the API Reference for detailed documentation | Visit our GitHub repository | Open an issue on GitHub | . ",
    "url": "/SPICE/tutorials.html#getting-help",
    
    "relUrl": "/tutorials.html#getting-help"
  },"82": {
    "doc": "Weinhardt et al. 2024 Case Study",
    "title": "Weinhardt et al. 2024 Case Study",
    "content": "This tutorial implements the model from Weinhardt et al. (2024) paper “Computational Discovery of Cognitive Dynamics”. You’ll learn how to: . | Implement a complex cognitive model combining multiple mechanisms | Work with both goal-directed and non-goal-directed behavior | Model choice perseveration bias | Combine RNN modules with hardcoded equations | . ",
    "url": "/SPICE/tutorials/weinhardt_2024.html",
    
    "relUrl": "/tutorials/weinhardt_2024.html"
  },"83": {
    "doc": "Weinhardt et al. 2024 Case Study",
    "title": "Prerequisites",
    "content": "Before starting this tutorial, make sure you have: . | Completed all previous tutorials | Understanding of reinforcement learning and choice behavior | Familiarity with participant embeddings and individual differences | . ",
    "url": "/SPICE/tutorials/weinhardt_2024.html#prerequisites",
    
    "relUrl": "/tutorials/weinhardt_2024.html#prerequisites"
  },"84": {
    "doc": "Weinhardt et al. 2024 Case Study",
    "title": "The Weinhardt 2024 Model",
    "content": "The model combines two key components: . | Goal-directed behavior (x_value_reward) . | Learning from rewards | Value-based decision making | . | Non-goal-directed behavior (x_value_choice) . | Choice perseveration bias | Previous action influence | Habit formation | . | . ",
    "url": "/SPICE/tutorials/weinhardt_2024.html#the-weinhardt-2024-model",
    
    "relUrl": "/tutorials/weinhardt_2024.html#the-weinhardt-2024-model"
  },"85": {
    "doc": "Weinhardt et al. 2024 Case Study",
    "title": "Tutorial Contents",
    "content": ". | Setting up the model architecture | Implementing reward and choice mechanisms | Combining multiple cognitive processes | Training and analyzing the model | Understanding the results | . ",
    "url": "/SPICE/tutorials/weinhardt_2024.html#tutorial-contents",
    
    "relUrl": "/tutorials/weinhardt_2024.html#tutorial-contents"
  },"86": {
    "doc": "Weinhardt et al. 2024 Case Study",
    "title": "Interactive Version",
    "content": "This is the static web version of the tutorial. For an interactive version: . | Go to the SPICE repository | Navigate to tutorials/5_weinhardt_et_al_2024.ipynb | Run the notebook in Jupyter | . ",
    "url": "/SPICE/tutorials/weinhardt_2024.html#interactive-version",
    
    "relUrl": "/tutorials/weinhardt_2024.html#interactive-version"
  },"87": {
    "doc": "Weinhardt et al. 2024 Case Study",
    "title": "Full Tutorial",
    "content": "View or download the complete notebook . ",
    "url": "/SPICE/tutorials/weinhardt_2024.html#full-tutorial",
    
    "relUrl": "/tutorials/weinhardt_2024.html#full-tutorial"
  },"88": {
    "doc": "Weinhardt et al. 2024 Case Study",
    "title": "Step-by-Step Guide",
    "content": "1. Setup and Imports . import numpy as np import torch from spice.resources.bandits import BanditsDrift, AgentQ, create_dataset from spice.precoded import Weinhardt2024RNN, WEINHARDT_2024_CONFIG from spice.estimator import SpiceEstimator # Set random seeds for reproducibility np.random.seed(42) torch.manual_seed(42) . 2. Create Environment and Agents . We’ll create agents that exhibit both goal-directed and habitual behavior: . # Set up the environment n_actions = 2 sigma = 0.2 environment = BanditsDrift(sigma=sigma, n_actions=n_actions) # Create agents with different parameters n_participants = 50 agents = [] for _ in range(n_participants): agent = AgentQ( n_actions=n_actions, alpha_reward=np.random.uniform(0.2, 0.8), # Learning rate for rewards alpha_penalty=np.random.uniform(0.2, 0.8), # Learning rate for penalties forget_rate=np.random.uniform(0.1, 0.5), # Forgetting rate beta_choice=np.random.uniform(0.5, 2.0), # Choice perseveration strength ) agents.append(agent) # Generate dataset for each agent datasets = [] for agent in agents: dataset, _, _ = create_dataset( agent=agent, environment=environment, n_trials=200, n_sessions=1, ) datasets.append(dataset) # Combine datasets combined_dataset = { 'xs': torch.cat([d.xs for d in datasets], dim=1), 'ys': torch.cat([d.ys for d in datasets], dim=1) } . 3. Using the Weinhardt 2024 RNN . The model includes both RNN modules and hardcoded equations: . # Create and train SPICE model spice_estimator = SpiceEstimator( rnn_class=Weinhardt2024RNN, spice_config=WEINHARDT_2024_CONFIG, hidden_size=8, learning_rate=5e-3, epochs=16, n_participants=n_participants, dropout=0.25, # Added dropout for regularization verbose=True ) spice_estimator.fit(combined_dataset.xs, combined_dataset.ys) . 4. Analyzing Model Components . Extract and examine the learned mechanisms: . # Get participant embeddings embeddings = spice_estimator.get_participant_embeddings() # Get learned features features = spice_estimator.get_spice_features() for id, feat in features.items(): print(f\"\\nParticipant {id}:\") for model_name, (feat_names, coeffs) in feat.items(): print(f\" {model_name}:\") for name, coeff in zip(feat_names, coeffs): print(f\" {name}: {coeff}\") # Analyze reward vs choice influence def analyze_value_components(features): reward_coeffs = [] choice_coeffs = [] for id, feat in features.items(): for model_name, (feat_names, coeffs) in feat.items(): if 'value_reward' in model_name: reward_coeffs.extend(coeffs) elif 'value_choice' in model_name: choice_coeffs.extend(coeffs) return np.mean(reward_coeffs), np.mean(choice_coeffs) reward_influence, choice_influence = analyze_value_components(features) print(f\"\\nAverage reward influence: {reward_influence:.3f}\") print(f\"Average choice influence: {choice_influence:.3f}\") . 5. Model Architecture Details . The Weinhardt 2024 model includes: . | Memory States init_values = { 'x_value_reward': 0.5, # Reward-based value 'x_value_choice': 0.0, # Choice-based value 'x_learning_rate_reward': 0.0, # Dynamic learning rate } . | RNN Modules # Learning rate module self.submodules_rnn['x_learning_rate_reward'] = self.setup_module( input_size=2+self.embedding_size, dropout=0.25 ) # Value update modules self.submodules_rnn['x_value_reward_not_chosen'] = self.setup_module( input_size=0+self.embedding_size, dropout=0.25 ) self.submodules_rnn['x_value_choice_chosen'] = self.setup_module( input_size=0+self.embedding_size, dropout=0.25 ) . | Hardcoded Equations # Reward prediction error update self.submodules_eq['x_value_reward_chosen'] = lambda value, inputs: ( value + inputs[..., 1] * (inputs[..., 0] - value) ) . | . ",
    "url": "/SPICE/tutorials/weinhardt_2024.html#step-by-step-guide",
    
    "relUrl": "/tutorials/weinhardt_2024.html#step-by-step-guide"
  },"89": {
    "doc": "Weinhardt et al. 2024 Case Study",
    "title": "Understanding the Results",
    "content": "When analyzing the model, look for: . | Balance of Mechanisms . | Relative influence of reward vs choice values | Individual differences in mechanism reliance | Learning rate adaptation | . | Choice Perseveration . | Strength of habit formation | Impact of previous choices | Individual variation in perseveration | . | Learning Dynamics . | Interaction between mechanisms | Adaptation to environment changes | Strategy shifts over time | . | . ",
    "url": "/SPICE/tutorials/weinhardt_2024.html#understanding-the-results",
    
    "relUrl": "/tutorials/weinhardt_2024.html#understanding-the-results"
  },"90": {
    "doc": "Weinhardt et al. 2024 Case Study",
    "title": "Best Practices",
    "content": "When working with complex models: . | Model Implementation . | Carefully balance components | Use appropriate regularization | Monitor mechanism interactions | . | Training . | Start with simpler versions | Validate each component | Use sufficient data | . | Analysis . | Examine mechanism contributions | Look for emergent behaviors | Consider individual differences | . | . ",
    "url": "/SPICE/tutorials/weinhardt_2024.html#best-practices",
    
    "relUrl": "/tutorials/weinhardt_2024.html#best-practices"
  },"91": {
    "doc": "Weinhardt et al. 2024 Case Study",
    "title": "Next Steps",
    "content": "After completing this tutorial, you can: . | Modify the model for your research | Add new cognitive mechanisms | Apply to different experimental paradigms | . ",
    "url": "/SPICE/tutorials/weinhardt_2024.html#next-steps",
    
    "relUrl": "/tutorials/weinhardt_2024.html#next-steps"
  },"92": {
    "doc": "Weinhardt et al. 2024 Case Study",
    "title": "Common Issues and Solutions",
    "content": ". | Component Dominance: Adjust scaling factors or learning rates | Training Instability: Reduce learning rate or add regularization | Poor Generalization: Increase dropout or data size | . ",
    "url": "/SPICE/tutorials/weinhardt_2024.html#common-issues-and-solutions",
    
    "relUrl": "/tutorials/weinhardt_2024.html#common-issues-and-solutions"
  },"93": {
    "doc": "Weinhardt et al. 2024 Case Study",
    "title": "Additional Resources",
    "content": ". | Original Paper | SPICE API Documentation | GitHub Repository | . ",
    "url": "/SPICE/tutorials/weinhardt_2024.html#additional-resources",
    
    "relUrl": "/tutorials/weinhardt_2024.html#additional-resources"
  }
}
