{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf7jlYw4NA0v",
        "outputId": "0969ca34-675d-422e-cbfb-7387d9bcd8ad"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/whyhardt/SPICE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oXIbg826NS5i",
        "outputId": "3825864a-cb2d-4ad5-f2e5-79a4e81dfc3e"
      },
      "outputs": [],
      "source": [
        "# !pip install -e SPICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f0uVlABYznR5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from spice.estimator import SpiceEstimator\n",
        "from spice.resources.spice_utils import SpiceConfig\n",
        "from spice.utils.convert_dataset import convert_dataset\n",
        "from spice.resources.rnn import BaseRNN\n",
        "from spice.utils.plotting import plot_session\n",
        "\n",
        "# For custom RNN\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load the data first with the `convert_dataset` method. This method returns a `SpiceDataset` object which we can use right away "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of dataset: torch.Size([1176, 24, 8])\n",
            "Number of participants: 98\n",
            "Number of actions in dataset: 2\n",
            "Number of additional inputs: 1\n"
          ]
        }
      ],
      "source": [
        "# Load your data\n",
        "dataset = convert_dataset(\n",
        "    file = '../data/ganesh2024a/ganesh2024a.csv',\n",
        "    df_participant_id='subjID',\n",
        "    df_choice='chose_high',\n",
        "    df_reward='reward',\n",
        "    df_block='blocks',\n",
        "    additional_inputs=['contrast_difference'],\n",
        "    timeshift_additional_inputs=True,\n",
        "    )\n",
        "\n",
        "# structure of dataset:\n",
        "# dataset has two main attributes: xs -> inputs; ys -> targets (next action)\n",
        "# shape: (n_participants*n_blocks*n_experiments, n_timesteps, features)\n",
        "# features are (n_actions * action, n_actions * reward, n_additional_inputs * additional_input, block_number, experiment_id, participant_id)\n",
        "\n",
        "# in order to set up the participant embedding we have to compute the number of unique participants in our data \n",
        "# to get the number of participants n_participants we do:\n",
        "n_participants = len(dataset.xs[..., -1].unique())\n",
        "\n",
        "print(f\"Shape of dataset: {dataset.xs.shape}\")\n",
        "print(f\"Number of participants: {n_participants}\")\n",
        "n_actions = dataset.ys.shape[-1]\n",
        "print(f\"Number of actions in dataset: {n_actions}\")\n",
        "print(f\"Number of additional inputs: {dataset.xs.shape[-1]-2*n_actions-3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPICE Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to define the configuration for SPICE with a `SpiceConfig` object.\n",
        "\n",
        "The `SpiceConfig` takes as arguments \n",
        "1. `library_setup (dict)`: Defining the variable names of each module.\n",
        "2. `memory_state (dict)`: Defining the memory state variables and their initial values.\n",
        "3. `states_in_logit (list)`: Defining which of the memory state variables are used later for the logit computation. This is necessary for some background processes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "spice_config = SpiceConfig(\n",
        "    library_setup={\n",
        "        'value_reward_chosen': ['contr_diff', 'reward'],\n",
        "        'value_reward_not_chosen': ['contr_diff'],\n",
        "        'value_choice': ['contr_diff', 'choice'],\n",
        "    },\n",
        "    \n",
        "    memory_state={\n",
        "            'value_reward': 0.,\n",
        "            'value_choice': 0.,\n",
        "        }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we are going to define the SPICE model which is a child of the `BaseRNN` and `torch.nn.Module` class and takes as required arguments:\n",
        "1. `spice_config (SpiceConfig)`: previously defined SpiceConfig object\n",
        "2. `n_actions (int)`: number of possible actions in your dataset (including non-displayed ones if applicable).\n",
        "3. `n_participants (int)`: number of participants in your dataset.\n",
        "\n",
        "As usual for a `torch.nn.Module` we have to define at least the `__init__` method and the `forward` method.\n",
        "The `forward` method gets called when computing a forward pass through the model and takes as inputs `(inputs (SpiceDataset.xs), prev_state (dict, default: None), batch_first (bool, default: False))` and returns `(logits (torch.Tensor, shape: (n_participants*n_blocks*n_experiments, timesteps, n_actions)), updated_state (dict))`. Two necessary method calls inside the forward pass are:\n",
        "1. `self.init_forward_pass(inputs, prev_state, batch_first) -> SpiceSignals`: returns a `SpiceSignals` object which carries all relevant information already processed.\n",
        "2. `self.post_forward_pass(SpiceSignals, batch_first) -> SpiceSignals`: does some re-arranging of the logits to adhere to `batch_first`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z0kOR2Qgz0FZ"
      },
      "outputs": [],
      "source": [
        "class SPICERNN(BaseRNN):\n",
        "    \n",
        "    def __init__(self, spice_config, **kwargs):\n",
        "        super().__init__(spice_config=spice_config, **kwargs)\n",
        "        \n",
        "        # participant embedding\n",
        "        self.participant_embedding = self.setup_embedding(num_embeddings=self.n_participants, embedding_size=self.embedding_size, dropout=0.)\n",
        "        \n",
        "        # set up the submodules\n",
        "        self.setup_module(key_module='value_reward_chosen', input_size=2+self.embedding_size)\n",
        "        self.setup_module(key_module='value_reward_not_chosen', input_size=1+self.embedding_size)\n",
        "        self.setup_module(key_module='value_choice', input_size=2+self.embedding_size)\n",
        "        \n",
        "    def forward(self, inputs, prev_state, batch_first=False):\n",
        "        \n",
        "        spice_signals = self.init_forward_pass(inputs, prev_state, batch_first)\n",
        "        \n",
        "        contr_diffs = spice_signals.additional_inputs.repeat(1, 1, self.n_actions)\n",
        "        rewards_chosen = (spice_signals.actions * spice_signals.rewards).sum(dim=-1, keepdim=True).repeat(1, 1, self.n_actions)\n",
        "        \n",
        "        # time-invariant participant features\n",
        "        participant_embeddings = self.participant_embedding(spice_signals.participant_ids)\n",
        "        \n",
        "        for timestep in spice_signals.timesteps:\n",
        "            \n",
        "            # update chosen value\n",
        "            self.call_module(\n",
        "                key_module='value_reward_chosen',\n",
        "                key_state='value_reward',\n",
        "                action_mask=spice_signals.actions[timestep],\n",
        "                inputs=(contr_diffs[timestep], rewards_chosen[timestep]),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "            )\n",
        "            \n",
        "            # update not chosen value\n",
        "            self.call_module(\n",
        "                key_module='value_reward_not_chosen',\n",
        "                key_state='value_reward',\n",
        "                action_mask=1-spice_signals.actions[timestep],\n",
        "                inputs=(contr_diffs[timestep]),  # add input rewards_chosen[timestep] for counterfactual updating (adjust in config as well)\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "            )\n",
        "            \n",
        "            # same for choice values\n",
        "            self.call_module(\n",
        "                key_module='value_choice',\n",
        "                key_state='value_choice',\n",
        "                action_mask=spice_signals.actions[timestep],\n",
        "                inputs=(contr_diffs[timestep], spice_signals.actions[timestep]),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "            )\n",
        "            \n",
        "            spice_signals.logits[timestep] = self.state['value_reward'] + self.state['value_choice']\n",
        "            \n",
        "        spice_signals = self.post_forward_pass(spice_signals, batch_first)\n",
        "        \n",
        "        return spice_signals.logits, self.get_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's setup now the `SpiceEstimator` object and fit it to the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_spice = '../params/ganesh2024a/spice_ganesh2024a.pkl'\n",
        "\n",
        "estimator = SpiceEstimator(\n",
        "        # model paramaeters\n",
        "        rnn_class=SPICERNN,\n",
        "        spice_config=spice_config,\n",
        "        n_actions=2,\n",
        "        n_participants=n_participants,\n",
        "        \n",
        "        # rnn training parameters\n",
        "        epochs=1000,\n",
        "        warmup_steps=200,\n",
        "        learning_rate=0.01,\n",
        "        \n",
        "        # sindy fitting parameters\n",
        "        sindy_weight=0.1,\n",
        "        sindy_threshold=0.05,\n",
        "        sindy_threshold_frequency=1,\n",
        "        sindy_threshold_terms=1,\n",
        "        sindy_cutoff_patience=100,\n",
        "        sindy_epochs=1000,\n",
        "        sindy_alpha=0.0001,\n",
        "        sindy_library_polynomial_degree=2,\n",
        "        sindy_ensemble_size=1,\n",
        "        \n",
        "        # additional generalization parameters\n",
        "        batch_size=1024,\n",
        "        bagging=True,\n",
        "        scheduler=True,\n",
        "        \n",
        "        verbose=True,\n",
        "        save_path_spice=path_spice,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "3EnmDiUMWq6e",
        "outputId": "e53b1bbd-4173-4d2c-bcdc-15832bc31bd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training on cpu...\n",
            "================================================================================\n",
            "\n",
            "Training the RNN...\n",
            "================================================================================\n",
            "Epoch 1/1000 --- L(Train): 0.6989506 --- L(Val, RNN): 0.6408207 --- L(Val, SINDy): 0.7097650 --- Time: 0.43s; --- Convergence: 6.80e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.006 1 + 0.999 value_reward_chosen[t] + -0.002 contr_diff + -0.003 reward + -0.0 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.001 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + -0.002 reward^2 \n",
            "value_reward_not_chosen[t+1] = 0.006 1 + 0.998 value_reward_not_chosen[t] + -0.007 contr_diff + -0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.008 1 + 0.996 value_choice[t] + 0.006 contr_diff + -0.008 choice + 0.003 value_choice^2 + -0.002 value_choice*contr_diff + -0.003 value_choice*choice + -0.003 contr_diff^2 + 0.006 contr_diff*choice + -0.006 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 2/1000 --- L(Train): 0.6397673 --- L(Val, RNN): 0.5986323 --- L(Val, SINDy): 0.6759062 --- Time: 0.30s; --- Convergence: 3.61e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.011 1 + 0.996 value_reward_chosen[t] + -0.003 contr_diff + 0.0 reward + -0.0 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.002 value_reward_chosen*reward + -0.003 contr_diff^2 + -0.001 contr_diff*reward + 0.001 reward^2 \n",
            "value_reward_not_chosen[t+1] = 0.013 1 + 0.996 value_reward_not_chosen[t] + -0.014 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.016 1 + 0.989 value_choice[t] + 0.009 contr_diff + -0.016 choice + 0.009 value_choice^2 + -0.001 value_choice*contr_diff + -0.01 value_choice*choice + -0.005 contr_diff^2 + 0.009 contr_diff*choice + -0.014 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 3/1000 --- L(Train): 0.6053283 --- L(Val, RNN): 0.5662715 --- L(Val, SINDy): 0.6216979 --- Time: 0.36s; --- Convergence: 1.97e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.01 1 + 0.993 value_reward_chosen[t] + -0.004 contr_diff + 0.006 reward + -0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.003 value_reward_chosen*reward + -0.005 contr_diff^2 + -0.001 contr_diff*reward + 0.007 reward^2 \n",
            "value_reward_not_chosen[t+1] = 0.018 1 + 0.994 value_reward_not_chosen[t] + -0.02 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.006 contr_diff^2 \n",
            "value_choice[t+1] = -0.011 1 + 0.981 value_choice[t] + 0.008 contr_diff + -0.011 choice + 0.014 value_choice^2 + 0.001 value_choice*contr_diff + -0.018 value_choice*choice + -0.004 contr_diff^2 + 0.007 contr_diff*choice + -0.01 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 4/1000 --- L(Train): 0.5640646 --- L(Val, RNN): 0.5404323 --- L(Val, SINDy): 0.5731428 --- Time: 0.40s; --- Convergence: 1.11e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.007 1 + 0.992 value_reward_chosen[t] + -0.004 contr_diff + 0.011 reward + -0.001 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.003 value_reward_chosen*reward + -0.005 contr_diff^2 + -0.001 contr_diff*reward + 0.012 reward^2 \n",
            "value_reward_not_chosen[t+1] = 0.015 1 + 0.991 value_reward_not_chosen[t] + -0.025 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.006 contr_diff^2 \n",
            "value_choice[t+1] = -0.004 1 + 0.977 value_choice[t] + 0.003 contr_diff + -0.004 choice + 0.019 value_choice^2 + 0.001 value_choice*contr_diff + -0.022 value_choice*choice + -0.0 contr_diff^2 + 0.002 contr_diff*choice + -0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 5/1000 --- L(Train): 0.5407761 --- L(Val, RNN): 0.5193925 --- L(Val, SINDy): 0.5379621 --- Time: 0.34s; --- Convergence: 6.61e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.004 1 + 0.989 value_reward_chosen[t] + -0.004 contr_diff + 0.009 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.005 value_reward_chosen*reward + -0.004 contr_diff^2 + -0.001 contr_diff*reward + 0.01 reward^2 \n",
            "value_reward_not_chosen[t+1] = 0.01 1 + 0.987 value_reward_not_chosen[t] + -0.028 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.003 1 + 0.979 value_choice[t] + -0.002 contr_diff + 0.003 choice + 0.026 value_choice^2 + -0.001 value_choice*contr_diff + -0.02 value_choice*choice + 0.004 contr_diff^2 + -0.003 contr_diff*choice + 0.005 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 6/1000 --- L(Train): 0.5092751 --- L(Val, RNN): 0.5019832 --- L(Val, SINDy): 0.5131063 --- Time: 0.27s; --- Convergence: 4.18e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.001 1 + 0.986 value_reward_chosen[t] + -0.004 contr_diff + 0.007 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.007 value_reward_chosen*reward + -0.003 contr_diff^2 + 0.0 contr_diff*reward + 0.008 reward^2 \n",
            "value_reward_not_chosen[t+1] = 0.003 1 + 0.984 value_reward_not_chosen[t] + -0.029 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.011 1 + 0.982 value_choice[t] + -0.009 contr_diff + 0.011 choice + 0.032 value_choice^2 + -0.004 value_choice*contr_diff + -0.017 value_choice*choice + 0.01 contr_diff^2 + -0.01 contr_diff*choice + 0.013 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 7/1000 --- L(Train): 0.5120275 --- L(Val, RNN): 0.4883202 --- L(Val, SINDy): 0.4967899 --- Time: 0.29s; --- Convergence: 2.77e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.006 1 + 0.982 value_reward_chosen[t] + -0.004 contr_diff + 0.007 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.01 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.008 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.005 1 + 0.98 value_reward_not_chosen[t] + -0.029 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.019 1 + 0.981 value_choice[t] + -0.015 contr_diff + 0.019 choice + 0.032 value_choice^2 + -0.008 value_choice*contr_diff + -0.018 value_choice*choice + 0.015 contr_diff^2 + -0.016 contr_diff*choice + 0.021 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 8/1000 --- L(Train): 0.4926227 --- L(Val, RNN): 0.4784069 --- L(Val, SINDy): 0.4886358 --- Time: 0.29s; --- Convergence: 1.88e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.012 1 + 0.977 value_reward_chosen[t] + -0.001 contr_diff + 0.012 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.014 value_reward_chosen*reward + 0.004 contr_diff^2 + 0.003 contr_diff*reward + 0.013 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.012 1 + 0.975 value_reward_not_chosen[t] + -0.026 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.007 contr_diff^2 \n",
            "value_choice[t+1] = 0.028 1 + 0.976 value_choice[t] + -0.022 contr_diff + 0.028 choice + 0.027 value_choice^2 + -0.011 value_choice*contr_diff + -0.022 value_choice*choice + 0.021 contr_diff^2 + -0.023 contr_diff*choice + 0.029 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 9/1000 --- L(Train): 0.4895141 --- L(Val, RNN): 0.4727741 --- L(Val, SINDy): 0.4882984 --- Time: 0.41s; --- Convergence: 1.22e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.019 1 + 0.971 value_reward_chosen[t] + 0.004 contr_diff + 0.017 reward + -0.002 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.02 value_reward_chosen*reward + 0.009 contr_diff^2 + 0.007 contr_diff*reward + 0.018 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.021 1 + 0.969 value_reward_not_chosen[t] + -0.023 contr_diff + -0.004 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.97 value_choice[t] + -0.029 contr_diff + 0.036 choice + 0.021 value_choice^2 + -0.011 value_choice*contr_diff + -0.029 value_choice*choice + 0.027 contr_diff^2 + -0.03 contr_diff*choice + 0.038 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 10/1000 --- L(Train): 0.4775447 --- L(Val, RNN): 0.4711669 --- L(Val, SINDy): 0.4946911 --- Time: 0.37s; --- Convergence: 6.92e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.026 1 + 0.964 value_reward_chosen[t] + 0.007 contr_diff + 0.02 reward + -0.004 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.026 value_reward_chosen*reward + 0.014 contr_diff^2 + 0.01 contr_diff*reward + 0.021 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.029 1 + 0.962 value_reward_not_chosen[t] + -0.02 contr_diff + -0.007 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.018 contr_diff^2 \n",
            "value_choice[t+1] = 0.045 1 + 0.963 value_choice[t] + -0.035 contr_diff + 0.045 choice + 0.014 value_choice^2 + -0.011 value_choice*contr_diff + -0.035 value_choice*choice + 0.032 contr_diff^2 + -0.036 contr_diff*choice + 0.047 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 11/1000 --- L(Train): 0.4690082 --- L(Val, RNN): 0.4714539 --- L(Val, SINDy): 0.5050050 --- Time: 0.41s; --- Convergence: 3.60e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.033 1 + 0.957 value_reward_chosen[t] + 0.012 contr_diff + 0.022 reward + -0.007 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.033 value_reward_chosen*reward + 0.02 contr_diff^2 + 0.015 contr_diff*reward + 0.023 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.038 1 + 0.955 value_reward_not_chosen[t] + -0.015 contr_diff + -0.011 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.024 contr_diff^2 \n",
            "value_choice[t+1] = 0.054 1 + 0.964 value_choice[t] + -0.038 contr_diff + 0.054 choice + 0.013 value_choice^2 + -0.006 value_choice*contr_diff + -0.035 value_choice*choice + 0.039 contr_diff^2 + -0.039 contr_diff*choice + 0.055 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 12/1000 --- L(Train): 0.4775236 --- L(Val, RNN): 0.4716891 --- L(Val, SINDy): 0.5145251 --- Time: 0.44s; --- Convergence: 1.92e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.042 1 + 0.949 value_reward_chosen[t] + 0.014 contr_diff + 0.025 reward + -0.012 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.04 value_reward_chosen*reward + 0.026 contr_diff^2 + 0.017 contr_diff*reward + 0.026 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.047 1 + 0.947 value_reward_not_chosen[t] + -0.01 contr_diff + -0.015 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.031 contr_diff^2 \n",
            "value_choice[t+1] = 0.063 1 + 0.967 value_choice[t] + -0.038 contr_diff + 0.063 choice + 0.017 value_choice^2 + -0.001 value_choice*contr_diff + -0.031 value_choice*choice + 0.045 contr_diff^2 + -0.039 contr_diff*choice + 0.065 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 13/1000 --- L(Train): 0.4651267 --- L(Val, RNN): 0.4704445 --- L(Val, SINDy): 0.5158859 --- Time: 0.38s; --- Convergence: 1.58e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.05 1 + 0.94 value_reward_chosen[t] + 0.013 contr_diff + 0.022 reward + -0.017 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.048 value_reward_chosen*reward + 0.033 contr_diff^2 + 0.016 contr_diff*reward + 0.023 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.057 1 + 0.939 value_reward_not_chosen[t] + -0.003 contr_diff + -0.02 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.038 contr_diff^2 \n",
            "value_choice[t+1] = 0.072 1 + 0.973 value_choice[t] + -0.036 contr_diff + 0.072 choice + 0.023 value_choice^2 + 0.006 value_choice*contr_diff + -0.026 value_choice*choice + 0.052 contr_diff^2 + -0.037 contr_diff*choice + 0.074 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 14/1000 --- L(Train): 0.4683891 --- L(Val, RNN): 0.4678108 --- L(Val, SINDy): 0.5102481 --- Time: 0.29s; --- Convergence: 2.11e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.058 1 + 0.932 value_reward_chosen[t] + 0.011 contr_diff + 0.019 reward + -0.024 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + -0.056 value_reward_chosen*reward + 0.041 contr_diff^2 + 0.014 contr_diff*reward + 0.02 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.067 1 + 0.93 value_reward_not_chosen[t] + 0.003 contr_diff + -0.027 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.046 contr_diff^2 \n",
            "value_choice[t+1] = 0.078 1 + 0.976 value_choice[t] + -0.032 contr_diff + 0.078 choice + 0.025 value_choice^2 + 0.014 value_choice*contr_diff + -0.023 value_choice*choice + 0.057 contr_diff^2 + -0.033 contr_diff*choice + 0.08 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 15/1000 --- L(Train): 0.4700564 --- L(Val, RNN): 0.4644897 --- L(Val, SINDy): 0.5009440 --- Time: 0.31s; --- Convergence: 2.71e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.068 1 + 0.923 value_reward_chosen[t] + 0.008 contr_diff + 0.017 reward + -0.031 value_reward_chosen^2 + -0.007 value_reward_chosen*contr_diff + -0.065 value_reward_chosen*reward + 0.049 contr_diff^2 + 0.011 contr_diff*reward + 0.018 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.077 1 + 0.921 value_reward_not_chosen[t] + 0.01 contr_diff + -0.033 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.054 contr_diff^2 \n",
            "value_choice[t+1] = 0.08 1 + 0.974 value_choice[t] + -0.027 contr_diff + 0.08 choice + 0.023 value_choice^2 + 0.021 value_choice*contr_diff + -0.025 value_choice*choice + 0.058 contr_diff^2 + -0.028 contr_diff*choice + 0.082 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 16/1000 --- L(Train): 0.4628086 --- L(Val, RNN): 0.4609782 --- L(Val, SINDy): 0.4913879 --- Time: 0.36s; --- Convergence: 3.11e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.076 1 + 0.913 value_reward_chosen[t] + 0.006 contr_diff + 0.013 reward + -0.038 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + -0.075 value_reward_chosen*reward + 0.056 contr_diff^2 + 0.009 contr_diff*reward + 0.014 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.085 1 + 0.912 value_reward_not_chosen[t] + 0.015 contr_diff + -0.038 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.061 contr_diff^2 \n",
            "value_choice[t+1] = 0.08 1 + 0.97 value_choice[t] + -0.022 contr_diff + 0.08 choice + 0.018 value_choice^2 + 0.028 value_choice*contr_diff + -0.028 value_choice*choice + 0.058 contr_diff^2 + -0.023 contr_diff*choice + 0.081 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 17/1000 --- L(Train): 0.4596286 --- L(Val, RNN): 0.4577130 --- L(Val, SINDy): 0.4823987 --- Time: 0.29s; --- Convergence: 3.19e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.085 1 + 0.903 value_reward_chosen[t] + 0.001 contr_diff + 0.01 reward + -0.046 value_reward_chosen^2 + -0.013 value_reward_chosen*contr_diff + -0.084 value_reward_chosen*reward + 0.065 contr_diff^2 + 0.004 contr_diff*reward + 0.011 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.094 1 + 0.902 value_reward_not_chosen[t] + 0.021 contr_diff + -0.043 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.069 contr_diff^2 \n",
            "value_choice[t+1] = 0.077 1 + 0.966 value_choice[t] + -0.017 contr_diff + 0.077 choice + 0.014 value_choice^2 + 0.035 value_choice*contr_diff + -0.032 value_choice*choice + 0.056 contr_diff^2 + -0.018 contr_diff*choice + 0.079 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 18/1000 --- L(Train): 0.4693031 --- L(Val, RNN): 0.4548655 --- L(Val, SINDy): 0.4743173 --- Time: 0.36s; --- Convergence: 3.02e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.095 1 + 0.893 value_reward_chosen[t] + -0.004 contr_diff + 0.008 reward + -0.054 value_reward_chosen^2 + -0.016 value_reward_chosen*contr_diff + -0.094 value_reward_chosen*reward + 0.074 contr_diff^2 + -0.001 contr_diff*reward + 0.009 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.101 1 + 0.892 value_reward_not_chosen[t] + 0.026 contr_diff + -0.047 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.077 contr_diff^2 \n",
            "value_choice[t+1] = 0.073 1 + 0.963 value_choice[t] + -0.012 contr_diff + 0.073 choice + 0.011 value_choice^2 + 0.04 value_choice*contr_diff + -0.036 value_choice*choice + 0.052 contr_diff^2 + -0.013 contr_diff*choice + 0.075 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 19/1000 --- L(Train): 0.4615328 --- L(Val, RNN): 0.4525275 --- L(Val, SINDy): 0.4675263 --- Time: 0.35s; --- Convergence: 2.68e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.104 1 + 0.882 value_reward_chosen[t] + -0.005 contr_diff + 0.008 reward + -0.063 value_reward_chosen^2 + -0.021 value_reward_chosen*contr_diff + -0.105 value_reward_chosen*reward + 0.083 contr_diff^2 + -0.003 contr_diff*reward + 0.008 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.106 1 + 0.882 value_reward_not_chosen[t] + 0.026 contr_diff + -0.048 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.085 contr_diff^2 \n",
            "value_choice[t+1] = 0.068 1 + 0.961 value_choice[t] + -0.007 contr_diff + 0.068 choice + 0.01 value_choice^2 + 0.043 value_choice*contr_diff + -0.038 value_choice*choice + 0.046 contr_diff^2 + -0.008 contr_diff*choice + 0.07 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 20/1000 --- L(Train): 0.4539353 --- L(Val, RNN): 0.4507542 --- L(Val, SINDy): 0.4624746 --- Time: 0.31s; --- Convergence: 2.23e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.115 1 + 0.872 value_reward_chosen[t] + -0.006 contr_diff + 0.01 reward + -0.072 value_reward_chosen^2 + -0.024 value_reward_chosen*contr_diff + -0.115 value_reward_chosen*reward + 0.092 contr_diff^2 + -0.004 contr_diff*reward + 0.01 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.11 1 + 0.871 value_reward_not_chosen[t] + 0.026 contr_diff + -0.048 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.093 contr_diff^2 \n",
            "value_choice[t+1] = 0.063 1 + 0.961 value_choice[t] + -0.002 contr_diff + 0.063 choice + 0.011 value_choice^2 + 0.045 value_choice*contr_diff + -0.038 value_choice*choice + 0.041 contr_diff^2 + -0.003 contr_diff*choice + 0.065 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 21/1000 --- L(Train): 0.4544056 --- L(Val, RNN): 0.4495764 --- L(Val, SINDy): 0.4589231 --- Time: 0.29s; --- Convergence: 1.70e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.125 1 + 0.861 value_reward_chosen[t] + -0.006 contr_diff + 0.013 reward + -0.081 value_reward_chosen^2 + -0.029 value_reward_chosen*contr_diff + -0.126 value_reward_chosen*reward + 0.101 contr_diff^2 + -0.006 contr_diff*reward + 0.014 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.114 1 + 0.86 value_reward_not_chosen[t] + 0.023 contr_diff + -0.049 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.1 contr_diff^2 \n",
            "value_choice[t+1] = 0.058 1 + 0.961 value_choice[t] + -0.0 contr_diff + 0.058 choice + 0.012 value_choice^2 + 0.044 value_choice*contr_diff + -0.038 value_choice*choice + 0.034 contr_diff^2 + -0.001 contr_diff*choice + 0.059 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 22/1000 --- L(Train): 0.4599746 --- L(Val, RNN): 0.4487923 --- L(Val, SINDy): 0.4554925 --- Time: 0.40s; --- Convergence: 1.24e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.136 1 + 0.849 value_reward_chosen[t] + -0.005 contr_diff + 0.015 reward + -0.091 value_reward_chosen^2 + -0.033 value_reward_chosen*contr_diff + -0.137 value_reward_chosen*reward + 0.111 contr_diff^2 + -0.007 contr_diff*reward + 0.016 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.117 1 + 0.849 value_reward_not_chosen[t] + 0.021 contr_diff + -0.048 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.107 contr_diff^2 \n",
            "value_choice[t+1] = 0.052 1 + 0.962 value_choice[t] + 0.001 contr_diff + 0.052 choice + 0.014 value_choice^2 + 0.042 value_choice*contr_diff + -0.037 value_choice*choice + 0.028 contr_diff^2 + 0.0 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 23/1000 --- L(Train): 0.4516614 --- L(Val, RNN): 0.4482671 --- L(Val, SINDy): 0.4521121 --- Time: 0.31s; --- Convergence: 8.84e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.147 1 + 0.838 value_reward_chosen[t] + -0.004 contr_diff + 0.016 reward + -0.101 value_reward_chosen^2 + -0.039 value_reward_chosen*contr_diff + -0.148 value_reward_chosen*reward + 0.12 contr_diff^2 + -0.009 contr_diff*reward + 0.017 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.118 1 + 0.838 value_reward_not_chosen[t] + 0.017 contr_diff + -0.046 value_reward_not_chosen^2 + 0.005 value_reward_not_chosen*contr_diff + -0.114 contr_diff^2 \n",
            "value_choice[t+1] = 0.047 1 + 0.963 value_choice[t] + -0.0 contr_diff + 0.047 choice + 0.016 value_choice^2 + 0.039 value_choice*contr_diff + -0.035 value_choice*choice + 0.021 contr_diff^2 + -0.001 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 24/1000 --- L(Train): 0.4468510 --- L(Val, RNN): 0.4477033 --- L(Val, SINDy): 0.4497763 --- Time: 0.34s; --- Convergence: 7.24e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.157 1 + 0.827 value_reward_chosen[t] + -0.003 contr_diff + 0.013 reward + -0.11 value_reward_chosen^2 + -0.045 value_reward_chosen*contr_diff + -0.16 value_reward_chosen*reward + 0.13 contr_diff^2 + -0.013 contr_diff*reward + 0.014 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.117 1 + 0.827 value_reward_not_chosen[t] + 0.014 contr_diff + -0.043 value_reward_not_chosen^2 + 0.007 value_reward_not_chosen*contr_diff + -0.12 contr_diff^2 \n",
            "value_choice[t+1] = 0.043 1 + 0.966 value_choice[t] + -0.003 contr_diff + 0.043 choice + 0.019 value_choice^2 + 0.035 value_choice*contr_diff + -0.032 value_choice*choice + 0.015 contr_diff^2 + -0.004 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 25/1000 --- L(Train): 0.4424058 --- L(Val, RNN): 0.4468991 --- L(Val, SINDy): 0.4483493 --- Time: 0.41s; --- Convergence: 7.64e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.167 1 + 0.815 value_reward_chosen[t] + -0.001 contr_diff + 0.011 reward + -0.12 value_reward_chosen^2 + -0.05 value_reward_chosen*contr_diff + -0.171 value_reward_chosen*reward + 0.139 contr_diff^2 + -0.014 contr_diff*reward + 0.012 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.116 1 + 0.815 value_reward_not_chosen[t] + 0.011 contr_diff + -0.039 value_reward_not_chosen^2 + 0.009 value_reward_not_chosen*contr_diff + -0.126 contr_diff^2 \n",
            "value_choice[t+1] = 0.039 1 + 0.969 value_choice[t] + -0.005 contr_diff + 0.039 choice + 0.022 value_choice^2 + 0.03 value_choice*contr_diff + -0.029 value_choice*choice + 0.009 contr_diff^2 + -0.006 contr_diff*choice + 0.04 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 26/1000 --- L(Train): 0.4301813 --- L(Val, RNN): 0.4459804 --- L(Val, SINDy): 0.4471935 --- Time: 0.31s; --- Convergence: 8.41e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.177 1 + 0.804 value_reward_chosen[t] + 0.002 contr_diff + 0.009 reward + -0.13 value_reward_chosen^2 + -0.053 value_reward_chosen*contr_diff + -0.182 value_reward_chosen*reward + 0.148 contr_diff^2 + -0.015 contr_diff*reward + 0.01 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.115 1 + 0.804 value_reward_not_chosen[t] + 0.008 contr_diff + -0.036 value_reward_not_chosen^2 + 0.009 value_reward_not_chosen*contr_diff + -0.132 contr_diff^2 \n",
            "value_choice[t+1] = 0.035 1 + 0.973 value_choice[t] + -0.008 contr_diff + 0.035 choice + 0.025 value_choice^2 + 0.026 value_choice*contr_diff + -0.026 value_choice*choice + 0.004 contr_diff^2 + -0.009 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 27/1000 --- L(Train): 0.4487169 --- L(Val, RNN): 0.4449351 --- L(Val, SINDy): 0.4466447 --- Time: 0.33s; --- Convergence: 9.43e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.187 1 + 0.792 value_reward_chosen[t] + 0.004 contr_diff + 0.007 reward + -0.14 value_reward_chosen^2 + -0.057 value_reward_chosen*contr_diff + -0.194 value_reward_chosen*reward + 0.156 contr_diff^2 + -0.017 contr_diff*reward + 0.008 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.114 1 + 0.793 value_reward_not_chosen[t] + 0.005 contr_diff + -0.033 value_reward_not_chosen^2 + 0.011 value_reward_not_chosen*contr_diff + -0.136 contr_diff^2 \n",
            "value_choice[t+1] = 0.033 1 + 0.976 value_choice[t] + -0.01 contr_diff + 0.033 choice + 0.028 value_choice^2 + 0.022 value_choice*contr_diff + -0.023 value_choice*choice + -0.0 contr_diff^2 + -0.011 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 28/1000 --- L(Train): 0.4463908 --- L(Val, RNN): 0.4438064 --- L(Val, SINDy): 0.4462127 --- Time: 0.29s; --- Convergence: 1.04e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.196 1 + 0.78 value_reward_chosen[t] + 0.007 contr_diff + 0.007 reward + -0.151 value_reward_chosen^2 + -0.059 value_reward_chosen*contr_diff + -0.205 value_reward_chosen*reward + 0.163 contr_diff^2 + -0.018 contr_diff*reward + 0.008 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.113 1 + 0.782 value_reward_not_chosen[t] + 0.002 contr_diff + -0.029 value_reward_not_chosen^2 + 0.012 value_reward_not_chosen*contr_diff + -0.141 contr_diff^2 \n",
            "value_choice[t+1] = 0.032 1 + 0.98 value_choice[t] + -0.012 contr_diff + 0.032 choice + 0.031 value_choice^2 + 0.018 value_choice*contr_diff + -0.019 value_choice*choice + -0.004 contr_diff^2 + -0.013 contr_diff*choice + 0.033 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 29/1000 --- L(Train): 0.4492468 --- L(Val, RNN): 0.4427207 --- L(Val, SINDy): 0.4455991 --- Time: 0.34s; --- Convergence: 1.06e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.204 1 + 0.768 value_reward_chosen[t] + 0.012 contr_diff + 0.009 reward + -0.162 value_reward_chosen^2 + -0.061 value_reward_chosen*contr_diff + -0.216 value_reward_chosen*reward + 0.17 contr_diff^2 + -0.017 contr_diff*reward + 0.01 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.112 1 + 0.771 value_reward_not_chosen[t] + -0.0 contr_diff + -0.026 value_reward_not_chosen^2 + 0.012 value_reward_not_chosen*contr_diff + -0.145 contr_diff^2 \n",
            "value_choice[t+1] = 0.031 1 + 0.983 value_choice[t] + -0.013 contr_diff + 0.031 choice + 0.033 value_choice^2 + 0.015 value_choice*contr_diff + -0.016 value_choice*choice + -0.006 contr_diff^2 + -0.014 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 30/1000 --- L(Train): 0.4538268 --- L(Val, RNN): 0.4417399 --- L(Val, SINDy): 0.4444873 --- Time: 0.37s; --- Convergence: 1.02e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.213 1 + 0.756 value_reward_chosen[t] + 0.015 contr_diff + 0.011 reward + -0.174 value_reward_chosen^2 + -0.063 value_reward_chosen*contr_diff + -0.228 value_reward_chosen*reward + 0.177 contr_diff^2 + -0.018 contr_diff*reward + 0.011 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.111 1 + 0.759 value_reward_not_chosen[t] + -0.001 contr_diff + -0.023 value_reward_not_chosen^2 + 0.013 value_reward_not_chosen*contr_diff + -0.149 contr_diff^2 \n",
            "value_choice[t+1] = 0.03 1 + 0.983 value_choice[t] + -0.014 contr_diff + 0.03 choice + 0.033 value_choice^2 + 0.012 value_choice*contr_diff + -0.016 value_choice*choice + -0.008 contr_diff^2 + -0.015 contr_diff*choice + 0.031 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 31/1000 --- L(Train): 0.4459712 --- L(Val, RNN): 0.4409661 --- L(Val, SINDy): 0.4441972 --- Time: 0.46s; --- Convergence: 8.97e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.223 1 + 0.744 value_reward_chosen[t] + 0.017 contr_diff + 0.015 reward + -0.185 value_reward_chosen^2 + -0.065 value_reward_chosen*contr_diff + -0.239 value_reward_chosen*reward + 0.183 contr_diff^2 + -0.019 contr_diff*reward + 0.015 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.111 1 + 0.747 value_reward_not_chosen[t] + -0.001 contr_diff + -0.023 value_reward_not_chosen^2 + 0.014 value_reward_not_chosen*contr_diff + -0.153 contr_diff^2 \n",
            "value_choice[t+1] = 0.029 1 + 0.982 value_choice[t] + -0.014 contr_diff + 0.029 choice + 0.031 value_choice^2 + 0.01 value_choice*contr_diff + -0.017 value_choice*choice + -0.01 contr_diff^2 + -0.015 contr_diff*choice + 0.031 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 32/1000 --- L(Train): 0.4412989 --- L(Val, RNN): 0.4403380 --- L(Val, SINDy): 0.4444178 --- Time: 0.29s; --- Convergence: 7.63e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.233 1 + 0.731 value_reward_chosen[t] + 0.021 contr_diff + 0.02 reward + -0.197 value_reward_chosen^2 + -0.065 value_reward_chosen*contr_diff + -0.251 value_reward_chosen*reward + 0.188 contr_diff^2 + -0.017 contr_diff*reward + 0.02 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.112 1 + 0.735 value_reward_not_chosen[t] + 0.001 contr_diff + -0.025 value_reward_not_chosen^2 + 0.013 value_reward_not_chosen*contr_diff + -0.156 contr_diff^2 \n",
            "value_choice[t+1] = 0.031 1 + 0.981 value_choice[t] + -0.013 contr_diff + 0.031 choice + 0.029 value_choice^2 + 0.009 value_choice*contr_diff + -0.018 value_choice*choice + -0.009 contr_diff^2 + -0.013 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 33/1000 --- L(Train): 0.4399400 --- L(Val, RNN): 0.4397188 --- L(Val, SINDy): 0.4447024 --- Time: 0.39s; --- Convergence: 6.91e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.242 1 + 0.719 value_reward_chosen[t] + 0.023 contr_diff + 0.025 reward + -0.208 value_reward_chosen^2 + -0.066 value_reward_chosen*contr_diff + -0.263 value_reward_chosen*reward + 0.193 contr_diff^2 + -0.016 contr_diff*reward + 0.026 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.113 1 + 0.724 value_reward_not_chosen[t] + 0.005 contr_diff + -0.028 value_reward_not_chosen^2 + 0.013 value_reward_not_chosen*contr_diff + -0.159 contr_diff^2 \n",
            "value_choice[t+1] = 0.033 1 + 0.981 value_choice[t] + -0.01 contr_diff + 0.033 choice + 0.027 value_choice^2 + 0.009 value_choice*contr_diff + -0.018 value_choice*choice + -0.008 contr_diff^2 + -0.011 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 34/1000 --- L(Train): 0.4468850 --- L(Val, RNN): 0.4391591 --- L(Val, SINDy): 0.4450260 --- Time: 0.31s; --- Convergence: 6.25e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.249 1 + 0.706 value_reward_chosen[t] + 0.026 contr_diff + 0.029 reward + -0.22 value_reward_chosen^2 + -0.066 value_reward_chosen*contr_diff + -0.275 value_reward_chosen*reward + 0.196 contr_diff^2 + -0.016 contr_diff*reward + 0.029 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.111 1 + 0.712 value_reward_not_chosen[t] + 0.011 contr_diff + -0.032 value_reward_not_chosen^2 + 0.011 value_reward_not_chosen*contr_diff + -0.16 contr_diff^2 \n",
            "value_choice[t+1] = 0.037 1 + 0.98 value_choice[t] + -0.007 contr_diff + 0.037 choice + 0.025 value_choice^2 + 0.01 value_choice*contr_diff + -0.019 value_choice*choice + -0.004 contr_diff^2 + -0.008 contr_diff*choice + 0.039 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 35/1000 --- L(Train): 0.4392131 --- L(Val, RNN): 0.4386636 --- L(Val, SINDy): 0.4453295 --- Time: 0.36s; --- Convergence: 5.60e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.254 1 + 0.693 value_reward_chosen[t] + 0.026 contr_diff + 0.03 reward + -0.233 value_reward_chosen^2 + -0.066 value_reward_chosen*contr_diff + -0.287 value_reward_chosen*reward + 0.199 contr_diff^2 + -0.016 contr_diff*reward + 0.031 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.107 1 + 0.7 value_reward_not_chosen[t] + 0.018 contr_diff + -0.034 value_reward_not_chosen^2 + 0.009 value_reward_not_chosen*contr_diff + -0.16 contr_diff^2 \n",
            "value_choice[t+1] = 0.042 1 + 0.98 value_choice[t] + -0.004 contr_diff + 0.042 choice + 0.021 value_choice^2 + 0.011 value_choice*contr_diff + -0.019 value_choice*choice + -0.0 contr_diff^2 + -0.005 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 36/1000 --- L(Train): 0.4421874 --- L(Val, RNN): 0.4380553 --- L(Val, SINDy): 0.4482693 --- Time: 0.30s; --- Convergence: 5.84e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.257 1 + 0.68 value_reward_chosen[t] + 0.026 contr_diff + 0.032 reward + -0.245 value_reward_chosen^2 + -0.065 value_reward_chosen*contr_diff + -0.299 value_reward_chosen*reward + 0.2 contr_diff^2 + -0.016 contr_diff*reward + 0.032 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.1 1 + 0.688 value_reward_not_chosen[t] + 0.026 contr_diff + -0.038 value_reward_not_chosen^2 + 0.006 value_reward_not_chosen*contr_diff + -0.157 contr_diff^2 \n",
            "value_choice[t+1] = 0.048 1 + 0.98 value_choice[t] + -0.001 contr_diff + 0.048 choice + 0.018 value_choice^2 + 0.012 value_choice*contr_diff + -0.019 value_choice*choice + 0.004 contr_diff^2 + -0.002 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 37/1000 --- L(Train): 0.4519296 --- L(Val, RNN): 0.4373521 --- L(Val, SINDy): 0.4483671 --- Time: 0.31s; --- Convergence: 6.44e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.261 1 + 0.667 value_reward_chosen[t] + 0.023 contr_diff + 0.035 reward + -0.258 value_reward_chosen^2 + -0.067 value_reward_chosen*contr_diff + -0.311 value_reward_chosen*reward + 0.2 contr_diff^2 + -0.018 contr_diff*reward + 0.036 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.091 1 + 0.676 value_reward_not_chosen[t] + 0.036 contr_diff + -0.041 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.151 contr_diff^2 \n",
            "value_choice[t+1] = 0.052 1 + 0.979 value_choice[t] + 0.001 contr_diff + 0.052 choice + 0.014 value_choice^2 + 0.012 value_choice*contr_diff + -0.02 value_choice*choice + 0.008 contr_diff^2 + 0.0 contr_diff*choice + 0.053 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 38/1000 --- L(Train): 0.4350812 --- L(Val, RNN): 0.4366230 --- L(Val, SINDy): 0.4467309 --- Time: 0.28s; --- Convergence: 6.86e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.265 1 + 0.654 value_reward_chosen[t] + 0.019 contr_diff + 0.041 reward + -0.271 value_reward_chosen^2 + -0.068 value_reward_chosen*contr_diff + -0.322 value_reward_chosen*reward + 0.2 contr_diff^2 + -0.02 contr_diff*reward + 0.041 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.082 1 + 0.664 value_reward_not_chosen[t] + 0.046 contr_diff + -0.046 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.145 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.977 value_choice[t] + 0.003 contr_diff + 0.055 choice + 0.011 value_choice^2 + 0.011 value_choice*contr_diff + -0.021 value_choice*choice + 0.012 contr_diff^2 + 0.002 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 39/1000 --- L(Train): 0.4368492 --- L(Val, RNN): 0.4359302 --- L(Val, SINDy): 0.4453634 --- Time: 0.31s; --- Convergence: 6.90e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.27 1 + 0.642 value_reward_chosen[t] + 0.017 contr_diff + 0.048 reward + -0.284 value_reward_chosen^2 + -0.067 value_reward_chosen*contr_diff + -0.332 value_reward_chosen*reward + 0.198 contr_diff^2 + -0.02 contr_diff*reward + 0.049 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.072 1 + 0.653 value_reward_not_chosen[t] + 0.055 contr_diff + -0.051 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + -0.137 contr_diff^2 \n",
            "value_choice[t+1] = 0.057 1 + 0.977 value_choice[t] + 0.003 contr_diff + 0.057 choice + 0.008 value_choice^2 + 0.009 value_choice*contr_diff + -0.022 value_choice*choice + 0.014 contr_diff^2 + 0.003 contr_diff*choice + 0.059 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 40/1000 --- L(Train): 0.4458511 --- L(Val, RNN): 0.4353245 --- L(Val, SINDy): 0.4442788 --- Time: 0.45s; --- Convergence: 6.48e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.276 1 + 0.63 value_reward_chosen[t] + 0.016 contr_diff + 0.057 reward + -0.296 value_reward_chosen^2 + -0.064 value_reward_chosen*contr_diff + -0.34 value_reward_chosen*reward + 0.197 contr_diff^2 + -0.02 contr_diff*reward + 0.058 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.061 1 + 0.642 value_reward_not_chosen[t] + 0.064 contr_diff + -0.058 value_reward_not_chosen^2 + -0.011 value_reward_not_chosen*contr_diff + -0.13 contr_diff^2 \n",
            "value_choice[t+1] = 0.058 1 + 0.977 value_choice[t] + 0.004 contr_diff + 0.058 choice + 0.006 value_choice^2 + 0.007 value_choice*contr_diff + -0.021 value_choice*choice + 0.015 contr_diff^2 + 0.003 contr_diff*choice + 0.06 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 41/1000 --- L(Train): 0.4361637 --- L(Val, RNN): 0.4347251 --- L(Val, SINDy): 0.4431199 --- Time: 0.30s; --- Convergence: 6.24e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.283 1 + 0.619 value_reward_chosen[t] + 0.014 contr_diff + 0.066 reward + -0.308 value_reward_chosen^2 + -0.06 value_reward_chosen*contr_diff + -0.348 value_reward_chosen*reward + 0.194 contr_diff^2 + -0.019 contr_diff*reward + 0.066 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.051 1 + 0.631 value_reward_not_chosen[t] + 0.073 contr_diff + -0.067 value_reward_not_chosen^2 + -0.016 value_reward_not_chosen*contr_diff + -0.122 contr_diff^2 \n",
            "value_choice[t+1] = 0.058 1 + 0.978 value_choice[t] + 0.004 contr_diff + 0.058 choice + 0.006 value_choice^2 + 0.004 value_choice*contr_diff + -0.02 value_choice*choice + 0.016 contr_diff^2 + 0.003 contr_diff*choice + 0.06 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 42/1000 --- L(Train): 0.4328451 --- L(Val, RNN): 0.4341613 --- L(Val, SINDy): 0.4417430 --- Time: 0.29s; --- Convergence: 5.94e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.288 1 + 0.608 value_reward_chosen[t] + 0.014 contr_diff + 0.073 reward + -0.32 value_reward_chosen^2 + -0.055 value_reward_chosen*contr_diff + -0.357 value_reward_chosen*reward + 0.191 contr_diff^2 + -0.018 contr_diff*reward + 0.073 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.04 1 + 0.621 value_reward_not_chosen[t] + 0.081 contr_diff + -0.076 value_reward_not_chosen^2 + -0.021 value_reward_not_chosen*contr_diff + -0.114 contr_diff^2 \n",
            "value_choice[t+1] = 0.057 1 + 0.98 value_choice[t] + 0.004 contr_diff + 0.057 choice + 0.006 value_choice^2 + 0.002 value_choice*contr_diff + -0.019 value_choice*choice + 0.015 contr_diff^2 + 0.004 contr_diff*choice + 0.059 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 43/1000 --- L(Train): 0.4332119 --- L(Val, RNN): 0.4336311 --- L(Val, SINDy): 0.4397190 --- Time: 0.37s; --- Convergence: 5.62e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.289 1 + 0.596 value_reward_chosen[t] + 0.016 contr_diff + 0.078 reward + -0.333 value_reward_chosen^2 + -0.047 value_reward_chosen*contr_diff + -0.365 value_reward_chosen*reward + 0.186 contr_diff^2 + -0.016 contr_diff*reward + 0.078 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.03 1 + 0.61 value_reward_not_chosen[t] + 0.089 contr_diff + -0.086 value_reward_not_chosen^2 + -0.027 value_reward_not_chosen*contr_diff + -0.107 contr_diff^2 \n",
            "value_choice[t+1] = 0.054 1 + 0.98 value_choice[t] + 0.004 contr_diff + 0.054 choice + 0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.019 value_choice*choice + 0.014 contr_diff^2 + 0.004 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 44/1000 --- L(Train): 0.4333129 --- L(Val, RNN): 0.4330528 --- L(Val, SINDy): 0.4388298 --- Time: 0.33s; --- Convergence: 5.70e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.287 1 + 0.584 value_reward_chosen[t] + 0.019 contr_diff + 0.082 reward + -0.346 value_reward_chosen^2 + -0.039 value_reward_chosen*contr_diff + -0.373 value_reward_chosen*reward + 0.179 contr_diff^2 + -0.014 contr_diff*reward + 0.082 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.021 1 + 0.6 value_reward_not_chosen[t] + 0.095 contr_diff + -0.097 value_reward_not_chosen^2 + -0.034 value_reward_not_chosen*contr_diff + -0.1 contr_diff^2 \n",
            "value_choice[t+1] = 0.051 1 + 0.981 value_choice[t] + 0.004 contr_diff + 0.05 choice + 0.006 value_choice^2 + -0.004 value_choice*contr_diff + -0.018 value_choice*choice + 0.011 contr_diff^2 + 0.003 contr_diff*choice + 0.052 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 45/1000 --- L(Train): 0.4383850 --- L(Val, RNN): 0.4324060 --- L(Val, SINDy): 0.4348480 --- Time: 0.47s; --- Convergence: 6.08e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.281 1 + 0.572 value_reward_chosen[t] + 0.025 contr_diff + 0.085 reward + -0.359 value_reward_chosen^2 + -0.029 value_reward_chosen*contr_diff + -0.38 value_reward_chosen*reward + 0.17 contr_diff^2 + -0.009 contr_diff*reward + 0.086 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.016 1 + 0.59 value_reward_not_chosen[t] + 0.098 contr_diff + -0.109 value_reward_not_chosen^2 + -0.042 value_reward_not_chosen*contr_diff + -0.097 contr_diff^2 \n",
            "value_choice[t+1] = 0.047 1 + 0.983 value_choice[t] + 0.003 contr_diff + 0.047 choice + 0.008 value_choice^2 + -0.008 value_choice*contr_diff + -0.016 value_choice*choice + 0.009 contr_diff^2 + 0.002 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 46/1000 --- L(Train): 0.4327628 --- L(Val, RNN): 0.4317138 --- L(Val, SINDy): 0.4338791 --- Time: 0.32s; --- Convergence: 6.50e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.273 1 + 0.559 value_reward_chosen[t] + 0.029 contr_diff + 0.092 reward + -0.373 value_reward_chosen^2 + -0.021 value_reward_chosen*contr_diff + -0.385 value_reward_chosen*reward + 0.161 contr_diff^2 + -0.005 contr_diff*reward + 0.092 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.016 1 + 0.579 value_reward_not_chosen[t] + 0.1 contr_diff + -0.121 value_reward_not_chosen^2 + -0.051 value_reward_not_chosen*contr_diff + -0.095 contr_diff^2 \n",
            "value_choice[t+1] = 0.045 1 + 0.986 value_choice[t] + 0.002 contr_diff + 0.045 choice + 0.01 value_choice^2 + -0.011 value_choice*contr_diff + -0.013 value_choice*choice + 0.006 contr_diff^2 + 0.001 contr_diff*choice + 0.046 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 47/1000 --- L(Train): 0.4410988 --- L(Val, RNN): 0.4310609 --- L(Val, SINDy): 0.4328248 --- Time: 0.33s; --- Convergence: 6.52e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.264 1 + 0.546 value_reward_chosen[t] + 0.036 contr_diff + 0.101 reward + -0.387 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + -0.384 value_reward_chosen*reward + 0.15 contr_diff^2 + -0.003 contr_diff*reward + 0.101 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.019 1 + 0.569 value_reward_not_chosen[t] + 0.1 contr_diff + -0.134 value_reward_not_chosen^2 + -0.06 value_reward_not_chosen*contr_diff + -0.098 contr_diff^2 \n",
            "value_choice[t+1] = 0.042 1 + 0.987 value_choice[t] + 0.002 contr_diff + 0.042 choice + 0.01 value_choice^2 + -0.011 value_choice*contr_diff + -0.012 value_choice*choice + 0.005 contr_diff^2 + 0.002 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 48/1000 --- L(Train): 0.4363113 --- L(Val, RNN): 0.4304158 --- L(Val, SINDy): 0.4319159 --- Time: 0.32s; --- Convergence: 6.48e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.253 1 + 0.532 value_reward_chosen[t] + 0.035 contr_diff + 0.111 reward + -0.401 value_reward_chosen^2 + -0.008 value_reward_chosen*contr_diff + -0.382 value_reward_chosen*reward + 0.139 contr_diff^2 + -0.003 contr_diff*reward + 0.111 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.026 1 + 0.559 value_reward_not_chosen[t] + 0.099 contr_diff + -0.147 value_reward_not_chosen^2 + -0.068 value_reward_not_chosen*contr_diff + -0.102 contr_diff^2 \n",
            "value_choice[t+1] = 0.041 1 + 0.987 value_choice[t] + 0.004 contr_diff + 0.041 choice + 0.01 value_choice^2 + -0.01 value_choice*contr_diff + -0.011 value_choice*choice + 0.004 contr_diff^2 + 0.003 contr_diff*choice + 0.043 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 49/1000 --- L(Train): 0.4194239 --- L(Val, RNN): 0.4296897 --- L(Val, SINDy): 0.4313591 --- Time: 0.33s; --- Convergence: 6.87e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.241 1 + 0.517 value_reward_chosen[t] + 0.029 contr_diff + 0.122 reward + -0.416 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + -0.378 value_reward_chosen*reward + 0.126 contr_diff^2 + -0.007 contr_diff*reward + 0.123 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.034 1 + 0.551 value_reward_not_chosen[t] + 0.095 contr_diff + -0.16 value_reward_not_chosen^2 + -0.076 value_reward_not_chosen*contr_diff + -0.109 contr_diff^2 \n",
            "value_choice[t+1] = 0.042 1 + 0.988 value_choice[t] + 0.006 contr_diff + 0.042 choice + 0.009 value_choice^2 + -0.008 value_choice*contr_diff + -0.011 value_choice*choice + 0.006 contr_diff^2 + 0.006 contr_diff*choice + 0.043 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 50/1000 --- L(Train): 0.4265115 --- L(Val, RNN): 0.4289716 --- L(Val, SINDy): 0.4318672 --- Time: 0.45s; --- Convergence: 7.03e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.228 1 + 0.503 value_reward_chosen[t] + 0.024 contr_diff + 0.135 reward + -0.431 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + -0.372 value_reward_chosen*reward + 0.113 contr_diff^2 + -0.011 contr_diff*reward + 0.135 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.044 1 + 0.545 value_reward_not_chosen[t] + 0.09 contr_diff + -0.173 value_reward_not_chosen^2 + -0.083 value_reward_not_chosen*contr_diff + -0.118 contr_diff^2 \n",
            "value_choice[t+1] = 0.044 1 + 0.988 value_choice[t] + 0.007 contr_diff + 0.044 choice + 0.007 value_choice^2 + -0.005 value_choice*contr_diff + -0.011 value_choice*choice + 0.009 contr_diff^2 + 0.006 contr_diff*choice + 0.045 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 51/1000 --- L(Train): 0.4201156 --- L(Val, RNN): 0.4284204 --- L(Val, SINDy): 0.4314379 --- Time: 0.34s; --- Convergence: 6.27e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.214 1 + 0.488 value_reward_chosen[t] + 0.017 contr_diff + 0.148 reward + -0.446 value_reward_chosen^2 + -0.014 value_reward_chosen*contr_diff + -0.365 value_reward_chosen*reward + 0.1 contr_diff^2 + -0.016 contr_diff*reward + 0.148 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.056 1 + 0.542 value_reward_not_chosen[t] + 0.083 contr_diff + -0.187 value_reward_not_chosen^2 + -0.087 value_reward_not_chosen*contr_diff + -0.129 contr_diff^2 \n",
            "value_choice[t+1] = 0.046 1 + 0.987 value_choice[t] + 0.007 contr_diff + 0.045 choice + 0.004 value_choice^2 + -0.003 value_choice*contr_diff + -0.012 value_choice*choice + 0.013 contr_diff^2 + 0.006 contr_diff*choice + 0.047 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 52/1000 --- L(Train): 0.4209555 --- L(Val, RNN): 0.4278685 --- L(Val, SINDy): 0.4308125 --- Time: 0.37s; --- Convergence: 5.89e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.201 1 + 0.473 value_reward_chosen[t] + 0.01 contr_diff + 0.161 reward + -0.461 value_reward_chosen^2 + -0.018 value_reward_chosen*contr_diff + -0.356 value_reward_chosen*reward + 0.087 contr_diff^2 + -0.021 contr_diff*reward + 0.162 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.068 1 + 0.542 value_reward_not_chosen[t] + 0.075 contr_diff + -0.201 value_reward_not_chosen^2 + -0.09 value_reward_not_chosen*contr_diff + -0.139 contr_diff^2 \n",
            "value_choice[t+1] = 0.048 1 + 0.986 value_choice[t] + 0.006 contr_diff + 0.048 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + -0.013 value_choice*choice + 0.016 contr_diff^2 + 0.006 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 53/1000 --- L(Train): 0.4164794 --- L(Val, RNN): 0.4273245 --- L(Val, SINDy): 0.4301040 --- Time: 0.44s; --- Convergence: 5.67e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.187 1 + 0.458 value_reward_chosen[t] + 0.002 contr_diff + 0.176 reward + -0.476 value_reward_chosen^2 + -0.023 value_reward_chosen*contr_diff + -0.347 value_reward_chosen*reward + 0.073 contr_diff^2 + -0.025 contr_diff*reward + 0.176 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.081 1 + 0.544 value_reward_not_chosen[t] + 0.066 contr_diff + -0.216 value_reward_not_chosen^2 + -0.091 value_reward_not_chosen*contr_diff + -0.151 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.986 value_choice[t] + 0.004 contr_diff + 0.05 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + -0.013 value_choice*choice + 0.019 contr_diff^2 + 0.003 contr_diff*choice + 0.052 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 54/1000 --- L(Train): 0.4280657 --- L(Val, RNN): 0.4267840 --- L(Val, SINDy): 0.4286969 --- Time: 0.30s; --- Convergence: 5.54e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.174 1 + 0.443 value_reward_chosen[t] + -0.001 contr_diff + 0.19 reward + -0.491 value_reward_chosen^2 + -0.024 value_reward_chosen*contr_diff + -0.335 value_reward_chosen*reward + 0.059 contr_diff^2 + -0.025 contr_diff*reward + 0.191 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.095 1 + 0.549 value_reward_not_chosen[t] + 0.057 contr_diff + -0.231 value_reward_not_chosen^2 + -0.09 value_reward_not_chosen*contr_diff + -0.162 contr_diff^2 \n",
            "value_choice[t+1] = 0.053 1 + 0.987 value_choice[t] + 0.001 contr_diff + 0.053 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + -0.012 value_choice*choice + 0.021 contr_diff^2 + 0.0 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 55/1000 --- L(Train): 0.4321833 --- L(Val, RNN): 0.4262613 --- L(Val, SINDy): 0.4277136 --- Time: 0.40s; --- Convergence: 5.38e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.16 1 + 0.428 value_reward_chosen[t] + -0.006 contr_diff + 0.206 reward + -0.505 value_reward_chosen^2 + -0.028 value_reward_chosen*contr_diff + -0.323 value_reward_chosen*reward + 0.045 contr_diff^2 + -0.025 contr_diff*reward + 0.206 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.109 1 + 0.555 value_reward_not_chosen[t] + 0.046 contr_diff + -0.246 value_reward_not_chosen^2 + -0.085 value_reward_not_chosen*contr_diff + -0.174 contr_diff^2 \n",
            "value_choice[t+1] = 0.053 1 + 0.986 value_choice[t] + -0.002 contr_diff + 0.053 choice + -0.002 value_choice^2 + -0.002 value_choice*contr_diff + -0.012 value_choice*choice + 0.021 contr_diff^2 + -0.002 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 56/1000 --- L(Train): 0.4321109 --- L(Val, RNN): 0.4257664 --- L(Val, SINDy): 0.4277558 --- Time: 0.32s; --- Convergence: 5.17e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.146 1 + 0.413 value_reward_chosen[t] + -0.008 contr_diff + 0.221 reward + -0.521 value_reward_chosen^2 + -0.03 value_reward_chosen*contr_diff + -0.309 value_reward_chosen*reward + 0.031 contr_diff^2 + -0.024 contr_diff*reward + 0.222 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.123 1 + 0.564 value_reward_not_chosen[t] + 0.035 contr_diff + -0.261 value_reward_not_chosen^2 + -0.077 value_reward_not_chosen*contr_diff + -0.186 contr_diff^2 \n",
            "value_choice[t+1] = 0.051 1 + 0.986 value_choice[t] + -0.002 contr_diff + 0.051 choice + -0.003 value_choice^2 + -0.001 value_choice*contr_diff + -0.012 value_choice*choice + 0.018 contr_diff^2 + -0.003 contr_diff*choice + 0.053 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 57/1000 --- L(Train): 0.4229356 --- L(Val, RNN): 0.4253267 --- L(Val, SINDy): 0.4269052 --- Time: 0.31s; --- Convergence: 4.78e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.133 1 + 0.398 value_reward_chosen[t] + -0.011 contr_diff + 0.237 reward + -0.535 value_reward_chosen^2 + -0.032 value_reward_chosen*contr_diff + -0.295 value_reward_chosen*reward + 0.017 contr_diff^2 + -0.023 contr_diff*reward + 0.238 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.137 1 + 0.574 value_reward_not_chosen[t] + 0.025 contr_diff + -0.275 value_reward_not_chosen^2 + -0.069 value_reward_not_chosen*contr_diff + -0.197 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.988 value_choice[t] + -0.002 contr_diff + 0.05 choice + -0.002 value_choice^2 + -0.0 value_choice*contr_diff + -0.011 value_choice*choice + 0.016 contr_diff^2 + -0.003 contr_diff*choice + 0.052 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 58/1000 --- L(Train): 0.4122690 --- L(Val, RNN): 0.4248681 --- L(Val, SINDy): 0.4261307 --- Time: 0.33s; --- Convergence: 4.68e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.122 1 + 0.386 value_reward_chosen[t] + -0.012 contr_diff + 0.253 reward + -0.545 value_reward_chosen^2 + -0.034 value_reward_chosen*contr_diff + -0.28 value_reward_chosen*reward + 0.004 contr_diff^2 + -0.019 contr_diff*reward + 0.254 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.149 1 + 0.584 value_reward_not_chosen[t] + 0.016 contr_diff + -0.282 value_reward_not_chosen^2 + -0.059 value_reward_not_chosen*contr_diff + -0.204 contr_diff^2 \n",
            "value_choice[t+1] = 0.049 1 + 0.989 value_choice[t] + -0.001 contr_diff + 0.049 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.009 value_choice*choice + 0.013 contr_diff^2 + -0.002 contr_diff*choice + 0.051 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 59/1000 --- L(Train): 0.4312513 --- L(Val, RNN): 0.4243651 --- L(Val, SINDy): 0.4256451 --- Time: 0.33s; --- Convergence: 4.86e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.115 1 + 0.378 value_reward_chosen[t] + -0.012 contr_diff + 0.27 reward + -0.55 value_reward_chosen^2 + -0.035 value_reward_chosen*contr_diff + -0.264 value_reward_chosen*reward + -0.008 contr_diff^2 + -0.012 contr_diff*reward + 0.271 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.158 1 + 0.594 value_reward_not_chosen[t] + 0.012 contr_diff + -0.281 value_reward_not_chosen^2 + -0.048 value_reward_not_chosen*contr_diff + -0.205 contr_diff^2 \n",
            "value_choice[t+1] = 0.048 1 + 0.99 value_choice[t] + -0.0 contr_diff + 0.048 choice + -0.0 value_choice^2 + 0.003 value_choice*contr_diff + -0.009 value_choice*choice + 0.01 contr_diff^2 + -0.001 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 60/1000 --- L(Train): 0.4289392 --- L(Val, RNN): 0.4237531 --- L(Val, SINDy): 0.4251977 --- Time: 0.32s; --- Convergence: 5.49e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.109 1 + 0.372 value_reward_chosen[t] + -0.014 contr_diff + 0.286 reward + -0.552 value_reward_chosen^2 + -0.037 value_reward_chosen*contr_diff + -0.248 value_reward_chosen*reward + -0.018 contr_diff^2 + -0.006 contr_diff*reward + 0.287 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.164 1 + 0.602 value_reward_not_chosen[t] + 0.01 contr_diff + -0.276 value_reward_not_chosen^2 + -0.036 value_reward_not_chosen*contr_diff + -0.203 contr_diff^2 \n",
            "value_choice[t+1] = 0.046 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.046 choice + -0.001 value_choice^2 + 0.004 value_choice*contr_diff + -0.009 value_choice*choice + 0.007 contr_diff^2 + -0.0 contr_diff*choice + 0.048 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 61/1000 --- L(Train): 0.4252118 --- L(Val, RNN): 0.4231731 --- L(Val, SINDy): 0.4248483 --- Time: 0.31s; --- Convergence: 5.64e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.105 1 + 0.367 value_reward_chosen[t] + -0.016 contr_diff + 0.302 reward + -0.553 value_reward_chosen^2 + -0.039 value_reward_chosen*contr_diff + -0.232 value_reward_chosen*reward + -0.028 contr_diff^2 + 0.0 contr_diff*reward + 0.303 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.167 1 + 0.609 value_reward_not_chosen[t] + 0.01 contr_diff + -0.269 value_reward_not_chosen^2 + -0.025 value_reward_not_chosen*contr_diff + -0.2 contr_diff^2 \n",
            "value_choice[t+1] = 0.046 1 + 0.99 value_choice[t] + 0.0 contr_diff + 0.046 choice + -0.001 value_choice^2 + 0.005 value_choice*contr_diff + -0.009 value_choice*choice + 0.006 contr_diff^2 + -0.0 contr_diff*choice + 0.048 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 62/1000 --- L(Train): 0.4205473 --- L(Val, RNN): 0.4228171 --- L(Val, SINDy): 0.4242209 --- Time: 0.35s; --- Convergence: 4.60e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.099 1 + 0.365 value_reward_chosen[t] + -0.018 contr_diff + 0.317 reward + -0.552 value_reward_chosen^2 + -0.041 value_reward_chosen*contr_diff + -0.217 value_reward_chosen*reward + -0.038 contr_diff^2 + 0.004 contr_diff*reward + 0.318 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.167 1 + 0.613 value_reward_not_chosen[t] + 0.01 contr_diff + -0.259 value_reward_not_chosen^2 + -0.014 value_reward_not_chosen*contr_diff + -0.194 contr_diff^2 \n",
            "value_choice[t+1] = 0.048 1 + 0.991 value_choice[t] + -0.0 contr_diff + 0.048 choice + -0.001 value_choice^2 + 0.005 value_choice*contr_diff + -0.008 value_choice*choice + 0.005 contr_diff^2 + -0.001 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\n",
            "Training interrupted. Continuing with further operations...\n",
            "\n",
            "================================================================================\n",
            "Starting second stage SINDy fitting (threshold=0.05, single model)\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 1/1000 --- L(Train): 0.1353876 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.011 1 + 0.99 value_reward_chosen[t] + 0.011 contr_diff + 0.01 reward + -0.012 value_reward_chosen^2 + 0.009 value_reward_chosen*contr_diff + -0.01 value_reward_chosen*reward + -0.008 contr_diff^2 + -0.01 contr_diff*reward + 0.011 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.009 1 + 0.99 value_reward_not_chosen[t] + 0.009 contr_diff + -0.009 value_reward_not_chosen^2 + -0.009 value_reward_not_chosen*contr_diff + 0.009 contr_diff^2 \n",
            "value_choice[t+1] = 0.009 1 + 1.012 value_choice[t] + 0.01 contr_diff + 0.01 choice + 0.011 value_choice^2 + -0.009 value_choice*contr_diff + 0.01 value_choice*choice + 0.012 contr_diff^2 + 0.009 contr_diff*choice + 0.01 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 2/1000 --- L(Train): 0.1249563 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.021 1 + 0.98 value_reward_chosen[t] + 0.021 contr_diff + 0.02 reward + -0.022 value_reward_chosen^2 + 0.013 value_reward_chosen*contr_diff + -0.02 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.006 contr_diff*reward + 0.021 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.019 1 + 0.982 value_reward_not_chosen[t] + 0.011 contr_diff + -0.018 value_reward_not_chosen^2 + -0.012 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.018 1 + 1.019 value_choice[t] + 0.005 contr_diff + 0.019 choice + 0.017 value_choice^2 + -0.009 value_choice*contr_diff + 0.018 value_choice*choice + 0.018 contr_diff^2 + 0.006 contr_diff*choice + 0.019 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 3/1000 --- L(Train): 0.1189471 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.03 1 + 0.97 value_reward_chosen[t] + 0.031 contr_diff + 0.03 reward + -0.032 value_reward_chosen^2 + 0.014 value_reward_chosen*contr_diff + -0.03 value_reward_chosen*reward + 0.005 contr_diff^2 + -0.0 contr_diff*reward + 0.031 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.029 1 + 0.973 value_reward_not_chosen[t] + 0.009 contr_diff + -0.025 value_reward_not_chosen^2 + -0.012 value_reward_not_chosen*contr_diff + -0.007 contr_diff^2 \n",
            "value_choice[t+1] = 0.025 1 + 1.022 value_choice[t] + -0.002 contr_diff + 0.026 choice + 0.017 value_choice^2 + -0.005 value_choice*contr_diff + 0.02 value_choice*choice + 0.016 contr_diff^2 + 0.0 contr_diff*choice + 0.025 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 4/1000 --- L(Train): 0.1132147 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.039 1 + 0.96 value_reward_chosen[t] + 0.04 contr_diff + 0.04 reward + -0.042 value_reward_chosen^2 + 0.012 value_reward_chosen*contr_diff + -0.04 value_reward_chosen*reward + 0.008 contr_diff^2 + 0.007 contr_diff*reward + 0.041 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.038 1 + 0.964 value_reward_not_chosen[t] + 0.006 contr_diff + -0.032 value_reward_not_chosen^2 + -0.01 value_reward_not_chosen*contr_diff + -0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.029 1 + 1.02 value_choice[t] + -0.004 contr_diff + 0.031 choice + 0.012 value_choice^2 + 0.002 value_choice*contr_diff + 0.018 value_choice*choice + 0.011 contr_diff^2 + -0.006 contr_diff*choice + 0.029 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 5/1000 --- L(Train): 0.1070987 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.045 1 + 0.95 value_reward_chosen[t] + 0.049 contr_diff + 0.05 reward + -0.052 value_reward_chosen^2 + 0.009 value_reward_chosen*contr_diff + -0.05 value_reward_chosen*reward + 0.008 contr_diff^2 + 0.01 contr_diff*reward + 0.051 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.048 1 + 0.954 value_reward_not_chosen[t] + 0.001 contr_diff + -0.038 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + -0.017 contr_diff^2 \n",
            "value_choice[t+1] = 0.032 1 + 1.016 value_choice[t] + -0.003 contr_diff + 0.033 choice + 0.007 value_choice^2 + 0.005 value_choice*contr_diff + 0.014 value_choice*choice + 0.004 contr_diff^2 + -0.009 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 6/1000 --- L(Train): 0.1012283 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.049 1 + 0.94 value_reward_chosen[t] + 0.057 contr_diff + 0.059 reward + -0.062 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + -0.061 value_reward_chosen*reward + 0.006 contr_diff^2 + 0.011 contr_diff*reward + 0.061 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.056 1 + 0.945 value_reward_not_chosen[t] + -0.004 contr_diff + -0.042 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.019 contr_diff^2 \n",
            "value_choice[t+1] = 0.033 1 + 1.011 value_choice[t] + 0.0 contr_diff + 0.034 choice + 0.0 value_choice^2 + 0.007 value_choice*contr_diff + 0.009 value_choice*choice + -0.003 contr_diff^2 + -0.008 contr_diff*choice + 0.033 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 7/1000 --- L(Train): 0.0961295 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.051 1 + 0.93 value_reward_chosen[t] + 0.065 contr_diff + 0.069 reward + -0.072 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.009 contr_diff*reward + 0.07 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.065 1 + 0.935 value_reward_not_chosen[t] + -0.006 contr_diff + -0.045 value_reward_not_chosen^2 + 0.004 value_reward_not_chosen*contr_diff + -0.02 contr_diff^2 \n",
            "value_choice[t+1] = 0.035 1 + 1.006 value_choice[t] + 0.001 contr_diff + 0.036 choice + -0.006 value_choice^2 + 0.006 value_choice*contr_diff + 0.003 value_choice*choice + -0.006 contr_diff^2 + -0.006 contr_diff*choice + 0.034 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 8/1000 --- L(Train): 0.0918714 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.05 1 + 0.92 value_reward_chosen[t] + 0.071 contr_diff + 0.078 reward + -0.082 value_reward_chosen^2 + -0.005 value_reward_chosen*contr_diff + -0.081 value_reward_chosen*reward + -0.004 contr_diff^2 + 0.006 contr_diff*reward + 0.08 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.072 1 + 0.925 value_reward_not_chosen[t] + -0.004 contr_diff + -0.047 value_reward_not_chosen^2 + 0.006 value_reward_not_chosen*contr_diff + -0.019 contr_diff^2 \n",
            "value_choice[t+1] = 0.037 1 + 1.002 value_choice[t] + 0.0 contr_diff + 0.037 choice + -0.012 value_choice^2 + 0.004 value_choice*contr_diff + -0.001 value_choice*choice + -0.007 contr_diff^2 + -0.003 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 9/1000 --- L(Train): 0.0881347 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.048 1 + 0.91 value_reward_chosen[t] + 0.077 contr_diff + 0.088 reward + -0.093 value_reward_chosen^2 + -0.006 value_reward_chosen*contr_diff + -0.091 value_reward_chosen*reward + -0.008 contr_diff^2 + 0.002 contr_diff*reward + 0.089 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.079 1 + 0.915 value_reward_not_chosen[t] + -0.001 contr_diff + -0.047 value_reward_not_chosen^2 + 0.005 value_reward_not_chosen*contr_diff + -0.017 contr_diff^2 \n",
            "value_choice[t+1] = 0.039 1 + 1.0 value_choice[t] + -0.003 contr_diff + 0.04 choice + -0.016 value_choice^2 + 0.0 value_choice*contr_diff + -0.004 value_choice*choice + -0.004 contr_diff^2 + 0.001 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 10/1000 --- L(Train): 0.0845599 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.044 1 + 0.9 value_reward_chosen[t] + 0.081 contr_diff + 0.097 reward + -0.103 value_reward_chosen^2 + -0.005 value_reward_chosen*contr_diff + -0.101 value_reward_chosen*reward + -0.011 contr_diff^2 + -0.003 contr_diff*reward + 0.098 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.085 1 + 0.905 value_reward_not_chosen[t] + 0.004 contr_diff + -0.046 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.043 1 + 0.998 value_choice[t] + -0.004 contr_diff + 0.043 choice + -0.018 value_choice^2 + -0.004 value_choice*contr_diff + -0.005 value_choice*choice + -0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.04 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 11/1000 --- L(Train): 0.0810317 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.039 1 + 0.89 value_reward_chosen[t] + 0.083 contr_diff + 0.106 reward + -0.113 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.111 value_reward_chosen*reward + -0.013 contr_diff^2 + -0.007 contr_diff*reward + 0.107 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.09 1 + 0.895 value_reward_not_chosen[t] + 0.007 contr_diff + -0.043 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.01 contr_diff^2 \n",
            "value_choice[t+1] = 0.047 1 + 0.999 value_choice[t] + -0.003 contr_diff + 0.047 choice + -0.019 value_choice^2 + -0.007 value_choice*contr_diff + -0.005 value_choice*choice + 0.004 contr_diff^2 + 0.004 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 12/1000 --- L(Train): 0.0776312 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.034 1 + 0.88 value_reward_chosen[t] + 0.085 contr_diff + 0.115 reward + -0.123 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.121 value_reward_chosen*reward + -0.015 contr_diff^2 + -0.009 contr_diff*reward + 0.116 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.095 1 + 0.885 value_reward_not_chosen[t] + 0.009 contr_diff + -0.039 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + -0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.052 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.052 choice + -0.019 value_choice^2 + -0.007 value_choice*contr_diff + -0.004 value_choice*choice + 0.008 contr_diff^2 + 0.002 contr_diff*choice + 0.048 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 13/1000 --- L(Train): 0.0744846 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.027 1 + 0.87 value_reward_chosen[t] + 0.085 contr_diff + 0.124 reward + -0.133 value_reward_chosen^2 + 0.006 value_reward_chosen*contr_diff + -0.131 value_reward_chosen*reward + -0.015 contr_diff^2 + -0.009 contr_diff*reward + 0.125 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.098 1 + 0.874 value_reward_not_chosen[t] + 0.01 contr_diff + -0.035 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.056 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.056 choice + -0.018 value_choice^2 + -0.007 value_choice*contr_diff + -0.002 value_choice*choice + 0.009 contr_diff^2 + 0.0 contr_diff*choice + 0.051 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 14/1000 --- L(Train): 0.0716189 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.02 1 + 0.86 value_reward_chosen[t] + 0.084 contr_diff + 0.133 reward + -0.143 value_reward_chosen^2 + 0.008 value_reward_chosen*contr_diff + -0.141 value_reward_chosen*reward + -0.015 contr_diff^2 + -0.009 contr_diff*reward + 0.134 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.101 1 + 0.864 value_reward_not_chosen[t] + 0.01 contr_diff + -0.03 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.059 1 + 1.003 value_choice[t] + 0.002 contr_diff + 0.059 choice + -0.018 value_choice^2 + -0.005 value_choice*contr_diff + -0.001 value_choice*choice + 0.01 contr_diff^2 + -0.003 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 15/1000 --- L(Train): 0.0689389 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.013 1 + 0.85 value_reward_chosen[t] + 0.082 contr_diff + 0.142 reward + -0.153 value_reward_chosen^2 + 0.009 value_reward_chosen*contr_diff + -0.151 value_reward_chosen*reward + -0.014 contr_diff^2 + -0.008 contr_diff*reward + 0.143 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.104 1 + 0.853 value_reward_not_chosen[t] + 0.009 contr_diff + -0.024 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + 0.006 contr_diff^2 \n",
            "value_choice[t+1] = 0.062 1 + 1.003 value_choice[t] + 0.002 contr_diff + 0.061 choice + -0.017 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.008 contr_diff^2 + -0.005 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 16/1000 --- L(Train): 0.0663288 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.006 1 + 0.841 value_reward_chosen[t] + 0.08 contr_diff + 0.15 reward + -0.163 value_reward_chosen^2 + 0.008 value_reward_chosen*contr_diff + -0.161 value_reward_chosen*reward + -0.012 contr_diff^2 + -0.006 contr_diff*reward + 0.151 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.106 1 + 0.843 value_reward_not_chosen[t] + 0.007 contr_diff + -0.017 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.006 contr_diff^2 \n",
            "value_choice[t+1] = 0.063 1 + 1.003 value_choice[t] + 0.001 contr_diff + 0.062 choice + -0.017 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.005 contr_diff^2 + -0.005 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 17/1000 --- L(Train): 0.0637422 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.001 1 + 0.831 value_reward_chosen[t] + 0.076 contr_diff + 0.159 reward + -0.173 value_reward_chosen^2 + 0.007 value_reward_chosen*contr_diff + -0.17 value_reward_chosen*reward + -0.01 contr_diff^2 + -0.003 contr_diff*reward + 0.16 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.108 1 + 0.833 value_reward_not_chosen[t] + 0.005 contr_diff + -0.011 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.063 1 + 1.002 value_choice[t] + -0.001 contr_diff + 0.062 choice + -0.018 value_choice^2 + 0.004 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 18/1000 --- L(Train): 0.0612113 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.008 1 + 0.822 value_reward_chosen[t] + 0.071 contr_diff + 0.168 reward + -0.182 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + -0.179 value_reward_chosen*reward + -0.007 contr_diff^2 + 0.0 contr_diff*reward + 0.169 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.11 1 + 0.823 value_reward_not_chosen[t] + 0.002 contr_diff + -0.004 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.062 1 + 1.001 value_choice[t] + -0.001 contr_diff + 0.061 choice + -0.018 value_choice^2 + 0.005 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + -0.001 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 19/1000 --- L(Train): 0.0587956 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.015 1 + 0.813 value_reward_chosen[t] + 0.066 contr_diff + 0.176 reward + -0.192 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.188 value_reward_chosen*reward + -0.003 contr_diff^2 + 0.002 contr_diff*reward + 0.177 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.112 1 + 0.813 value_reward_not_chosen[t] + -0.001 contr_diff + 0.003 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.061 1 + 0.999 value_choice[t] + -0.0 contr_diff + 0.059 choice + -0.019 value_choice^2 + 0.005 value_choice*contr_diff + -0.003 value_choice*choice + -0.007 contr_diff^2 + 0.002 contr_diff*choice + 0.052 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 20/1000 --- L(Train): 0.0565193 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.022 1 + 0.803 value_reward_chosen[t] + 0.061 contr_diff + 0.185 reward + -0.201 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.197 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.186 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.113 1 + 0.804 value_reward_not_chosen[t] + -0.002 contr_diff + 0.008 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.059 1 + 0.997 value_choice[t] + 0.001 contr_diff + 0.057 choice + -0.02 value_choice^2 + 0.003 value_choice*contr_diff + -0.004 value_choice*choice + -0.01 contr_diff^2 + 0.004 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 21/1000 --- L(Train): 0.0543565 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.028 1 + 0.794 value_reward_chosen[t] + 0.055 contr_diff + 0.193 reward + -0.21 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.206 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.0 contr_diff*reward + 0.194 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.116 1 + 0.795 value_reward_not_chosen[t] + -0.001 contr_diff + 0.011 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.057 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.055 choice + -0.02 value_choice^2 + 0.0 value_choice*contr_diff + -0.005 value_choice*choice + -0.011 contr_diff^2 + 0.004 contr_diff*choice + 0.047 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 22/1000 --- L(Train): 0.0522700 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.034 1 + 0.785 value_reward_chosen[t] + 0.049 contr_diff + 0.202 reward + -0.219 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.214 value_reward_chosen*reward + 0.003 contr_diff^2 + -0.002 contr_diff*reward + 0.203 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.118 1 + 0.787 value_reward_not_chosen[t] + 0.002 contr_diff + 0.013 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.056 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.054 choice + -0.019 value_choice^2 + -0.003 value_choice*contr_diff + -0.005 value_choice*choice + -0.011 contr_diff^2 + 0.003 contr_diff*choice + 0.045 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 23/1000 --- L(Train): 0.0502443 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.04 1 + 0.776 value_reward_chosen[t] + 0.044 contr_diff + 0.21 reward + -0.228 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.222 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.003 contr_diff*reward + 0.211 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.12 1 + 0.779 value_reward_not_chosen[t] + 0.003 contr_diff + 0.013 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.052 choice + -0.018 value_choice^2 + -0.005 value_choice*contr_diff + -0.004 value_choice*choice + -0.01 contr_diff^2 + 0.001 contr_diff*choice + 0.043 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 24/1000 --- L(Train): 0.0482880 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.044 1 + 0.768 value_reward_chosen[t] + 0.038 contr_diff + 0.218 reward + -0.236 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.23 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.219 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.123 1 + 0.772 value_reward_not_chosen[t] + 0.005 contr_diff + 0.012 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.997 value_choice[t] + 0.0 contr_diff + 0.052 choice + -0.016 value_choice^2 + -0.006 value_choice*contr_diff + -0.003 value_choice*choice + -0.009 contr_diff^2 + -0.002 contr_diff*choice + 0.042 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 25/1000 --- L(Train): 0.0464187 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.049 1 + 0.759 value_reward_chosen[t] + 0.032 contr_diff + 0.227 reward + -0.245 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.237 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.003 contr_diff*reward + 0.228 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.125 1 + 0.765 value_reward_not_chosen[t] + 0.005 contr_diff + 0.01 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.998 value_choice[t] + 0.0 contr_diff + 0.051 choice + -0.014 value_choice^2 + -0.006 value_choice*contr_diff + -0.001 value_choice*choice + -0.006 contr_diff^2 + -0.004 contr_diff*choice + 0.041 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 26/1000 --- L(Train): 0.0446533 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.053 1 + 0.751 value_reward_chosen[t] + 0.027 contr_diff + 0.235 reward + -0.253 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + -0.245 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.236 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.128 1 + 0.759 value_reward_not_chosen[t] + 0.006 contr_diff + 0.008 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.999 value_choice[t] + -0.001 contr_diff + 0.051 choice + -0.012 value_choice^2 + -0.005 value_choice*contr_diff + 0.001 value_choice*choice + -0.003 contr_diff^2 + -0.004 contr_diff*choice + 0.04 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 27/1000 --- L(Train): 0.0429834 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.057 1 + 0.742 value_reward_chosen[t] + 0.022 contr_diff + 0.243 reward + -0.261 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.251 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.244 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.131 1 + 0.753 value_reward_not_chosen[t] + 0.005 contr_diff + 0.004 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 1.0 value_choice[t] + -0.0 contr_diff + 0.051 choice + -0.01 value_choice^2 + -0.002 value_choice*contr_diff + 0.002 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.039 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 28/1000 --- L(Train): 0.0413956 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.06 1 + 0.734 value_reward_chosen[t] + 0.018 contr_diff + 0.251 reward + -0.269 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.258 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.003 contr_diff*reward + 0.252 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.134 1 + 0.748 value_reward_not_chosen[t] + 0.005 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.054 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.05 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + 0.003 value_choice*choice + 0.004 contr_diff^2 + -0.001 contr_diff*choice + 0.038 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 29/1000 --- L(Train): 0.0398865 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.063 1 + 0.726 value_reward_chosen[t] + 0.014 contr_diff + 0.259 reward + -0.276 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.264 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.003 contr_diff*reward + 0.26 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.137 1 + 0.743 value_reward_not_chosen[t] + 0.004 contr_diff + -0.003 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.053 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.049 choice + -0.007 value_choice^2 + 0.003 value_choice*contr_diff + 0.003 value_choice*choice + 0.005 contr_diff^2 + 0.002 contr_diff*choice + 0.036 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 30/1000 --- L(Train): 0.0384513 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.066 1 + 0.718 value_reward_chosen[t] + 0.011 contr_diff + 0.267 reward + -0.283 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.27 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.002 contr_diff*reward + 0.268 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.139 1 + 0.738 value_reward_not_chosen[t] + 0.002 contr_diff + -0.005 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.052 1 + 1.001 value_choice[t] + 0.0 contr_diff + 0.047 choice + -0.006 value_choice^2 + 0.003 value_choice*contr_diff + 0.003 value_choice*choice + 0.005 contr_diff^2 + 0.003 contr_diff*choice + 0.034 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 31/1000 --- L(Train): 0.0370968 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.069 1 + 0.71 value_reward_chosen[t] + 0.008 contr_diff + 0.275 reward + -0.291 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.276 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.275 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.141 1 + 0.734 value_reward_not_chosen[t] + 0.001 contr_diff + -0.006 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.051 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.046 choice + -0.006 value_choice^2 + 0.003 value_choice*contr_diff + 0.002 value_choice*choice + 0.004 contr_diff^2 + 0.003 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 32/1000 --- L(Train): 0.0358177 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.072 1 + 0.703 value_reward_chosen[t] + 0.006 contr_diff + 0.282 reward + -0.298 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.282 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.283 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.73 value_reward_not_chosen[t] + -0.001 contr_diff + -0.005 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.999 value_choice[t] + -0.003 contr_diff + 0.045 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.003 contr_diff^2 + 0.002 contr_diff*choice + 0.03 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 33/1000 --- L(Train): 0.0346115 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.075 1 + 0.695 value_reward_chosen[t] + 0.005 contr_diff + 0.289 reward + -0.305 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.287 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.29 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.145 1 + 0.726 value_reward_not_chosen[t] + -0.001 contr_diff + -0.004 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.998 value_choice[t] + -0.003 contr_diff + 0.044 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.029 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 34/1000 --- L(Train): 0.0334692 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.078 1 + 0.688 value_reward_chosen[t] + 0.004 contr_diff + 0.297 reward + -0.311 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.292 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.297 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.146 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.998 value_choice[t] + -0.002 contr_diff + 0.043 choice + -0.006 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + -0.002 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 35/1000 --- L(Train): 0.0323954 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.081 1 + 0.681 value_reward_chosen[t] + 0.003 contr_diff + 0.304 reward + -0.318 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.297 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.304 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.147 1 + 0.718 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.998 value_choice[t] + -0.0 contr_diff + 0.043 choice + -0.005 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.004 contr_diff^2 + -0.004 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 36/1000 --- L(Train): 0.0313797 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.085 1 + 0.673 value_reward_chosen[t] + 0.003 contr_diff + 0.31 reward + -0.324 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.302 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.311 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.148 1 + 0.715 value_reward_not_chosen[t] + 0.004 contr_diff + 0.003 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.051 1 + 0.998 value_choice[t] + 0.003 contr_diff + 0.044 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.004 contr_diff^2 + -0.004 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 37/1000 --- L(Train): 0.0304146 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.089 1 + 0.666 value_reward_chosen[t] + 0.004 contr_diff + 0.317 reward + -0.331 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.306 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.317 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.148 1 + 0.712 value_reward_not_chosen[t] + 0.005 contr_diff + 0.003 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.052 1 + 0.999 value_choice[t] + 0.004 contr_diff + 0.045 choice + -0.004 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + -0.003 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 38/1000 --- L(Train): 0.0295013 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.092 1 + 0.659 value_reward_chosen[t] + 0.004 contr_diff + 0.323 reward + -0.337 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.311 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.323 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.149 1 + 0.709 value_reward_not_chosen[t] + 0.005 contr_diff + 0.003 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.054 1 + 0.999 value_choice[t] + 0.004 contr_diff + 0.046 choice + -0.004 value_choice^2 + 0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 39/1000 --- L(Train): 0.0286399 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.096 1 + 0.653 value_reward_chosen[t] + 0.005 contr_diff + 0.329 reward + -0.343 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.315 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.329 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.149 1 + 0.707 value_reward_not_chosen[t] + 0.005 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.999 value_choice[t] + 0.003 contr_diff + 0.047 choice + -0.003 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.002 contr_diff^2 + 0.0 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 40/1000 --- L(Train): 0.0278237 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.1 1 + 0.646 value_reward_chosen[t] + 0.005 contr_diff + 0.335 reward + -0.349 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.318 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.335 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.705 value_reward_not_chosen[t] + 0.004 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.056 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.048 choice + -0.003 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.003 contr_diff^2 + 0.001 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 41/1000 --- L(Train): 0.0270533 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.104 1 + 0.64 value_reward_chosen[t] + 0.006 contr_diff + 0.341 reward + -0.354 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.322 value_reward_chosen*reward + -0.004 contr_diff^2 + 0.002 contr_diff*reward + 0.341 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.703 value_reward_not_chosen[t] + 0.004 contr_diff + -0.002 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.057 1 + 0.999 value_choice[t] + -0.002 contr_diff + 0.048 choice + -0.004 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.004 contr_diff^2 + 0.001 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 42/1000 --- L(Train): 0.0263197 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.109 1 + 0.633 value_reward_chosen[t] + 0.007 contr_diff + 0.347 reward + -0.36 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.325 value_reward_chosen*reward + -0.005 contr_diff^2 + 0.001 contr_diff*reward + 0.346 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.702 value_reward_not_chosen[t] + 0.003 contr_diff + -0.003 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.058 1 + 0.999 value_choice[t] + -0.003 contr_diff + 0.049 choice + -0.004 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.004 contr_diff^2 + 0.0 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 43/1000 --- L(Train): 0.0256242 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.113 1 + 0.627 value_reward_chosen[t] + 0.007 contr_diff + 0.352 reward + -0.365 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.328 value_reward_chosen*reward + -0.004 contr_diff^2 + -0.001 contr_diff*reward + 0.352 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.7 value_reward_not_chosen[t] + 0.001 contr_diff + -0.003 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.059 1 + 0.998 value_choice[t] + -0.004 contr_diff + 0.049 choice + -0.005 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 44/1000 --- L(Train): 0.0249687 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.116 1 + 0.621 value_reward_chosen[t] + 0.007 contr_diff + 0.357 reward + -0.37 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.331 value_reward_chosen*reward + -0.003 contr_diff^2 + -0.001 contr_diff*reward + 0.357 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.7 value_reward_not_chosen[t] + -0.0 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.059 1 + 0.997 value_choice[t] + -0.003 contr_diff + 0.049 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + -0.003 value_choice*choice + 0.0 contr_diff^2 + -0.003 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 45/1000 --- L(Train): 0.0243485 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.12 1 + 0.615 value_reward_chosen[t] + 0.007 contr_diff + 0.362 reward + -0.375 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.334 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.362 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.699 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.06 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.05 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.003 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 46/1000 --- L(Train): 0.0237631 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.124 1 + 0.609 value_reward_chosen[t] + 0.006 contr_diff + 0.367 reward + -0.38 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.336 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.366 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.699 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.061 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.05 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.003 value_choice*choice + -0.004 contr_diff^2 + -0.001 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 47/1000 --- L(Train): 0.0232105 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.127 1 + 0.603 value_reward_chosen[t] + 0.005 contr_diff + 0.372 reward + -0.384 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.338 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.371 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.699 value_reward_not_chosen[t] + 0.001 contr_diff + 0.003 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.062 1 + 0.996 value_choice[t] + 0.002 contr_diff + 0.051 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + -0.003 value_choice*choice + -0.004 contr_diff^2 + 0.001 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 48/1000 --- L(Train): 0.0226844 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.131 1 + 0.598 value_reward_chosen[t] + 0.004 contr_diff + 0.377 reward + -0.388 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.34 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.375 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.699 value_reward_not_chosen[t] + 0.001 contr_diff + 0.003 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.063 1 + 0.996 value_choice[t] + 0.002 contr_diff + 0.052 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.003 value_choice*choice + -0.003 contr_diff^2 + 0.002 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 49/1000 --- L(Train): 0.0221864 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.133 1 + 0.592 value_reward_chosen[t] + 0.003 contr_diff + 0.381 reward + -0.392 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.341 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.38 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.7 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.064 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.052 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 50/1000 --- L(Train): 0.0217170 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.136 1 + 0.587 value_reward_chosen[t] + 0.001 contr_diff + 0.385 reward + -0.396 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.342 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.384 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.701 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.066 1 + 0.997 value_choice[t] + -0.001 contr_diff + 0.053 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.025 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 51/1000 --- L(Train): 0.0212722 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.139 1 + 0.582 value_reward_chosen[t] + -0.001 contr_diff + 0.389 reward + -0.4 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.343 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.388 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.702 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.066 1 + 0.997 value_choice[t] + -0.002 contr_diff + 0.054 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.025 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 52/1000 --- L(Train): 0.0208500 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.141 1 + 0.577 value_reward_chosen[t] + 0.001 contr_diff + 0.393 reward + -0.403 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.344 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.391 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.703 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.067 1 + 0.997 value_choice[t] + -0.002 contr_diff + 0.054 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.003 contr_diff^2 + -0.003 contr_diff*choice + 0.024 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 53/1000 --- L(Train): 0.0204512 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.143 1 + 0.572 value_reward_chosen[t] + 0.001 contr_diff + 0.397 reward + -0.407 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.345 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.395 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.704 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.067 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.054 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.024 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 54/1000 --- L(Train): 0.0200735 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.145 1 + 0.567 value_reward_chosen[t] + 0.002 contr_diff + 0.4 reward + -0.41 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.345 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.398 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.705 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.067 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.053 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.022 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 55/1000 --- L(Train): 0.0197156 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.147 1 + 0.563 value_reward_chosen[t] + 0.002 contr_diff + 0.404 reward + -0.413 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.345 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.401 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.706 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.068 1 + 0.996 value_choice[t] + 0.002 contr_diff + 0.053 choice + -0.009 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.021 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 56/1000 --- L(Train): 0.0193737 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.148 1 + 0.558 value_reward_chosen[t] + 0.001 contr_diff + 0.407 reward + -0.415 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.345 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.404 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.707 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.068 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.053 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.003 contr_diff^2 + 0.002 contr_diff*choice + 0.02 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 57/1000 --- L(Train): 0.0190477 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.15 1 + 0.554 value_reward_chosen[t] + 0.0 contr_diff + 0.41 reward + -0.418 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.344 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.407 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.149 1 + 0.708 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.068 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.053 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.004 contr_diff^2 + 0.001 contr_diff*choice + 0.019 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 58/1000 --- L(Train): 0.0187381 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.152 1 + 0.55 value_reward_chosen[t] + -0.001 contr_diff + 0.413 reward + -0.42 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.344 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.41 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.149 1 + 0.709 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.068 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.052 choice + -0.009 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.003 contr_diff^2 + -0.0 contr_diff*choice + 0.018 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 59/1000 --- L(Train): 0.0184415 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.153 1 + 0.545 value_reward_chosen[t] + 0.0 contr_diff + 0.416 reward + -0.422 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.343 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.412 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.148 1 + 0.71 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.069 1 + 0.996 value_choice[t] + -0.003 contr_diff + 0.052 choice + -0.009 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.017 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 60/1000 --- L(Train): 0.0181580 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.541 value_reward_chosen[t] + 0.001 contr_diff + 0.418 reward + -0.424 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.343 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.415 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.148 1 + 0.711 value_reward_not_chosen[t] + -0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.069 1 + 0.996 value_choice[t] + -0.003 contr_diff + 0.053 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.002 contr_diff^2 + 0.0 contr_diff*choice + 0.016 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 61/1000 --- L(Train): 0.0178854 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.537 value_reward_chosen[t] + 0.001 contr_diff + 0.42 reward + -0.426 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.342 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.417 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.147 1 + 0.712 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.07 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.053 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.003 contr_diff^2 + 0.0 contr_diff*choice + 0.016 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 62/1000 --- L(Train): 0.0176252 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.533 value_reward_chosen[t] + 0.001 contr_diff + 0.423 reward + -0.428 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.341 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.419 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.146 1 + 0.713 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.07 1 + 0.997 value_choice[t] + -0.0 contr_diff + 0.053 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.003 contr_diff^2 + -0.001 contr_diff*choice + 0.015 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 63/1000 --- L(Train): 0.0173770 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.53 value_reward_chosen[t] + -0.0 contr_diff + 0.425 reward + -0.43 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.339 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.421 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.146 1 + 0.714 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.071 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.053 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.003 contr_diff^2 + -0.001 contr_diff*choice + 0.014 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 64/1000 --- L(Train): 0.0171362 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.526 value_reward_chosen[t] + 0.001 contr_diff + 0.427 reward + -0.431 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.338 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.422 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.146 1 + 0.715 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.071 1 + 0.997 value_choice[t] + 0.004 contr_diff + 0.053 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.013 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 65/1000 --- L(Train): 0.0169067 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.522 value_reward_chosen[t] + 0.002 contr_diff + 0.428 reward + -0.433 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.337 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.424 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.145 1 + 0.716 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.072 1 + 0.997 value_choice[t] + 0.004 contr_diff + 0.053 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.012 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 66/1000 --- L(Train): 0.0166839 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.519 value_reward_chosen[t] + 0.002 contr_diff + 0.43 reward + -0.434 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.335 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.425 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.145 1 + 0.717 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.072 1 + 0.997 value_choice[t] + 0.003 contr_diff + 0.053 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.002 contr_diff^2 + 0.002 contr_diff*choice + 0.011 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 67/1000 --- L(Train): 0.0164695 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.515 value_reward_chosen[t] + 0.001 contr_diff + 0.431 reward + -0.435 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.334 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.426 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.718 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.073 1 + 0.997 value_choice[t] + 0.001 contr_diff + 0.053 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.011 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 68/1000 --- L(Train): 0.0162634 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.512 value_reward_chosen[t] + -0.0 contr_diff + 0.433 reward + -0.436 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.332 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.427 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.719 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.074 1 + 0.998 value_choice[t] + -0.002 contr_diff + 0.053 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.01 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 69/1000 --- L(Train): 0.0160637 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.166 1 + 0.509 value_reward_chosen[t] + 0.001 contr_diff + 0.434 reward + -0.437 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.33 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.428 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.72 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.074 1 + 0.998 value_choice[t] + -0.004 contr_diff + 0.054 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.009 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 70/1000 --- L(Train): 0.0158700 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.505 value_reward_chosen[t] + 0.001 contr_diff + 0.435 reward + -0.438 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.328 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.429 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.075 1 + 0.998 value_choice[t] + -0.004 contr_diff + 0.054 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.003 contr_diff^2 + -0.0 contr_diff*choice + 0.008 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 71/1000 --- L(Train): 0.0156852 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.502 value_reward_chosen[t] + 0.001 contr_diff + 0.436 reward + -0.439 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.326 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.43 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.076 1 + 0.998 value_choice[t] + -0.004 contr_diff + 0.054 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.007 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 72/1000 --- L(Train): 0.0155035 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.499 value_reward_chosen[t] + -0.001 contr_diff + 0.437 reward + -0.439 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.324 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.431 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.076 1 + 0.998 value_choice[t] + -0.002 contr_diff + 0.054 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.007 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 73/1000 --- L(Train): 0.0153252 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.496 value_reward_chosen[t] + 0.0 contr_diff + 0.438 reward + -0.44 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.322 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.431 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.724 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.077 1 + 0.998 value_choice[t] + 0.0 contr_diff + 0.054 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.006 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 74/1000 --- L(Train): 0.0151530 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.493 value_reward_chosen[t] + 0.001 contr_diff + 0.438 reward + -0.44 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.32 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.724 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.078 1 + 0.998 value_choice[t] + 0.001 contr_diff + 0.054 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.005 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 75/1000 --- L(Train): 0.0149865 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.49 value_reward_chosen[t] + 0.0 contr_diff + 0.439 reward + -0.44 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.317 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + -0.0 contr_diff + -0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.078 1 + 0.998 value_choice[t] + 0.001 contr_diff + 0.054 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.004 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 76/1000 --- L(Train): 0.0148249 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.487 value_reward_chosen[t] + -0.001 contr_diff + 0.44 reward + -0.44 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.315 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.079 1 + 0.998 value_choice[t] + -0.0 contr_diff + 0.055 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 77/1000 --- L(Train): 0.0146669 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.485 value_reward_chosen[t] + 0.0 contr_diff + 0.44 reward + -0.44 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.312 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.08 1 + 0.998 value_choice[t] + -0.0 contr_diff + 0.055 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 78/1000 --- L(Train): 0.0145113 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.482 value_reward_chosen[t] + 0.001 contr_diff + 0.44 reward + -0.44 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.31 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.433 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.726 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.08 1 + 0.998 value_choice[t] + 0.001 contr_diff + 0.055 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 79/1000 --- L(Train): 0.0143580 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.479 value_reward_chosen[t] + 0.0 contr_diff + 0.441 reward + -0.44 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.307 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.433 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.726 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.081 1 + 0.998 value_choice[t] + 0.0 contr_diff + 0.055 choice + -0.008 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 80/1000 --- L(Train): 0.0142110 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.477 value_reward_chosen[t] + -0.001 contr_diff + 0.441 reward + -0.44 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.305 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.082 1 + 0.998 value_choice[t] + -0.001 contr_diff + 0.055 choice + -0.008 value_choice^2 + -0.003 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 81/1000 --- L(Train): 0.0140704 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.474 value_reward_chosen[t] + -0.0 contr_diff + 0.441 reward + -0.44 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.302 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.083 1 + 0.998 value_choice[t] + -0.001 contr_diff + 0.056 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 82/1000 --- L(Train): 0.0139286 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.471 value_reward_chosen[t] + 0.003 contr_diff + 0.441 reward + -0.439 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.3 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.083 1 + 0.998 value_choice[t] + -0.0 contr_diff + 0.056 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 83/1000 --- L(Train): 0.0137879 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.469 value_reward_chosen[t] + 0.005 contr_diff + 0.441 reward + -0.439 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.297 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.084 1 + 0.998 value_choice[t] + 0.002 contr_diff + 0.056 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 84/1000 --- L(Train): 0.0136518 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.466 value_reward_chosen[t] + 0.005 contr_diff + 0.441 reward + -0.439 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.294 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.084 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.056 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 85/1000 --- L(Train): 0.0135203 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.464 value_reward_chosen[t] + 0.005 contr_diff + 0.441 reward + -0.438 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.292 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.431 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.085 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.056 choice + -0.009 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 86/1000 --- L(Train): 0.0133903 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.461 value_reward_chosen[t] + 0.004 contr_diff + 0.441 reward + -0.438 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.289 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.431 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.085 1 + 0.997 value_choice[t] + 0.0 contr_diff + 0.056 choice + -0.009 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 87/1000 --- L(Train): 0.0132622 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.459 value_reward_chosen[t] + 0.002 contr_diff + 0.441 reward + -0.437 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.286 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.43 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.725 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.086 1 + 0.997 value_choice[t] + -0.002 contr_diff + 0.056 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 88/1000 --- L(Train): 0.0131357 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.457 value_reward_chosen[t] + -0.001 contr_diff + 0.441 reward + -0.436 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.284 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.43 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.086 1 + 0.997 value_choice[t] + -0.003 contr_diff + 0.055 choice + -0.009 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.003 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 89/1000 --- L(Train): 0.0130134 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.454 value_reward_chosen[t] + -0.001 contr_diff + 0.441 reward + -0.436 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.281 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.429 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.086 1 + 0.997 value_choice[t] + -0.004 contr_diff + 0.055 choice + -0.009 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 90/1000 --- L(Train): 0.0128941 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.452 value_reward_chosen[t] + 0.0 contr_diff + 0.44 reward + -0.435 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.278 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.429 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.997 value_choice[t] + -0.003 contr_diff + 0.055 choice + -0.009 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 91/1000 --- L(Train): 0.0127742 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.45 value_reward_chosen[t] + 0.001 contr_diff + 0.44 reward + -0.434 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.275 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.428 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.997 value_choice[t] + -0.001 contr_diff + 0.055 choice + -0.009 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 92/1000 --- L(Train): 0.0126582 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.166 1 + 0.447 value_reward_chosen[t] + 0.001 contr_diff + 0.44 reward + -0.433 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.273 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.427 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + 0.003 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.054 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 93/1000 --- L(Train): 0.0125449 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.166 1 + 0.445 value_reward_chosen[t] + -0.0 contr_diff + 0.439 reward + -0.433 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.27 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.427 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.724 value_reward_not_chosen[t] + 0.006 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.997 value_choice[t] + 0.003 contr_diff + 0.054 choice + -0.009 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 94/1000 --- L(Train): 0.0124330 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.443 value_reward_chosen[t] + 0.001 contr_diff + 0.439 reward + -0.432 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.267 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.426 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.724 value_reward_not_chosen[t] + 0.007 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.997 value_choice[t] + 0.003 contr_diff + 0.053 choice + -0.009 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 95/1000 --- L(Train): 0.0123226 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.44 value_reward_chosen[t] + 0.001 contr_diff + 0.439 reward + -0.431 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.265 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.425 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.724 value_reward_not_chosen[t] + 0.008 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.053 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 96/1000 --- L(Train): 0.0122144 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.438 value_reward_chosen[t] + 0.0 contr_diff + 0.439 reward + -0.43 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.262 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.425 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.724 value_reward_not_chosen[t] + 0.009 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + 0.0 contr_diff + 0.052 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 97/1000 --- L(Train): 0.0121085 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.436 value_reward_chosen[t] + -0.001 contr_diff + 0.438 reward + -0.429 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.259 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.424 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.723 value_reward_not_chosen[t] + 0.008 contr_diff + 0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + -0.003 contr_diff + 0.052 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 98/1000 --- L(Train): 0.0120062 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.434 value_reward_chosen[t] + -0.0 contr_diff + 0.438 reward + -0.428 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.256 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.423 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.723 value_reward_not_chosen[t] + 0.008 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + -0.004 contr_diff + 0.051 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 99/1000 --- L(Train): 0.0119041 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.432 value_reward_chosen[t] + 0.002 contr_diff + 0.438 reward + -0.427 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.254 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.003 contr_diff*reward + 0.423 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.723 value_reward_not_chosen[t] + 0.006 contr_diff + -0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + -0.005 contr_diff + 0.051 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 100/1000 --- L(Train): 0.0118013 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.43 value_reward_chosen[t] + 0.004 contr_diff + 0.437 reward + -0.426 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.251 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.002 contr_diff*reward + 0.422 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.723 value_reward_not_chosen[t] + 0.005 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + -0.004 contr_diff + 0.05 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 101/1000 --- L(Train): 0.0117029 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.427 value_reward_chosen[t] + 0.004 contr_diff + 0.437 reward + -0.425 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.248 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.001 contr_diff*reward + 0.421 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + -0.003 contr_diff + 0.05 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 102/1000 --- L(Train): 0.0116076 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.425 value_reward_chosen[t] + 0.003 contr_diff + 0.436 reward + -0.424 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.246 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.421 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + -0.0 contr_diff + 0.05 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 103/1000 --- L(Train): 0.0115134 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.423 value_reward_chosen[t] + 0.002 contr_diff + 0.436 reward + -0.423 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.243 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.42 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.089 1 + 0.997 value_choice[t] + 0.003 contr_diff + 0.049 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 104/1000 --- L(Train): 0.0114195 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.421 value_reward_chosen[t] + -0.0 contr_diff + 0.436 reward + -0.422 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.24 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.419 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.089 1 + 0.997 value_choice[t] + 0.005 contr_diff + 0.049 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 105/1000 --- L(Train): 0.0113265 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.419 value_reward_chosen[t] + -0.0 contr_diff + 0.435 reward + -0.42 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.238 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.418 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.089 1 + 0.997 value_choice[t] + 0.005 contr_diff + 0.049 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 106/1000 --- L(Train): 0.0112344 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.417 value_reward_chosen[t] + 0.001 contr_diff + 0.435 reward + -0.419 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.235 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.418 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.09 1 + 0.997 value_choice[t] + 0.004 contr_diff + 0.049 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 107/1000 --- L(Train): 0.0111453 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.415 value_reward_chosen[t] + 0.002 contr_diff + 0.435 reward + -0.418 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.233 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.417 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.004 contr_diff + 0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.09 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.048 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 108/1000 --- L(Train): 0.0110557 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.412 value_reward_chosen[t] + 0.002 contr_diff + 0.434 reward + -0.417 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.23 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.416 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.005 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.09 1 + 0.997 value_choice[t] + -0.0 contr_diff + 0.048 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 109/1000 --- L(Train): 0.0109689 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.41 value_reward_chosen[t] + 0.001 contr_diff + 0.434 reward + -0.416 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.228 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.416 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.005 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.09 1 + 0.997 value_choice[t] + -0.002 contr_diff + 0.047 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 110/1000 --- L(Train): 0.0108829 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.408 value_reward_chosen[t] + -0.001 contr_diff + 0.434 reward + -0.415 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.225 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.415 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.005 contr_diff + -0.002 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.091 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.047 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 111/1000 --- L(Train): 0.0107985 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.406 value_reward_chosen[t] + -0.0 contr_diff + 0.434 reward + -0.414 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.223 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.414 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.004 contr_diff + -0.002 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.091 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.047 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 112/1000 --- L(Train): 0.0107144 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.404 value_reward_chosen[t] + 0.002 contr_diff + 0.433 reward + -0.412 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.22 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.414 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.003 contr_diff + -0.002 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.092 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.047 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 113/1000 --- L(Train): 0.0106319 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.402 value_reward_chosen[t] + 0.003 contr_diff + 0.433 reward + -0.411 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.218 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.413 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.092 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.047 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 114/1000 --- L(Train): 0.0105499 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.4 value_reward_chosen[t] + 0.003 contr_diff + 0.433 reward + -0.41 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.215 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.412 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.092 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.046 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 115/1000 --- L(Train): 0.0104706 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.398 value_reward_chosen[t] + 0.002 contr_diff + 0.432 reward + -0.409 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.213 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.412 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + -0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.093 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.046 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 116/1000 --- L(Train): 0.0103934 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.396 value_reward_chosen[t] + 0.001 contr_diff + 0.432 reward + -0.408 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.211 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.411 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.093 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.046 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 117/1000 --- L(Train): 0.0103156 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.394 value_reward_chosen[t] + -0.002 contr_diff + 0.432 reward + -0.406 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.208 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.411 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.094 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.046 choice + -0.008 value_choice^2 + 0.003 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 118/1000 --- L(Train): 0.0102380 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.392 value_reward_chosen[t] + -0.002 contr_diff + 0.432 reward + -0.405 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.206 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.41 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.004 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.094 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.046 choice + -0.008 value_choice^2 + 0.003 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 119/1000 --- L(Train): 0.0101628 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.39 value_reward_chosen[t] + -0.0 contr_diff + 0.432 reward + -0.404 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.204 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.409 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.005 contr_diff + -0.002 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.095 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.045 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 120/1000 --- L(Train): 0.0100908 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.388 value_reward_chosen[t] + 0.003 contr_diff + 0.432 reward + -0.403 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.201 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.409 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.006 contr_diff + -0.002 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.095 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.045 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 121/1000 --- L(Train): 0.0100191 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.386 value_reward_chosen[t] + 0.005 contr_diff + 0.431 reward + -0.401 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.199 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.408 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.007 contr_diff + -0.001 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.095 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.045 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 122/1000 --- L(Train): 0.0099447 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.384 value_reward_chosen[t] + 0.006 contr_diff + 0.431 reward + -0.4 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.197 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.408 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.006 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.096 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.044 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 123/1000 --- L(Train): 0.0098723 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.382 value_reward_chosen[t] + 0.006 contr_diff + 0.431 reward + -0.399 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.195 value_reward_chosen*reward + -0.003 contr_diff^2 + 0.001 contr_diff*reward + 0.407 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.006 contr_diff + 0.002 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.096 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.044 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 124/1000 --- L(Train): 0.0098037 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.38 value_reward_chosen[t] + 0.005 contr_diff + 0.431 reward + -0.397 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.193 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.407 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.005 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.096 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.044 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 125/1000 --- L(Train): 0.0097360 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.378 value_reward_chosen[t] + 0.003 contr_diff + 0.431 reward + -0.396 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.191 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.406 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.096 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.043 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 126/1000 --- L(Train): 0.0096687 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.376 value_reward_chosen[t] + 0.0 contr_diff + 0.431 reward + -0.395 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.188 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.406 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.097 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.043 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 127/1000 --- L(Train): 0.0096019 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.374 value_reward_chosen[t] + -0.003 contr_diff + 0.431 reward + -0.394 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.186 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.405 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + -0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.097 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.042 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.003 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 128/1000 --- L(Train): 0.0095362 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.372 value_reward_chosen[t] + -0.004 contr_diff + 0.431 reward + -0.392 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.184 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.405 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.097 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.042 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 129/1000 --- L(Train): 0.0094719 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.37 value_reward_chosen[t] + -0.003 contr_diff + 0.431 reward + -0.391 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.182 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.404 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.097 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.041 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 130/1000 --- L(Train): 0.0094099 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.368 value_reward_chosen[t] + -0.0 contr_diff + 0.431 reward + -0.39 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.18 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.404 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.002 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.098 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.041 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 131/1000 --- L(Train): 0.0093470 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.366 value_reward_chosen[t] + 0.004 contr_diff + 0.43 reward + -0.388 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.179 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.003 contr_diff*reward + 0.404 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + 0.003 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.098 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.041 choice + -0.008 value_choice^2 + 0.003 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 132/1000 --- L(Train): 0.0092867 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.364 value_reward_chosen[t] + 0.006 contr_diff + 0.43 reward + -0.387 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.177 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.403 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.003 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.099 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.041 choice + -0.007 value_choice^2 + 0.003 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 133/1000 --- L(Train): 0.0092260 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.362 value_reward_chosen[t] + 0.007 contr_diff + 0.43 reward + -0.385 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.175 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.403 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.099 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.04 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 134/1000 --- L(Train): 0.0091651 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.36 value_reward_chosen[t] + 0.008 contr_diff + 0.43 reward + -0.384 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.173 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.402 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.004 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.099 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.04 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 135/1000 --- L(Train): 0.0091064 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.358 value_reward_chosen[t] + 0.007 contr_diff + 0.43 reward + -0.383 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.171 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.004 contr_diff*reward + 0.402 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.004 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.099 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.04 choice + -0.008 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 136/1000 --- L(Train): 0.0090491 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.356 value_reward_chosen[t] + 0.005 contr_diff + 0.43 reward + -0.381 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.169 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.005 contr_diff*reward + 0.401 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.004 contr_diff + -0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.1 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.039 choice + -0.008 value_choice^2 + -0.004 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 137/1000 --- L(Train): 0.0089939 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.354 value_reward_chosen[t] + 0.003 contr_diff + 0.43 reward + -0.38 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.167 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.004 contr_diff*reward + 0.401 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.1 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.039 choice + -0.008 value_choice^2 + -0.004 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 138/1000 --- L(Train): 0.0089413 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.352 value_reward_chosen[t] + -0.0 contr_diff + 0.43 reward + -0.379 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.166 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.401 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.101 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.039 choice + -0.008 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + 0.003 contr_diff^2 + 0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 139/1000 --- L(Train): 0.0088828 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.35 value_reward_chosen[t] + -0.001 contr_diff + 0.43 reward + -0.377 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.164 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.4 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.101 1 + 0.996 value_choice[t] + 0.002 contr_diff + 0.038 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.003 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 140/1000 --- L(Train): 0.0088254 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.348 value_reward_chosen[t] + -0.0 contr_diff + 0.43 reward + -0.376 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.162 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.4 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.101 1 + 0.996 value_choice[t] + 0.002 contr_diff + 0.038 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 141/1000 --- L(Train): 0.0087736 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.346 value_reward_chosen[t] + 0.002 contr_diff + 0.431 reward + -0.374 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.16 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.004 contr_diff*reward + 0.4 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.102 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.038 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 142/1000 --- L(Train): 0.0087227 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.344 value_reward_chosen[t] + 0.003 contr_diff + 0.431 reward + -0.373 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.159 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.005 contr_diff*reward + 0.399 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.102 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.037 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 143/1000 --- L(Train): 0.0086709 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.342 value_reward_chosen[t] + 0.003 contr_diff + 0.431 reward + -0.371 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.157 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.004 contr_diff*reward + 0.399 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.103 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.037 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 144/1000 --- L(Train): 0.0086198 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.341 value_reward_chosen[t] + 0.002 contr_diff + 0.431 reward + -0.37 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.156 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.003 contr_diff*reward + 0.398 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.103 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.037 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 145/1000 --- L(Train): 0.0085685 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.339 value_reward_chosen[t] + 0.001 contr_diff + 0.431 reward + -0.368 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.154 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.398 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.104 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.037 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 146/1000 --- L(Train): 0.0085187 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.337 value_reward_chosen[t] + -0.002 contr_diff + 0.431 reward + -0.367 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.152 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.398 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.104 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.036 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 147/1000 --- L(Train): 0.0084748 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.335 value_reward_chosen[t] + -0.002 contr_diff + 0.431 reward + -0.365 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.151 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.005 contr_diff*reward + 0.397 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.104 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.036 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 148/1000 --- L(Train): 0.0084290 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.333 value_reward_chosen[t] + -0.001 contr_diff + 0.431 reward + -0.364 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.149 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.006 contr_diff*reward + 0.397 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.105 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.035 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 149/1000 --- L(Train): 0.0083797 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.331 value_reward_chosen[t] + 0.002 contr_diff + 0.431 reward + -0.362 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.148 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.005 contr_diff*reward + 0.397 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.105 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.035 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 150/1000 --- L(Train): 0.0083330 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.33 value_reward_chosen[t] + 0.004 contr_diff + 0.431 reward + -0.361 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.146 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.004 contr_diff*reward + 0.396 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.105 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.035 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 151/1000 --- L(Train): 0.0082878 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.328 value_reward_chosen[t] + 0.004 contr_diff + 0.431 reward + -0.359 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.145 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.396 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.106 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.034 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 152/1000 --- L(Train): 0.0082445 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.326 value_reward_chosen[t] + 0.004 contr_diff + 0.431 reward + -0.358 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.143 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.396 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.106 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.034 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 153/1000 --- L(Train): 0.0082022 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.324 value_reward_chosen[t] + 0.003 contr_diff + 0.431 reward + -0.356 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.142 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.395 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.106 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.033 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 154/1000 --- L(Train): 0.0081587 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.322 value_reward_chosen[t] + 0.001 contr_diff + 0.431 reward + -0.354 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.141 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.004 contr_diff*reward + 0.395 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.106 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.033 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 155/1000 --- L(Train): 0.0081151 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.321 value_reward_chosen[t] + -0.002 contr_diff + 0.432 reward + -0.353 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.139 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.395 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.107 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.033 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 156/1000 --- L(Train): 0.0080747 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.319 value_reward_chosen[t] + -0.003 contr_diff + 0.432 reward + -0.351 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.138 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.394 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.107 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.032 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 157/1000 --- L(Train): 0.0080344 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.317 value_reward_chosen[t] + -0.002 contr_diff + 0.432 reward + -0.35 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.137 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.394 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.108 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.032 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 158/1000 --- L(Train): 0.0079959 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.315 value_reward_chosen[t] + 0.001 contr_diff + 0.432 reward + -0.348 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.135 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.394 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.108 1 + 0.995 value_choice[t] + -0.0 contr_diff + 0.032 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 159/1000 --- L(Train): 0.0079556 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.314 value_reward_chosen[t] + 0.002 contr_diff + 0.432 reward + -0.346 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.134 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.393 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.108 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.031 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 160/1000 --- L(Train): 0.0079151 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.312 value_reward_chosen[t] + 0.002 contr_diff + 0.432 reward + -0.345 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.133 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.393 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.109 1 + 0.995 value_choice[t] + 0.003 contr_diff + 0.031 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 161/1000 --- L(Train): 0.0078756 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.31 value_reward_chosen[t] + 0.002 contr_diff + 0.432 reward + -0.343 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.131 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.393 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + -0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.109 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.031 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 162/1000 --- L(Train): 0.0078413 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.308 value_reward_chosen[t] + 0.0 contr_diff + 0.432 reward + -0.341 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.13 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.392 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.11 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.03 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 163/1000 --- L(Train): 0.0078034 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.307 value_reward_chosen[t] + -0.002 contr_diff + 0.432 reward + -0.34 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.129 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.392 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.11 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.03 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 164/1000 --- L(Train): 0.0077655 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.305 value_reward_chosen[t] + -0.002 contr_diff + 0.433 reward + -0.338 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.128 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.392 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.11 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.029 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 165/1000 --- L(Train): 0.0077305 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.303 value_reward_chosen[t] + -0.001 contr_diff + 0.433 reward + -0.336 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.126 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.391 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.111 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.029 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 166/1000 --- L(Train): 0.0076967 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.302 value_reward_chosen[t] + 0.002 contr_diff + 0.433 reward + -0.334 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.125 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.391 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.111 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.028 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 167/1000 --- L(Train): 0.0076625 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.3 value_reward_chosen[t] + 0.003 contr_diff + 0.433 reward + -0.333 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.124 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.391 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.111 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.028 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 168/1000 --- L(Train): 0.0076266 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.299 value_reward_chosen[t] + 0.004 contr_diff + 0.433 reward + -0.331 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.123 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.39 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + -0.003 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.112 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.028 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 169/1000 --- L(Train): 0.0075934 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.297 value_reward_chosen[t] + 0.003 contr_diff + 0.433 reward + -0.329 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.122 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.39 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.003 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.112 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.027 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 170/1000 --- L(Train): 0.0075616 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.295 value_reward_chosen[t] + 0.002 contr_diff + 0.433 reward + -0.327 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.121 value_reward_chosen*reward + 0.003 contr_diff^2 + -0.001 contr_diff*reward + 0.39 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.113 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.027 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 171/1000 --- L(Train): 0.0075302 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.294 value_reward_chosen[t] + -0.0 contr_diff + 0.433 reward + -0.326 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.12 value_reward_chosen*reward + 0.003 contr_diff^2 + -0.001 contr_diff*reward + 0.39 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.113 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.027 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 172/1000 --- L(Train): 0.0074971 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.292 value_reward_chosen[t] + -0.001 contr_diff + 0.434 reward + -0.324 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.118 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.389 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.003 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.113 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.026 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 173/1000 --- L(Train): 0.0074659 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.29 value_reward_chosen[t] + 0.0 contr_diff + 0.434 reward + -0.322 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.117 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.389 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.004 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.114 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.026 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 174/1000 --- L(Train): 0.0074361 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.289 value_reward_chosen[t] + 0.001 contr_diff + 0.434 reward + -0.32 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.116 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.389 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.004 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.114 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.026 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 175/1000 --- L(Train): 0.0074060 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.287 value_reward_chosen[t] + -0.0 contr_diff + 0.434 reward + -0.318 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.115 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.388 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.004 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.115 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.026 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 176/1000 --- L(Train): 0.0073764 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.286 value_reward_chosen[t] + 0.001 contr_diff + 0.434 reward + -0.317 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.114 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.388 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.116 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.025 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 177/1000 --- L(Train): 0.0073462 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.284 value_reward_chosen[t] + 0.0 contr_diff + 0.434 reward + -0.315 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.113 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.388 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.116 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.025 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.003 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 178/1000 --- L(Train): 0.0073173 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.283 value_reward_chosen[t] + -0.0 contr_diff + 0.434 reward + -0.313 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.112 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.387 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.116 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.024 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.003 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 179/1000 --- L(Train): 0.0072886 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.281 value_reward_chosen[t] + 0.0 contr_diff + 0.435 reward + -0.311 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.111 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.387 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.116 1 + 0.995 value_choice[t] + -0.0 contr_diff + 0.024 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.003 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 180/1000 --- L(Train): 0.0072605 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.28 value_reward_chosen[t] + -0.0 contr_diff + 0.435 reward + -0.309 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.11 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.387 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.116 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.023 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 181/1000 --- L(Train): 0.0072334 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.278 value_reward_chosen[t] + 0.001 contr_diff + 0.435 reward + -0.307 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.109 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.386 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.117 1 + 0.995 value_choice[t] + 0.003 contr_diff + 0.022 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 182/1000 --- L(Train): 0.0072071 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.277 value_reward_chosen[t] + 0.001 contr_diff + 0.435 reward + -0.305 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.108 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.386 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.002 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.117 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.022 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.003 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 183/1000 --- L(Train): 0.0071808 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.275 value_reward_chosen[t] + 0.0 contr_diff + 0.435 reward + -0.304 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.107 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.386 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.117 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.022 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.003 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 184/1000 --- L(Train): 0.0071551 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.274 value_reward_chosen[t] + -0.001 contr_diff + 0.435 reward + -0.302 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.106 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.385 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.118 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.021 choice + -0.007 value_choice^2 + -0.003 value_choice*contr_diff + 0.001 value_choice*choice + -0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 185/1000 --- L(Train): 0.0071307 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.272 value_reward_chosen[t] + -0.001 contr_diff + 0.436 reward + -0.3 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.106 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.385 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.119 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.021 choice + -0.007 value_choice^2 + -0.003 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 186/1000 --- L(Train): 0.0071074 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.271 value_reward_chosen[t] + 0.0 contr_diff + 0.436 reward + -0.298 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.105 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.385 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.119 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.021 choice + -0.007 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 187/1000 --- L(Train): 0.0070821 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.269 value_reward_chosen[t] + 0.001 contr_diff + 0.436 reward + -0.296 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.104 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.385 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.12 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.02 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.004 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 188/1000 --- L(Train): 0.0070570 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.268 value_reward_chosen[t] + 0.001 contr_diff + 0.436 reward + -0.294 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.103 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.384 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.12 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.02 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.004 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 189/1000 --- L(Train): 0.0070344 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.266 value_reward_chosen[t] + -0.001 contr_diff + 0.436 reward + -0.292 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.102 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.384 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.12 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.02 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.003 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 190/1000 --- L(Train): 0.0070120 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.265 value_reward_chosen[t] + -0.0 contr_diff + 0.436 reward + -0.29 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.101 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.384 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.121 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.019 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + -0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 191/1000 --- L(Train): 0.0069894 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.264 value_reward_chosen[t] + 0.001 contr_diff + 0.437 reward + -0.288 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.1 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.383 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.121 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.019 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 192/1000 --- L(Train): 0.0069678 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.262 value_reward_chosen[t] + 0.002 contr_diff + 0.437 reward + -0.286 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.1 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.383 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.122 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.019 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.002 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 193/1000 --- L(Train): 0.0069466 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.261 value_reward_chosen[t] + 0.002 contr_diff + 0.437 reward + -0.284 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.099 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.383 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.122 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.018 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 194/1000 --- L(Train): 0.0069243 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.259 value_reward_chosen[t] + 0.001 contr_diff + 0.437 reward + -0.282 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.098 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.382 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.123 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.018 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 195/1000 --- L(Train): 0.0069027 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.258 value_reward_chosen[t] + -0.001 contr_diff + 0.437 reward + -0.28 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.097 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.382 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.123 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.017 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 196/1000 --- L(Train): 0.0068831 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.257 value_reward_chosen[t] + -0.002 contr_diff + 0.438 reward + -0.278 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.097 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.382 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.123 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.016 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 197/1000 --- L(Train): 0.0068637 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.255 value_reward_chosen[t] + -0.001 contr_diff + 0.438 reward + -0.276 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.096 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.382 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.123 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.016 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 198/1000 --- L(Train): 0.0068424 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.254 value_reward_chosen[t] + 0.002 contr_diff + 0.438 reward + -0.274 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.095 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.381 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + 0.003 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.123 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.015 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 199/1000 --- L(Train): 0.0068214 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.253 value_reward_chosen[t] + 0.003 contr_diff + 0.438 reward + -0.272 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.094 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.381 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.003 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.124 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.015 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 200/1000 --- L(Train): 0.0068021 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.251 value_reward_chosen[t] + 0.003 contr_diff + 0.438 reward + -0.27 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.094 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.381 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.125 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.015 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.002 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 201/1000 --- L(Train): 0.0067833 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.25 value_reward_chosen[t] + 0.003 contr_diff + 0.439 reward + -0.268 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.093 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.38 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.125 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.015 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 202/1000 --- L(Train): 0.0067662 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.249 value_reward_chosen[t] + 0.001 contr_diff + 0.439 reward + -0.266 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.092 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.38 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.126 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.014 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 203/1000 --- L(Train): 0.0067477 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.247 value_reward_chosen[t] + -0.001 contr_diff + 0.439 reward + -0.264 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.092 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.38 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.127 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.014 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 204/1000 --- L(Train): 0.0067289 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.246 value_reward_chosen[t] + -0.002 contr_diff + 0.439 reward + -0.262 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.091 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.379 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.127 1 + 0.995 value_choice[t] + -0.0 contr_diff + 0.014 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.003 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 205/1000 --- L(Train): 0.0067132 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.245 value_reward_chosen[t] + -0.001 contr_diff + 0.439 reward + -0.26 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.091 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.379 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.128 1 + 0.996 value_choice[t] + 0.002 contr_diff + 0.013 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.003 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 206/1000 --- L(Train): 0.0066963 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.243 value_reward_chosen[t] + 0.001 contr_diff + 0.44 reward + -0.258 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.09 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.379 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.128 1 + 0.996 value_choice[t] + 0.003 contr_diff + 0.013 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 207/1000 --- L(Train): 0.0066804 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.242 value_reward_chosen[t] + 0.002 contr_diff + 0.44 reward + -0.256 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.089 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.379 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.128 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.012 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + -0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 208/1000 --- L(Train): 0.0066638 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.241 value_reward_chosen[t] + 0.002 contr_diff + 0.44 reward + -0.254 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.089 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.378 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.129 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.012 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 209/1000 --- L(Train): 0.0066474 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.239 value_reward_chosen[t] + 0.001 contr_diff + 0.44 reward + -0.252 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.088 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.378 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.129 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.011 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 210/1000 --- L(Train): 0.0066324 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.238 value_reward_chosen[t] + -0.001 contr_diff + 0.44 reward + -0.25 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.088 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.378 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.129 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.011 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 211/1000 --- L(Train): 0.0066165 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.237 value_reward_chosen[t] + -0.001 contr_diff + 0.441 reward + -0.248 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.087 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.377 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.13 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.011 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 212/1000 --- L(Train): 0.0065990 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.166 1 + 0.236 value_reward_chosen[t] + 0.0 contr_diff + 0.441 reward + -0.246 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.086 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.377 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.13 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.01 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 213/1000 --- L(Train): 0.0065849 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.166 1 + 0.234 value_reward_chosen[t] + 0.0 contr_diff + 0.441 reward + -0.244 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.086 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.377 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.13 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.009 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 214/1000 --- L(Train): 0.0065709 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.166 1 + 0.233 value_reward_chosen[t] + -0.0 contr_diff + 0.441 reward + -0.242 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.085 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.377 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.131 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.009 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 215/1000 --- L(Train): 0.0065559 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.232 value_reward_chosen[t] + 0.0 contr_diff + 0.442 reward + -0.239 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.085 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.376 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.131 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.008 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 216/1000 --- L(Train): 0.0065399 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.231 value_reward_chosen[t] + 0.0 contr_diff + 0.442 reward + -0.237 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.084 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.376 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.131 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.007 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 217/1000 --- L(Train): 0.0065263 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.229 value_reward_chosen[t] + -0.001 contr_diff + 0.442 reward + -0.235 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.084 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.376 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.132 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.007 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 218/1000 --- L(Train): 0.0065135 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.228 value_reward_chosen[t] + -0.001 contr_diff + 0.442 reward + -0.233 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.083 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.375 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.133 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.007 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 219/1000 --- L(Train): 0.0065011 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.227 value_reward_chosen[t] + 0.001 contr_diff + 0.443 reward + -0.231 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.083 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.375 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.133 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.007 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 220/1000 --- L(Train): 0.0064874 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.226 value_reward_chosen[t] + 0.002 contr_diff + 0.443 reward + -0.229 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.082 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.375 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.134 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.007 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 221/1000 --- L(Train): 0.0064752 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.225 value_reward_chosen[t] + 0.001 contr_diff + 0.443 reward + -0.227 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.082 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.375 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.134 1 + 0.995 value_choice[t] + -0.0 contr_diff + 0.006 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 222/1000 --- L(Train): 0.0064619 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.223 value_reward_chosen[t] + -0.0 contr_diff + 0.443 reward + -0.225 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.082 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.374 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.135 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.006 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 223/1000 --- L(Train): 0.0064485 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.222 value_reward_chosen[t] + 0.0 contr_diff + 0.443 reward + -0.223 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.081 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.374 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.135 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.005 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 224/1000 --- L(Train): 0.0064349 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.221 value_reward_chosen[t] + -0.001 contr_diff + 0.444 reward + -0.221 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.081 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.374 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.135 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.004 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 225/1000 --- L(Train): 0.0064223 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.17 1 + 0.22 value_reward_chosen[t] + -0.0 contr_diff + 0.444 reward + -0.218 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.08 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.374 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.136 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.004 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 226/1000 --- L(Train): 0.0064113 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.17 1 + 0.219 value_reward_chosen[t] + 0.002 contr_diff + 0.444 reward + -0.216 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.08 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.373 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.136 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.004 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 227/1000 --- L(Train): 0.0063975 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.17 1 + 0.218 value_reward_chosen[t] + 0.003 contr_diff + 0.444 reward + -0.214 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.079 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.373 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.137 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.003 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 228/1000 --- L(Train): 0.0063860 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.17 1 + 0.216 value_reward_chosen[t] + 0.002 contr_diff + 0.445 reward + -0.212 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.079 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.373 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.137 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.003 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 229/1000 --- L(Train): 0.0063769 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.171 1 + 0.215 value_reward_chosen[t] + 0.001 contr_diff + 0.445 reward + -0.21 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.079 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.372 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.138 1 + 0.995 value_choice[t] + -0.0 contr_diff + 0.003 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 230/1000 --- L(Train): 0.0063661 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.171 1 + 0.214 value_reward_chosen[t] + -0.001 contr_diff + 0.445 reward + -0.208 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.078 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.0 contr_diff*reward + 0.372 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.139 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.002 choice + -0.007 value_choice^2 + 0.003 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 231/1000 --- L(Train): 0.0063530 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.171 1 + 0.213 value_reward_chosen[t] + -0.001 contr_diff + 0.446 reward + -0.206 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.078 value_reward_chosen*reward + 0.003 contr_diff^2 + -0.0 contr_diff*reward + 0.372 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.139 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.002 choice + -0.007 value_choice^2 + 0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 232/1000 --- L(Train): 0.0063430 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.172 1 + 0.212 value_reward_chosen[t] + -0.0 contr_diff + 0.446 reward + -0.204 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.078 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.372 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.139 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 233/1000 --- L(Train): 0.0063344 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.172 1 + 0.211 value_reward_chosen[t] + 0.002 contr_diff + 0.446 reward + -0.202 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.077 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.371 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.14 1 + 0.995 value_choice[t] + -0.0 contr_diff + 0.0 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 234/1000 --- L(Train): 0.0063241 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.172 1 + 0.21 value_reward_chosen[t] + 0.003 contr_diff + 0.446 reward + -0.199 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.077 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.371 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.14 1 + 0.995 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 235/1000 --- L(Train): 0.0063150 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.173 1 + 0.209 value_reward_chosen[t] + 0.003 contr_diff + 0.447 reward + -0.197 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.077 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.371 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.14 1 + 0.995 value_choice[t] + -0.0 contr_diff + -0.0 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 236/1000 --- L(Train): 0.0063048 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.173 1 + 0.207 value_reward_chosen[t] + 0.002 contr_diff + 0.447 reward + -0.195 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.076 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.37 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 237/1000 --- L(Train): 0.0062955 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.173 1 + 0.206 value_reward_chosen[t] + -0.0 contr_diff + 0.447 reward + -0.193 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.076 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.37 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 238/1000 --- L(Train): 0.0062846 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.174 1 + 0.205 value_reward_chosen[t] + -0.001 contr_diff + 0.447 reward + -0.191 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.076 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.37 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.995 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 239/1000 --- L(Train): 0.0062750 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.174 1 + 0.204 value_reward_chosen[t] + 0.0 contr_diff + 0.448 reward + -0.189 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.075 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.37 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.003 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 240/1000 --- L(Train): 0.0062666 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.174 1 + 0.203 value_reward_chosen[t] + 0.0 contr_diff + 0.448 reward + -0.187 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.075 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.369 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.003 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\u001b[H\u001b[2J\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 241/1000 --- L(Train): 0.0062578 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.175 1 + 0.202 value_reward_chosen[t] + -0.001 contr_diff + 0.448 reward + -0.184 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.075 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.369 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 242/1000 --- L(Train): 0.0062479 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.175 1 + 0.201 value_reward_chosen[t] + -0.0 contr_diff + 0.448 reward + -0.182 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.074 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.369 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + -0.0 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 243/1000 --- L(Train): 0.0062392 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.175 1 + 0.2 value_reward_chosen[t] + 0.001 contr_diff + 0.449 reward + -0.18 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.074 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.369 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + 0.002 contr_diff + -0.0 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 244/1000 --- L(Train): 0.0062304 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.176 1 + 0.199 value_reward_chosen[t] + 0.002 contr_diff + 0.449 reward + -0.178 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.074 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.368 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.007 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 245/1000 --- L(Train): 0.0062215 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.176 1 + 0.198 value_reward_chosen[t] + 0.002 contr_diff + 0.449 reward + -0.176 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.074 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.368 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + 0.002 contr_diff + -0.0 choice + -0.007 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 246/1000 --- L(Train): 0.0062140 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.176 1 + 0.197 value_reward_chosen[t] + 0.0 contr_diff + 0.45 reward + -0.174 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.073 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.368 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 247/1000 --- L(Train): 0.0062062 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.177 1 + 0.196 value_reward_chosen[t] + -0.002 contr_diff + 0.45 reward + -0.172 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.073 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.368 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.003 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 248/1000 --- L(Train): 0.0061980 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.177 1 + 0.195 value_reward_chosen[t] + -0.002 contr_diff + 0.45 reward + -0.169 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.073 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.367 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.993 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.003 value_choice*contr_diff + -0.001 value_choice*choice + 0.003 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 249/1000 --- L(Train): 0.0061883 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.177 1 + 0.194 value_reward_chosen[t] + -0.002 contr_diff + 0.45 reward + -0.167 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.073 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.367 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.993 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.003 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 250/1000 --- L(Train): 0.0061800 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.178 1 + 0.193 value_reward_chosen[t] + 0.0 contr_diff + 0.451 reward + -0.165 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.072 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.367 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.993 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 251/1000 --- L(Train): 0.0061733 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.178 1 + 0.191 value_reward_chosen[t] + 0.001 contr_diff + 0.451 reward + -0.163 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.072 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.366 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 1, 0, 0, 1, 0, 1, 1, 0\n",
            "value_reward_not_chosen: 0, 0, 1, 1, 1, 1\n",
            "value_choice: 0, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 252/1000 --- L(Train): 0.0061664 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.178 1 + 0.19 value_reward_chosen[t] + 0.001 contr_diff + 0.451 reward + -0.161 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.072 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.366 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.007 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.003 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 2, 0, 0, 2, 0, 2, 2, 0\n",
            "value_reward_not_chosen: 0, 0, 2, 2, 2, 2\n",
            "value_choice: 0, 2, 2, 2, 2, 2, 2, 2, 2, 2\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 253/1000 --- L(Train): 0.0061577 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.179 1 + 0.189 value_reward_chosen[t] + 0.0 contr_diff + 0.452 reward + -0.159 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.072 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.366 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.993 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.007 value_choice^2 + -0.004 value_choice*contr_diff + 0.0 value_choice*choice + -0.004 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 3, 0, 0, 3, 0, 3, 3, 0\n",
            "value_reward_not_chosen: 0, 0, 3, 3, 3, 3\n",
            "value_choice: 0, 3, 3, 3, 3, 3, 3, 3, 3, 3\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 254/1000 --- L(Train): 0.0061494 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.179 1 + 0.188 value_reward_chosen[t] + -0.002 contr_diff + 0.452 reward + -0.157 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.072 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.366 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.993 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.007 value_choice^2 + -0.004 value_choice*contr_diff + -0.0 value_choice*choice + -0.003 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 4, 0, 0, 4, 0, 4, 4, 0\n",
            "value_reward_not_chosen: 0, 0, 4, 4, 4, 4\n",
            "value_choice: 0, 4, 4, 4, 4, 4, 4, 4, 4, 4\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 255/1000 --- L(Train): 0.0061432 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.179 1 + 0.187 value_reward_chosen[t] + -0.002 contr_diff + 0.452 reward + -0.154 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.365 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.993 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.007 value_choice^2 + -0.004 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 5, 0, 0, 5, 0, 5, 5, 0\n",
            "value_reward_not_chosen: 0, 0, 5, 5, 5, 5\n",
            "value_choice: 0, 5, 5, 5, 5, 5, 5, 5, 5, 5\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 256/1000 --- L(Train): 0.0061381 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.18 1 + 0.186 value_reward_chosen[t] + -0.001 contr_diff + 0.452 reward + -0.152 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.365 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.993 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.007 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 6, 0, 0, 6, 0, 6, 6, 0\n",
            "value_reward_not_chosen: 0, 0, 6, 6, 6, 6\n",
            "value_choice: 0, 6, 6, 6, 6, 6, 6, 6, 6, 6\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 257/1000 --- L(Train): 0.0061310 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.18 1 + 0.185 value_reward_chosen[t] + 0.001 contr_diff + 0.453 reward + -0.15 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.365 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.993 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.003 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 7, 0, 0, 7, 0, 7, 7, 0\n",
            "value_reward_not_chosen: 0, 0, 7, 7, 7, 7\n",
            "value_choice: 0, 7, 7, 7, 7, 7, 7, 7, 7, 7\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 258/1000 --- L(Train): 0.0061223 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.18 1 + 0.184 value_reward_chosen[t] + 0.002 contr_diff + 0.453 reward + -0.148 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.365 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.003 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 8, 0, 0, 8, 0, 8, 8, 0\n",
            "value_reward_not_chosen: 0, 0, 8, 8, 8, 8\n",
            "value_choice: 0, 8, 8, 8, 8, 8, 8, 8, 8, 8\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 259/1000 --- L(Train): 0.0061156 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.181 1 + 0.183 value_reward_chosen[t] + 0.002 contr_diff + 0.453 reward + -0.146 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.003 contr_diff*reward + 0.364 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 9, 0, 0, 9, 0, 9, 9, 0\n",
            "value_reward_not_chosen: 0, 0, 9, 9, 9, 9\n",
            "value_choice: 0, 9, 9, 9, 9, 9, 9, 9, 9, 9\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 260/1000 --- L(Train): 0.0061092 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.181 1 + 0.182 value_reward_chosen[t] + 0.001 contr_diff + 0.454 reward + -0.144 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.364 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 10, 0, 0, 10, 0, 10, 10, 0\n",
            "value_reward_not_chosen: 0, 0, 10, 10, 10, 10\n",
            "value_choice: 0, 10, 10, 10, 10, 10, 10, 10, 10, 10\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 261/1000 --- L(Train): 0.0061034 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.181 1 + 0.181 value_reward_chosen[t] + -0.001 contr_diff + 0.454 reward + -0.142 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.07 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.364 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 11, 0, 0, 11, 0, 11, 11, 0\n",
            "value_reward_not_chosen: 0, 0, 11, 11, 11, 11\n",
            "value_choice: 0, 11, 11, 11, 11, 11, 11, 11, 11, 11\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 262/1000 --- L(Train): 0.0060976 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.182 1 + 0.18 value_reward_chosen[t] + -0.001 contr_diff + 0.454 reward + -0.139 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.07 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.364 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 12, 0, 0, 12, 0, 12, 12, 0\n",
            "value_reward_not_chosen: 0, 0, 12, 12, 12, 12\n",
            "value_choice: 0, 12, 12, 12, 12, 12, 12, 12, 12, 12\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 263/1000 --- L(Train): 0.0060923 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.182 1 + 0.179 value_reward_chosen[t] + -0.0 contr_diff + 0.455 reward + -0.137 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.07 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.363 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + -0.0 contr_diff + 0.001 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 13, 0, 0, 13, 0, 13, 13, 0\n",
            "value_reward_not_chosen: 0, 0, 13, 13, 13, 13\n",
            "value_choice: 0, 13, 13, 13, 13, 13, 13, 13, 13, 13\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 264/1000 --- L(Train): 0.0060844 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.182 1 + 0.179 value_reward_chosen[t] + 0.002 contr_diff + 0.455 reward + -0.135 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.07 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.363 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 14, 0, 0, 14, 0, 14, 14, 0\n",
            "value_reward_not_chosen: 0, 0, 14, 14, 14, 14\n",
            "value_choice: 0, 14, 14, 14, 14, 14, 14, 14, 14, 14\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 265/1000 --- L(Train): 0.0060802 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.183 1 + 0.178 value_reward_chosen[t] + 0.003 contr_diff + 0.455 reward + -0.133 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.07 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.363 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + 0.003 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 15, 0, 0, 15, 0, 15, 15, 0\n",
            "value_reward_not_chosen: 0, 0, 15, 15, 15, 15\n",
            "value_choice: 0, 15, 15, 15, 15, 15, 15, 15, 15, 15\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 266/1000 --- L(Train): 0.0060730 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.183 1 + 0.177 value_reward_chosen[t] + 0.003 contr_diff + 0.455 reward + -0.131 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.07 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.362 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 16, 0, 0, 16, 0, 16, 16, 0\n",
            "value_reward_not_chosen: 0, 0, 16, 16, 16, 16\n",
            "value_choice: 0, 16, 16, 16, 16, 16, 16, 16, 16, 16\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 267/1000 --- L(Train): 0.0060672 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.183 1 + 0.176 value_reward_chosen[t] + 0.001 contr_diff + 0.456 reward + -0.129 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.362 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 17, 0, 0, 17, 0, 17, 17, 0\n",
            "value_reward_not_chosen: 0, 0, 17, 17, 17, 17\n",
            "value_choice: 0, 17, 17, 17, 17, 17, 17, 17, 17, 17\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 268/1000 --- L(Train): 0.0060611 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.184 1 + 0.175 value_reward_chosen[t] + -0.001 contr_diff + 0.456 reward + -0.127 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.362 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 18, 0, 0, 18, 0, 18, 18, 0\n",
            "value_reward_not_chosen: 0, 0, 18, 18, 18, 18\n",
            "value_choice: 0, 18, 18, 18, 18, 18, 18, 18, 18, 18\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 269/1000 --- L(Train): 0.0060553 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.184 1 + 0.174 value_reward_chosen[t] + -0.001 contr_diff + 0.456 reward + -0.125 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.362 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.992 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 19, 0, 0, 19, 0, 19, 19, 0\n",
            "value_reward_not_chosen: 0, 0, 19, 19, 19, 19\n",
            "value_choice: 0, 19, 19, 19, 19, 19, 19, 19, 19, 19\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 270/1000 --- L(Train): 0.0060498 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.184 1 + 0.173 value_reward_chosen[t] + -0.0 contr_diff + 0.457 reward + -0.122 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.361 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.992 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 20, 0, 0, 20, 0, 20, 20, 0\n",
            "value_reward_not_chosen: 0, 0, 20, 20, 20, 20\n",
            "value_choice: 0, 20, 20, 20, 20, 20, 20, 20, 20, 20\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 271/1000 --- L(Train): 0.0060449 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.185 1 + 0.172 value_reward_chosen[t] + 0.002 contr_diff + 0.457 reward + -0.12 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.361 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.992 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 21, 0, 0, 21, 0, 21, 21, 0\n",
            "value_reward_not_chosen: 0, 0, 21, 21, 21, 21\n",
            "value_choice: 0, 21, 21, 21, 21, 21, 21, 21, 21, 21\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 272/1000 --- L(Train): 0.0060398 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.185 1 + 0.171 value_reward_chosen[t] + 0.002 contr_diff + 0.457 reward + -0.118 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.361 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.992 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 22, 0, 0, 22, 0, 22, 22, 0\n",
            "value_reward_not_chosen: 0, 0, 22, 22, 22, 22\n",
            "value_choice: 0, 22, 22, 22, 22, 22, 22, 22, 22, 22\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 273/1000 --- L(Train): 0.0060347 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.186 1 + 0.17 value_reward_chosen[t] + 0.002 contr_diff + 0.458 reward + -0.116 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.361 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 23, 0, 0, 23, 0, 23, 23, 0\n",
            "value_reward_not_chosen: 0, 0, 23, 23, 23, 23\n",
            "value_choice: 0, 23, 23, 23, 23, 23, 23, 23, 23, 23\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 274/1000 --- L(Train): 0.0060302 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.186 1 + 0.169 value_reward_chosen[t] + 0.001 contr_diff + 0.458 reward + -0.114 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.36 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 24, 0, 0, 24, 0, 24, 24, 0\n",
            "value_reward_not_chosen: 0, 0, 24, 24, 24, 24\n",
            "value_choice: 0, 24, 24, 24, 24, 24, 24, 24, 24, 24\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 275/1000 --- L(Train): 0.0060264 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.186 1 + 0.168 value_reward_chosen[t] + -0.001 contr_diff + 0.458 reward + -0.112 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.36 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 25, 0, 0, 25, 0, 25, 25, 0\n",
            "value_reward_not_chosen: 0, 0, 25, 25, 25, 25\n",
            "value_choice: 0, 25, 25, 25, 25, 25, 25, 25, 25, 25\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 276/1000 --- L(Train): 0.0060198 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.187 1 + 0.167 value_reward_chosen[t] + -0.002 contr_diff + 0.459 reward + -0.11 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.36 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.992 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 26, 0, 0, 26, 0, 26, 26, 0\n",
            "value_reward_not_chosen: 0, 0, 26, 26, 26, 26\n",
            "value_choice: 0, 26, 26, 26, 26, 26, 26, 26, 26, 26\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 277/1000 --- L(Train): 0.0060139 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.187 1 + 0.166 value_reward_chosen[t] + -0.001 contr_diff + 0.459 reward + -0.108 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.359 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.003 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.992 value_choice[t] + -0.0 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 27, 0, 0, 27, 0, 27, 27, 0\n",
            "value_reward_not_chosen: 0, 0, 27, 27, 27, 27\n",
            "value_choice: 0, 27, 27, 27, 27, 27, 27, 27, 27, 27\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 278/1000 --- L(Train): 0.0060105 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.187 1 + 0.165 value_reward_chosen[t] + 0.001 contr_diff + 0.459 reward + -0.105 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.359 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.003 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 28, 0, 0, 28, 0, 28, 28, 0\n",
            "value_reward_not_chosen: 0, 0, 28, 28, 28, 28\n",
            "value_choice: 0, 28, 28, 28, 28, 28, 28, 28, 28, 28\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 279/1000 --- L(Train): 0.0060064 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.188 1 + 0.164 value_reward_chosen[t] + 0.001 contr_diff + 0.46 reward + -0.103 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.359 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 29, 0, 0, 29, 0, 29, 29, 0\n",
            "value_reward_not_chosen: 0, 0, 29, 29, 29, 29\n",
            "value_choice: 0, 29, 29, 29, 29, 29, 29, 29, 29, 29\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 280/1000 --- L(Train): 0.0060017 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.188 1 + 0.164 value_reward_chosen[t] + 0.001 contr_diff + 0.46 reward + -0.101 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.359 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 30, 0, 0, 30, 0, 30, 30, 0\n",
            "value_reward_not_chosen: 0, 0, 30, 30, 30, 30\n",
            "value_choice: 0, 30, 30, 30, 30, 30, 30, 30, 30, 30\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 281/1000 --- L(Train): 0.0059965 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.188 1 + 0.163 value_reward_chosen[t] + -0.0 contr_diff + 0.46 reward + -0.099 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.358 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 31, 0, 0, 31, 0, 31, 31, 0\n",
            "value_reward_not_chosen: 0, 0, 31, 31, 31, 31\n",
            "value_choice: 0, 31, 31, 31, 31, 31, 31, 31, 31, 31\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 282/1000 --- L(Train): 0.0059921 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.189 1 + 0.162 value_reward_chosen[t] + 0.0 contr_diff + 0.461 reward + -0.097 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.358 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.004 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.004 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 32, 0, 0, 32, 0, 32, 32, 0\n",
            "value_reward_not_chosen: 0, 0, 32, 32, 32, 32\n",
            "value_choice: 0, 32, 32, 32, 32, 32, 32, 32, 32, 32\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 283/1000 --- L(Train): 0.0059882 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.189 1 + 0.161 value_reward_chosen[t] + -0.001 contr_diff + 0.461 reward + -0.095 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.358 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.004 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.004 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 33, 0, 0, 33, 0, 33, 33, 0\n",
            "value_reward_not_chosen: 0, 0, 33, 33, 33, 33\n",
            "value_choice: 0, 33, 33, 33, 33, 33, 33, 33, 33, 33\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 284/1000 --- L(Train): 0.0059840 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.189 1 + 0.16 value_reward_chosen[t] + -0.0 contr_diff + 0.461 reward + -0.093 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.358 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.003 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 34, 0, 0, 34, 0, 34, 34, 0\n",
            "value_reward_not_chosen: 0, 0, 34, 34, 34, 34\n",
            "value_choice: 0, 34, 34, 34, 34, 34, 34, 34, 34, 34\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 285/1000 --- L(Train): 0.0059808 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.19 1 + 0.159 value_reward_chosen[t] + 0.001 contr_diff + 0.462 reward + -0.091 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.357 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 35, 0, 0, 35, 0, 35, 35, 0\n",
            "value_reward_not_chosen: 0, 0, 35, 35, 35, 35\n",
            "value_choice: 0, 35, 35, 35, 35, 35, 35, 35, 35, 35\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 286/1000 --- L(Train): 0.0059768 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.19 1 + 0.158 value_reward_chosen[t] + 0.002 contr_diff + 0.462 reward + -0.089 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.357 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.721 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 36, 0, 0, 36, 0, 36, 36, 0\n",
            "value_reward_not_chosen: 0, 0, 36, 36, 36, 36\n",
            "value_choice: 0, 36, 36, 36, 36, 36, 36, 36, 36, 36\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 287/1000 --- L(Train): 0.0059749 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.191 1 + 0.157 value_reward_chosen[t] + 0.001 contr_diff + 0.462 reward + -0.087 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.357 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.721 value_reward_not_chosen[t] + 0.0 contr_diff + 0.003 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 37, 0, 0, 37, 0, 37, 37, 0\n",
            "value_reward_not_chosen: 0, 0, 37, 37, 37, 37\n",
            "value_choice: 0, 37, 37, 37, 37, 37, 37, 37, 37, 37\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 288/1000 --- L(Train): 0.0059703 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.191 1 + 0.156 value_reward_chosen[t] + -0.0 contr_diff + 0.462 reward + -0.084 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.357 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.003 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 38, 0, 0, 38, 0, 38, 38, 0\n",
            "value_reward_not_chosen: 0, 0, 38, 38, 38, 38\n",
            "value_choice: 0, 38, 38, 38, 38, 38, 38, 38, 38, 38\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 289/1000 --- L(Train): 0.0059649 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.191 1 + 0.156 value_reward_chosen[t] + -0.0 contr_diff + 0.463 reward + -0.082 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.356 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 39, 0, 0, 39, 0, 39, 39, 0\n",
            "value_reward_not_chosen: 0, 0, 39, 39, 39, 39\n",
            "value_choice: 0, 39, 39, 39, 39, 39, 39, 39, 39, 39\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 290/1000 --- L(Train): 0.0059602 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.191 1 + 0.155 value_reward_chosen[t] + 0.001 contr_diff + 0.463 reward + -0.08 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.356 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 40, 0, 0, 40, 0, 40, 40, 0\n",
            "value_reward_not_chosen: 0, 0, 40, 40, 40, 40\n",
            "value_choice: 0, 40, 40, 40, 40, 40, 40, 40, 40, 40\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 291/1000 --- L(Train): 0.0059591 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.192 1 + 0.154 value_reward_chosen[t] + 0.001 contr_diff + 0.463 reward + -0.078 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.356 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 41, 0, 0, 41, 0, 41, 41, 0\n",
            "value_reward_not_chosen: 0, 0, 41, 41, 41, 41\n",
            "value_choice: 0, 41, 41, 41, 41, 41, 41, 41, 41, 41\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 292/1000 --- L(Train): 0.0059553 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.192 1 + 0.153 value_reward_chosen[t] + -0.0 contr_diff + 0.464 reward + -0.076 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.355 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.003 contr_diff + -0.003 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.0 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.002 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 42, 0, 0, 42, 0, 42, 42, 0\n",
            "value_reward_not_chosen: 0, 0, 42, 42, 42, 42\n",
            "value_choice: 0, 42, 42, 42, 42, 42, 42, 42, 42, 42\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 293/1000 --- L(Train): 0.0059504 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.193 1 + 0.152 value_reward_chosen[t] + 0.0 contr_diff + 0.464 reward + -0.074 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.355 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.002 contr_diff + -0.003 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 43, 0, 0, 43, 0, 43, 43, 0\n",
            "value_reward_not_chosen: 0, 0, 43, 43, 43, 43\n",
            "value_choice: 0, 43, 43, 43, 43, 43, 43, 43, 43, 43\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 294/1000 --- L(Train): 0.0059477 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.193 1 + 0.151 value_reward_chosen[t] + -0.0 contr_diff + 0.464 reward + -0.072 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.355 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 44, 0, 0, 44, 0, 44, 44, 0\n",
            "value_reward_not_chosen: 0, 0, 44, 44, 44, 44\n",
            "value_choice: 0, 44, 44, 44, 44, 44, 44, 44, 44, 44\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 295/1000 --- L(Train): 0.0059445 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.193 1 + 0.15 value_reward_chosen[t] + 0.0 contr_diff + 0.465 reward + -0.07 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.355 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 45, 0, 0, 45, 0, 45, 45, 0\n",
            "value_reward_not_chosen: 0, 0, 45, 45, 45, 45\n",
            "value_choice: 0, 45, 45, 45, 45, 45, 45, 45, 45, 45\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 296/1000 --- L(Train): 0.0059395 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.194 1 + 0.149 value_reward_chosen[t] + -0.0 contr_diff + 0.465 reward + -0.068 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.354 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 46, 0, 0, 46, 0, 46, 46, 0\n",
            "value_reward_not_chosen: 0, 0, 46, 46, 46, 46\n",
            "value_choice: 0, 46, 46, 46, 46, 46, 46, 46, 46, 46\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 297/1000 --- L(Train): 0.0059363 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.194 1 + 0.149 value_reward_chosen[t] + 0.001 contr_diff + 0.465 reward + -0.066 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.354 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.004 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 47, 0, 0, 47, 0, 47, 47, 0\n",
            "value_reward_not_chosen: 0, 0, 47, 47, 47, 47\n",
            "value_choice: 0, 47, 47, 47, 47, 47, 47, 47, 47, 47\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 298/1000 --- L(Train): 0.0059353 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.194 1 + 0.148 value_reward_chosen[t] + 0.001 contr_diff + 0.466 reward + -0.064 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.354 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.004 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 48, 0, 0, 48, 0, 48, 48, 0\n",
            "value_reward_not_chosen: 0, 0, 48, 48, 48, 48\n",
            "value_choice: 0, 48, 48, 48, 48, 48, 48, 48, 48, 48\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 299/1000 --- L(Train): 0.0059346 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.195 1 + 0.147 value_reward_chosen[t] + -0.001 contr_diff + 0.466 reward + -0.062 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.003 contr_diff^2 + -0.001 contr_diff*reward + 0.353 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.003 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.0 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 49, 0, 0, 49, 0, 49, 49, 0\n",
            "value_reward_not_chosen: 0, 0, 49, 49, 49, 49\n",
            "value_choice: 0, 49, 49, 49, 49, 49, 49, 49, 49, 49\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 300/1000 --- L(Train): 0.0059301 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.195 1 + 0.146 value_reward_chosen[t] + -0.0 contr_diff + 0.466 reward + -0.06 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.353 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 50, 0, 0, 50, 0, 50, 50, 0\n",
            "value_reward_not_chosen: 0, 0, 50, 50, 50, 50\n",
            "value_choice: 0, 50, 50, 50, 50, 50, 50, 50, 50, 50\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 301/1000 --- L(Train): 0.0059247 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.195 1 + 0.145 value_reward_chosen[t] + 0.001 contr_diff + 0.467 reward + -0.058 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.353 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 51, 0, 0, 51, 0, 51, 51, 0\n",
            "value_reward_not_chosen: 0, 0, 51, 51, 51, 51\n",
            "value_choice: 0, 51, 51, 51, 51, 51, 51, 51, 51, 51\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 302/1000 --- L(Train): 0.0059218 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.196 1 + 0.144 value_reward_chosen[t] + 0.001 contr_diff + 0.467 reward + -0.056 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.353 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.0 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 52, 0, 0, 52, 0, 52, 52, 0\n",
            "value_reward_not_chosen: 0, 0, 52, 52, 52, 52\n",
            "value_choice: 0, 52, 52, 52, 52, 52, 52, 52, 52, 52\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 303/1000 --- L(Train): 0.0059213 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.196 1 + 0.144 value_reward_chosen[t] + 0.001 contr_diff + 0.468 reward + -0.054 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.352 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + -0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.0 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 53, 0, 0, 53, 0, 53, 53, 0\n",
            "value_reward_not_chosen: 0, 0, 53, 53, 53, 53\n",
            "value_choice: 0, 53, 53, 53, 53, 53, 53, 53, 53, 53\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 304/1000 --- L(Train): 0.0059184 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.197 1 + 0.143 value_reward_chosen[t] + -0.001 contr_diff + 0.468 reward + -0.051 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.352 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 54, 0, 0, 54, 0, 54, 54, 0\n",
            "value_reward_not_chosen: 0, 0, 54, 54, 54, 54\n",
            "value_choice: 0, 54, 54, 54, 54, 54, 54, 54, 54, 54\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 305/1000 --- L(Train): 0.0059136 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.197 1 + 0.142 value_reward_chosen[t] + -0.001 contr_diff + 0.468 reward + -0.049 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.352 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.003 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 55, 0, 1, 55, 0, 55, 55, 0\n",
            "value_reward_not_chosen: 0, 0, 55, 55, 55, 55\n",
            "value_choice: 0, 55, 55, 55, 55, 55, 55, 55, 55, 55\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 306/1000 --- L(Train): 0.0059123 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.197 1 + 0.141 value_reward_chosen[t] + -0.0 contr_diff + 0.469 reward + -0.047 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.352 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.003 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 56, 0, 2, 56, 0, 56, 56, 0\n",
            "value_reward_not_chosen: 0, 0, 56, 56, 56, 56\n",
            "value_choice: 0, 56, 56, 56, 56, 56, 56, 56, 56, 56\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 307/1000 --- L(Train): 0.0059093 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.198 1 + 0.14 value_reward_chosen[t] + 0.002 contr_diff + 0.469 reward + -0.045 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.351 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.002 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 57, 0, 3, 57, 0, 57, 57, 0\n",
            "value_reward_not_chosen: 0, 0, 57, 57, 57, 57\n",
            "value_choice: 0, 57, 57, 57, 57, 57, 57, 57, 57, 57\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 308/1000 --- L(Train): 0.0059048 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.198 1 + 0.14 value_reward_chosen[t] + 0.003 contr_diff + 0.469 reward + -0.043 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.351 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 58, 0, 4, 58, 0, 58, 58, 0\n",
            "value_reward_not_chosen: 0, 0, 58, 58, 58, 58\n",
            "value_choice: 0, 58, 58, 58, 58, 58, 58, 58, 58, 58\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 309/1000 --- L(Train): 0.0059028 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.198 1 + 0.139 value_reward_chosen[t] + 0.002 contr_diff + 0.47 reward + -0.041 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.351 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 59, 0, 5, 59, 0, 59, 59, 0\n",
            "value_reward_not_chosen: 0, 0, 59, 59, 59, 59\n",
            "value_choice: 0, 59, 59, 59, 59, 59, 59, 59, 59, 59\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 310/1000 --- L(Train): 0.0059003 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.199 1 + 0.138 value_reward_chosen[t] + 0.001 contr_diff + 0.47 reward + -0.039 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.35 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 60, 0, 6, 60, 0, 60, 60, 0\n",
            "value_reward_not_chosen: 0, 0, 60, 60, 60, 60\n",
            "value_choice: 0, 60, 60, 60, 60, 60, 60, 60, 60, 60\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 311/1000 --- L(Train): 0.0058988 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.199 1 + 0.137 value_reward_chosen[t] + -0.001 contr_diff + 0.47 reward + -0.037 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.35 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 61, 0, 7, 61, 0, 61, 61, 0\n",
            "value_reward_not_chosen: 0, 0, 61, 61, 61, 61\n",
            "value_choice: 0, 61, 61, 61, 61, 61, 61, 61, 61, 61\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 312/1000 --- L(Train): 0.0058953 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.199 1 + 0.136 value_reward_chosen[t] + -0.002 contr_diff + 0.471 reward + -0.035 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.35 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 62, 0, 8, 62, 0, 62, 62, 0\n",
            "value_reward_not_chosen: 0, 0, 62, 62, 62, 62\n",
            "value_choice: 0, 62, 62, 62, 62, 62, 62, 62, 62, 62\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 313/1000 --- L(Train): 0.0058934 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.2 1 + 0.135 value_reward_chosen[t] + -0.001 contr_diff + 0.471 reward + -0.033 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.35 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 63, 0, 9, 63, 0, 63, 63, 0\n",
            "value_reward_not_chosen: 0, 0, 63, 63, 63, 63\n",
            "value_choice: 0, 63, 63, 63, 63, 63, 63, 63, 63, 63\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 314/1000 --- L(Train): 0.0058905 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.2 1 + 0.135 value_reward_chosen[t] + 0.001 contr_diff + 0.471 reward + -0.031 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.349 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 64, 0, 10, 64, 0, 64, 64, 0\n",
            "value_reward_not_chosen: 0, 0, 64, 64, 64, 64\n",
            "value_choice: 0, 64, 64, 64, 64, 64, 64, 64, 64, 64\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 315/1000 --- L(Train): 0.0058888 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.2 1 + 0.134 value_reward_chosen[t] + 0.001 contr_diff + 0.472 reward + -0.029 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.349 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 65, 0, 11, 65, 0, 65, 65, 0\n",
            "value_reward_not_chosen: 0, 0, 65, 65, 65, 65\n",
            "value_choice: 0, 65, 65, 65, 65, 65, 65, 65, 65, 65\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 316/1000 --- L(Train): 0.0058867 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.201 1 + 0.133 value_reward_chosen[t] + 0.001 contr_diff + 0.472 reward + -0.027 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.349 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.003 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + -0.001 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 66, 0, 12, 66, 0, 66, 66, 0\n",
            "value_reward_not_chosen: 0, 0, 66, 66, 66, 66\n",
            "value_choice: 0, 66, 66, 66, 66, 66, 66, 66, 66, 66\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 317/1000 --- L(Train): 0.0058826 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.201 1 + 0.132 value_reward_chosen[t] + -0.001 contr_diff + 0.472 reward + -0.025 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.348 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.002 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 67, 0, 13, 67, 0, 67, 67, 0\n",
            "value_reward_not_chosen: 0, 0, 67, 67, 67, 67\n",
            "value_choice: 0, 67, 67, 67, 67, 67, 67, 67, 67, 67\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 318/1000 --- L(Train): 0.0058793 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.201 1 + 0.131 value_reward_chosen[t] + -0.001 contr_diff + 0.473 reward + -0.023 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.348 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 68, 0, 14, 68, 0, 68, 68, 0\n",
            "value_reward_not_chosen: 0, 0, 68, 68, 68, 68\n",
            "value_choice: 0, 68, 68, 68, 68, 68, 68, 68, 68, 68\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 319/1000 --- L(Train): 0.0058784 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.131 value_reward_chosen[t] + 0.001 contr_diff + 0.473 reward + -0.021 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.348 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 69, 0, 15, 69, 0, 69, 69, 0\n",
            "value_reward_not_chosen: 0, 0, 69, 69, 69, 69\n",
            "value_choice: 0, 69, 69, 69, 69, 69, 69, 69, 69, 69\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 320/1000 --- L(Train): 0.0058761 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.13 value_reward_chosen[t] + 0.001 contr_diff + 0.473 reward + -0.019 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.348 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 70, 0, 16, 70, 0, 70, 70, 0\n",
            "value_reward_not_chosen: 0, 0, 70, 70, 70, 70\n",
            "value_choice: 0, 70, 70, 70, 70, 70, 70, 70, 70, 70\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 321/1000 --- L(Train): 0.0058739 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.129 value_reward_chosen[t] + -0.0 contr_diff + 0.474 reward + -0.017 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.347 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.004 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 71, 0, 17, 71, 0, 71, 71, 0\n",
            "value_reward_not_chosen: 0, 0, 71, 71, 71, 71\n",
            "value_choice: 0, 71, 71, 71, 71, 71, 71, 71, 71, 71\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 322/1000 --- L(Train): 0.0058728 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.128 value_reward_chosen[t] + -0.0 contr_diff + 0.474 reward + -0.015 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.347 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.002 choice + -0.005 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.004 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 72, 0, 18, 72, 0, 72, 72, 0\n",
            "value_reward_not_chosen: 0, 0, 72, 72, 72, 72\n",
            "value_choice: 0, 72, 72, 72, 72, 72, 72, 72, 72, 72\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 323/1000 --- L(Train): 0.0058712 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.128 value_reward_chosen[t] + 0.001 contr_diff + 0.474 reward + -0.013 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.347 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.002 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 73, 0, 19, 73, 0, 73, 73, 0\n",
            "value_reward_not_chosen: 0, 0, 73, 73, 73, 73\n",
            "value_choice: 0, 73, 73, 73, 73, 73, 73, 73, 73, 73\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 324/1000 --- L(Train): 0.0058686 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.127 value_reward_chosen[t] + 0.002 contr_diff + 0.475 reward + -0.011 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.346 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.0 contr_diff + 0.002 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 74, 0, 20, 74, 0, 74, 74, 0\n",
            "value_reward_not_chosen: 0, 0, 74, 74, 74, 74\n",
            "value_choice: 0, 74, 74, 74, 74, 74, 74, 74, 74, 74\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 325/1000 --- L(Train): 0.0058667 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.126 value_reward_chosen[t] + 0.001 contr_diff + 0.475 reward + -0.009 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.346 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 75, 0, 21, 75, 0, 75, 75, 0\n",
            "value_reward_not_chosen: 0, 0, 75, 75, 75, 75\n",
            "value_choice: 0, 75, 75, 75, 75, 75, 75, 75, 75, 75\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 326/1000 --- L(Train): 0.0058626 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.125 value_reward_chosen[t] + -0.001 contr_diff + 0.476 reward + -0.008 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.346 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 76, 0, 22, 76, 0, 76, 76, 0\n",
            "value_reward_not_chosen: 0, 0, 76, 76, 76, 76\n",
            "value_choice: 0, 76, 76, 76, 76, 76, 76, 76, 76, 76\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 327/1000 --- L(Train): 0.0058605 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.125 value_reward_chosen[t] + -0.001 contr_diff + 0.476 reward + -0.006 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.346 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 77, 0, 23, 77, 0, 77, 77, 0\n",
            "value_reward_not_chosen: 0, 0, 77, 77, 77, 77\n",
            "value_choice: 0, 77, 77, 77, 77, 77, 77, 77, 77, 77\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 328/1000 --- L(Train): 0.0058594 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.124 value_reward_chosen[t] + -0.0 contr_diff + 0.476 reward + -0.004 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.345 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 78, 0, 24, 78, 0, 78, 78, 0\n",
            "value_reward_not_chosen: 0, 0, 78, 78, 78, 78\n",
            "value_choice: 0, 78, 78, 78, 78, 78, 78, 78, 78, 78\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 329/1000 --- L(Train): 0.0058581 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.123 value_reward_chosen[t] + 0.002 contr_diff + 0.477 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.345 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 79, 0, 25, 79, 0, 79, 79, 0\n",
            "value_reward_not_chosen: 0, 0, 79, 79, 79, 79\n",
            "value_choice: 0, 79, 79, 79, 79, 79, 79, 79, 79, 79\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 330/1000 --- L(Train): 0.0058547 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.122 value_reward_chosen[t] + 0.002 contr_diff + 0.477 reward + 0.0 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.345 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 80, 0, 26, 80, 0, 80, 80, 0\n",
            "value_reward_not_chosen: 0, 0, 80, 80, 80, 80\n",
            "value_choice: 0, 80, 80, 80, 80, 80, 80, 80, 80, 80\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 331/1000 --- L(Train): 0.0058521 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.121 value_reward_chosen[t] + 0.002 contr_diff + 0.477 reward + 0.001 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.344 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 81, 0, 27, 81, 0, 81, 81, 0\n",
            "value_reward_not_chosen: 0, 0, 81, 81, 81, 81\n",
            "value_choice: 0, 81, 81, 81, 81, 81, 81, 81, 81, 81\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 332/1000 --- L(Train): 0.0058498 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.121 value_reward_chosen[t] + 0.001 contr_diff + 0.478 reward + 0.002 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.344 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.003 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 82, 0, 28, 82, 0, 82, 82, 0\n",
            "value_reward_not_chosen: 0, 0, 82, 82, 82, 82\n",
            "value_choice: 0, 82, 82, 82, 82, 82, 82, 82, 82, 82\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 333/1000 --- L(Train): 0.0058491 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.12 value_reward_chosen[t] + -0.002 contr_diff + 0.478 reward + 0.002 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.344 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.003 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.0 contr_diff + 0.002 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 83, 0, 29, 83, 0, 83, 83, 0\n",
            "value_reward_not_chosen: 0, 0, 83, 83, 83, 83\n",
            "value_choice: 0, 83, 83, 83, 83, 83, 83, 83, 83, 83\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 334/1000 --- L(Train): 0.0058496 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.119 value_reward_chosen[t] + -0.002 contr_diff + 0.478 reward + 0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.344 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.002 choice + -0.005 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 84, 0, 30, 84, 0, 84, 84, 0\n",
            "value_reward_not_chosen: 0, 0, 84, 84, 84, 84\n",
            "value_choice: 0, 84, 84, 84, 84, 84, 84, 84, 84, 84\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 335/1000 --- L(Train): 0.0058472 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.119 value_reward_chosen[t] + -0.002 contr_diff + 0.479 reward + 0.0 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.343 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.0 contr_diff + 0.002 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 85, 0, 31, 85, 0, 85, 85, 0\n",
            "value_reward_not_chosen: 0, 0, 85, 85, 85, 85\n",
            "value_choice: 0, 85, 85, 85, 85, 85, 85, 85, 85, 85\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 336/1000 --- L(Train): 0.0058437 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.118 value_reward_chosen[t] + -0.0 contr_diff + 0.479 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.343 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 86, 0, 32, 86, 0, 86, 86, 0\n",
            "value_reward_not_chosen: 0, 0, 86, 86, 86, 86\n",
            "value_choice: 0, 86, 86, 86, 86, 86, 86, 86, 86, 86\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 337/1000 --- L(Train): 0.0058425 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.117 value_reward_chosen[t] + 0.002 contr_diff + 0.48 reward + -0.002 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.066 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.343 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 87, 0, 33, 87, 0, 87, 87, 0\n",
            "value_reward_not_chosen: 0, 0, 87, 87, 87, 87\n",
            "value_choice: 0, 87, 87, 87, 87, 87, 87, 87, 87, 87\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 338/1000 --- L(Train): 0.0058408 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.117 value_reward_chosen[t] + 0.004 contr_diff + 0.48 reward + -0.003 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.066 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.343 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 88, 0, 34, 88, 0, 88, 88, 0\n",
            "value_reward_not_chosen: 0, 0, 88, 88, 88, 88\n",
            "value_choice: 0, 88, 88, 88, 88, 88, 88, 88, 88, 88\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 339/1000 --- L(Train): 0.0058405 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.116 value_reward_chosen[t] + 0.004 contr_diff + 0.481 reward + -0.003 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.066 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.343 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 89, 0, 35, 89, 0, 89, 89, 0\n",
            "value_reward_not_chosen: 0, 0, 89, 89, 89, 89\n",
            "value_choice: 0, 89, 89, 89, 89, 89, 89, 89, 89, 89\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 340/1000 --- L(Train): 0.0058372 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.116 value_reward_chosen[t] + 0.003 contr_diff + 0.481 reward + -0.003 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.065 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.342 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.005 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 90, 0, 36, 90, 0, 90, 90, 0\n",
            "value_reward_not_chosen: 0, 0, 90, 90, 90, 90\n",
            "value_choice: 0, 90, 90, 90, 90, 90, 90, 90, 90, 90\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 341/1000 --- L(Train): 0.0058357 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.115 value_reward_chosen[t] + 0.001 contr_diff + 0.482 reward + -0.003 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.065 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.342 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.0 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 91, 0, 37, 91, 0, 91, 91, 0\n",
            "value_reward_not_chosen: 0, 0, 91, 91, 91, 91\n",
            "value_choice: 0, 91, 91, 91, 91, 91, 91, 91, 91, 91\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 342/1000 --- L(Train): 0.0058356 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.115 value_reward_chosen[t] + -0.002 contr_diff + 0.482 reward + -0.002 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.064 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.342 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 92, 0, 38, 92, 0, 92, 92, 0\n",
            "value_reward_not_chosen: 0, 0, 92, 92, 92, 92\n",
            "value_choice: 0, 92, 92, 92, 92, 92, 92, 92, 92, 92\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 343/1000 --- L(Train): 0.0058331 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.114 value_reward_chosen[t] + -0.003 contr_diff + 0.482 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.064 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.342 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 93, 0, 39, 93, 0, 93, 93, 0\n",
            "value_reward_not_chosen: 0, 0, 93, 93, 93, 93\n",
            "value_choice: 0, 93, 93, 93, 93, 93, 93, 93, 93, 93\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 344/1000 --- L(Train): 0.0058308 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.113 value_reward_chosen[t] + -0.003 contr_diff + 0.483 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.064 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.341 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 94, 0, 40, 94, 0, 94, 94, 0\n",
            "value_reward_not_chosen: 0, 0, 94, 94, 94, 94\n",
            "value_choice: 0, 94, 94, 94, 94, 94, 94, 94, 94, 94\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 345/1000 --- L(Train): 0.0058296 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.113 value_reward_chosen[t] + -0.002 contr_diff + 0.483 reward + -0.0 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.063 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.003 contr_diff*reward + 0.341 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.0 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 95, 0, 41, 95, 0, 95, 95, 0\n",
            "value_reward_not_chosen: 0, 0, 95, 95, 95, 95\n",
            "value_choice: 0, 95, 95, 95, 95, 95, 95, 95, 95, 95\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 346/1000 --- L(Train): 0.0058296 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.112 value_reward_chosen[t] + 0.0 contr_diff + 0.483 reward + 0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.063 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.341 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 96, 0, 42, 96, 0, 96, 96, 0\n",
            "value_reward_not_chosen: 0, 0, 96, 96, 96, 96\n",
            "value_choice: 0, 96, 96, 96, 96, 96, 96, 96, 96, 96\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 347/1000 --- L(Train): 0.0058271 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.112 value_reward_chosen[t] + 0.001 contr_diff + 0.484 reward + 0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.063 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.34 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.003 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 97, 0, 43, 97, 0, 97, 97, 0\n",
            "value_reward_not_chosen: 0, 0, 97, 97, 97, 97\n",
            "value_choice: 0, 97, 97, 97, 97, 97, 97, 97, 97, 97\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 348/1000 --- L(Train): 0.0058253 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.112 value_reward_chosen[t] + 0.001 contr_diff + 0.484 reward + 0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.062 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.34 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 98, 0, 44, 98, 0, 98, 98, 0\n",
            "value_reward_not_chosen: 0, 0, 98, 98, 98, 98\n",
            "value_choice: 0, 98, 98, 98, 98, 98, 98, 98, 98, 98\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 349/1000 --- L(Train): 0.0058233 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.111 value_reward_chosen[t] + 0.0 contr_diff + 0.484 reward + 0.0 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.062 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.339 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.005 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 99, 0, 45, 99, 0, 99, 99, 0\n",
            "value_reward_not_chosen: 0, 0, 99, 99, 99, 99\n",
            "value_choice: 0, 99, 99, 99, 99, 99, 99, 99, 99, 99\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 350/1000 --- L(Train): 0.0058197 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 25):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.111 value_reward_chosen[t] + -0.002 contr_diff + 0.484 reward + -0.001 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.061 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.339 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 100, 0, 46, 100, 0, 100, 100, 0\n",
            "value_reward_not_chosen: 0, 0, 100, 100, 100, 100\n",
            "value_choice: 0, 100, 100, 100, 100, 100, 100, -, 100, 100\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 351/1000 --- L(Train): 0.0058194 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 24):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.11 value_reward_chosen[t] + -0.003 contr_diff + 0.485 reward + -0.002 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.061 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.339 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.002 contr_diff + -0.0 choice + -0.005 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 101, 0, 47, 101, 0, 101, 101, 0\n",
            "value_reward_not_chosen: 0, 0, 101, 101, 101, 101\n",
            "value_choice: 0, 101, 101, 101, 101, 101, 101, -, 101, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 352/1000 --- L(Train): 0.0058183 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 23):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.11 value_reward_chosen[t] + -0.002 contr_diff + 0.485 reward + -0.002 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.06 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.001 contr_diff*reward + 0.338 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.002 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 102, 0, 48, 102, 0, 102, 102, 0\n",
            "value_reward_not_chosen: 0, 0, 102, -, 102, 102\n",
            "value_choice: 0, 102, 102, 102, 102, 102, 102, -, 102, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 353/1000 --- L(Train): 0.0058162 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 22):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.11 value_reward_chosen[t] + -0.0 contr_diff + 0.486 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.059 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.0 contr_diff*reward + 0.338 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 103, 0, 49, 103, 0, 103, 103, 0\n",
            "value_reward_not_chosen: 0, 0, 103, -, 103, 103\n",
            "value_choice: 0, 103, 103, 103, 103, 103, 103, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 354/1000 --- L(Train): 0.0058141 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 21):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.109 value_reward_chosen[t] + 0.002 contr_diff + 0.486 reward + -0.002 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.059 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.338 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 104, 0, 50, 104, 0, 104, 104, 0\n",
            "value_reward_not_chosen: 0, 0, 104, -, 104, 104\n",
            "value_choice: 0, 104, 104, 104, 104, 104, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 355/1000 --- L(Train): 0.0058116 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 20):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.109 value_reward_chosen[t] + 0.004 contr_diff + 0.486 reward + -0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.058 value_reward_chosen*reward + -0.002 contr_diff*reward + 0.338 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 105, 0, 51, 105, 0, -, 105, 0\n",
            "value_reward_not_chosen: 0, 0, 105, -, 105, 105\n",
            "value_choice: 0, 105, 105, 105, 105, 105, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 356/1000 --- L(Train): 0.0058098 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 19):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.109 value_reward_chosen[t] + 0.004 contr_diff + 0.487 reward + -0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.058 value_reward_chosen*reward + -0.001 contr_diff*reward + 0.337 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 106, 0, 52, 106, 0, -, 106, 0\n",
            "value_reward_not_chosen: 0, 0, 106, -, 106, 106\n",
            "value_choice: 0, 106, 106, 106, 106, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 357/1000 --- L(Train): 0.0058074 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 18):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.108 value_reward_chosen[t] + 0.003 contr_diff + 0.487 reward + 0.0 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.057 value_reward_chosen*reward + 0.337 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.0 contr_diff + 0.001 choice + -0.006 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 107, 0, 53, 107, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, 107, -, 107, 107\n",
            "value_choice: 0, 107, 107, 107, 107, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 358/1000 --- L(Train): 0.0058048 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 17):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.108 value_reward_chosen[t] + 0.001 contr_diff + 0.487 reward + 0.0 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.057 value_reward_chosen*reward + 0.336 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 108, 0, 54, 108, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, 108, -, -, 108\n",
            "value_choice: 0, 108, 108, 108, 108, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 359/1000 --- L(Train): 0.0058032 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.108 value_reward_chosen[t] + -0.002 contr_diff + 0.487 reward + 0.0 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.056 value_reward_chosen*reward + 0.336 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.0 choice + -0.006 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 109, 0, 55, 109, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, 109, -, -, 109\n",
            "value_choice: 0, 109, -, 109, 109, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 360/1000 --- L(Train): 0.0058015 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.107 value_reward_chosen[t] + -0.003 contr_diff + 0.488 reward + -0.001 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.056 value_reward_chosen*reward + 0.335 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.006 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 110, 0, 56, 110, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, 110, -, -, 110\n",
            "value_choice: 0, 110, -, -, 110, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 361/1000 --- L(Train): 0.0058010 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.107 value_reward_chosen[t] + -0.003 contr_diff + 0.488 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.056 value_reward_chosen*reward + 0.335 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.005 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 111, 0, 57, 111, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, 111, -, -, -\n",
            "value_choice: 0, 111, -, -, 111, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 362/1000 --- L(Train): 0.0057985 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.107 value_reward_chosen[t] + -0.002 contr_diff + 0.488 reward + -0.002 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.055 value_reward_chosen*reward + 0.335 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.005 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 112, 0, 58, 112, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, 112, -, -, 112, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 363/1000 --- L(Train): 0.0057960 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 12):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.107 value_reward_chosen[t] + 0.488 reward + -0.001 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.055 value_reward_chosen*reward + 0.334 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.005 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 59, 113, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, 113, -, -, 113, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 364/1000 --- L(Train): 0.0057939 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 11):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.106 value_reward_chosen[t] + 0.489 reward + -0.001 value_reward_chosen^2 + -0.054 value_reward_chosen*reward + 0.334 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.006 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 60, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, 114, -, -, 114, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 365/1000 --- L(Train): 0.0057973 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 10):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.106 value_reward_chosen[t] + 0.489 reward + -0.0 value_reward_chosen^2 + -0.054 value_reward_chosen*reward + 0.333 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 61, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, 115, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 366/1000 --- L(Train): 0.0058204 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 9):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.106 value_reward_chosen[t] + 0.489 reward + 0.0 value_reward_chosen^2 + -0.053 value_reward_chosen*reward + 0.333 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.141 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 62, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 367/1000 --- L(Train): 0.0059190 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.106 value_reward_chosen[t] + 0.49 reward + 0.001 value_reward_chosen^2 + -0.053 value_reward_chosen*reward + 0.333 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.138 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 63, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 368/1000 --- L(Train): 0.0060194 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.105 value_reward_chosen[t] + 0.49 reward + 0.0 value_reward_chosen^2 + -0.052 value_reward_chosen*reward + 0.332 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.133 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 64, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 369/1000 --- L(Train): 0.0060125 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.105 value_reward_chosen[t] + 0.49 reward + -0.001 value_reward_chosen^2 + -0.052 value_reward_chosen*reward + 0.332 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.128 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 65, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 370/1000 --- L(Train): 0.0060429 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.105 value_reward_chosen[t] + 0.49 reward + -0.001 value_reward_chosen^2 + -0.052 value_reward_chosen*reward + 0.331 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 66, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 371/1000 --- L(Train): 0.0060457 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.105 value_reward_chosen[t] + 0.491 reward + -0.001 value_reward_chosen^2 + -0.052 value_reward_chosen*reward + 0.331 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.117 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 67, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 372/1000 --- L(Train): 0.0060073 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.104 value_reward_chosen[t] + 0.491 reward + -0.001 value_reward_chosen^2 + -0.051 value_reward_chosen*reward + 0.331 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.114 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 68, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 373/1000 --- L(Train): 0.0059874 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.104 value_reward_chosen[t] + 0.491 reward + -0.001 value_reward_chosen^2 + -0.051 value_reward_chosen*reward + 0.33 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.111 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 69, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 374/1000 --- L(Train): 0.0059848 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.104 value_reward_chosen[t] + 0.492 reward + -0.0 value_reward_chosen^2 + -0.051 value_reward_chosen*reward + 0.33 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.111 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 70, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 375/1000 --- L(Train): 0.0059928 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.104 value_reward_chosen[t] + 0.492 reward + 0.0 value_reward_chosen^2 + -0.05 value_reward_chosen*reward + 0.329 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.111 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 71, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 376/1000 --- L(Train): 0.0060048 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.103 value_reward_chosen[t] + 0.492 reward + 0.001 value_reward_chosen^2 + -0.05 value_reward_chosen*reward + 0.329 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.113 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 72, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 377/1000 --- L(Train): 0.0060078 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.103 value_reward_chosen[t] + 0.492 reward + 0.0 value_reward_chosen^2 + -0.05 value_reward_chosen*reward + 0.328 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.116 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 73, -, 1, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 378/1000 --- L(Train): 0.0059990 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.103 value_reward_chosen[t] + 0.493 reward + -0.001 value_reward_chosen^2 + -0.05 value_reward_chosen*reward + 0.328 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.12 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 74, -, 2, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 379/1000 --- L(Train): 0.0059832 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.103 value_reward_chosen[t] + 0.493 reward + -0.001 value_reward_chosen^2 + -0.049 value_reward_chosen*reward + 0.328 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 75, -, 3, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 380/1000 --- L(Train): 0.0059668 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.102 value_reward_chosen[t] + 0.493 reward + -0.001 value_reward_chosen^2 + -0.049 value_reward_chosen*reward + 0.327 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.126 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 76, -, 4, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 381/1000 --- L(Train): 0.0059538 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.102 value_reward_chosen[t] + 0.494 reward + -0.001 value_reward_chosen^2 + -0.049 value_reward_chosen*reward + 0.327 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.129 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 77, -, 5, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 382/1000 --- L(Train): 0.0059438 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.102 value_reward_chosen[t] + 0.494 reward + -0.001 value_reward_chosen^2 + -0.049 value_reward_chosen*reward + 0.327 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.13 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 78, -, 6, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 383/1000 --- L(Train): 0.0059356 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.102 value_reward_chosen[t] + 0.494 reward + -0.0 value_reward_chosen^2 + -0.048 value_reward_chosen*reward + 0.326 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.13 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 79, -, 7, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 384/1000 --- L(Train): 0.0059348 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.102 value_reward_chosen[t] + 0.495 reward + 0.001 value_reward_chosen^2 + -0.048 value_reward_chosen*reward + 0.326 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.13 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 80, -, 8, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 385/1000 --- L(Train): 0.0059363 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.102 value_reward_chosen[t] + 0.495 reward + 0.001 value_reward_chosen^2 + -0.048 value_reward_chosen*reward + 0.325 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.129 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 81, -, 9, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 386/1000 --- L(Train): 0.0059377 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.101 value_reward_chosen[t] + 0.495 reward + 0.0 value_reward_chosen^2 + -0.048 value_reward_chosen*reward + 0.325 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.127 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 82, -, 10, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 387/1000 --- L(Train): 0.0059358 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.101 value_reward_chosen[t] + 0.496 reward + -0.001 value_reward_chosen^2 + -0.048 value_reward_chosen*reward + 0.325 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.125 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 83, -, 11, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 388/1000 --- L(Train): 0.0059313 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.101 value_reward_chosen[t] + 0.496 reward + -0.001 value_reward_chosen^2 + -0.047 value_reward_chosen*reward + 0.324 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 84, -, 12, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 389/1000 --- L(Train): 0.0059242 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.101 value_reward_chosen[t] + 0.496 reward + -0.001 value_reward_chosen^2 + -0.047 value_reward_chosen*reward + 0.324 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.12 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 85, -, 13, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 390/1000 --- L(Train): 0.0059182 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.101 value_reward_chosen[t] + 0.497 reward + -0.001 value_reward_chosen^2 + -0.047 value_reward_chosen*reward + 0.323 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.119 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 86, -, 14, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 391/1000 --- L(Train): 0.0059140 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.497 reward + -0.001 value_reward_chosen^2 + -0.047 value_reward_chosen*reward + 0.323 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.118 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 87, -, 15, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 392/1000 --- L(Train): 0.0059098 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.497 reward + -0.0 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.323 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.118 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 88, -, 16, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 393/1000 --- L(Train): 0.0059194 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.498 reward + 0.001 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.322 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.118 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 89, -, 17, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 394/1000 --- L(Train): 0.0059177 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.498 reward + 0.001 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.322 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.119 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 90, -, 18, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 395/1000 --- L(Train): 0.0059168 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.498 reward + 0.001 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.322 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.121 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 91, -, 19, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 396/1000 --- L(Train): 0.0059146 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.499 reward + -0.0 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.321 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 92, -, 20, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 397/1000 --- L(Train): 0.0059114 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.499 reward + -0.001 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.321 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 93, -, 21, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 398/1000 --- L(Train): 0.0059080 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.499 reward + -0.001 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.32 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 94, -, 22, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 399/1000 --- L(Train): 0.0059050 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.5 reward + -0.001 value_reward_chosen^2 + -0.045 value_reward_chosen*reward + 0.32 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.125 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 95, -, 23, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 400/1000 --- L(Train): 0.0059043 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.5 reward + -0.0 value_reward_chosen^2 + -0.045 value_reward_chosen*reward + 0.32 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.126 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 96, -, 24, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 401/1000 --- L(Train): 0.0059030 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.5 reward + 0.0 value_reward_chosen^2 + -0.045 value_reward_chosen*reward + 0.319 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.126 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 97, -, 25, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 402/1000 --- L(Train): 0.0059039 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.501 reward + 0.0 value_reward_chosen^2 + -0.045 value_reward_chosen*reward + 0.319 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.126 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 98, -, 26, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 403/1000 --- L(Train): 0.0059045 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.501 reward + -0.0 value_reward_chosen^2 + -0.045 value_reward_chosen*reward + 0.319 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.125 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 99, -, 27, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 404/1000 --- L(Train): 0.0059033 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.502 reward + -0.045 value_reward_chosen*reward + 0.318 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 28, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 405/1000 --- L(Train): 0.0059007 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.502 reward + -0.044 value_reward_chosen*reward + 0.318 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 29, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 406/1000 --- L(Train): 0.0059011 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.502 reward + -0.044 value_reward_chosen*reward + 0.318 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 30, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 407/1000 --- L(Train): 0.0058986 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.503 reward + -0.044 value_reward_chosen*reward + 0.317 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 31, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 408/1000 --- L(Train): 0.0058962 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.503 reward + -0.044 value_reward_chosen*reward + 0.317 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.121 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 32, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 409/1000 --- L(Train): 0.0058947 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.503 reward + -0.044 value_reward_chosen*reward + 0.316 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.121 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 33, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 410/1000 --- L(Train): 0.0058937 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.504 reward + -0.044 value_reward_chosen*reward + 0.316 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.121 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 34, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 411/1000 --- L(Train): 0.0058904 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.504 reward + -0.044 value_reward_chosen*reward + 0.316 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.121 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 35, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 412/1000 --- L(Train): 0.0058890 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.504 reward + -0.044 value_reward_chosen*reward + 0.315 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 36, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 413/1000 --- L(Train): 0.0058863 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.505 reward + -0.044 value_reward_chosen*reward + 0.315 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 37, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 414/1000 --- L(Train): 0.0058862 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.505 reward + -0.044 value_reward_chosen*reward + 0.315 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 38, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 415/1000 --- L(Train): 0.0058852 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.505 reward + -0.043 value_reward_chosen*reward + 0.314 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 39, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 416/1000 --- L(Train): 0.0058832 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.506 reward + -0.043 value_reward_chosen*reward + 0.314 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 40, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 417/1000 --- L(Train): 0.0058824 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.506 reward + -0.043 value_reward_chosen*reward + 0.313 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 41, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\u001b[H\u001b[2J\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 418/1000 --- L(Train): 0.0058830 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.507 reward + -0.043 value_reward_chosen*reward + 0.313 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 42, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 419/1000 --- L(Train): 0.0058803 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.507 reward + -0.043 value_reward_chosen*reward + 0.313 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 43, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 420/1000 --- L(Train): 0.0058790 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.507 reward + -0.043 value_reward_chosen*reward + 0.312 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 44, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 421/1000 --- L(Train): 0.0058801 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.508 reward + -0.043 value_reward_chosen*reward + 0.312 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 45, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 422/1000 --- L(Train): 0.0058795 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.508 reward + -0.043 value_reward_chosen*reward + 0.311 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 46, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 423/1000 --- L(Train): 0.0058778 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.508 reward + -0.043 value_reward_chosen*reward + 0.311 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 47, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 424/1000 --- L(Train): 0.0058780 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.509 reward + -0.043 value_reward_chosen*reward + 0.311 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 48, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 425/1000 --- L(Train): 0.0058778 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.509 reward + -0.043 value_reward_chosen*reward + 0.31 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 49, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 426/1000 --- L(Train): 0.0058767 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.509 reward + -0.042 value_reward_chosen*reward + 0.31 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 50, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 427/1000 --- L(Train): 0.0058755 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.097 value_reward_chosen[t] + 0.51 reward + -0.042 value_reward_chosen*reward + 0.31 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 51, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 428/1000 --- L(Train): 0.0058749 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.51 reward + -0.042 value_reward_chosen*reward + 0.309 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 52, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 429/1000 --- L(Train): 0.0058746 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.511 reward + -0.042 value_reward_chosen*reward + 0.309 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 53, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 430/1000 --- L(Train): 0.0058747 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.511 reward + -0.042 value_reward_chosen*reward + 0.308 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 54, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 431/1000 --- L(Train): 0.0058743 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.511 reward + -0.042 value_reward_chosen*reward + 0.308 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 55, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 432/1000 --- L(Train): 0.0058737 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.512 reward + -0.042 value_reward_chosen*reward + 0.308 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 56, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 433/1000 --- L(Train): 0.0058723 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.512 reward + -0.042 value_reward_chosen*reward + 0.307 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 57, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 434/1000 --- L(Train): 0.0058712 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.512 reward + -0.042 value_reward_chosen*reward + 0.307 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 58, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 435/1000 --- L(Train): 0.0058693 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.513 reward + -0.042 value_reward_chosen*reward + 0.306 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 59, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 436/1000 --- L(Train): 0.0058695 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.513 reward + -0.042 value_reward_chosen*reward + 0.306 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 60, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 437/1000 --- L(Train): 0.0058685 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.514 reward + -0.042 value_reward_chosen*reward + 0.306 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 61, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 438/1000 --- L(Train): 0.0058673 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.514 reward + -0.042 value_reward_chosen*reward + 0.305 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 62, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 439/1000 --- L(Train): 0.0058661 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.514 reward + -0.042 value_reward_chosen*reward + 0.305 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 63, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 440/1000 --- L(Train): 0.0058652 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.515 reward + -0.041 value_reward_chosen*reward + 0.305 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 64, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 441/1000 --- L(Train): 0.0058635 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.515 reward + -0.041 value_reward_chosen*reward + 0.304 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 65, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 442/1000 --- L(Train): 0.0058629 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.515 reward + -0.041 value_reward_chosen*reward + 0.304 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 66, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 443/1000 --- L(Train): 0.0058600 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.516 reward + -0.041 value_reward_chosen*reward + 0.303 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 67, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 444/1000 --- L(Train): 0.0058591 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.516 reward + -0.041 value_reward_chosen*reward + 0.303 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 68, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 445/1000 --- L(Train): 0.0058597 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.517 reward + -0.041 value_reward_chosen*reward + 0.303 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 69, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 446/1000 --- L(Train): 0.0058596 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.517 reward + -0.041 value_reward_chosen*reward + 0.302 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 70, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 447/1000 --- L(Train): 0.0058580 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.517 reward + -0.041 value_reward_chosen*reward + 0.302 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 71, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 448/1000 --- L(Train): 0.0058579 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.518 reward + -0.041 value_reward_chosen*reward + 0.301 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 72, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 449/1000 --- L(Train): 0.0058563 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.518 reward + -0.041 value_reward_chosen*reward + 0.301 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 73, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 450/1000 --- L(Train): 0.0058558 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.519 reward + -0.041 value_reward_chosen*reward + 0.301 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 74, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 451/1000 --- L(Train): 0.0058563 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.519 reward + -0.041 value_reward_chosen*reward + 0.3 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 75, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 452/1000 --- L(Train): 0.0058552 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.519 reward + -0.041 value_reward_chosen*reward + 0.3 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 76, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 453/1000 --- L(Train): 0.0058547 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.52 reward + -0.041 value_reward_chosen*reward + 0.299 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 77, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 454/1000 --- L(Train): 0.0058561 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.52 reward + -0.041 value_reward_chosen*reward + 0.299 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 78, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 455/1000 --- L(Train): 0.0058533 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.52 reward + -0.041 value_reward_chosen*reward + 0.299 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 79, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 456/1000 --- L(Train): 0.0058529 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.521 reward + -0.041 value_reward_chosen*reward + 0.298 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 80, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 457/1000 --- L(Train): 0.0058542 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.521 reward + -0.041 value_reward_chosen*reward + 0.298 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 81, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 458/1000 --- L(Train): 0.0058538 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.522 reward + -0.041 value_reward_chosen*reward + 0.297 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 82, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 459/1000 --- L(Train): 0.0058519 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.522 reward + -0.041 value_reward_chosen*reward + 0.297 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 83, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 460/1000 --- L(Train): 0.0058523 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.522 reward + -0.041 value_reward_chosen*reward + 0.297 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 84, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 461/1000 --- L(Train): 0.0058511 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.523 reward + -0.041 value_reward_chosen*reward + 0.296 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 85, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 462/1000 --- L(Train): 0.0058505 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.523 reward + -0.041 value_reward_chosen*reward + 0.296 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 86, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 463/1000 --- L(Train): 0.0058505 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.524 reward + -0.041 value_reward_chosen*reward + 0.295 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 87, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 464/1000 --- L(Train): 0.0058498 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.524 reward + -0.04 value_reward_chosen*reward + 0.295 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 88, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 465/1000 --- L(Train): 0.0058486 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.524 reward + -0.04 value_reward_chosen*reward + 0.295 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 89, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 466/1000 --- L(Train): 0.0058484 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.525 reward + -0.04 value_reward_chosen*reward + 0.294 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 90, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 467/1000 --- L(Train): 0.0058459 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.525 reward + -0.04 value_reward_chosen*reward + 0.294 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 91, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 468/1000 --- L(Train): 0.0058447 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.526 reward + -0.04 value_reward_chosen*reward + 0.293 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 92, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 469/1000 --- L(Train): 0.0058438 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.526 reward + -0.04 value_reward_chosen*reward + 0.293 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 93, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 470/1000 --- L(Train): 0.0058441 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.526 reward + -0.04 value_reward_chosen*reward + 0.293 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 94, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 471/1000 --- L(Train): 0.0058430 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.527 reward + -0.04 value_reward_chosen*reward + 0.292 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 95, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 472/1000 --- L(Train): 0.0058431 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.527 reward + -0.04 value_reward_chosen*reward + 0.292 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 96, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 473/1000 --- L(Train): 0.0058426 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.528 reward + -0.04 value_reward_chosen*reward + 0.291 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 97, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 474/1000 --- L(Train): 0.0058418 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.528 reward + -0.04 value_reward_chosen*reward + 0.291 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 98, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 475/1000 --- L(Train): 0.0058412 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.528 reward + -0.04 value_reward_chosen*reward + 0.291 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 99, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 476/1000 --- L(Train): 0.0058408 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.529 reward + 0.29 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 477/1000 --- L(Train): 0.0058414 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.094 value_reward_chosen[t] + 0.529 reward + 0.29 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 478/1000 --- L(Train): 0.0058411 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.094 value_reward_chosen[t] + 0.529 reward + 0.288 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 479/1000 --- L(Train): 0.0058394 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.209 1 + 0.093 value_reward_chosen[t] + 0.528 reward + 0.287 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 480/1000 --- L(Train): 0.0058387 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.212 1 + 0.093 value_reward_chosen[t] + 0.528 reward + 0.286 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nStarting training on {estimator.device}...\")\n",
        "print(\"=\" * 80)\n",
        "estimator.fit(dataset.xs, dataset.ys, dataset.xs, dataset.ys)\n",
        "# estimator.load_spice(args.model)\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Print example SPICE model for first participant\n",
        "print(\"\\nExample SPICE model (participant 0):\")\n",
        "print(\"-\" * 80)\n",
        "estimator.print_spice_model(participant_id=0)\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "estimator.load_spice(path_spice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRU for benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('../..')\n",
        "from weinhardt2025.benchmarking.benchmarking_gru import GRU, training, setup_agent_gru\n",
        "\n",
        "path_gru = '../../weinhardt2025/params/ganesh2024a/gru_ganesh2024a.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10000: L(Train): 0.7024821639060974; L(Test): 0.6310489177703857\n",
            "Epoch 2/10000: L(Train): 0.633144736289978; L(Test): 0.5667180418968201\n",
            "Epoch 3/10000: L(Train): 0.5679585933685303; L(Test): 0.5049673318862915\n",
            "Epoch 4/10000: L(Train): 0.4973967671394348; L(Test): 0.4549196660518646\n",
            "Epoch 5/10000: L(Train): 0.46094459295272827; L(Test): 0.43841224908828735\n",
            "Epoch 6/10000: L(Train): 0.44079843163490295; L(Test): 0.4531238377094269\n",
            "Epoch 7/10000: L(Train): 0.43541082739830017; L(Test): 0.46989983320236206\n",
            "Epoch 8/10000: L(Train): 0.4694954752922058; L(Test): 0.4722661077976227\n",
            "Epoch 9/10000: L(Train): 0.4716421365737915; L(Test): 0.46296119689941406\n",
            "Epoch 10/10000: L(Train): 0.4702780544757843; L(Test): 0.44972705841064453\n",
            "Epoch 11/10000: L(Train): 0.4453604519367218; L(Test): 0.43842417001724243\n",
            "Epoch 12/10000: L(Train): 0.4390490651130676; L(Test): 0.4312731623649597\n",
            "Epoch 13/10000: L(Train): 0.43314412236213684; L(Test): 0.4287739098072052\n",
            "Epoch 14/10000: L(Train): 0.4243510663509369; L(Test): 0.4297647178173065\n",
            "Epoch 15/10000: L(Train): 0.4327476918697357; L(Test): 0.43206825852394104\n",
            "Epoch 16/10000: L(Train): 0.4446254372596741; L(Test): 0.43344584107398987\n",
            "Epoch 17/10000: L(Train): 0.43633174896240234; L(Test): 0.4323686957359314\n",
            "Epoch 18/10000: L(Train): 0.43249955773353577; L(Test): 0.42922091484069824\n",
            "Epoch 19/10000: L(Train): 0.4329946041107178; L(Test): 0.4252740442752838\n",
            "Epoch 20/10000: L(Train): 0.4251154959201813; L(Test): 0.4216115176677704\n",
            "Epoch 21/10000: L(Train): 0.42388084530830383; L(Test): 0.41892948746681213\n",
            "Epoch 22/10000: L(Train): 0.41398516297340393; L(Test): 0.41765448451042175\n",
            "Epoch 23/10000: L(Train): 0.4169330894947052; L(Test): 0.417776882648468\n",
            "Epoch 24/10000: L(Train): 0.4120670557022095; L(Test): 0.4186638593673706\n",
            "Epoch 25/10000: L(Train): 0.4237697422504425; L(Test): 0.41958147287368774\n",
            "Epoch 26/10000: L(Train): 0.4093981981277466; L(Test): 0.41977861523628235\n",
            "Epoch 27/10000: L(Train): 0.43360692262649536; L(Test): 0.4188399910926819\n",
            "Epoch 28/10000: L(Train): 0.42327645421028137; L(Test): 0.41732481122016907\n",
            "Epoch 29/10000: L(Train): 0.41528230905532837; L(Test): 0.41589346528053284\n",
            "Epoch 30/10000: L(Train): 0.4153040051460266; L(Test): 0.41475608944892883\n",
            "Epoch 31/10000: L(Train): 0.4186977744102478; L(Test): 0.413880854845047\n",
            "Epoch 32/10000: L(Train): 0.4289022386074066; L(Test): 0.41373854875564575\n",
            "Epoch 33/10000: L(Train): 0.4145466685295105; L(Test): 0.4142815172672272\n",
            "Epoch 34/10000: L(Train): 0.41357696056365967; L(Test): 0.41490691900253296\n",
            "Epoch 35/10000: L(Train): 0.41755563020706177; L(Test): 0.41483643651008606\n",
            "Epoch 36/10000: L(Train): 0.4169156849384308; L(Test): 0.4142904579639435\n",
            "Epoch 37/10000: L(Train): 0.41612669825553894; L(Test): 0.4135891795158386\n",
            "Epoch 38/10000: L(Train): 0.4160141050815582; L(Test): 0.4128483235836029\n",
            "Epoch 39/10000: L(Train): 0.41577574610710144; L(Test): 0.41231900453567505\n",
            "Epoch 40/10000: L(Train): 0.4131613075733185; L(Test): 0.4120883345603943\n",
            "Epoch 41/10000: L(Train): 0.41633206605911255; L(Test): 0.412261039018631\n",
            "Epoch 42/10000: L(Train): 0.41924095153808594; L(Test): 0.41237735748291016\n",
            "Epoch 43/10000: L(Train): 0.4155784845352173; L(Test): 0.4122113287448883\n",
            "Epoch 44/10000: L(Train): 0.4313792884349823; L(Test): 0.41175729036331177\n",
            "Epoch 45/10000: L(Train): 0.400645911693573; L(Test): 0.4113536775112152\n",
            "Epoch 46/10000: L(Train): 0.42389822006225586; L(Test): 0.410866379737854\n",
            "Epoch 47/10000: L(Train): 0.41169679164886475; L(Test): 0.4104674458503723\n",
            "Epoch 48/10000: L(Train): 0.4043372571468353; L(Test): 0.41036152839660645\n",
            "Epoch 49/10000: L(Train): 0.4129469394683838; L(Test): 0.4104931056499481\n",
            "Epoch 50/10000: L(Train): 0.4121848940849304; L(Test): 0.41024473309516907\n",
            "Epoch 51/10000: L(Train): 0.41257959604263306; L(Test): 0.4098406136035919\n",
            "Epoch 52/10000: L(Train): 0.4132137596607208; L(Test): 0.409904420375824\n",
            "Epoch 53/10000: L(Train): 0.41583678126335144; L(Test): 0.4098816215991974\n",
            "Epoch 54/10000: L(Train): 0.41144099831581116; L(Test): 0.4093434512615204\n",
            "Epoch 55/10000: L(Train): 0.4096020460128784; L(Test): 0.40893954038619995\n",
            "Epoch 56/10000: L(Train): 0.41201797127723694; L(Test): 0.40902888774871826\n",
            "Epoch 57/10000: L(Train): 0.4160138964653015; L(Test): 0.40927523374557495\n",
            "Epoch 58/10000: L(Train): 0.4071137011051178; L(Test): 0.4088117182254791\n",
            "Epoch 59/10000: L(Train): 0.4086897075176239; L(Test): 0.4083133339881897\n",
            "Epoch 60/10000: L(Train): 0.40961501002311707; L(Test): 0.4082607328891754\n",
            "Epoch 61/10000: L(Train): 0.40091732144355774; L(Test): 0.40802815556526184\n",
            "Epoch 62/10000: L(Train): 0.41043809056282043; L(Test): 0.4076629877090454\n",
            "Epoch 63/10000: L(Train): 0.40420225262641907; L(Test): 0.4074849784374237\n",
            "Epoch 64/10000: L(Train): 0.4149913191795349; L(Test): 0.407483845949173\n",
            "Epoch 65/10000: L(Train): 0.41725417971611023; L(Test): 0.40715739130973816\n",
            "Epoch 66/10000: L(Train): 0.41254985332489014; L(Test): 0.4069417715072632\n",
            "Epoch 67/10000: L(Train): 0.40880706906318665; L(Test): 0.40689605474472046\n",
            "Epoch 68/10000: L(Train): 0.4158923029899597; L(Test): 0.4066430330276489\n",
            "Epoch 69/10000: L(Train): 0.4181744158267975; L(Test): 0.4063802659511566\n",
            "Epoch 70/10000: L(Train): 0.39734673500061035; L(Test): 0.40673205256462097\n",
            "Epoch 71/10000: L(Train): 0.4163687527179718; L(Test): 0.4065009653568268\n",
            "Epoch 72/10000: L(Train): 0.40254664421081543; L(Test): 0.40591540932655334\n",
            "Epoch 73/10000: L(Train): 0.413196325302124; L(Test): 0.4058796167373657\n",
            "Epoch 74/10000: L(Train): 0.41973963379859924; L(Test): 0.4060578942298889\n",
            "Epoch 75/10000: L(Train): 0.41366061568260193; L(Test): 0.4054092764854431\n",
            "Epoch 76/10000: L(Train): 0.401714026927948; L(Test): 0.40525004267692566\n",
            "Epoch 77/10000: L(Train): 0.4089162051677704; L(Test): 0.4058634340763092\n",
            "Epoch 78/10000: L(Train): 0.4118628203868866; L(Test): 0.40563392639160156\n",
            "Epoch 79/10000: L(Train): 0.41755571961402893; L(Test): 0.40470850467681885\n",
            "Epoch 80/10000: L(Train): 0.4017811119556427; L(Test): 0.40506452322006226\n",
            "Epoch 81/10000: L(Train): 0.40915438532829285; L(Test): 0.40535297989845276\n",
            "Epoch 82/10000: L(Train): 0.4101635813713074; L(Test): 0.4044402539730072\n",
            "Epoch 83/10000: L(Train): 0.4055958092212677; L(Test): 0.4040326774120331\n",
            "Epoch 84/10000: L(Train): 0.409363716840744; L(Test): 0.4041839838027954\n",
            "Epoch 85/10000: L(Train): 0.4070080518722534; L(Test): 0.403927206993103\n",
            "Epoch 86/10000: L(Train): 0.3998628556728363; L(Test): 0.4036148190498352\n",
            "Epoch 87/10000: L(Train): 0.3984316289424896; L(Test): 0.40344133973121643\n",
            "Epoch 88/10000: L(Train): 0.41296514868736267; L(Test): 0.40332990884780884\n",
            "Epoch 89/10000: L(Train): 0.4032275974750519; L(Test): 0.40312445163726807\n",
            "Epoch 90/10000: L(Train): 0.4027971625328064; L(Test): 0.40286344289779663\n",
            "Epoch 91/10000: L(Train): 0.40659603476524353; L(Test): 0.4027445912361145\n",
            "Epoch 92/10000: L(Train): 0.4151914417743683; L(Test): 0.4028799533843994\n",
            "Epoch 93/10000: L(Train): 0.39774268865585327; L(Test): 0.4026508033275604\n",
            "Epoch 94/10000: L(Train): 0.4031469225883484; L(Test): 0.40222617983818054\n",
            "Epoch 95/10000: L(Train): 0.4026522934436798; L(Test): 0.40216684341430664\n",
            "Epoch 96/10000: L(Train): 0.41320958733558655; L(Test): 0.4020345211029053\n",
            "Epoch 97/10000: L(Train): 0.4039013981819153; L(Test): 0.4017638564109802\n",
            "Epoch 98/10000: L(Train): 0.3990069329738617; L(Test): 0.40171509981155396\n",
            "Epoch 99/10000: L(Train): 0.39726686477661133; L(Test): 0.4015325903892517\n",
            "Epoch 100/10000: L(Train): 0.41612958908081055; L(Test): 0.40135645866394043\n",
            "Epoch 101/10000: L(Train): 0.41132837533950806; L(Test): 0.40113702416419983\n",
            "Epoch 102/10000: L(Train): 0.40372923016548157; L(Test): 0.4008367657661438\n",
            "Epoch 103/10000: L(Train): 0.3991623520851135; L(Test): 0.40071484446525574\n",
            "Epoch 104/10000: L(Train): 0.4052065908908844; L(Test): 0.40065282583236694\n",
            "Epoch 105/10000: L(Train): 0.4034045338630676; L(Test): 0.40038955211639404\n",
            "Epoch 106/10000: L(Train): 0.3998594880104065; L(Test): 0.40020108222961426\n",
            "Epoch 107/10000: L(Train): 0.4086076021194458; L(Test): 0.4000617265701294\n",
            "Epoch 108/10000: L(Train): 0.3994866609573364; L(Test): 0.3999652862548828\n",
            "Epoch 109/10000: L(Train): 0.40695303678512573; L(Test): 0.3999102711677551\n",
            "Epoch 110/10000: L(Train): 0.4046781063079834; L(Test): 0.3998267352581024\n",
            "Epoch 111/10000: L(Train): 0.40603095293045044; L(Test): 0.3997753858566284\n",
            "Epoch 112/10000: L(Train): 0.40458470582962036; L(Test): 0.3996073007583618\n",
            "Epoch 113/10000: L(Train): 0.4136664569377899; L(Test): 0.399427205324173\n",
            "Epoch 114/10000: L(Train): 0.40080246329307556; L(Test): 0.3993948996067047\n",
            "Epoch 115/10000: L(Train): 0.40317076444625854; L(Test): 0.3992425799369812\n",
            "Epoch 116/10000: L(Train): 0.39596229791641235; L(Test): 0.3990981876850128\n",
            "Epoch 117/10000: L(Train): 0.3988284766674042; L(Test): 0.39920952916145325\n",
            "Epoch 118/10000: L(Train): 0.40390482544898987; L(Test): 0.39916372299194336\n",
            "Epoch 119/10000: L(Train): 0.40234842896461487; L(Test): 0.39901140332221985\n",
            "Epoch 120/10000: L(Train): 0.4009672999382019; L(Test): 0.39878198504447937\n",
            "Epoch 121/10000: L(Train): 0.40936902165412903; L(Test): 0.3987162411212921\n",
            "Epoch 122/10000: L(Train): 0.4039396643638611; L(Test): 0.39859431982040405\n",
            "Epoch 123/10000: L(Train): 0.40321388840675354; L(Test): 0.3985372483730316\n",
            "Epoch 124/10000: L(Train): 0.3994900584220886; L(Test): 0.3984133005142212\n",
            "Epoch 125/10000: L(Train): 0.40257805585861206; L(Test): 0.39831021428108215\n",
            "Epoch 126/10000: L(Train): 0.3935847878456116; L(Test): 0.3982754349708557\n",
            "Epoch 127/10000: L(Train): 0.40872228145599365; L(Test): 0.39831051230430603\n",
            "Epoch 128/10000: L(Train): 0.3996182680130005; L(Test): 0.3984631597995758\n",
            "Epoch 129/10000: L(Train): 0.4000589847564697; L(Test): 0.39826449751853943\n",
            "Epoch 130/10000: L(Train): 0.40202438831329346; L(Test): 0.3979879915714264\n",
            "Epoch 131/10000: L(Train): 0.4009547531604767; L(Test): 0.3979276716709137\n",
            "Epoch 132/10000: L(Train): 0.3955360949039459; L(Test): 0.39780393242836\n",
            "Epoch 133/10000: L(Train): 0.38232484459877014; L(Test): 0.3977179229259491\n",
            "Epoch 134/10000: L(Train): 0.4049590229988098; L(Test): 0.39759647846221924\n",
            "Epoch 135/10000: L(Train): 0.3940168023109436; L(Test): 0.39744803309440613\n",
            "Epoch 136/10000: L(Train): 0.3997168242931366; L(Test): 0.3973790109157562\n",
            "Epoch 137/10000: L(Train): 0.39649251103401184; L(Test): 0.39729389548301697\n",
            "Epoch 138/10000: L(Train): 0.4005109667778015; L(Test): 0.3972642421722412\n",
            "Epoch 139/10000: L(Train): 0.4104365408420563; L(Test): 0.39706552028656006\n",
            "Epoch 140/10000: L(Train): 0.40464383363723755; L(Test): 0.3969990015029907\n",
            "Epoch 141/10000: L(Train): 0.4070431590080261; L(Test): 0.3969126045703888\n",
            "Epoch 142/10000: L(Train): 0.40588855743408203; L(Test): 0.39690732955932617\n",
            "Epoch 143/10000: L(Train): 0.3990497887134552; L(Test): 0.3968620300292969\n",
            "Epoch 144/10000: L(Train): 0.39704635739326477; L(Test): 0.396696001291275\n",
            "Epoch 145/10000: L(Train): 0.404754638671875; L(Test): 0.39662444591522217\n",
            "Epoch 146/10000: L(Train): 0.3994235694408417; L(Test): 0.3965787887573242\n",
            "Epoch 147/10000: L(Train): 0.39893850684165955; L(Test): 0.39650437235832214\n",
            "Epoch 148/10000: L(Train): 0.3936232924461365; L(Test): 0.3965148627758026\n",
            "Epoch 149/10000: L(Train): 0.40318602323532104; L(Test): 0.39639341831207275\n",
            "Epoch 150/10000: L(Train): 0.4071866571903229; L(Test): 0.39630961418151855\n",
            "Epoch 151/10000: L(Train): 0.3952461779117584; L(Test): 0.3963196277618408\n",
            "Epoch 152/10000: L(Train): 0.3881815969944; L(Test): 0.3962247669696808\n",
            "Epoch 153/10000: L(Train): 0.40586724877357483; L(Test): 0.3962833881378174\n",
            "Epoch 154/10000: L(Train): 0.4082304835319519; L(Test): 0.39604341983795166\n",
            "Epoch 155/10000: L(Train): 0.40021657943725586; L(Test): 0.39597630500793457\n",
            "Epoch 156/10000: L(Train): 0.3983634114265442; L(Test): 0.3959140479564667\n",
            "Epoch 157/10000: L(Train): 0.4132537543773651; L(Test): 0.3960055410861969\n",
            "Epoch 158/10000: L(Train): 0.41299596428871155; L(Test): 0.3958340287208557\n",
            "Epoch 159/10000: L(Train): 0.39064711332321167; L(Test): 0.39595332741737366\n",
            "Epoch 160/10000: L(Train): 0.3958738446235657; L(Test): 0.39602556824684143\n",
            "Epoch 161/10000: L(Train): 0.4109600782394409; L(Test): 0.3957039713859558\n",
            "Epoch 162/10000: L(Train): 0.4022384583950043; L(Test): 0.3957695960998535\n",
            "Epoch 163/10000: L(Train): 0.4020403325557709; L(Test): 0.3955373167991638\n",
            "Epoch 164/10000: L(Train): 0.3796038031578064; L(Test): 0.39548006653785706\n",
            "Epoch 165/10000: L(Train): 0.40579894185066223; L(Test): 0.3953375816345215\n",
            "Epoch 166/10000: L(Train): 0.3952007591724396; L(Test): 0.39521947503089905\n",
            "Epoch 167/10000: L(Train): 0.399069219827652; L(Test): 0.3951706886291504\n",
            "Epoch 168/10000: L(Train): 0.38225337862968445; L(Test): 0.39521074295043945\n",
            "Epoch 169/10000: L(Train): 0.37741610407829285; L(Test): 0.39526596665382385\n",
            "Epoch 170/10000: L(Train): 0.4079526662826538; L(Test): 0.39555373787879944\n",
            "Epoch 171/10000: L(Train): 0.40189921855926514; L(Test): 0.395103394985199\n",
            "Epoch 172/10000: L(Train): 0.3944326341152191; L(Test): 0.39495742321014404\n",
            "Epoch 173/10000: L(Train): 0.39812904596328735; L(Test): 0.39523881673812866\n",
            "Epoch 174/10000: L(Train): 0.39831671118736267; L(Test): 0.3947221338748932\n",
            "Epoch 175/10000: L(Train): 0.3985344171524048; L(Test): 0.39502790570259094\n",
            "Epoch 176/10000: L(Train): 0.39644646644592285; L(Test): 0.3947400748729706\n",
            "Epoch 177/10000: L(Train): 0.38835546374320984; L(Test): 0.39498814940452576\n",
            "Epoch 178/10000: L(Train): 0.39690089225769043; L(Test): 0.3950633108615875\n",
            "Epoch 179/10000: L(Train): 0.3933737874031067; L(Test): 0.39454010128974915\n",
            "Epoch 180/10000: L(Train): 0.4032719135284424; L(Test): 0.3949678838253021\n",
            "Epoch 181/10000: L(Train): 0.38961052894592285; L(Test): 0.39470118284225464\n",
            "Epoch 182/10000: L(Train): 0.39659926295280457; L(Test): 0.39445117115974426\n",
            "Epoch 183/10000: L(Train): 0.40326353907585144; L(Test): 0.3949081599712372\n",
            "Epoch 184/10000: L(Train): 0.4064525365829468; L(Test): 0.39419785141944885\n",
            "Epoch 185/10000: L(Train): 0.39655035734176636; L(Test): 0.3944931924343109\n",
            "Epoch 186/10000: L(Train): 0.40520724654197693; L(Test): 0.3945252001285553\n",
            "Epoch 187/10000: L(Train): 0.39620736241340637; L(Test): 0.39397358894348145\n",
            "Epoch 188/10000: L(Train): 0.3930230140686035; L(Test): 0.394284725189209\n",
            "Epoch 189/10000: L(Train): 0.3964425325393677; L(Test): 0.39408206939697266\n",
            "Epoch 190/10000: L(Train): 0.394192099571228; L(Test): 0.3940618336200714\n",
            "Epoch 191/10000: L(Train): 0.39759138226509094; L(Test): 0.39418140053749084\n",
            "Epoch 192/10000: L(Train): 0.3824256658554077; L(Test): 0.39389684796333313\n",
            "Epoch 193/10000: L(Train): 0.39225295186042786; L(Test): 0.3937002420425415\n",
            "Epoch 194/10000: L(Train): 0.3887067139148712; L(Test): 0.39379608631134033\n",
            "Epoch 195/10000: L(Train): 0.3916932940483093; L(Test): 0.3935821056365967\n",
            "Epoch 196/10000: L(Train): 0.39968764781951904; L(Test): 0.39341798424720764\n",
            "Epoch 197/10000: L(Train): 0.40278857946395874; L(Test): 0.3934178352355957\n",
            "Epoch 198/10000: L(Train): 0.4008113741874695; L(Test): 0.3933884799480438\n",
            "Epoch 199/10000: L(Train): 0.3968237340450287; L(Test): 0.3932350277900696\n",
            "Epoch 200/10000: L(Train): 0.39584553241729736; L(Test): 0.3935455083847046\n",
            "Epoch 201/10000: L(Train): 0.3931790888309479; L(Test): 0.3934441804885864\n",
            "Epoch 202/10000: L(Train): 0.4043349325656891; L(Test): 0.3932020664215088\n",
            "Epoch 203/10000: L(Train): 0.38705316185951233; L(Test): 0.3930682837963104\n",
            "Epoch 204/10000: L(Train): 0.39085137844085693; L(Test): 0.39301109313964844\n",
            "Epoch 205/10000: L(Train): 0.4060443639755249; L(Test): 0.3929591774940491\n",
            "Epoch 206/10000: L(Train): 0.40412434935569763; L(Test): 0.3928532302379608\n",
            "Epoch 207/10000: L(Train): 0.39185941219329834; L(Test): 0.3928893506526947\n",
            "Epoch 208/10000: L(Train): 0.40471288561820984; L(Test): 0.39292463660240173\n",
            "Epoch 209/10000: L(Train): 0.38791942596435547; L(Test): 0.39290669560432434\n",
            "Epoch 210/10000: L(Train): 0.3968576490879059; L(Test): 0.39274704456329346\n",
            "Epoch 211/10000: L(Train): 0.3900435268878937; L(Test): 0.39276137948036194\n",
            "Epoch 212/10000: L(Train): 0.396298348903656; L(Test): 0.3928012251853943\n",
            "Epoch 213/10000: L(Train): 0.3899407386779785; L(Test): 0.3925749361515045\n",
            "Epoch 214/10000: L(Train): 0.4056619703769684; L(Test): 0.39266589283943176\n",
            "Epoch 215/10000: L(Train): 0.39434289932250977; L(Test): 0.3925139904022217\n",
            "Epoch 216/10000: L(Train): 0.3906106650829315; L(Test): 0.3925154209136963\n",
            "Epoch 217/10000: L(Train): 0.3971295952796936; L(Test): 0.3928130865097046\n",
            "Epoch 218/10000: L(Train): 0.3893064856529236; L(Test): 0.39233314990997314\n",
            "Epoch 219/10000: L(Train): 0.4006034731864929; L(Test): 0.3923278748989105\n",
            "Epoch 220/10000: L(Train): 0.40329310297966003; L(Test): 0.3924735486507416\n",
            "Epoch 221/10000: L(Train): 0.404781699180603; L(Test): 0.3921085596084595\n",
            "Epoch 222/10000: L(Train): 0.3883565366268158; L(Test): 0.39190295338630676\n",
            "Epoch 223/10000: L(Train): 0.3998652398586273; L(Test): 0.3919737935066223\n",
            "Epoch 224/10000: L(Train): 0.3968447148799896; L(Test): 0.39198556542396545\n",
            "Epoch 225/10000: L(Train): 0.4038519263267517; L(Test): 0.3918406069278717\n",
            "Epoch 226/10000: L(Train): 0.3901807367801666; L(Test): 0.391683429479599\n",
            "Epoch 227/10000: L(Train): 0.3952651023864746; L(Test): 0.3917449116706848\n",
            "Epoch 228/10000: L(Train): 0.39138948917388916; L(Test): 0.3916621208190918\n",
            "Epoch 229/10000: L(Train): 0.401884526014328; L(Test): 0.3914240300655365\n",
            "Epoch 230/10000: L(Train): 0.4002417027950287; L(Test): 0.3915191888809204\n",
            "Epoch 231/10000: L(Train): 0.40886425971984863; L(Test): 0.3917503356933594\n",
            "Epoch 232/10000: L(Train): 0.40138688683509827; L(Test): 0.3915903866291046\n",
            "Epoch 233/10000: L(Train): 0.39827296137809753; L(Test): 0.39128029346466064\n",
            "Epoch 234/10000: L(Train): 0.4002093970775604; L(Test): 0.3911893963813782\n",
            "Epoch 235/10000: L(Train): 0.3873959183692932; L(Test): 0.39149266481399536\n",
            "Epoch 236/10000: L(Train): 0.39243045449256897; L(Test): 0.39140745997428894\n",
            "Epoch 237/10000: L(Train): 0.3879872262477875; L(Test): 0.39100366830825806\n",
            "Epoch 238/10000: L(Train): 0.3887938857078552; L(Test): 0.3909253776073456\n",
            "Epoch 239/10000: L(Train): 0.3924020230770111; L(Test): 0.3909947872161865\n",
            "Epoch 240/10000: L(Train): 0.3923404812812805; L(Test): 0.39068546891212463\n",
            "Epoch 241/10000: L(Train): 0.39573395252227783; L(Test): 0.39060279726982117\n",
            "Epoch 242/10000: L(Train): 0.39473241567611694; L(Test): 0.3907011151313782\n",
            "Epoch 243/10000: L(Train): 0.39531731605529785; L(Test): 0.39071959257125854\n",
            "Epoch 244/10000: L(Train): 0.3990325927734375; L(Test): 0.39068692922592163\n",
            "Epoch 245/10000: L(Train): 0.39194318652153015; L(Test): 0.39034008979797363\n",
            "Epoch 246/10000: L(Train): 0.3892529308795929; L(Test): 0.3903862535953522\n",
            "Epoch 247/10000: L(Train): 0.39680761098861694; L(Test): 0.3902691900730133\n",
            "Epoch 248/10000: L(Train): 0.40341436862945557; L(Test): 0.3900868892669678\n",
            "Epoch 249/10000: L(Train): 0.3913249969482422; L(Test): 0.3899509608745575\n",
            "Epoch 250/10000: L(Train): 0.39177393913269043; L(Test): 0.39022159576416016\n",
            "Epoch 251/10000: L(Train): 0.3894646465778351; L(Test): 0.3902865946292877\n",
            "Epoch 252/10000: L(Train): 0.3866840600967407; L(Test): 0.3899209797382355\n",
            "Epoch 253/10000: L(Train): 0.4007226824760437; L(Test): 0.38978928327560425\n",
            "Epoch 254/10000: L(Train): 0.3994356095790863; L(Test): 0.3899897634983063\n",
            "Epoch 255/10000: L(Train): 0.39103373885154724; L(Test): 0.38994643092155457\n",
            "Epoch 256/10000: L(Train): 0.3963906168937683; L(Test): 0.39001569151878357\n",
            "Epoch 257/10000: L(Train): 0.38108476996421814; L(Test): 0.38997289538383484\n",
            "Epoch 258/10000: L(Train): 0.39465731382369995; L(Test): 0.38952919840812683\n",
            "Epoch 259/10000: L(Train): 0.40177762508392334; L(Test): 0.38921689987182617\n",
            "Epoch 260/10000: L(Train): 0.3917785882949829; L(Test): 0.3890945315361023\n",
            "Epoch 261/10000: L(Train): 0.3880632221698761; L(Test): 0.38904789090156555\n",
            "Epoch 262/10000: L(Train): 0.3951859474182129; L(Test): 0.3888992667198181\n",
            "Epoch 263/10000: L(Train): 0.39057478308677673; L(Test): 0.3889773488044739\n",
            "Epoch 264/10000: L(Train): 0.38347986340522766; L(Test): 0.3891908824443817\n",
            "Epoch 265/10000: L(Train): 0.39131471514701843; L(Test): 0.38910478353500366\n",
            "Epoch 266/10000: L(Train): 0.40140560269355774; L(Test): 0.38859087228775024\n",
            "Epoch 267/10000: L(Train): 0.39813119173049927; L(Test): 0.38867709040641785\n",
            "Epoch 268/10000: L(Train): 0.39350464940071106; L(Test): 0.38899192214012146\n",
            "Epoch 269/10000: L(Train): 0.4061753749847412; L(Test): 0.38833123445510864\n",
            "Epoch 270/10000: L(Train): 0.3854813277721405; L(Test): 0.3882291316986084\n",
            "Epoch 271/10000: L(Train): 0.38776862621307373; L(Test): 0.3887051045894623\n",
            "Epoch 272/10000: L(Train): 0.3995611071586609; L(Test): 0.3882656991481781\n",
            "Epoch 273/10000: L(Train): 0.3876129388809204; L(Test): 0.3879176080226898\n",
            "Epoch 274/10000: L(Train): 0.3936879336833954; L(Test): 0.3881653845310211\n",
            "Epoch 275/10000: L(Train): 0.3909791111946106; L(Test): 0.38863062858581543\n",
            "Epoch 276/10000: L(Train): 0.3886930048465729; L(Test): 0.3876534402370453\n",
            "Epoch 277/10000: L(Train): 0.39036116003990173; L(Test): 0.38798269629478455\n",
            "Epoch 278/10000: L(Train): 0.3894222676753998; L(Test): 0.3880479335784912\n",
            "Epoch 279/10000: L(Train): 0.3905707597732544; L(Test): 0.38739660382270813\n",
            "Epoch 280/10000: L(Train): 0.4026137590408325; L(Test): 0.387536883354187\n",
            "Epoch 281/10000: L(Train): 0.38999274373054504; L(Test): 0.3874934911727905\n",
            "Epoch 282/10000: L(Train): 0.39649698138237; L(Test): 0.3873007893562317\n",
            "Epoch 283/10000: L(Train): 0.38375064730644226; L(Test): 0.3868260383605957\n",
            "Epoch 284/10000: L(Train): 0.3782404363155365; L(Test): 0.3872787356376648\n",
            "Epoch 285/10000: L(Train): 0.3922561705112457; L(Test): 0.38717958331108093\n",
            "Epoch 286/10000: L(Train): 0.38408225774765015; L(Test): 0.38681623339653015\n",
            "Epoch 287/10000: L(Train): 0.39131006598472595; L(Test): 0.3866017460823059\n",
            "Epoch 288/10000: L(Train): 0.394937127828598; L(Test): 0.3868713974952698\n",
            "Epoch 289/10000: L(Train): 0.390600323677063; L(Test): 0.38643231987953186\n",
            "Epoch 290/10000: L(Train): 0.3806713819503784; L(Test): 0.3859717547893524\n",
            "Epoch 291/10000: L(Train): 0.3931671679019928; L(Test): 0.38721147179603577\n",
            "Epoch 292/10000: L(Train): 0.38634729385375977; L(Test): 0.38696932792663574\n",
            "Epoch 293/10000: L(Train): 0.396074503660202; L(Test): 0.3861279785633087\n",
            "Epoch 294/10000: L(Train): 0.37767088413238525; L(Test): 0.3870920240879059\n",
            "Epoch 295/10000: L(Train): 0.3889085054397583; L(Test): 0.3862091302871704\n",
            "Epoch 296/10000: L(Train): 0.3747130036354065; L(Test): 0.38636514544487\n",
            "Epoch 297/10000: L(Train): 0.38784146308898926; L(Test): 0.38618096709251404\n",
            "Epoch 298/10000: L(Train): 0.39123624563217163; L(Test): 0.3859851062297821\n",
            "Epoch 299/10000: L(Train): 0.38189852237701416; L(Test): 0.3859979212284088\n",
            "Epoch 300/10000: L(Train): 0.39048340916633606; L(Test): 0.3852418065071106\n",
            "Epoch 301/10000: L(Train): 0.3821600675582886; L(Test): 0.38540026545524597\n",
            "Epoch 302/10000: L(Train): 0.38530170917510986; L(Test): 0.385415256023407\n",
            "Epoch 303/10000: L(Train): 0.3895529806613922; L(Test): 0.385017067193985\n",
            "Epoch 304/10000: L(Train): 0.3891642987728119; L(Test): 0.3847692608833313\n",
            "Epoch 305/10000: L(Train): 0.3946409821510315; L(Test): 0.3850392699241638\n",
            "Epoch 306/10000: L(Train): 0.38559597730636597; L(Test): 0.38517430424690247\n",
            "Epoch 307/10000: L(Train): 0.3897501826286316; L(Test): 0.38478559255599976\n",
            "Epoch 308/10000: L(Train): 0.3938950300216675; L(Test): 0.3844797611236572\n",
            "Epoch 309/10000: L(Train): 0.3895017206668854; L(Test): 0.3844738304615021\n",
            "Epoch 310/10000: L(Train): 0.38986945152282715; L(Test): 0.38454753160476685\n",
            "Epoch 311/10000: L(Train): 0.3826025128364563; L(Test): 0.3849036693572998\n",
            "Epoch 312/10000: L(Train): 0.3878391683101654; L(Test): 0.38501065969467163\n",
            "Epoch 313/10000: L(Train): 0.3900481164455414; L(Test): 0.38436654210090637\n",
            "Epoch 314/10000: L(Train): 0.38225144147872925; L(Test): 0.38365739583969116\n",
            "Epoch 315/10000: L(Train): 0.38654613494873047; L(Test): 0.3841094374656677\n",
            "Epoch 316/10000: L(Train): 0.3889901041984558; L(Test): 0.38392144441604614\n",
            "Epoch 317/10000: L(Train): 0.3849143087863922; L(Test): 0.3835313320159912\n",
            "Epoch 318/10000: L(Train): 0.3873637020587921; L(Test): 0.38389137387275696\n",
            "Epoch 319/10000: L(Train): 0.38581761717796326; L(Test): 0.3833502531051636\n",
            "Epoch 320/10000: L(Train): 0.37045395374298096; L(Test): 0.38315635919570923\n",
            "Epoch 321/10000: L(Train): 0.3856987953186035; L(Test): 0.38330334424972534\n",
            "Epoch 322/10000: L(Train): 0.38730984926223755; L(Test): 0.3828551173210144\n",
            "Epoch 323/10000: L(Train): 0.39118170738220215; L(Test): 0.3830806314945221\n",
            "Epoch 324/10000: L(Train): 0.3889561891555786; L(Test): 0.3824513554573059\n",
            "Epoch 325/10000: L(Train): 0.3833431303501129; L(Test): 0.3823487460613251\n",
            "Epoch 326/10000: L(Train): 0.3897056579589844; L(Test): 0.38238421082496643\n",
            "Epoch 327/10000: L(Train): 0.3942927122116089; L(Test): 0.38207143545150757\n",
            "Epoch 328/10000: L(Train): 0.378976970911026; L(Test): 0.38248211145401\n",
            "Epoch 329/10000: L(Train): 0.3925204873085022; L(Test): 0.38181400299072266\n",
            "Epoch 330/10000: L(Train): 0.38206496834754944; L(Test): 0.38188251852989197\n",
            "Epoch 331/10000: L(Train): 0.3933798372745514; L(Test): 0.3820168972015381\n",
            "Epoch 332/10000: L(Train): 0.38816317915916443; L(Test): 0.38151460886001587\n",
            "Epoch 333/10000: L(Train): 0.3847580552101135; L(Test): 0.38170498609542847\n",
            "Epoch 334/10000: L(Train): 0.38483065366744995; L(Test): 0.38131803274154663\n",
            "Epoch 335/10000: L(Train): 0.3958872854709625; L(Test): 0.38126426935195923\n",
            "Epoch 336/10000: L(Train): 0.38994336128234863; L(Test): 0.3813532292842865\n",
            "Epoch 337/10000: L(Train): 0.378791481256485; L(Test): 0.38088124990463257\n",
            "Epoch 338/10000: L(Train): 0.38221853971481323; L(Test): 0.3808143138885498\n",
            "Epoch 339/10000: L(Train): 0.39379775524139404; L(Test): 0.3806096613407135\n",
            "Epoch 340/10000: L(Train): 0.38297760486602783; L(Test): 0.3803977966308594\n",
            "Epoch 341/10000: L(Train): 0.38386279344558716; L(Test): 0.3803097903728485\n",
            "Epoch 342/10000: L(Train): 0.3808443546295166; L(Test): 0.3802039623260498\n",
            "Epoch 343/10000: L(Train): 0.37631523609161377; L(Test): 0.3801300525665283\n",
            "Epoch 344/10000: L(Train): 0.3853103518486023; L(Test): 0.38032981753349304\n",
            "Epoch 345/10000: L(Train): 0.37395912408828735; L(Test): 0.3800067603588104\n",
            "Epoch 346/10000: L(Train): 0.3887535333633423; L(Test): 0.3799956142902374\n",
            "Epoch 347/10000: L(Train): 0.38024652004241943; L(Test): 0.3800414800643921\n",
            "Epoch 348/10000: L(Train): 0.38503849506378174; L(Test): 0.37978240847587585\n",
            "Epoch 349/10000: L(Train): 0.3767565190792084; L(Test): 0.3794521391391754\n",
            "Epoch 350/10000: L(Train): 0.3839053511619568; L(Test): 0.37937241792678833\n",
            "Epoch 351/10000: L(Train): 0.3871806859970093; L(Test): 0.37916722893714905\n",
            "Epoch 352/10000: L(Train): 0.39081501960754395; L(Test): 0.3792259097099304\n",
            "Epoch 353/10000: L(Train): 0.38863980770111084; L(Test): 0.3794824481010437\n",
            "Epoch 354/10000: L(Train): 0.386258989572525; L(Test): 0.3795977532863617\n",
            "Epoch 355/10000: L(Train): 0.3861190974712372; L(Test): 0.37933969497680664\n",
            "Epoch 356/10000: L(Train): 0.3804551661014557; L(Test): 0.37940382957458496\n",
            "Epoch 357/10000: L(Train): 0.3851306140422821; L(Test): 0.3790590465068817\n",
            "Epoch 358/10000: L(Train): 0.3860587179660797; L(Test): 0.3790607750415802\n",
            "Epoch 359/10000: L(Train): 0.3832862973213196; L(Test): 0.3785145878791809\n",
            "Epoch 360/10000: L(Train): 0.3803653419017792; L(Test): 0.37974822521209717\n",
            "Epoch 361/10000: L(Train): 0.37200117111206055; L(Test): 0.37813690304756165\n",
            "Epoch 362/10000: L(Train): 0.38532790541648865; L(Test): 0.3782684803009033\n",
            "Epoch 363/10000: L(Train): 0.3983938992023468; L(Test): 0.377983421087265\n",
            "Epoch 364/10000: L(Train): 0.39047694206237793; L(Test): 0.3779642581939697\n",
            "Epoch 365/10000: L(Train): 0.37533676624298096; L(Test): 0.3778741657733917\n",
            "Epoch 366/10000: L(Train): 0.3822217881679535; L(Test): 0.37764522433280945\n",
            "Epoch 367/10000: L(Train): 0.3830015957355499; L(Test): 0.37732625007629395\n",
            "Epoch 368/10000: L(Train): 0.39249110221862793; L(Test): 0.377130389213562\n",
            "Epoch 369/10000: L(Train): 0.372409462928772; L(Test): 0.37702661752700806\n",
            "Epoch 370/10000: L(Train): 0.3826024830341339; L(Test): 0.37691232562065125\n",
            "Epoch 371/10000: L(Train): 0.38004544377326965; L(Test): 0.3768949508666992\n",
            "Epoch 372/10000: L(Train): 0.38016369938850403; L(Test): 0.37669801712036133\n",
            "Epoch 373/10000: L(Train): 0.3779914081096649; L(Test): 0.37673142552375793\n",
            "Epoch 374/10000: L(Train): 0.3836127817630768; L(Test): 0.376295268535614\n",
            "Epoch 375/10000: L(Train): 0.3722589612007141; L(Test): 0.37589791417121887\n",
            "Epoch 376/10000: L(Train): 0.384405255317688; L(Test): 0.37597647309303284\n",
            "Epoch 377/10000: L(Train): 0.3782530725002289; L(Test): 0.3757515251636505\n",
            "Epoch 378/10000: L(Train): 0.382793128490448; L(Test): 0.3758242130279541\n",
            "Epoch 379/10000: L(Train): 0.3821965456008911; L(Test): 0.37608572840690613\n",
            "Epoch 380/10000: L(Train): 0.3808073103427887; L(Test): 0.375673770904541\n",
            "Epoch 381/10000: L(Train): 0.3692583441734314; L(Test): 0.3755837678909302\n",
            "Epoch 382/10000: L(Train): 0.3882778584957123; L(Test): 0.3750511109828949\n",
            "Epoch 383/10000: L(Train): 0.3815896511077881; L(Test): 0.3750147521495819\n",
            "Epoch 384/10000: L(Train): 0.3832366466522217; L(Test): 0.375334769487381\n",
            "Epoch 385/10000: L(Train): 0.3811713457107544; L(Test): 0.37518468499183655\n",
            "Epoch 386/10000: L(Train): 0.3864326477050781; L(Test): 0.37486132979393005\n",
            "Epoch 387/10000: L(Train): 0.38278213143348694; L(Test): 0.3750099539756775\n",
            "Epoch 388/10000: L(Train): 0.37634044885635376; L(Test): 0.37472933530807495\n",
            "Epoch 389/10000: L(Train): 0.3770431578159332; L(Test): 0.3750495910644531\n",
            "Epoch 390/10000: L(Train): 0.3813721835613251; L(Test): 0.3741976022720337\n",
            "Epoch 391/10000: L(Train): 0.3766244649887085; L(Test): 0.37394243478775024\n",
            "Epoch 392/10000: L(Train): 0.36839213967323303; L(Test): 0.37376266717910767\n",
            "Epoch 393/10000: L(Train): 0.3875921964645386; L(Test): 0.3734721839427948\n",
            "Epoch 394/10000: L(Train): 0.3722400963306427; L(Test): 0.37397655844688416\n",
            "Epoch 395/10000: L(Train): 0.36808136105537415; L(Test): 0.37386181950569153\n",
            "Epoch 396/10000: L(Train): 0.37763315439224243; L(Test): 0.37347763776779175\n",
            "Epoch 397/10000: L(Train): 0.3831482231616974; L(Test): 0.37335067987442017\n",
            "Epoch 398/10000: L(Train): 0.3821241557598114; L(Test): 0.3732013702392578\n",
            "Epoch 399/10000: L(Train): 0.39820489287376404; L(Test): 0.37334513664245605\n",
            "Epoch 400/10000: L(Train): 0.38336506485939026; L(Test): 0.37280333042144775\n",
            "Epoch 401/10000: L(Train): 0.38422754406929016; L(Test): 0.37313976883888245\n",
            "Epoch 402/10000: L(Train): 0.37430599331855774; L(Test): 0.3724197447299957\n",
            "Epoch 403/10000: L(Train): 0.3783746361732483; L(Test): 0.37258288264274597\n",
            "Epoch 404/10000: L(Train): 0.39059963822364807; L(Test): 0.37240710854530334\n",
            "Epoch 405/10000: L(Train): 0.38320785760879517; L(Test): 0.37246018648147583\n",
            "Epoch 406/10000: L(Train): 0.37793970108032227; L(Test): 0.37279993295669556\n",
            "Epoch 407/10000: L(Train): 0.3815155029296875; L(Test): 0.3722326457500458\n",
            "Epoch 408/10000: L(Train): 0.3732045292854309; L(Test): 0.37208881974220276\n",
            "Epoch 409/10000: L(Train): 0.3714623749256134; L(Test): 0.3720310628414154\n",
            "Epoch 410/10000: L(Train): 0.38128018379211426; L(Test): 0.3718743622303009\n",
            "Epoch 411/10000: L(Train): 0.3742160201072693; L(Test): 0.3721199333667755\n",
            "Epoch 412/10000: L(Train): 0.37992045283317566; L(Test): 0.37162697315216064\n",
            "Epoch 413/10000: L(Train): 0.3797992765903473; L(Test): 0.3711257576942444\n",
            "Epoch 414/10000: L(Train): 0.3707458972930908; L(Test): 0.37099382281303406\n",
            "Epoch 415/10000: L(Train): 0.3733132779598236; L(Test): 0.37128010392189026\n",
            "Epoch 416/10000: L(Train): 0.37065422534942627; L(Test): 0.37073686718940735\n",
            "Epoch 417/10000: L(Train): 0.3679676651954651; L(Test): 0.37106987833976746\n",
            "Epoch 418/10000: L(Train): 0.38064709305763245; L(Test): 0.3704244792461395\n",
            "Epoch 419/10000: L(Train): 0.3789045214653015; L(Test): 0.3708598017692566\n",
            "Epoch 420/10000: L(Train): 0.3626379668712616; L(Test): 0.37136805057525635\n",
            "Epoch 421/10000: L(Train): 0.38195177912712097; L(Test): 0.3703131675720215\n",
            "Epoch 422/10000: L(Train): 0.38292771577835083; L(Test): 0.3712714910507202\n",
            "Epoch 423/10000: L(Train): 0.38279038667678833; L(Test): 0.3708721697330475\n",
            "Epoch 424/10000: L(Train): 0.38007843494415283; L(Test): 0.3707449734210968\n",
            "Epoch 425/10000: L(Train): 0.3855266571044922; L(Test): 0.37020280957221985\n",
            "Epoch 426/10000: L(Train): 0.369304895401001; L(Test): 0.37020963430404663\n",
            "Epoch 427/10000: L(Train): 0.3803177773952484; L(Test): 0.37001878023147583\n",
            "Epoch 428/10000: L(Train): 0.36253419518470764; L(Test): 0.36932572722435\n",
            "Epoch 429/10000: L(Train): 0.37739285826683044; L(Test): 0.36975154280662537\n",
            "Epoch 430/10000: L(Train): 0.37199947237968445; L(Test): 0.36988508701324463\n",
            "Epoch 431/10000: L(Train): 0.3693959712982178; L(Test): 0.36965039372444153\n",
            "Epoch 432/10000: L(Train): 0.3791258931159973; L(Test): 0.36991703510284424\n",
            "Epoch 433/10000: L(Train): 0.37155601382255554; L(Test): 0.3691260516643524\n",
            "Epoch 434/10000: L(Train): 0.3814897835254669; L(Test): 0.36941662430763245\n",
            "Epoch 435/10000: L(Train): 0.3676924705505371; L(Test): 0.3705761730670929\n",
            "Epoch 436/10000: L(Train): 0.3718670904636383; L(Test): 0.3689332902431488\n",
            "Epoch 437/10000: L(Train): 0.37934425473213196; L(Test): 0.36951372027397156\n",
            "Epoch 438/10000: L(Train): 0.3756692707538605; L(Test): 0.36818885803222656\n",
            "Epoch 439/10000: L(Train): 0.3687402606010437; L(Test): 0.3692004382610321\n",
            "Epoch 440/10000: L(Train): 0.37580013275146484; L(Test): 0.368294894695282\n",
            "Epoch 441/10000: L(Train): 0.38616904616355896; L(Test): 0.3687286674976349\n",
            "Epoch 442/10000: L(Train): 0.38392373919487; L(Test): 0.3680608868598938\n",
            "Epoch 443/10000: L(Train): 0.3845101296901703; L(Test): 0.36757686734199524\n",
            "Epoch 444/10000: L(Train): 0.3761003017425537; L(Test): 0.36819887161254883\n",
            "Epoch 445/10000: L(Train): 0.3743894398212433; L(Test): 0.36810174584388733\n",
            "Epoch 446/10000: L(Train): 0.38354969024658203; L(Test): 0.36844465136528015\n",
            "Epoch 447/10000: L(Train): 0.3810614049434662; L(Test): 0.3691839873790741\n",
            "Epoch 448/10000: L(Train): 0.36436790227890015; L(Test): 0.36800703406333923\n",
            "Epoch 449/10000: L(Train): 0.3801020085811615; L(Test): 0.36879685521125793\n",
            "Epoch 450/10000: L(Train): 0.3809492588043213; L(Test): 0.368364155292511\n",
            "Epoch 451/10000: L(Train): 0.3711690306663513; L(Test): 0.3675024211406708\n",
            "Epoch 452/10000: L(Train): 0.37780436873435974; L(Test): 0.36781781911849976\n",
            "Epoch 453/10000: L(Train): 0.38320058584213257; L(Test): 0.36718928813934326\n",
            "Epoch 454/10000: L(Train): 0.3833642303943634; L(Test): 0.3679961860179901\n",
            "Epoch 455/10000: L(Train): 0.37380722165107727; L(Test): 0.36818820238113403\n",
            "Epoch 456/10000: L(Train): 0.3683689534664154; L(Test): 0.36726564168930054\n",
            "Epoch 457/10000: L(Train): 0.3714185059070587; L(Test): 0.36682572960853577\n",
            "Epoch 458/10000: L(Train): 0.364214688539505; L(Test): 0.3669217824935913\n",
            "Epoch 459/10000: L(Train): 0.38237303495407104; L(Test): 0.36695167422294617\n",
            "Epoch 460/10000: L(Train): 0.37366983294487; L(Test): 0.36806178092956543\n",
            "Epoch 461/10000: L(Train): 0.3853619396686554; L(Test): 0.366499125957489\n",
            "Epoch 462/10000: L(Train): 0.37889018654823303; L(Test): 0.36637234687805176\n",
            "Epoch 463/10000: L(Train): 0.37489327788352966; L(Test): 0.36589717864990234\n",
            "Epoch 464/10000: L(Train): 0.37588247656822205; L(Test): 0.3662773370742798\n",
            "Epoch 465/10000: L(Train): 0.37863683700561523; L(Test): 0.3664283752441406\n",
            "Epoch 466/10000: L(Train): 0.3669167459011078; L(Test): 0.3658475875854492\n",
            "Epoch 467/10000: L(Train): 0.3792775571346283; L(Test): 0.3653721213340759\n",
            "Epoch 468/10000: L(Train): 0.3773881494998932; L(Test): 0.36546263098716736\n",
            "Epoch 469/10000: L(Train): 0.36942005157470703; L(Test): 0.36542096734046936\n",
            "Epoch 470/10000: L(Train): 0.3783586919307709; L(Test): 0.3653371036052704\n",
            "Epoch 471/10000: L(Train): 0.37100520730018616; L(Test): 0.3648417890071869\n",
            "Epoch 472/10000: L(Train): 0.3728489875793457; L(Test): 0.36457738280296326\n",
            "Epoch 473/10000: L(Train): 0.37760061025619507; L(Test): 0.36459293961524963\n",
            "Epoch 474/10000: L(Train): 0.37170177698135376; L(Test): 0.36444997787475586\n",
            "Epoch 475/10000: L(Train): 0.37655577063560486; L(Test): 0.36397311091423035\n",
            "Epoch 476/10000: L(Train): 0.37587669491767883; L(Test): 0.36405909061431885\n",
            "Epoch 477/10000: L(Train): 0.37997740507125854; L(Test): 0.36370226740837097\n",
            "Epoch 478/10000: L(Train): 0.37519949674606323; L(Test): 0.36366981267929077\n",
            "Epoch 479/10000: L(Train): 0.36613649129867554; L(Test): 0.36402493715286255\n",
            "Epoch 480/10000: L(Train): 0.3731057941913605; L(Test): 0.3636433780193329\n",
            "Epoch 481/10000: L(Train): 0.36446452140808105; L(Test): 0.36402836441993713\n",
            "Epoch 482/10000: L(Train): 0.3632086515426636; L(Test): 0.3632289469242096\n",
            "Epoch 483/10000: L(Train): 0.3734315037727356; L(Test): 0.3633587062358856\n",
            "Epoch 484/10000: L(Train): 0.3618016541004181; L(Test): 0.36327028274536133\n",
            "Epoch 485/10000: L(Train): 0.3790215849876404; L(Test): 0.36349913477897644\n",
            "Epoch 486/10000: L(Train): 0.37599435448646545; L(Test): 0.3625134229660034\n",
            "Epoch 487/10000: L(Train): 0.3724071681499481; L(Test): 0.36325812339782715\n",
            "Epoch 488/10000: L(Train): 0.37079378962516785; L(Test): 0.3623591959476471\n",
            "Epoch 489/10000: L(Train): 0.37788382172584534; L(Test): 0.36327725648880005\n",
            "Epoch 490/10000: L(Train): 0.36471274495124817; L(Test): 0.3626379668712616\n",
            "Epoch 491/10000: L(Train): 0.3688373863697052; L(Test): 0.36300766468048096\n",
            "Epoch 492/10000: L(Train): 0.37195512652397156; L(Test): 0.3618831932544708\n",
            "Epoch 493/10000: L(Train): 0.36081019043922424; L(Test): 0.3618864417076111\n",
            "Epoch 494/10000: L(Train): 0.36922287940979004; L(Test): 0.3616228997707367\n",
            "Epoch 495/10000: L(Train): 0.36553409695625305; L(Test): 0.36152106523513794\n",
            "Epoch 496/10000: L(Train): 0.369078665971756; L(Test): 0.36149588227272034\n",
            "Epoch 497/10000: L(Train): 0.361473023891449; L(Test): 0.36162903904914856\n",
            "Epoch 498/10000: L(Train): 0.3766978979110718; L(Test): 0.36169496178627014\n",
            "Epoch 499/10000: L(Train): 0.3750031888484955; L(Test): 0.3609399199485779\n",
            "Epoch 500/10000: L(Train): 0.37954431772232056; L(Test): 0.36119675636291504\n",
            "Epoch 501/10000: L(Train): 0.3544541001319885; L(Test): 0.3618748188018799\n",
            "Epoch 502/10000: L(Train): 0.3609006404876709; L(Test): 0.3611508011817932\n",
            "Epoch 503/10000: L(Train): 0.3679068982601166; L(Test): 0.36065250635147095\n",
            "Epoch 504/10000: L(Train): 0.3598291277885437; L(Test): 0.3607042729854584\n",
            "Epoch 505/10000: L(Train): 0.3682284951210022; L(Test): 0.36084064841270447\n",
            "Epoch 506/10000: L(Train): 0.3643365502357483; L(Test): 0.36074164509773254\n",
            "Epoch 507/10000: L(Train): 0.3692263960838318; L(Test): 0.3613668382167816\n",
            "Epoch 508/10000: L(Train): 0.37475448846817017; L(Test): 0.3612731099128723\n",
            "Epoch 509/10000: L(Train): 0.37109315395355225; L(Test): 0.359828382730484\n",
            "Epoch 510/10000: L(Train): 0.3731605112552643; L(Test): 0.3610362410545349\n",
            "Epoch 511/10000: L(Train): 0.3818070590496063; L(Test): 0.36009106040000916\n",
            "Epoch 512/10000: L(Train): 0.3687725365161896; L(Test): 0.3606833517551422\n",
            "Epoch 513/10000: L(Train): 0.3850792646408081; L(Test): 0.36047109961509705\n",
            "Epoch 514/10000: L(Train): 0.3641912639141083; L(Test): 0.36018452048301697\n",
            "Epoch 515/10000: L(Train): 0.3634115755558014; L(Test): 0.3607124090194702\n",
            "Epoch 516/10000: L(Train): 0.36755096912384033; L(Test): 0.36022618412971497\n",
            "Epoch 517/10000: L(Train): 0.36488041281700134; L(Test): 0.35997843742370605\n",
            "Epoch 518/10000: L(Train): 0.3618374466896057; L(Test): 0.36053550243377686\n",
            "Epoch 519/10000: L(Train): 0.3709261417388916; L(Test): 0.3612832725048065\n",
            "Epoch 520/10000: L(Train): 0.3721393644809723; L(Test): 0.3597926199436188\n",
            "Epoch 521/10000: L(Train): 0.36400529742240906; L(Test): 0.3597303032875061\n",
            "Epoch 522/10000: L(Train): 0.36967578530311584; L(Test): 0.359488308429718\n",
            "Epoch 523/10000: L(Train): 0.3665063977241516; L(Test): 0.3599357604980469\n",
            "Epoch 524/10000: L(Train): 0.37604713439941406; L(Test): 0.359890341758728\n",
            "Epoch 525/10000: L(Train): 0.36267951130867004; L(Test): 0.35967007279396057\n",
            "Epoch 526/10000: L(Train): 0.3731866180896759; L(Test): 0.3588084876537323\n",
            "Epoch 527/10000: L(Train): 0.37497416138648987; L(Test): 0.35878413915634155\n",
            "Epoch 528/10000: L(Train): 0.36368340253829956; L(Test): 0.35887211561203003\n",
            "Epoch 529/10000: L(Train): 0.36893847584724426; L(Test): 0.35881972312927246\n",
            "Epoch 530/10000: L(Train): 0.3665764629840851; L(Test): 0.35872596502304077\n",
            "Epoch 531/10000: L(Train): 0.3680224120616913; L(Test): 0.35967499017715454\n",
            "Epoch 532/10000: L(Train): 0.365164577960968; L(Test): 0.3587081730365753\n",
            "Epoch 533/10000: L(Train): 0.3702516555786133; L(Test): 0.35901209712028503\n",
            "Epoch 534/10000: L(Train): 0.3689629137516022; L(Test): 0.3582507371902466\n",
            "Epoch 535/10000: L(Train): 0.36483681201934814; L(Test): 0.35862940549850464\n",
            "Epoch 536/10000: L(Train): 0.36233311891555786; L(Test): 0.3582189679145813\n",
            "Epoch 537/10000: L(Train): 0.3696078360080719; L(Test): 0.3580091595649719\n",
            "Epoch 538/10000: L(Train): 0.36796310544013977; L(Test): 0.35803085565567017\n",
            "Epoch 539/10000: L(Train): 0.3619077205657959; L(Test): 0.3584645986557007\n",
            "Epoch 540/10000: L(Train): 0.3665890395641327; L(Test): 0.35805925726890564\n",
            "Epoch 541/10000: L(Train): 0.371776819229126; L(Test): 0.35911062359809875\n",
            "Epoch 542/10000: L(Train): 0.3576962947845459; L(Test): 0.357994019985199\n",
            "Epoch 543/10000: L(Train): 0.37291592359542847; L(Test): 0.35855600237846375\n",
            "Epoch 544/10000: L(Train): 0.36587047576904297; L(Test): 0.35745885968208313\n",
            "Epoch 545/10000: L(Train): 0.36756789684295654; L(Test): 0.3590906858444214\n",
            "Epoch 546/10000: L(Train): 0.3726120591163635; L(Test): 0.35808223485946655\n",
            "Epoch 547/10000: L(Train): 0.36034300923347473; L(Test): 0.3577108681201935\n",
            "Epoch 548/10000: L(Train): 0.36597949266433716; L(Test): 0.35695767402648926\n",
            "Epoch 549/10000: L(Train): 0.3649185299873352; L(Test): 0.3570457696914673\n",
            "Epoch 550/10000: L(Train): 0.3751937747001648; L(Test): 0.3576508164405823\n",
            "Epoch 551/10000: L(Train): 0.37393587827682495; L(Test): 0.35668617486953735\n",
            "Epoch 552/10000: L(Train): 0.36402928829193115; L(Test): 0.35729479789733887\n",
            "Epoch 553/10000: L(Train): 0.36618465185165405; L(Test): 0.3556574881076813\n",
            "Epoch 554/10000: L(Train): 0.3610343337059021; L(Test): 0.35603296756744385\n",
            "Epoch 555/10000: L(Train): 0.36696305871009827; L(Test): 0.3562038242816925\n",
            "Epoch 556/10000: L(Train): 0.37443453073501587; L(Test): 0.35538724064826965\n",
            "Epoch 557/10000: L(Train): 0.3652549386024475; L(Test): 0.3558503985404968\n",
            "Epoch 558/10000: L(Train): 0.36956435441970825; L(Test): 0.3555816411972046\n",
            "Epoch 559/10000: L(Train): 0.3691179156303406; L(Test): 0.3559040129184723\n",
            "Epoch 560/10000: L(Train): 0.3626018762588501; L(Test): 0.35493648052215576\n",
            "Epoch 561/10000: L(Train): 0.36848926544189453; L(Test): 0.3553105294704437\n",
            "Epoch 562/10000: L(Train): 0.376086950302124; L(Test): 0.3547916114330292\n",
            "Epoch 563/10000: L(Train): 0.365141898393631; L(Test): 0.35524752736091614\n",
            "Epoch 564/10000: L(Train): 0.3577686846256256; L(Test): 0.3548611104488373\n",
            "Epoch 565/10000: L(Train): 0.3642549514770508; L(Test): 0.35501423478126526\n",
            "Epoch 566/10000: L(Train): 0.3717300593852997; L(Test): 0.3543897569179535\n",
            "Epoch 567/10000: L(Train): 0.3534068763256073; L(Test): 0.35472285747528076\n",
            "Epoch 568/10000: L(Train): 0.3744838237762451; L(Test): 0.35416457056999207\n",
            "Epoch 569/10000: L(Train): 0.3533402383327484; L(Test): 0.3546014428138733\n",
            "Epoch 570/10000: L(Train): 0.36324071884155273; L(Test): 0.3548714816570282\n",
            "Epoch 571/10000: L(Train): 0.37003493309020996; L(Test): 0.3545408844947815\n",
            "Epoch 572/10000: L(Train): 0.35308584570884705; L(Test): 0.3551333248615265\n",
            "Epoch 573/10000: L(Train): 0.3712948262691498; L(Test): 0.3540675938129425\n",
            "Epoch 574/10000: L(Train): 0.36223113536834717; L(Test): 0.35425034165382385\n",
            "Epoch 575/10000: L(Train): 0.3645535707473755; L(Test): 0.35398685932159424\n",
            "Epoch 576/10000: L(Train): 0.3635369539260864; L(Test): 0.3541316092014313\n",
            "Epoch 577/10000: L(Train): 0.3734496533870697; L(Test): 0.35321560502052307\n",
            "Epoch 578/10000: L(Train): 0.3664468824863434; L(Test): 0.3536493480205536\n",
            "Epoch 579/10000: L(Train): 0.3685220181941986; L(Test): 0.35317885875701904\n",
            "Epoch 580/10000: L(Train): 0.36688774824142456; L(Test): 0.35286444425582886\n",
            "Epoch 581/10000: L(Train): 0.35740652680397034; L(Test): 0.35267919301986694\n",
            "Epoch 582/10000: L(Train): 0.3578099310398102; L(Test): 0.3525201380252838\n",
            "Epoch 583/10000: L(Train): 0.3575640618801117; L(Test): 0.3527507483959198\n",
            "Epoch 584/10000: L(Train): 0.37072017788887024; L(Test): 0.3530089855194092\n",
            "Epoch 585/10000: L(Train): 0.3560164272785187; L(Test): 0.35317185521125793\n",
            "Epoch 586/10000: L(Train): 0.36076298356056213; L(Test): 0.35245755314826965\n",
            "Epoch 587/10000: L(Train): 0.3537577986717224; L(Test): 0.3536570966243744\n",
            "Epoch 588/10000: L(Train): 0.360869437456131; L(Test): 0.3535691499710083\n",
            "Epoch 589/10000: L(Train): 0.366855651140213; L(Test): 0.35281097888946533\n",
            "Epoch 590/10000: L(Train): 0.356171578168869; L(Test): 0.3535352051258087\n",
            "Epoch 591/10000: L(Train): 0.3623189628124237; L(Test): 0.3526216149330139\n",
            "Epoch 592/10000: L(Train): 0.36613836884498596; L(Test): 0.3528968095779419\n",
            "Epoch 593/10000: L(Train): 0.3678411841392517; L(Test): 0.35271942615509033\n",
            "Epoch 594/10000: L(Train): 0.3546009957790375; L(Test): 0.3520244061946869\n",
            "Epoch 595/10000: L(Train): 0.35742467641830444; L(Test): 0.3528485894203186\n",
            "Epoch 596/10000: L(Train): 0.35827070474624634; L(Test): 0.35221609473228455\n",
            "Epoch 597/10000: L(Train): 0.36249780654907227; L(Test): 0.3538661599159241\n",
            "Epoch 598/10000: L(Train): 0.3611723780632019; L(Test): 0.3537728488445282\n",
            "Epoch 599/10000: L(Train): 0.3679092526435852; L(Test): 0.3525445759296417\n",
            "Epoch 600/10000: L(Train): 0.36044546961784363; L(Test): 0.353838711977005\n",
            "Epoch 601/10000: L(Train): 0.370623379945755; L(Test): 0.35281574726104736\n",
            "Epoch 602/10000: L(Train): 0.3670307695865631; L(Test): 0.35340970754623413\n",
            "Epoch 603/10000: L(Train): 0.35120224952697754; L(Test): 0.3550942838191986\n",
            "Epoch 604/10000: L(Train): 0.36994534730911255; L(Test): 0.3540500998497009\n",
            "Epoch 605/10000: L(Train): 0.3620438575744629; L(Test): 0.3528628647327423\n",
            "Epoch 606/10000: L(Train): 0.3640676736831665; L(Test): 0.3532351851463318\n",
            "Epoch 607/10000: L(Train): 0.3566315174102783; L(Test): 0.3519991934299469\n",
            "Epoch 608/10000: L(Train): 0.3572131097316742; L(Test): 0.35307109355926514\n",
            "Epoch 609/10000: L(Train): 0.36571449041366577; L(Test): 0.3538293242454529\n",
            "Epoch 610/10000: L(Train): 0.3584970533847809; L(Test): 0.3516683876514435\n",
            "Epoch 611/10000: L(Train): 0.35125067830085754; L(Test): 0.3522264063358307\n",
            "Epoch 612/10000: L(Train): 0.35729455947875977; L(Test): 0.35238540172576904\n",
            "Epoch 613/10000: L(Train): 0.36966755986213684; L(Test): 0.350970596075058\n",
            "Epoch 614/10000: L(Train): 0.35995569825172424; L(Test): 0.35358110070228577\n",
            "Epoch 615/10000: L(Train): 0.3598647713661194; L(Test): 0.35154297947883606\n",
            "Epoch 616/10000: L(Train): 0.3572964072227478; L(Test): 0.35028356313705444\n",
            "Epoch 617/10000: L(Train): 0.3643089830875397; L(Test): 0.35090845823287964\n",
            "Epoch 618/10000: L(Train): 0.3517795205116272; L(Test): 0.3498329520225525\n",
            "Epoch 619/10000: L(Train): 0.35908403992652893; L(Test): 0.3506338894367218\n",
            "Epoch 620/10000: L(Train): 0.34907427430152893; L(Test): 0.35001105070114136\n",
            "Epoch 621/10000: L(Train): 0.3588216304779053; L(Test): 0.35037761926651\n",
            "Epoch 622/10000: L(Train): 0.3569518029689789; L(Test): 0.35049352049827576\n",
            "Epoch 623/10000: L(Train): 0.3665008842945099; L(Test): 0.349522203207016\n",
            "Epoch 624/10000: L(Train): 0.366268128156662; L(Test): 0.34984204173088074\n",
            "Epoch 625/10000: L(Train): 0.35648584365844727; L(Test): 0.34951117634773254\n",
            "Epoch 626/10000: L(Train): 0.3570350110530853; L(Test): 0.35066699981689453\n",
            "Epoch 627/10000: L(Train): 0.3593086302280426; L(Test): 0.3492812216281891\n",
            "Epoch 628/10000: L(Train): 0.36311817169189453; L(Test): 0.3489997982978821\n",
            "Epoch 629/10000: L(Train): 0.3571823835372925; L(Test): 0.34920260310173035\n",
            "Epoch 630/10000: L(Train): 0.3638361692428589; L(Test): 0.3489072620868683\n",
            "Epoch 631/10000: L(Train): 0.35490572452545166; L(Test): 0.3488079905509949\n",
            "Epoch 632/10000: L(Train): 0.35784709453582764; L(Test): 0.34872063994407654\n",
            "Epoch 633/10000: L(Train): 0.3535938262939453; L(Test): 0.3494681417942047\n",
            "Epoch 634/10000: L(Train): 0.3538232445716858; L(Test): 0.34826579689979553\n",
            "Epoch 635/10000: L(Train): 0.35491517186164856; L(Test): 0.3486277163028717\n",
            "Epoch 636/10000: L(Train): 0.34501707553863525; L(Test): 0.3483094573020935\n",
            "Epoch 637/10000: L(Train): 0.360255628824234; L(Test): 0.3483525812625885\n",
            "Epoch 638/10000: L(Train): 0.35576367378234863; L(Test): 0.3485521376132965\n",
            "Epoch 639/10000: L(Train): 0.3613906502723694; L(Test): 0.3478788137435913\n",
            "Epoch 640/10000: L(Train): 0.35533466935157776; L(Test): 0.3477221727371216\n",
            "Epoch 641/10000: L(Train): 0.36214420199394226; L(Test): 0.3476867973804474\n",
            "Epoch 642/10000: L(Train): 0.36189004778862; L(Test): 0.3479689657688141\n",
            "Epoch 643/10000: L(Train): 0.35412248969078064; L(Test): 0.3480283319950104\n",
            "Epoch 644/10000: L(Train): 0.354002982378006; L(Test): 0.34770891070365906\n",
            "Epoch 645/10000: L(Train): 0.3525664806365967; L(Test): 0.34823545813560486\n",
            "Epoch 646/10000: L(Train): 0.3526134490966797; L(Test): 0.34800058603286743\n",
            "Epoch 647/10000: L(Train): 0.3501075208187103; L(Test): 0.34823504090309143\n",
            "Epoch 648/10000: L(Train): 0.3556019961833954; L(Test): 0.3493136763572693\n",
            "Epoch 649/10000: L(Train): 0.36683717370033264; L(Test): 0.34893128275871277\n",
            "Epoch 650/10000: L(Train): 0.36791372299194336; L(Test): 0.3493669927120209\n",
            "Epoch 651/10000: L(Train): 0.3697374761104584; L(Test): 0.35010913014411926\n",
            "Epoch 652/10000: L(Train): 0.36287081241607666; L(Test): 0.3494716286659241\n",
            "Epoch 653/10000: L(Train): 0.34778931736946106; L(Test): 0.3488996922969818\n",
            "Epoch 654/10000: L(Train): 0.35934534668922424; L(Test): 0.3486807346343994\n",
            "Epoch 655/10000: L(Train): 0.35971149802207947; L(Test): 0.34941917657852173\n",
            "Epoch 656/10000: L(Train): 0.36057600378990173; L(Test): 0.34920647740364075\n",
            "Epoch 657/10000: L(Train): 0.35496222972869873; L(Test): 0.34878021478652954\n",
            "Epoch 658/10000: L(Train): 0.3634226620197296; L(Test): 0.34786468744277954\n",
            "Epoch 659/10000: L(Train): 0.36931830644607544; L(Test): 0.34833431243896484\n",
            "Epoch 660/10000: L(Train): 0.3621715307235718; L(Test): 0.3489363193511963\n",
            "Epoch 661/10000: L(Train): 0.3617178499698639; L(Test): 0.34774863719940186\n",
            "Epoch 662/10000: L(Train): 0.357837438583374; L(Test): 0.3475087881088257\n",
            "Epoch 663/10000: L(Train): 0.35470104217529297; L(Test): 0.347896546125412\n",
            "Epoch 664/10000: L(Train): 0.35334140062332153; L(Test): 0.3481024205684662\n",
            "Epoch 665/10000: L(Train): 0.34821078181266785; L(Test): 0.34769973158836365\n",
            "Epoch 666/10000: L(Train): 0.34263288974761963; L(Test): 0.3479548394680023\n",
            "Epoch 667/10000: L(Train): 0.35723936557769775; L(Test): 0.34689509868621826\n",
            "Epoch 668/10000: L(Train): 0.3607546091079712; L(Test): 0.34711208939552307\n",
            "Epoch 669/10000: L(Train): 0.3582701086997986; L(Test): 0.3471173048019409\n",
            "Epoch 670/10000: L(Train): 0.3628702759742737; L(Test): 0.3459315896034241\n",
            "Epoch 671/10000: L(Train): 0.35739850997924805; L(Test): 0.34606316685676575\n",
            "Epoch 672/10000: L(Train): 0.36249402165412903; L(Test): 0.3459109365940094\n",
            "Epoch 673/10000: L(Train): 0.3465745747089386; L(Test): 0.34670591354370117\n",
            "Epoch 674/10000: L(Train): 0.3599257469177246; L(Test): 0.3468192517757416\n",
            "Epoch 675/10000: L(Train): 0.361044704914093; L(Test): 0.3467370569705963\n",
            "Epoch 676/10000: L(Train): 0.3576943278312683; L(Test): 0.34627029299736023\n",
            "Epoch 677/10000: L(Train): 0.3485552668571472; L(Test): 0.34569087624549866\n",
            "Epoch 678/10000: L(Train): 0.3649236857891083; L(Test): 0.3457479178905487\n",
            "Epoch 679/10000: L(Train): 0.3575846552848816; L(Test): 0.3452392518520355\n",
            "Epoch 680/10000: L(Train): 0.35472211241722107; L(Test): 0.3455255925655365\n",
            "Epoch 681/10000: L(Train): 0.35742810368537903; L(Test): 0.34541964530944824\n",
            "Epoch 682/10000: L(Train): 0.346760630607605; L(Test): 0.3457105755805969\n",
            "Epoch 683/10000: L(Train): 0.35549843311309814; L(Test): 0.34536266326904297\n",
            "Epoch 684/10000: L(Train): 0.36234772205352783; L(Test): 0.3447985351085663\n",
            "Epoch 685/10000: L(Train): 0.3634074330329895; L(Test): 0.34467193484306335\n",
            "Epoch 686/10000: L(Train): 0.3620894253253937; L(Test): 0.34448692202568054\n",
            "Epoch 687/10000: L(Train): 0.353774756193161; L(Test): 0.3448568284511566\n",
            "Epoch 688/10000: L(Train): 0.366420716047287; L(Test): 0.3446095585823059\n",
            "Epoch 689/10000: L(Train): 0.34719762206077576; L(Test): 0.3438703417778015\n",
            "Epoch 690/10000: L(Train): 0.3517717719078064; L(Test): 0.344125896692276\n",
            "Epoch 691/10000: L(Train): 0.36086004972457886; L(Test): 0.34464171528816223\n",
            "Epoch 692/10000: L(Train): 0.36391422152519226; L(Test): 0.3446066081523895\n",
            "Epoch 693/10000: L(Train): 0.36026445031166077; L(Test): 0.34438422322273254\n",
            "Epoch 694/10000: L(Train): 0.3619169294834137; L(Test): 0.34423235058784485\n",
            "Epoch 695/10000: L(Train): 0.3509572744369507; L(Test): 0.34363996982574463\n",
            "Epoch 696/10000: L(Train): 0.3679771423339844; L(Test): 0.34332871437072754\n",
            "Epoch 697/10000: L(Train): 0.35475051403045654; L(Test): 0.34459352493286133\n",
            "Epoch 698/10000: L(Train): 0.35450300574302673; L(Test): 0.3436049520969391\n",
            "Epoch 699/10000: L(Train): 0.35821396112442017; L(Test): 0.3446141481399536\n",
            "Epoch 700/10000: L(Train): 0.3541003167629242; L(Test): 0.34386223554611206\n",
            "Epoch 701/10000: L(Train): 0.36110419034957886; L(Test): 0.34453800320625305\n",
            "Epoch 702/10000: L(Train): 0.36622700095176697; L(Test): 0.34386518597602844\n",
            "Epoch 703/10000: L(Train): 0.3499433100223541; L(Test): 0.3430652916431427\n",
            "Epoch 704/10000: L(Train): 0.35474663972854614; L(Test): 0.3432227373123169\n",
            "Epoch 705/10000: L(Train): 0.3596729338169098; L(Test): 0.3432314097881317\n",
            "Epoch 706/10000: L(Train): 0.3569469153881073; L(Test): 0.343056321144104\n",
            "Epoch 707/10000: L(Train): 0.36411014199256897; L(Test): 0.3421308994293213\n",
            "Epoch 708/10000: L(Train): 0.3551196753978729; L(Test): 0.34238192439079285\n",
            "Epoch 709/10000: L(Train): 0.3662334978580475; L(Test): 0.34313005208969116\n",
            "Epoch 710/10000: L(Train): 0.35422733426094055; L(Test): 0.34314629435539246\n",
            "Epoch 711/10000: L(Train): 0.3604331910610199; L(Test): 0.3430880010128021\n",
            "Epoch 712/10000: L(Train): 0.34996291995048523; L(Test): 0.34263351559638977\n",
            "Epoch 713/10000: L(Train): 0.36071160435676575; L(Test): 0.34289973974227905\n",
            "Epoch 714/10000: L(Train): 0.35760653018951416; L(Test): 0.34260496497154236\n",
            "Epoch 715/10000: L(Train): 0.3516497313976288; L(Test): 0.3437291979789734\n",
            "Epoch 716/10000: L(Train): 0.3559473156929016; L(Test): 0.3427508771419525\n",
            "Epoch 717/10000: L(Train): 0.3494946360588074; L(Test): 0.34277471899986267\n",
            "Epoch 718/10000: L(Train): 0.3470090925693512; L(Test): 0.3425942659378052\n",
            "Epoch 719/10000: L(Train): 0.3585674464702606; L(Test): 0.34289252758026123\n",
            "Epoch 720/10000: L(Train): 0.3634927272796631; L(Test): 0.34251949191093445\n",
            "Epoch 721/10000: L(Train): 0.35276609659194946; L(Test): 0.34345659613609314\n",
            "Epoch 722/10000: L(Train): 0.35277846455574036; L(Test): 0.34249863028526306\n",
            "Epoch 723/10000: L(Train): 0.35540831089019775; L(Test): 0.3423745334148407\n",
            "Epoch 724/10000: L(Train): 0.3491382300853729; L(Test): 0.3428575098514557\n",
            "Epoch 725/10000: L(Train): 0.3569302260875702; L(Test): 0.3429650068283081\n",
            "Epoch 726/10000: L(Train): 0.3503802418708801; L(Test): 0.3427026867866516\n",
            "Epoch 727/10000: L(Train): 0.35308557748794556; L(Test): 0.3417187035083771\n",
            "Epoch 728/10000: L(Train): 0.3614005446434021; L(Test): 0.34206581115722656\n",
            "Epoch 729/10000: L(Train): 0.3535704016685486; L(Test): 0.3436090052127838\n",
            "Epoch 730/10000: L(Train): 0.35541102290153503; L(Test): 0.3433265686035156\n",
            "Epoch 731/10000: L(Train): 0.35147029161453247; L(Test): 0.3434087336063385\n",
            "Epoch 732/10000: L(Train): 0.34934869408607483; L(Test): 0.34257251024246216\n",
            "Epoch 733/10000: L(Train): 0.35488826036453247; L(Test): 0.34190016984939575\n",
            "Epoch 734/10000: L(Train): 0.35076144337654114; L(Test): 0.3419390022754669\n",
            "Epoch 735/10000: L(Train): 0.35006898641586304; L(Test): 0.3420289158821106\n",
            "Epoch 736/10000: L(Train): 0.3541751205921173; L(Test): 0.3425661325454712\n",
            "Epoch 737/10000: L(Train): 0.3537712097167969; L(Test): 0.341343492269516\n",
            "Epoch 738/10000: L(Train): 0.3601197302341461; L(Test): 0.3408010005950928\n",
            "Epoch 739/10000: L(Train): 0.35405632853507996; L(Test): 0.3411533534526825\n",
            "Epoch 740/10000: L(Train): 0.34659385681152344; L(Test): 0.3407728374004364\n",
            "Epoch 741/10000: L(Train): 0.3515210747718811; L(Test): 0.3410145342350006\n",
            "Epoch 742/10000: L(Train): 0.3611697256565094; L(Test): 0.3415062725543976\n",
            "Epoch 743/10000: L(Train): 0.3609887957572937; L(Test): 0.3403179347515106\n",
            "Epoch 744/10000: L(Train): 0.36997753381729126; L(Test): 0.3404878079891205\n",
            "Epoch 745/10000: L(Train): 0.344599187374115; L(Test): 0.34054306149482727\n",
            "Epoch 746/10000: L(Train): 0.3572632372379303; L(Test): 0.3404305875301361\n",
            "Epoch 747/10000: L(Train): 0.3472156226634979; L(Test): 0.3405151963233948\n",
            "Epoch 748/10000: L(Train): 0.3482379615306854; L(Test): 0.34092622995376587\n",
            "Epoch 749/10000: L(Train): 0.34204334020614624; L(Test): 0.34100863337516785\n",
            "Epoch 750/10000: L(Train): 0.35824114084243774; L(Test): 0.34032389521598816\n",
            "Epoch 751/10000: L(Train): 0.3546629548072815; L(Test): 0.340257465839386\n",
            "Epoch 752/10000: L(Train): 0.35405102372169495; L(Test): 0.3406251072883606\n",
            "Epoch 753/10000: L(Train): 0.3505669832229614; L(Test): 0.3414086699485779\n",
            "Epoch 754/10000: L(Train): 0.359465092420578; L(Test): 0.3428650498390198\n",
            "Epoch 755/10000: L(Train): 0.34590619802474976; L(Test): 0.3403506577014923\n",
            "Epoch 756/10000: L(Train): 0.3638401925563812; L(Test): 0.3412987291812897\n",
            "Epoch 757/10000: L(Train): 0.3528253734111786; L(Test): 0.3418055474758148\n",
            "Epoch 758/10000: L(Train): 0.35214582085609436; L(Test): 0.34160658717155457\n",
            "Epoch 759/10000: L(Train): 0.36078891158103943; L(Test): 0.34251847863197327\n",
            "Epoch 760/10000: L(Train): 0.35017356276512146; L(Test): 0.3411126434803009\n",
            "Epoch 761/10000: L(Train): 0.3551423251628876; L(Test): 0.3416348695755005\n",
            "Epoch 762/10000: L(Train): 0.34784960746765137; L(Test): 0.34090888500213623\n",
            "Epoch 763/10000: L(Train): 0.34496912360191345; L(Test): 0.3408811390399933\n",
            "Epoch 764/10000: L(Train): 0.35434389114379883; L(Test): 0.3414594531059265\n",
            "Epoch 765/10000: L(Train): 0.3681371212005615; L(Test): 0.34043580293655396\n",
            "Epoch 766/10000: L(Train): 0.36109158396720886; L(Test): 0.34229177236557007\n",
            "Epoch 767/10000: L(Train): 0.3518218994140625; L(Test): 0.3408536911010742\n",
            "Epoch 768/10000: L(Train): 0.3592260777950287; L(Test): 0.34026238322257996\n",
            "Epoch 769/10000: L(Train): 0.35595187544822693; L(Test): 0.3410612940788269\n",
            "Epoch 770/10000: L(Train): 0.3591173589229584; L(Test): 0.34038054943084717\n",
            "Epoch 771/10000: L(Train): 0.35422006249427795; L(Test): 0.34085679054260254\n",
            "Epoch 772/10000: L(Train): 0.36225417256355286; L(Test): 0.3414788246154785\n",
            "Epoch 773/10000: L(Train): 0.35647478699684143; L(Test): 0.3415682017803192\n",
            "Epoch 774/10000: L(Train): 0.35736706852912903; L(Test): 0.34128934144973755\n",
            "Epoch 775/10000: L(Train): 0.35242322087287903; L(Test): 0.3406341075897217\n",
            "Epoch 776/10000: L(Train): 0.35020938515663147; L(Test): 0.3402861952781677\n",
            "Epoch 777/10000: L(Train): 0.35651353001594543; L(Test): 0.3400593400001526\n",
            "Epoch 778/10000: L(Train): 0.3557274639606476; L(Test): 0.33937954902648926\n",
            "Epoch 779/10000: L(Train): 0.3490675091743469; L(Test): 0.3398279845714569\n",
            "Epoch 780/10000: L(Train): 0.3501087427139282; L(Test): 0.34004080295562744\n",
            "Epoch 781/10000: L(Train): 0.34821048378944397; L(Test): 0.3398357927799225\n",
            "Epoch 782/10000: L(Train): 0.34543806314468384; L(Test): 0.33925941586494446\n",
            "Epoch 783/10000: L(Train): 0.3494216203689575; L(Test): 0.3394218981266022\n",
            "Epoch 784/10000: L(Train): 0.3472641706466675; L(Test): 0.33810126781463623\n",
            "Epoch 785/10000: L(Train): 0.34332185983657837; L(Test): 0.33998361229896545\n",
            "Epoch 786/10000: L(Train): 0.35437819361686707; L(Test): 0.33951708674430847\n",
            "Epoch 787/10000: L(Train): 0.35090547800064087; L(Test): 0.3381863534450531\n",
            "Epoch 788/10000: L(Train): 0.3478907644748688; L(Test): 0.3382643461227417\n",
            "Epoch 789/10000: L(Train): 0.3581620454788208; L(Test): 0.33760303258895874\n",
            "Epoch 790/10000: L(Train): 0.3418339192867279; L(Test): 0.3382311761379242\n",
            "Epoch 791/10000: L(Train): 0.35038039088249207; L(Test): 0.3387657403945923\n",
            "Epoch 792/10000: L(Train): 0.3427339494228363; L(Test): 0.33786776661872864\n",
            "Epoch 793/10000: L(Train): 0.35096311569213867; L(Test): 0.33927103877067566\n",
            "Epoch 794/10000: L(Train): 0.3497179448604584; L(Test): 0.33810633420944214\n",
            "Epoch 795/10000: L(Train): 0.35316845774650574; L(Test): 0.33744940161705017\n",
            "Epoch 796/10000: L(Train): 0.3412511348724365; L(Test): 0.338777631521225\n",
            "Epoch 797/10000: L(Train): 0.3496663570404053; L(Test): 0.3381720781326294\n",
            "Epoch 798/10000: L(Train): 0.35849758982658386; L(Test): 0.33730635046958923\n",
            "Epoch 799/10000: L(Train): 0.3523208498954773; L(Test): 0.33846578001976013\n",
            "Epoch 800/10000: L(Train): 0.3564572036266327; L(Test): 0.3377116024494171\n",
            "Epoch 801/10000: L(Train): 0.35869526863098145; L(Test): 0.33774885535240173\n",
            "Epoch 802/10000: L(Train): 0.35496750473976135; L(Test): 0.338949978351593\n",
            "Epoch 803/10000: L(Train): 0.3503701686859131; L(Test): 0.33771029114723206\n",
            "Epoch 804/10000: L(Train): 0.35423919558525085; L(Test): 0.33703088760375977\n",
            "Epoch 805/10000: L(Train): 0.3547922670841217; L(Test): 0.3372576832771301\n",
            "Epoch 806/10000: L(Train): 0.34952235221862793; L(Test): 0.33746206760406494\n",
            "Epoch 807/10000: L(Train): 0.34464895725250244; L(Test): 0.3373102843761444\n",
            "Epoch 808/10000: L(Train): 0.34711000323295593; L(Test): 0.3372756242752075\n",
            "Epoch 809/10000: L(Train): 0.35465171933174133; L(Test): 0.33660414814949036\n",
            "Epoch 810/10000: L(Train): 0.3537837862968445; L(Test): 0.3362744450569153\n",
            "Epoch 811/10000: L(Train): 0.3454127609729767; L(Test): 0.3365027606487274\n",
            "Epoch 812/10000: L(Train): 0.3557744324207306; L(Test): 0.3365345597267151\n",
            "Epoch 813/10000: L(Train): 0.35211846232414246; L(Test): 0.3359871208667755\n",
            "Epoch 814/10000: L(Train): 0.3545302748680115; L(Test): 0.33617404103279114\n",
            "Epoch 815/10000: L(Train): 0.3546566665172577; L(Test): 0.33606091141700745\n",
            "Epoch 816/10000: L(Train): 0.35679712891578674; L(Test): 0.3359302580356598\n",
            "Epoch 817/10000: L(Train): 0.34779471158981323; L(Test): 0.3357637822628021\n",
            "Epoch 818/10000: L(Train): 0.3556860685348511; L(Test): 0.3358176052570343\n",
            "Epoch 819/10000: L(Train): 0.3551759719848633; L(Test): 0.33608025312423706\n",
            "Epoch 820/10000: L(Train): 0.34448516368865967; L(Test): 0.3357603847980499\n",
            "Epoch 821/10000: L(Train): 0.3554188311100006; L(Test): 0.3359789252281189\n",
            "Epoch 822/10000: L(Train): 0.34959328174591064; L(Test): 0.3361521065235138\n",
            "Epoch 823/10000: L(Train): 0.34782981872558594; L(Test): 0.33540064096450806\n",
            "Epoch 824/10000: L(Train): 0.34646081924438477; L(Test): 0.3356397747993469\n",
            "Epoch 825/10000: L(Train): 0.34832707047462463; L(Test): 0.3353007137775421\n",
            "Epoch 826/10000: L(Train): 0.3491748571395874; L(Test): 0.3351823091506958\n",
            "Epoch 827/10000: L(Train): 0.34588006138801575; L(Test): 0.3355356752872467\n",
            "Epoch 828/10000: L(Train): 0.3547614812850952; L(Test): 0.3349657952785492\n",
            "Epoch 829/10000: L(Train): 0.3536020815372467; L(Test): 0.3371955454349518\n",
            "Epoch 830/10000: L(Train): 0.36055484414100647; L(Test): 0.335014283657074\n",
            "Epoch 831/10000: L(Train): 0.35795339941978455; L(Test): 0.33669599890708923\n",
            "Epoch 832/10000: L(Train): 0.3538323938846588; L(Test): 0.3362756371498108\n",
            "Epoch 833/10000: L(Train): 0.3513771593570709; L(Test): 0.33610668778419495\n",
            "Epoch 834/10000: L(Train): 0.3470454812049866; L(Test): 0.3366848826408386\n",
            "Epoch 835/10000: L(Train): 0.3531259298324585; L(Test): 0.33678004145622253\n",
            "Epoch 836/10000: L(Train): 0.35081154108047485; L(Test): 0.33773332834243774\n",
            "Epoch 837/10000: L(Train): 0.3395295739173889; L(Test): 0.3357212245464325\n",
            "Epoch 838/10000: L(Train): 0.35783231258392334; L(Test): 0.3362092971801758\n",
            "Epoch 839/10000: L(Train): 0.35160011053085327; L(Test): 0.3382888436317444\n",
            "Epoch 840/10000: L(Train): 0.3478321135044098; L(Test): 0.3355536162853241\n",
            "Epoch 841/10000: L(Train): 0.3536379933357239; L(Test): 0.3361504077911377\n",
            "Epoch 842/10000: L(Train): 0.34095606207847595; L(Test): 0.3367501497268677\n",
            "Epoch 843/10000: L(Train): 0.34563586115837097; L(Test): 0.33538952469825745\n",
            "Epoch 844/10000: L(Train): 0.3491627871990204; L(Test): 0.3348647952079773\n",
            "Epoch 845/10000: L(Train): 0.35305628180503845; L(Test): 0.3346875011920929\n",
            "Epoch 846/10000: L(Train): 0.3401726186275482; L(Test): 0.33498793840408325\n",
            "Epoch 847/10000: L(Train): 0.3541036546230316; L(Test): 0.33641311526298523\n",
            "Epoch 848/10000: L(Train): 0.36219266057014465; L(Test): 0.3358646333217621\n",
            "Epoch 849/10000: L(Train): 0.35588276386260986; L(Test): 0.33610111474990845\n",
            "Epoch 850/10000: L(Train): 0.3569050431251526; L(Test): 0.3357725143432617\n",
            "Epoch 851/10000: L(Train): 0.3530426323413849; L(Test): 0.335264652967453\n",
            "Epoch 852/10000: L(Train): 0.3510652780532837; L(Test): 0.33566713333129883\n",
            "Epoch 853/10000: L(Train): 0.34339842200279236; L(Test): 0.3357830047607422\n",
            "Epoch 854/10000: L(Train): 0.35412707924842834; L(Test): 0.3348621129989624\n",
            "Epoch 855/10000: L(Train): 0.34434616565704346; L(Test): 0.3343774378299713\n",
            "Epoch 856/10000: L(Train): 0.35355642437934875; L(Test): 0.3347182273864746\n",
            "Epoch 857/10000: L(Train): 0.34295353293418884; L(Test): 0.33390772342681885\n",
            "Epoch 858/10000: L(Train): 0.3425397574901581; L(Test): 0.33350086212158203\n",
            "Epoch 859/10000: L(Train): 0.35629865527153015; L(Test): 0.33362045884132385\n",
            "Epoch 860/10000: L(Train): 0.34453070163726807; L(Test): 0.33368372917175293\n",
            "Epoch 861/10000: L(Train): 0.35159191489219666; L(Test): 0.3338751196861267\n",
            "Epoch 862/10000: L(Train): 0.343946248292923; L(Test): 0.3342452645301819\n",
            "Epoch 863/10000: L(Train): 0.3433249890804291; L(Test): 0.33397892117500305\n",
            "Epoch 864/10000: L(Train): 0.3495427370071411; L(Test): 0.3339909017086029\n",
            "Epoch 865/10000: L(Train): 0.3473167419433594; L(Test): 0.3337969481945038\n",
            "Epoch 866/10000: L(Train): 0.34805828332901; L(Test): 0.33402642607688904\n",
            "Epoch 867/10000: L(Train): 0.3538450598716736; L(Test): 0.33421409130096436\n",
            "Epoch 868/10000: L(Train): 0.34634673595428467; L(Test): 0.3336562216281891\n",
            "Epoch 869/10000: L(Train): 0.3517864942550659; L(Test): 0.3338780701160431\n",
            "Epoch 870/10000: L(Train): 0.35167741775512695; L(Test): 0.3345060348510742\n",
            "Epoch 871/10000: L(Train): 0.34487900137901306; L(Test): 0.3337734639644623\n",
            "Epoch 872/10000: L(Train): 0.33447542786598206; L(Test): 0.33526548743247986\n",
            "Epoch 873/10000: L(Train): 0.3501754701137543; L(Test): 0.33603784441947937\n",
            "Epoch 874/10000: L(Train): 0.3427791893482208; L(Test): 0.3341444134712219\n",
            "Epoch 875/10000: L(Train): 0.3454822301864624; L(Test): 0.3348380923271179\n",
            "Epoch 876/10000: L(Train): 0.349539577960968; L(Test): 0.33491769433021545\n",
            "Epoch 877/10000: L(Train): 0.35051682591438293; L(Test): 0.3338607847690582\n",
            "Epoch 878/10000: L(Train): 0.35685649514198303; L(Test): 0.33450382947921753\n",
            "Epoch 879/10000: L(Train): 0.34653475880622864; L(Test): 0.3348434567451477\n",
            "Epoch 880/10000: L(Train): 0.35405993461608887; L(Test): 0.33433377742767334\n",
            "Epoch 881/10000: L(Train): 0.34251663088798523; L(Test): 0.33440861105918884\n",
            "Epoch 882/10000: L(Train): 0.3506936728954315; L(Test): 0.3342682719230652\n",
            "Epoch 883/10000: L(Train): 0.3582528829574585; L(Test): 0.3332214653491974\n",
            "Epoch 884/10000: L(Train): 0.35121941566467285; L(Test): 0.3335344195365906\n",
            "Epoch 885/10000: L(Train): 0.35042741894721985; L(Test): 0.33379441499710083\n",
            "Epoch 886/10000: L(Train): 0.3461969494819641; L(Test): 0.33359968662261963\n",
            "Epoch 887/10000: L(Train): 0.35657116770744324; L(Test): 0.33333706855773926\n",
            "Epoch 888/10000: L(Train): 0.34609994292259216; L(Test): 0.3341694176197052\n",
            "Epoch 889/10000: L(Train): 0.35013383626937866; L(Test): 0.33406949043273926\n",
            "Epoch 890/10000: L(Train): 0.35349372029304504; L(Test): 0.3327660858631134\n",
            "Epoch 891/10000: L(Train): 0.34056979417800903; L(Test): 0.3340666890144348\n",
            "Epoch 892/10000: L(Train): 0.36360061168670654; L(Test): 0.3329929709434509\n",
            "Epoch 893/10000: L(Train): 0.3487970232963562; L(Test): 0.3318316638469696\n",
            "Epoch 894/10000: L(Train): 0.34248194098472595; L(Test): 0.33345532417297363\n",
            "Epoch 895/10000: L(Train): 0.33453109860420227; L(Test): 0.3326953649520874\n",
            "Epoch 896/10000: L(Train): 0.3427242338657379; L(Test): 0.33286014199256897\n",
            "Epoch 897/10000: L(Train): 0.34390944242477417; L(Test): 0.33234304189682007\n",
            "Epoch 898/10000: L(Train): 0.34288525581359863; L(Test): 0.33231350779533386\n",
            "Epoch 899/10000: L(Train): 0.3407706618309021; L(Test): 0.3319384753704071\n",
            "Epoch 900/10000: L(Train): 0.3397531807422638; L(Test): 0.331673264503479\n",
            "Epoch 901/10000: L(Train): 0.3459952473640442; L(Test): 0.3315449059009552\n",
            "Epoch 902/10000: L(Train): 0.3435651361942291; L(Test): 0.3317718207836151\n",
            "Epoch 903/10000: L(Train): 0.35033226013183594; L(Test): 0.33147159218788147\n",
            "Epoch 904/10000: L(Train): 0.3412338197231293; L(Test): 0.3326428234577179\n",
            "Epoch 905/10000: L(Train): 0.3452901542186737; L(Test): 0.3318455219268799\n",
            "Epoch 906/10000: L(Train): 0.3416748046875; L(Test): 0.3315507173538208\n",
            "Epoch 907/10000: L(Train): 0.3411274254322052; L(Test): 0.3321095407009125\n",
            "Epoch 908/10000: L(Train): 0.34582775831222534; L(Test): 0.3312551975250244\n",
            "Epoch 909/10000: L(Train): 0.3321549594402313; L(Test): 0.33108729124069214\n",
            "Epoch 910/10000: L(Train): 0.342640221118927; L(Test): 0.331609308719635\n",
            "Epoch 911/10000: L(Train): 0.34316566586494446; L(Test): 0.3312073051929474\n",
            "Epoch 912/10000: L(Train): 0.3550317585468292; L(Test): 0.33119064569473267\n",
            "Epoch 913/10000: L(Train): 0.3534821569919586; L(Test): 0.33091628551483154\n",
            "Epoch 914/10000: L(Train): 0.3470708131790161; L(Test): 0.33097314834594727\n",
            "Epoch 915/10000: L(Train): 0.3440319001674652; L(Test): 0.3311557471752167\n",
            "Epoch 916/10000: L(Train): 0.34551331400871277; L(Test): 0.33095070719718933\n",
            "Epoch 917/10000: L(Train): 0.3355996608734131; L(Test): 0.33114632964134216\n",
            "Epoch 918/10000: L(Train): 0.35529977083206177; L(Test): 0.3304758667945862\n",
            "Epoch 919/10000: L(Train): 0.34302374720573425; L(Test): 0.3303113281726837\n",
            "Epoch 920/10000: L(Train): 0.3471166789531708; L(Test): 0.33037272095680237\n",
            "Epoch 921/10000: L(Train): 0.3472748100757599; L(Test): 0.33101215958595276\n",
            "Epoch 922/10000: L(Train): 0.35485824942588806; L(Test): 0.33118152618408203\n",
            "Epoch 923/10000: L(Train): 0.35020798444747925; L(Test): 0.33066874742507935\n",
            "Epoch 924/10000: L(Train): 0.3467547297477722; L(Test): 0.3310893476009369\n",
            "Epoch 925/10000: L(Train): 0.3491896092891693; L(Test): 0.3312128484249115\n",
            "Epoch 926/10000: L(Train): 0.3494838774204254; L(Test): 0.3301450312137604\n",
            "Epoch 927/10000: L(Train): 0.3507489264011383; L(Test): 0.33053022623062134\n",
            "Epoch 928/10000: L(Train): 0.3385430872440338; L(Test): 0.33058902621269226\n",
            "Epoch 929/10000: L(Train): 0.34681060910224915; L(Test): 0.3299325406551361\n",
            "Epoch 930/10000: L(Train): 0.34539467096328735; L(Test): 0.33007678389549255\n",
            "Epoch 931/10000: L(Train): 0.3489995002746582; L(Test): 0.32986629009246826\n",
            "Epoch 932/10000: L(Train): 0.3569326102733612; L(Test): 0.3292286992073059\n",
            "Epoch 933/10000: L(Train): 0.3476116955280304; L(Test): 0.3291998505592346\n",
            "Epoch 934/10000: L(Train): 0.35084664821624756; L(Test): 0.32963284850120544\n",
            "Epoch 935/10000: L(Train): 0.3521708548069; L(Test): 0.330026239156723\n",
            "Epoch 936/10000: L(Train): 0.35353174805641174; L(Test): 0.33038240671157837\n",
            "Epoch 937/10000: L(Train): 0.3412383496761322; L(Test): 0.3306240737438202\n",
            "Epoch 938/10000: L(Train): 0.3483041822910309; L(Test): 0.32919439673423767\n",
            "Epoch 939/10000: L(Train): 0.33961278200149536; L(Test): 0.3290422856807709\n",
            "Epoch 940/10000: L(Train): 0.3505242168903351; L(Test): 0.32955271005630493\n",
            "Epoch 941/10000: L(Train): 0.3438389301300049; L(Test): 0.3293497562408447\n",
            "Epoch 942/10000: L(Train): 0.3395440876483917; L(Test): 0.32849469780921936\n",
            "Epoch 943/10000: L(Train): 0.3433164656162262; L(Test): 0.32832399010658264\n",
            "Epoch 944/10000: L(Train): 0.348871648311615; L(Test): 0.328830748796463\n",
            "Epoch 945/10000: L(Train): 0.34661900997161865; L(Test): 0.3286941647529602\n",
            "Epoch 946/10000: L(Train): 0.3385781943798065; L(Test): 0.3286270797252655\n",
            "Epoch 947/10000: L(Train): 0.34032949805259705; L(Test): 0.3283202052116394\n",
            "Epoch 948/10000: L(Train): 0.3427521586418152; L(Test): 0.3280545473098755\n",
            "Epoch 949/10000: L(Train): 0.34758102893829346; L(Test): 0.32831671833992004\n",
            "Epoch 950/10000: L(Train): 0.33769461512565613; L(Test): 0.32844582200050354\n",
            "Epoch 951/10000: L(Train): 0.3371630609035492; L(Test): 0.32807326316833496\n",
            "Epoch 952/10000: L(Train): 0.3551906645298004; L(Test): 0.327732115983963\n",
            "Epoch 953/10000: L(Train): 0.34240832924842834; L(Test): 0.32825326919555664\n",
            "Epoch 954/10000: L(Train): 0.3475538194179535; L(Test): 0.3289380669593811\n",
            "Epoch 955/10000: L(Train): 0.3503611087799072; L(Test): 0.3297869861125946\n",
            "Epoch 956/10000: L(Train): 0.3515884578227997; L(Test): 0.3289611339569092\n",
            "Epoch 957/10000: L(Train): 0.3447433114051819; L(Test): 0.3294612765312195\n",
            "Epoch 958/10000: L(Train): 0.34468188881874084; L(Test): 0.32927292585372925\n",
            "Epoch 959/10000: L(Train): 0.34647586941719055; L(Test): 0.3291390836238861\n",
            "Epoch 960/10000: L(Train): 0.35118603706359863; L(Test): 0.32920509576797485\n",
            "Epoch 961/10000: L(Train): 0.34825432300567627; L(Test): 0.3286136984825134\n",
            "Epoch 962/10000: L(Train): 0.34501883387565613; L(Test): 0.32871347665786743\n",
            "Epoch 963/10000: L(Train): 0.3524746000766754; L(Test): 0.32834112644195557\n",
            "Epoch 964/10000: L(Train): 0.34845536947250366; L(Test): 0.3281833827495575\n",
            "Epoch 965/10000: L(Train): 0.3428036868572235; L(Test): 0.32867318391799927\n",
            "Epoch 966/10000: L(Train): 0.35049861669540405; L(Test): 0.32871341705322266\n",
            "Epoch 967/10000: L(Train): 0.3357943296432495; L(Test): 0.32851433753967285\n",
            "Epoch 968/10000: L(Train): 0.33494511246681213; L(Test): 0.3284357190132141\n",
            "Epoch 969/10000: L(Train): 0.342403769493103; L(Test): 0.3282940089702606\n",
            "Epoch 970/10000: L(Train): 0.3482542037963867; L(Test): 0.32806551456451416\n",
            "Epoch 971/10000: L(Train): 0.344666987657547; L(Test): 0.3281654417514801\n",
            "Epoch 972/10000: L(Train): 0.35915541648864746; L(Test): 0.3278135657310486\n",
            "Epoch 973/10000: L(Train): 0.3340991139411926; L(Test): 0.32811489701271057\n",
            "Epoch 974/10000: L(Train): 0.3422715663909912; L(Test): 0.3285595178604126\n",
            "Epoch 975/10000: L(Train): 0.3428850471973419; L(Test): 0.328005313873291\n",
            "Epoch 976/10000: L(Train): 0.33839476108551025; L(Test): 0.32814672589302063\n",
            "Epoch 977/10000: L(Train): 0.3427903652191162; L(Test): 0.32819992303848267\n",
            "Epoch 978/10000: L(Train): 0.35020291805267334; L(Test): 0.32777535915374756\n",
            "Epoch 979/10000: L(Train): 0.3478003144264221; L(Test): 0.3278373181819916\n",
            "Epoch 980/10000: L(Train): 0.34514302015304565; L(Test): 0.3279883563518524\n",
            "Epoch 981/10000: L(Train): 0.34979286789894104; L(Test): 0.3277882933616638\n",
            "Epoch 982/10000: L(Train): 0.3414487838745117; L(Test): 0.3277615010738373\n",
            "Epoch 983/10000: L(Train): 0.33955711126327515; L(Test): 0.32941314578056335\n",
            "Epoch 984/10000: L(Train): 0.34963178634643555; L(Test): 0.3280421793460846\n",
            "Epoch 985/10000: L(Train): 0.34544825553894043; L(Test): 0.3279494345188141\n",
            "Epoch 986/10000: L(Train): 0.35043075680732727; L(Test): 0.32911404967308044\n",
            "Epoch 987/10000: L(Train): 0.35017311573028564; L(Test): 0.3287544846534729\n",
            "Epoch 988/10000: L(Train): 0.333736389875412; L(Test): 0.330285906791687\n",
            "Epoch 989/10000: L(Train): 0.337446391582489; L(Test): 0.3302372097969055\n",
            "Epoch 990/10000: L(Train): 0.34911859035491943; L(Test): 0.3281158208847046\n",
            "Epoch 991/10000: L(Train): 0.345517098903656; L(Test): 0.3290213942527771\n",
            "Epoch 992/10000: L(Train): 0.33941784501075745; L(Test): 0.3283633589744568\n",
            "Epoch 993/10000: L(Train): 0.34549158811569214; L(Test): 0.32901909947395325\n",
            "Epoch 994/10000: L(Train): 0.3458329141139984; L(Test): 0.3298991024494171\n",
            "Epoch 995/10000: L(Train): 0.34439486265182495; L(Test): 0.32895126938819885\n",
            "Epoch 996/10000: L(Train): 0.34090226888656616; L(Test): 0.32984158396720886\n",
            "Epoch 997/10000: L(Train): 0.34482648968696594; L(Test): 0.3281747102737427\n",
            "Epoch 998/10000: L(Train): 0.33396872878074646; L(Test): 0.32876715064048767\n",
            "Epoch 999/10000: L(Train): 0.33928951621055603; L(Test): 0.331667423248291\n",
            "Epoch 1000/10000: L(Train): 0.34214064478874207; L(Test): 0.32864320278167725\n",
            "Epoch 1001/10000: L(Train): 0.3400631248950958; L(Test): 0.327828586101532\n",
            "Epoch 1002/10000: L(Train): 0.33687618374824524; L(Test): 0.32878372073173523\n",
            "Epoch 1003/10000: L(Train): 0.34295889735221863; L(Test): 0.3288538455963135\n",
            "Epoch 1004/10000: L(Train): 0.34677764773368835; L(Test): 0.3297073245048523\n",
            "Epoch 1005/10000: L(Train): 0.3524840772151947; L(Test): 0.3291456401348114\n",
            "Epoch 1006/10000: L(Train): 0.345874160528183; L(Test): 0.32924407720565796\n",
            "Epoch 1007/10000: L(Train): 0.3324737548828125; L(Test): 0.3302479684352875\n",
            "Epoch 1008/10000: L(Train): 0.3424173593521118; L(Test): 0.3289375901222229\n",
            "Epoch 1009/10000: L(Train): 0.3471258878707886; L(Test): 0.3295339345932007\n",
            "Epoch 1010/10000: L(Train): 0.3391456604003906; L(Test): 0.3291527330875397\n",
            "Epoch 1011/10000: L(Train): 0.34471848607063293; L(Test): 0.3282453715801239\n",
            "Epoch 1012/10000: L(Train): 0.3399713635444641; L(Test): 0.32927030324935913\n",
            "Epoch 1013/10000: L(Train): 0.33784806728363037; L(Test): 0.3279677629470825\n",
            "Epoch 1014/10000: L(Train): 0.3385644257068634; L(Test): 0.32778650522232056\n",
            "Epoch 1015/10000: L(Train): 0.3423820734024048; L(Test): 0.32835203409194946\n",
            "Epoch 1016/10000: L(Train): 0.34895503520965576; L(Test): 0.3273080289363861\n",
            "Epoch 1017/10000: L(Train): 0.3344409763813019; L(Test): 0.32699158787727356\n",
            "Epoch 1018/10000: L(Train): 0.3483298420906067; L(Test): 0.3273991048336029\n",
            "Epoch 1019/10000: L(Train): 0.34583422541618347; L(Test): 0.3273387849330902\n",
            "Epoch 1020/10000: L(Train): 0.34466075897216797; L(Test): 0.32695990800857544\n",
            "Epoch 1021/10000: L(Train): 0.3458697497844696; L(Test): 0.3270743489265442\n",
            "Epoch 1022/10000: L(Train): 0.34091269969940186; L(Test): 0.32670357823371887\n",
            "Epoch 1023/10000: L(Train): 0.33882057666778564; L(Test): 0.32580626010894775\n",
            "Epoch 1024/10000: L(Train): 0.34251412749290466; L(Test): 0.3259313404560089\n",
            "Epoch 1025/10000: L(Train): 0.33680251240730286; L(Test): 0.32576778531074524\n",
            "Epoch 1026/10000: L(Train): 0.3457258641719818; L(Test): 0.3254550099372864\n",
            "Epoch 1027/10000: L(Train): 0.3365126848220825; L(Test): 0.32697823643684387\n",
            "Epoch 1028/10000: L(Train): 0.35378170013427734; L(Test): 0.3262038230895996\n",
            "Epoch 1029/10000: L(Train): 0.35395538806915283; L(Test): 0.32587116956710815\n",
            "Epoch 1030/10000: L(Train): 0.3469797670841217; L(Test): 0.32610002160072327\n",
            "Epoch 1031/10000: L(Train): 0.3449288606643677; L(Test): 0.3260131776332855\n",
            "Epoch 1032/10000: L(Train): 0.34186986088752747; L(Test): 0.32610443234443665\n",
            "Epoch 1033/10000: L(Train): 0.3403435945510864; L(Test): 0.3262898921966553\n",
            "Epoch 1034/10000: L(Train): 0.34086206555366516; L(Test): 0.32553374767303467\n",
            "Epoch 1035/10000: L(Train): 0.3432902693748474; L(Test): 0.3252316415309906\n",
            "Epoch 1036/10000: L(Train): 0.3416540324687958; L(Test): 0.3259246349334717\n",
            "Epoch 1037/10000: L(Train): 0.3493850827217102; L(Test): 0.32587572932243347\n",
            "Epoch 1038/10000: L(Train): 0.3422628343105316; L(Test): 0.32597997784614563\n",
            "Epoch 1039/10000: L(Train): 0.3361065685749054; L(Test): 0.326951801776886\n",
            "Epoch 1040/10000: L(Train): 0.3537830412387848; L(Test): 0.3255411982536316\n",
            "Epoch 1041/10000: L(Train): 0.3402644991874695; L(Test): 0.3263104557991028\n",
            "Epoch 1042/10000: L(Train): 0.34721899032592773; L(Test): 0.32701200246810913\n",
            "Epoch 1043/10000: L(Train): 0.3414106070995331; L(Test): 0.3262666165828705\n",
            "Epoch 1044/10000: L(Train): 0.3356283903121948; L(Test): 0.3258705735206604\n",
            "Epoch 1045/10000: L(Train): 0.344789057970047; L(Test): 0.32634875178337097\n",
            "Epoch 1046/10000: L(Train): 0.3386702537536621; L(Test): 0.32545793056488037\n",
            "Epoch 1047/10000: L(Train): 0.35181161761283875; L(Test): 0.32515984773635864\n",
            "Epoch 1048/10000: L(Train): 0.3445405960083008; L(Test): 0.32619020342826843\n",
            "Epoch 1049/10000: L(Train): 0.353700190782547; L(Test): 0.3254750669002533\n",
            "Epoch 1050/10000: L(Train): 0.34839993715286255; L(Test): 0.3247181475162506\n",
            "Epoch 1051/10000: L(Train): 0.3340142071247101; L(Test): 0.32564839720726013\n",
            "Epoch 1052/10000: L(Train): 0.33386868238449097; L(Test): 0.3253956139087677\n",
            "Epoch 1053/10000: L(Train): 0.34072190523147583; L(Test): 0.32541629672050476\n",
            "Epoch 1054/10000: L(Train): 0.33141130208969116; L(Test): 0.3258099853992462\n",
            "Epoch 1055/10000: L(Train): 0.3451973795890808; L(Test): 0.3251964747905731\n",
            "Epoch 1056/10000: L(Train): 0.3402186930179596; L(Test): 0.32505542039871216\n",
            "Epoch 1057/10000: L(Train): 0.34739384055137634; L(Test): 0.32425013184547424\n",
            "Epoch 1058/10000: L(Train): 0.3418286442756653; L(Test): 0.3266671895980835\n",
            "Epoch 1059/10000: L(Train): 0.3435397744178772; L(Test): 0.32497334480285645\n",
            "Epoch 1060/10000: L(Train): 0.3401302695274353; L(Test): 0.325569212436676\n",
            "Epoch 1061/10000: L(Train): 0.3409670293331146; L(Test): 0.32503512501716614\n",
            "Epoch 1062/10000: L(Train): 0.3393419682979584; L(Test): 0.3252089321613312\n",
            "Epoch 1063/10000: L(Train): 0.3317151963710785; L(Test): 0.3265189826488495\n",
            "Epoch 1064/10000: L(Train): 0.34039074182510376; L(Test): 0.3255372643470764\n",
            "Epoch 1065/10000: L(Train): 0.33647671341896057; L(Test): 0.3255183696746826\n",
            "Epoch 1066/10000: L(Train): 0.3474225699901581; L(Test): 0.3254282772541046\n",
            "Epoch 1067/10000: L(Train): 0.3369860053062439; L(Test): 0.3244827389717102\n",
            "Epoch 1068/10000: L(Train): 0.3411809206008911; L(Test): 0.32707515358924866\n",
            "Epoch 1069/10000: L(Train): 0.3417090177536011; L(Test): 0.32551297545433044\n",
            "Epoch 1070/10000: L(Train): 0.34691721200942993; L(Test): 0.32545655965805054\n",
            "Epoch 1071/10000: L(Train): 0.3474939167499542; L(Test): 0.3244289457798004\n",
            "Epoch 1072/10000: L(Train): 0.3397308886051178; L(Test): 0.32481926679611206\n",
            "Epoch 1073/10000: L(Train): 0.3386189043521881; L(Test): 0.3252027928829193\n",
            "Epoch 1074/10000: L(Train): 0.3352871239185333; L(Test): 0.3239816427230835\n",
            "Epoch 1075/10000: L(Train): 0.3435308635234833; L(Test): 0.3242802023887634\n",
            "Epoch 1076/10000: L(Train): 0.344009131193161; L(Test): 0.32333293557167053\n",
            "Epoch 1077/10000: L(Train): 0.33410921692848206; L(Test): 0.3252524137496948\n",
            "Epoch 1078/10000: L(Train): 0.3365541398525238; L(Test): 0.3248242437839508\n",
            "Epoch 1079/10000: L(Train): 0.3447533845901489; L(Test): 0.3242253363132477\n",
            "Epoch 1080/10000: L(Train): 0.3410489559173584; L(Test): 0.3253929018974304\n",
            "Epoch 1081/10000: L(Train): 0.33625736832618713; L(Test): 0.3238813579082489\n",
            "Epoch 1082/10000: L(Train): 0.3407058119773865; L(Test): 0.3250834047794342\n",
            "Epoch 1083/10000: L(Train): 0.34086740016937256; L(Test): 0.3243151903152466\n",
            "Epoch 1084/10000: L(Train): 0.35027068853378296; L(Test): 0.3241015374660492\n",
            "Epoch 1085/10000: L(Train): 0.346538245677948; L(Test): 0.3249436318874359\n",
            "Epoch 1086/10000: L(Train): 0.3441794216632843; L(Test): 0.32484376430511475\n",
            "Epoch 1087/10000: L(Train): 0.3409392833709717; L(Test): 0.32519662380218506\n",
            "Epoch 1088/10000: L(Train): 0.34352555871009827; L(Test): 0.3253169059753418\n",
            "Epoch 1089/10000: L(Train): 0.3449435830116272; L(Test): 0.3242001235485077\n",
            "Epoch 1090/10000: L(Train): 0.34975913166999817; L(Test): 0.32434168457984924\n",
            "Epoch 1091/10000: L(Train): 0.33807307481765747; L(Test): 0.32490774989128113\n",
            "Epoch 1092/10000: L(Train): 0.34221887588500977; L(Test): 0.3256365954875946\n",
            "Epoch 1093/10000: L(Train): 0.33988696336746216; L(Test): 0.32513755559921265\n",
            "Epoch 1094/10000: L(Train): 0.3414485454559326; L(Test): 0.3243653476238251\n",
            "Epoch 1095/10000: L(Train): 0.3440116345882416; L(Test): 0.3254318833351135\n",
            "Epoch 1096/10000: L(Train): 0.338357537984848; L(Test): 0.32492339611053467\n",
            "Epoch 1097/10000: L(Train): 0.3429546058177948; L(Test): 0.32494276762008667\n",
            "Epoch 1098/10000: L(Train): 0.35027259588241577; L(Test): 0.3246176540851593\n",
            "Epoch 1099/10000: L(Train): 0.34024778008461; L(Test): 0.32382023334503174\n",
            "Epoch 1100/10000: L(Train): 0.34753507375717163; L(Test): 0.3245488703250885\n",
            "Epoch 1101/10000: L(Train): 0.3528438210487366; L(Test): 0.32431402802467346\n",
            "Epoch 1102/10000: L(Train): 0.3387836515903473; L(Test): 0.32537004351615906\n",
            "Epoch 1103/10000: L(Train): 0.34600886702537537; L(Test): 0.3246690034866333\n",
            "Epoch 1104/10000: L(Train): 0.3447282612323761; L(Test): 0.32380571961402893\n",
            "Epoch 1105/10000: L(Train): 0.34289219975471497; L(Test): 0.32403624057769775\n",
            "Epoch 1106/10000: L(Train): 0.34748736023902893; L(Test): 0.32294923067092896\n",
            "Epoch 1107/10000: L(Train): 0.32839834690093994; L(Test): 0.32334619760513306\n",
            "Epoch 1108/10000: L(Train): 0.349272757768631; L(Test): 0.3233327567577362\n",
            "Epoch 1109/10000: L(Train): 0.3482522666454315; L(Test): 0.3237673044204712\n",
            "Epoch 1110/10000: L(Train): 0.3378370702266693; L(Test): 0.3241625130176544\n",
            "Epoch 1111/10000: L(Train): 0.3420450687408447; L(Test): 0.3243141770362854\n",
            "Epoch 1112/10000: L(Train): 0.3402409255504608; L(Test): 0.32467249035835266\n",
            "Epoch 1113/10000: L(Train): 0.3342803716659546; L(Test): 0.3242519199848175\n",
            "Epoch 1114/10000: L(Train): 0.3380986750125885; L(Test): 0.32416409254074097\n",
            "Epoch 1115/10000: L(Train): 0.33665117621421814; L(Test): 0.3246495723724365\n",
            "Epoch 1116/10000: L(Train): 0.340205579996109; L(Test): 0.32348930835723877\n",
            "Epoch 1117/10000: L(Train): 0.33567020297050476; L(Test): 0.3236066401004791\n",
            "Epoch 1118/10000: L(Train): 0.33912017941474915; L(Test): 0.32345372438430786\n",
            "Epoch 1119/10000: L(Train): 0.34088224172592163; L(Test): 0.322830468416214\n",
            "Epoch 1120/10000: L(Train): 0.3386658728122711; L(Test): 0.3234650790691376\n",
            "Epoch 1121/10000: L(Train): 0.3397752046585083; L(Test): 0.3239733874797821\n",
            "Epoch 1122/10000: L(Train): 0.34756964445114136; L(Test): 0.3234878182411194\n",
            "Epoch 1123/10000: L(Train): 0.3400425314903259; L(Test): 0.3240756094455719\n",
            "Epoch 1124/10000: L(Train): 0.3388800024986267; L(Test): 0.32426413893699646\n",
            "Epoch 1125/10000: L(Train): 0.34202948212623596; L(Test): 0.32436656951904297\n",
            "Epoch 1126/10000: L(Train): 0.3354070484638214; L(Test): 0.32406458258628845\n",
            "Epoch 1127/10000: L(Train): 0.3454880714416504; L(Test): 0.3238850235939026\n",
            "Epoch 1128/10000: L(Train): 0.3385397493839264; L(Test): 0.32432612776756287\n",
            "Epoch 1129/10000: L(Train): 0.34093016386032104; L(Test): 0.32386642694473267\n",
            "Epoch 1130/10000: L(Train): 0.34728291630744934; L(Test): 0.3249158561229706\n",
            "Epoch 1131/10000: L(Train): 0.336990624666214; L(Test): 0.3261306881904602\n",
            "Epoch 1132/10000: L(Train): 0.3553694486618042; L(Test): 0.32670000195503235\n",
            "Epoch 1133/10000: L(Train): 0.33431190252304077; L(Test): 0.3294106423854828\n",
            "Epoch 1134/10000: L(Train): 0.34903547167778015; L(Test): 0.32886794209480286\n",
            "Epoch 1135/10000: L(Train): 0.3509858548641205; L(Test): 0.32804155349731445\n",
            "Epoch 1136/10000: L(Train): 0.344877153635025; L(Test): 0.32696542143821716\n",
            "Epoch 1137/10000: L(Train): 0.3463200628757477; L(Test): 0.3262457251548767\n",
            "Epoch 1138/10000: L(Train): 0.34277889132499695; L(Test): 0.3299656808376312\n",
            "Epoch 1139/10000: L(Train): 0.3496139347553253; L(Test): 0.32645314931869507\n",
            "Epoch 1140/10000: L(Train): 0.3347443640232086; L(Test): 0.326482355594635\n",
            "Epoch 1141/10000: L(Train): 0.3420315384864807; L(Test): 0.32857564091682434\n",
            "Epoch 1142/10000: L(Train): 0.3366943299770355; L(Test): 0.3264670968055725\n",
            "Epoch 1143/10000: L(Train): 0.33933353424072266; L(Test): 0.3261592388153076\n",
            "Epoch 1144/10000: L(Train): 0.33369186520576477; L(Test): 0.3277033567428589\n",
            "Epoch 1145/10000: L(Train): 0.3488621413707733; L(Test): 0.32684966921806335\n",
            "Epoch 1146/10000: L(Train): 0.3418121933937073; L(Test): 0.3264187276363373\n",
            "Epoch 1147/10000: L(Train): 0.3397974669933319; L(Test): 0.3258342742919922\n",
            "Epoch 1148/10000: L(Train): 0.34613147377967834; L(Test): 0.3256711959838867\n",
            "Epoch 1149/10000: L(Train): 0.34744271636009216; L(Test): 0.3267744183540344\n",
            "Epoch 1150/10000: L(Train): 0.3393424153327942; L(Test): 0.3253529667854309\n",
            "Epoch 1151/10000: L(Train): 0.3442128598690033; L(Test): 0.3240855038166046\n",
            "Epoch 1152/10000: L(Train): 0.34364017844200134; L(Test): 0.3249227702617645\n",
            "Epoch 1153/10000: L(Train): 0.3368596136569977; L(Test): 0.3253767490386963\n",
            "Epoch 1154/10000: L(Train): 0.33987873792648315; L(Test): 0.32425370812416077\n",
            "Epoch 1155/10000: L(Train): 0.35133635997772217; L(Test): 0.32390645146369934\n",
            "Epoch 1156/10000: L(Train): 0.33716773986816406; L(Test): 0.32565027475357056\n",
            "Epoch 1157/10000: L(Train): 0.3449436128139496; L(Test): 0.32522353529930115\n",
            "Epoch 1158/10000: L(Train): 0.34463444352149963; L(Test): 0.32394304871559143\n",
            "Epoch 1159/10000: L(Train): 0.34571918845176697; L(Test): 0.324440062046051\n",
            "Epoch 1160/10000: L(Train): 0.34541553258895874; L(Test): 0.3242172300815582\n",
            "Epoch 1161/10000: L(Train): 0.3297118544578552; L(Test): 0.32409924268722534\n",
            "Epoch 1162/10000: L(Train): 0.34593725204467773; L(Test): 0.322660356760025\n",
            "Epoch 1163/10000: L(Train): 0.34367215633392334; L(Test): 0.32271841168403625\n",
            "Epoch 1164/10000: L(Train): 0.3393496870994568; L(Test): 0.3223631680011749\n",
            "Epoch 1165/10000: L(Train): 0.34886813163757324; L(Test): 0.32150471210479736\n",
            "Epoch 1166/10000: L(Train): 0.3388672471046448; L(Test): 0.32143062353134155\n",
            "Epoch 1167/10000: L(Train): 0.33430373668670654; L(Test): 0.3218727707862854\n",
            "Epoch 1168/10000: L(Train): 0.340503990650177; L(Test): 0.3227945864200592\n",
            "Epoch 1169/10000: L(Train): 0.3412100374698639; L(Test): 0.32297465205192566\n",
            "Epoch 1170/10000: L(Train): 0.3345862627029419; L(Test): 0.32238709926605225\n",
            "Epoch 1171/10000: L(Train): 0.3410228192806244; L(Test): 0.3225950300693512\n",
            "Epoch 1172/10000: L(Train): 0.3405441343784332; L(Test): 0.3226448595523834\n",
            "Epoch 1173/10000: L(Train): 0.34082290530204773; L(Test): 0.3226134181022644\n",
            "Epoch 1174/10000: L(Train): 0.3404025733470917; L(Test): 0.3217453956604004\n",
            "Epoch 1175/10000: L(Train): 0.33212998509407043; L(Test): 0.3212848901748657\n",
            "Epoch 1176/10000: L(Train): 0.3420385718345642; L(Test): 0.3209092319011688\n",
            "Epoch 1177/10000: L(Train): 0.3458861708641052; L(Test): 0.32118940353393555\n",
            "Epoch 1178/10000: L(Train): 0.33809688687324524; L(Test): 0.3213551938533783\n",
            "Epoch 1179/10000: L(Train): 0.33593451976776123; L(Test): 0.32194626331329346\n",
            "Epoch 1180/10000: L(Train): 0.3404812514781952; L(Test): 0.3220192492008209\n",
            "Epoch 1181/10000: L(Train): 0.3503653109073639; L(Test): 0.32156485319137573\n",
            "Epoch 1182/10000: L(Train): 0.33806312084198; L(Test): 0.32152897119522095\n",
            "Epoch 1183/10000: L(Train): 0.33707669377326965; L(Test): 0.32194599509239197\n",
            "Epoch 1184/10000: L(Train): 0.342201292514801; L(Test): 0.3220808207988739\n",
            "Epoch 1185/10000: L(Train): 0.3357071578502655; L(Test): 0.32276296615600586\n",
            "Epoch 1186/10000: L(Train): 0.34406498074531555; L(Test): 0.3215863108634949\n",
            "Epoch 1187/10000: L(Train): 0.3523400127887726; L(Test): 0.32110312581062317\n",
            "Epoch 1188/10000: L(Train): 0.33562377095222473; L(Test): 0.3209136128425598\n",
            "Epoch 1189/10000: L(Train): 0.33928102254867554; L(Test): 0.32109084725379944\n",
            "Epoch 1190/10000: L(Train): 0.3378791809082031; L(Test): 0.32162195444107056\n",
            "Epoch 1191/10000: L(Train): 0.34110307693481445; L(Test): 0.32183021306991577\n",
            "Epoch 1192/10000: L(Train): 0.35630398988723755; L(Test): 0.32154107093811035\n",
            "Epoch 1193/10000: L(Train): 0.33731764554977417; L(Test): 0.32150521874427795\n",
            "Epoch 1194/10000: L(Train): 0.33868351578712463; L(Test): 0.321357399225235\n",
            "Epoch 1195/10000: L(Train): 0.3517513573169708; L(Test): 0.32103317975997925\n",
            "Epoch 1196/10000: L(Train): 0.34554946422576904; L(Test): 0.3213210999965668\n",
            "Epoch 1197/10000: L(Train): 0.34332606196403503; L(Test): 0.32146310806274414\n",
            "Epoch 1198/10000: L(Train): 0.3333776891231537; L(Test): 0.3219926953315735\n",
            "Epoch 1199/10000: L(Train): 0.3420148193836212; L(Test): 0.32187795639038086\n",
            "Epoch 1200/10000: L(Train): 0.3425394296646118; L(Test): 0.3213384449481964\n",
            "Epoch 1201/10000: L(Train): 0.32920536398887634; L(Test): 0.32164719700813293\n",
            "Epoch 1202/10000: L(Train): 0.33801203966140747; L(Test): 0.32225120067596436\n",
            "Epoch 1203/10000: L(Train): 0.3408384919166565; L(Test): 0.32270848751068115\n",
            "Epoch 1204/10000: L(Train): 0.3390656113624573; L(Test): 0.3221447169780731\n",
            "Epoch 1205/10000: L(Train): 0.34842923283576965; L(Test): 0.3231804668903351\n",
            "Epoch 1206/10000: L(Train): 0.33076801896095276; L(Test): 0.3250485062599182\n",
            "Epoch 1207/10000: L(Train): 0.3383941054344177; L(Test): 0.3231421709060669\n",
            "Epoch 1208/10000: L(Train): 0.33911725878715515; L(Test): 0.32230740785598755\n",
            "Epoch 1209/10000: L(Train): 0.3443223536014557; L(Test): 0.323122501373291\n",
            "Epoch 1210/10000: L(Train): 0.34520208835601807; L(Test): 0.3225756585597992\n",
            "Epoch 1211/10000: L(Train): 0.34602898359298706; L(Test): 0.32193678617477417\n",
            "Epoch 1212/10000: L(Train): 0.3453842103481293; L(Test): 0.3221673369407654\n",
            "Epoch 1213/10000: L(Train): 0.33636969327926636; L(Test): 0.3218100070953369\n",
            "Epoch 1214/10000: L(Train): 0.3386600613594055; L(Test): 0.3216613233089447\n",
            "Epoch 1215/10000: L(Train): 0.3377765715122223; L(Test): 0.3220721185207367\n",
            "Epoch 1216/10000: L(Train): 0.3422318994998932; L(Test): 0.3217558264732361\n",
            "Epoch 1217/10000: L(Train): 0.33967283368110657; L(Test): 0.32161664962768555\n",
            "Epoch 1218/10000: L(Train): 0.342043936252594; L(Test): 0.32167917490005493\n",
            "Epoch 1219/10000: L(Train): 0.33689406514167786; L(Test): 0.3227785527706146\n",
            "Epoch 1220/10000: L(Train): 0.3399498760700226; L(Test): 0.32164403796195984\n",
            "Epoch 1221/10000: L(Train): 0.33978524804115295; L(Test): 0.32118865847587585\n",
            "Epoch 1222/10000: L(Train): 0.3461797535419464; L(Test): 0.32233136892318726\n",
            "Epoch 1223/10000: L(Train): 0.3416263163089752; L(Test): 0.3224831521511078\n",
            "Epoch 1224/10000: L(Train): 0.3380858898162842; L(Test): 0.32107359170913696\n",
            "Epoch 1225/10000: L(Train): 0.3410044312477112; L(Test): 0.32060706615448\n",
            "Epoch 1226/10000: L(Train): 0.3305385112762451; L(Test): 0.32179969549179077\n",
            "Epoch 1227/10000: L(Train): 0.3441660702228546; L(Test): 0.3223692774772644\n",
            "Epoch 1228/10000: L(Train): 0.3354009985923767; L(Test): 0.3205304443836212\n",
            "Epoch 1229/10000: L(Train): 0.3412138521671295; L(Test): 0.32147032022476196\n",
            "Epoch 1230/10000: L(Train): 0.34345945715904236; L(Test): 0.32113179564476013\n",
            "Epoch 1231/10000: L(Train): 0.34001287817955017; L(Test): 0.32039037346839905\n",
            "Epoch 1232/10000: L(Train): 0.33166229724884033; L(Test): 0.3207875192165375\n",
            "Epoch 1233/10000: L(Train): 0.3415624797344208; L(Test): 0.32051002979278564\n",
            "Epoch 1234/10000: L(Train): 0.33475297689437866; L(Test): 0.31954050064086914\n",
            "Epoch 1235/10000: L(Train): 0.33342796564102173; L(Test): 0.32005688548088074\n",
            "Epoch 1236/10000: L(Train): 0.3383258879184723; L(Test): 0.3209087550640106\n",
            "Epoch 1237/10000: L(Train): 0.33930298686027527; L(Test): 0.3206147253513336\n",
            "Epoch 1238/10000: L(Train): 0.33021342754364014; L(Test): 0.32064223289489746\n",
            "Epoch 1239/10000: L(Train): 0.3471572995185852; L(Test): 0.3208491802215576\n",
            "Epoch 1240/10000: L(Train): 0.3371245265007019; L(Test): 0.3205942213535309\n",
            "Epoch 1241/10000: L(Train): 0.3430107533931732; L(Test): 0.32084742188453674\n",
            "Epoch 1242/10000: L(Train): 0.34225207567214966; L(Test): 0.3199339509010315\n",
            "Epoch 1243/10000: L(Train): 0.34282198548316956; L(Test): 0.3197691738605499\n",
            "Epoch 1244/10000: L(Train): 0.33426812291145325; L(Test): 0.31995895504951477\n",
            "Epoch 1245/10000: L(Train): 0.3323309123516083; L(Test): 0.32037103176116943\n",
            "Epoch 1246/10000: L(Train): 0.3397888243198395; L(Test): 0.3207438886165619\n",
            "Epoch 1247/10000: L(Train): 0.33809173107147217; L(Test): 0.320101797580719\n",
            "Epoch 1248/10000: L(Train): 0.33465635776519775; L(Test): 0.3203590512275696\n",
            "Epoch 1249/10000: L(Train): 0.3419358730316162; L(Test): 0.3216981887817383\n",
            "Epoch 1250/10000: L(Train): 0.33747342228889465; L(Test): 0.32200026512145996\n",
            "Epoch 1251/10000: L(Train): 0.3402085304260254; L(Test): 0.3222220540046692\n",
            "Epoch 1252/10000: L(Train): 0.33886486291885376; L(Test): 0.3210376501083374\n",
            "Epoch 1253/10000: L(Train): 0.33519256114959717; L(Test): 0.3208398222923279\n",
            "Epoch 1254/10000: L(Train): 0.33874720335006714; L(Test): 0.32107076048851013\n",
            "Epoch 1255/10000: L(Train): 0.3287484645843506; L(Test): 0.32012268900871277\n",
            "Epoch 1256/10000: L(Train): 0.32903608679771423; L(Test): 0.31930607557296753\n",
            "Epoch 1257/10000: L(Train): 0.3272573947906494; L(Test): 0.31963419914245605\n",
            "Epoch 1258/10000: L(Train): 0.336063414812088; L(Test): 0.3194256126880646\n",
            "Epoch 1259/10000: L(Train): 0.33362072706222534; L(Test): 0.3193836510181427\n",
            "Epoch 1260/10000: L(Train): 0.3358291685581207; L(Test): 0.3196108043193817\n",
            "Epoch 1261/10000: L(Train): 0.33359283208847046; L(Test): 0.3195715546607971\n",
            "Epoch 1262/10000: L(Train): 0.3387782573699951; L(Test): 0.31869059801101685\n",
            "Epoch 1263/10000: L(Train): 0.3336030840873718; L(Test): 0.31796911358833313\n",
            "Epoch 1264/10000: L(Train): 0.35123375058174133; L(Test): 0.3183553218841553\n",
            "Epoch 1265/10000: L(Train): 0.33835309743881226; L(Test): 0.3182203769683838\n",
            "Epoch 1266/10000: L(Train): 0.3373977839946747; L(Test): 0.3182179927825928\n",
            "Epoch 1267/10000: L(Train): 0.33856821060180664; L(Test): 0.31837329268455505\n",
            "Epoch 1268/10000: L(Train): 0.33276301622390747; L(Test): 0.3187567889690399\n",
            "Epoch 1269/10000: L(Train): 0.34023159742355347; L(Test): 0.319366455078125\n",
            "Epoch 1270/10000: L(Train): 0.33804917335510254; L(Test): 0.3194279372692108\n",
            "Epoch 1271/10000: L(Train): 0.34030336141586304; L(Test): 0.3187941908836365\n",
            "Epoch 1272/10000: L(Train): 0.3349800109863281; L(Test): 0.31902799010276794\n",
            "Epoch 1273/10000: L(Train): 0.33858194947242737; L(Test): 0.3189123272895813\n",
            "Epoch 1274/10000: L(Train): 0.3461291193962097; L(Test): 0.3196527361869812\n",
            "Epoch 1275/10000: L(Train): 0.33870700001716614; L(Test): 0.3194831609725952\n",
            "Epoch 1276/10000: L(Train): 0.3451929986476898; L(Test): 0.3190712034702301\n",
            "Epoch 1277/10000: L(Train): 0.33672836422920227; L(Test): 0.3204939067363739\n",
            "Epoch 1278/10000: L(Train): 0.33856675028800964; L(Test): 0.32011470198631287\n",
            "Epoch 1279/10000: L(Train): 0.33743715286254883; L(Test): 0.319306343793869\n",
            "Epoch 1280/10000: L(Train): 0.3278573453426361; L(Test): 0.3192717432975769\n",
            "Epoch 1281/10000: L(Train): 0.33554449677467346; L(Test): 0.31932908296585083\n",
            "Epoch 1282/10000: L(Train): 0.3419353663921356; L(Test): 0.31951940059661865\n",
            "Epoch 1283/10000: L(Train): 0.3395403325557709; L(Test): 0.31868889927864075\n",
            "Epoch 1284/10000: L(Train): 0.3361801505088806; L(Test): 0.3195238411426544\n",
            "Epoch 1285/10000: L(Train): 0.3303646743297577; L(Test): 0.32071876525878906\n",
            "Epoch 1286/10000: L(Train): 0.34339073300361633; L(Test): 0.3195860981941223\n",
            "Epoch 1287/10000: L(Train): 0.33493486046791077; L(Test): 0.31918591260910034\n",
            "Epoch 1288/10000: L(Train): 0.3366670608520508; L(Test): 0.31902503967285156\n",
            "Epoch 1289/10000: L(Train): 0.3424510955810547; L(Test): 0.31924504041671753\n",
            "Epoch 1290/10000: L(Train): 0.33346661925315857; L(Test): 0.3191757798194885\n",
            "Epoch 1291/10000: L(Train): 0.34582802653312683; L(Test): 0.3180989921092987\n",
            "Epoch 1292/10000: L(Train): 0.3368397057056427; L(Test): 0.3181625306606293\n",
            "Epoch 1293/10000: L(Train): 0.3408537805080414; L(Test): 0.3186858594417572\n",
            "Epoch 1294/10000: L(Train): 0.3392447233200073; L(Test): 0.31865429878234863\n",
            "Epoch 1295/10000: L(Train): 0.3413742780685425; L(Test): 0.3183019161224365\n",
            "Epoch 1296/10000: L(Train): 0.32913997769355774; L(Test): 0.31796795129776\n",
            "Epoch 1297/10000: L(Train): 0.33865028619766235; L(Test): 0.31938838958740234\n",
            "Epoch 1298/10000: L(Train): 0.3314068019390106; L(Test): 0.3200435936450958\n",
            "Epoch 1299/10000: L(Train): 0.33932411670684814; L(Test): 0.3194079101085663\n",
            "Epoch 1300/10000: L(Train): 0.32841870188713074; L(Test): 0.31851446628570557\n",
            "Epoch 1301/10000: L(Train): 0.334576815366745; L(Test): 0.31835079193115234\n",
            "Epoch 1302/10000: L(Train): 0.3284270167350769; L(Test): 0.31926804780960083\n",
            "Epoch 1303/10000: L(Train): 0.3240492045879364; L(Test): 0.32036635279655457\n",
            "Epoch 1304/10000: L(Train): 0.33637821674346924; L(Test): 0.32117435336112976\n",
            "Epoch 1305/10000: L(Train): 0.3333851993083954; L(Test): 0.32047519087791443\n",
            "Epoch 1306/10000: L(Train): 0.34083035588264465; L(Test): 0.31873825192451477\n",
            "Epoch 1307/10000: L(Train): 0.3352099359035492; L(Test): 0.3185708820819855\n",
            "Epoch 1308/10000: L(Train): 0.33548644185066223; L(Test): 0.3192194402217865\n",
            "Epoch 1309/10000: L(Train): 0.3359779715538025; L(Test): 0.31885597109794617\n",
            "Epoch 1310/10000: L(Train): 0.33679670095443726; L(Test): 0.3182922601699829\n",
            "Epoch 1311/10000: L(Train): 0.3357294797897339; L(Test): 0.3198455572128296\n",
            "Epoch 1312/10000: L(Train): 0.33836236596107483; L(Test): 0.31906336545944214\n",
            "Epoch 1313/10000: L(Train): 0.33573654294013977; L(Test): 0.31925278902053833\n",
            "Epoch 1314/10000: L(Train): 0.3315572440624237; L(Test): 0.3209061324596405\n",
            "Epoch 1315/10000: L(Train): 0.34402066469192505; L(Test): 0.31948259472846985\n",
            "Epoch 1316/10000: L(Train): 0.34437093138694763; L(Test): 0.32048293948173523\n",
            "Epoch 1317/10000: L(Train): 0.32997533679008484; L(Test): 0.3188922107219696\n",
            "Epoch 1318/10000: L(Train): 0.33577805757522583; L(Test): 0.31941911578178406\n",
            "Epoch 1319/10000: L(Train): 0.3379116952419281; L(Test): 0.31902629137039185\n",
            "Epoch 1320/10000: L(Train): 0.34082189202308655; L(Test): 0.3188517391681671\n",
            "Epoch 1321/10000: L(Train): 0.3368212878704071; L(Test): 0.318485826253891\n",
            "Epoch 1322/10000: L(Train): 0.34474971890449524; L(Test): 0.31776076555252075\n",
            "Epoch 1323/10000: L(Train): 0.34667789936065674; L(Test): 0.3185097575187683\n",
            "Epoch 1324/10000: L(Train): 0.33264607191085815; L(Test): 0.31915292143821716\n",
            "Epoch 1325/10000: L(Train): 0.3291284441947937; L(Test): 0.31858327984809875\n",
            "Epoch 1326/10000: L(Train): 0.33557331562042236; L(Test): 0.3185635507106781\n",
            "Epoch 1327/10000: L(Train): 0.33591732382774353; L(Test): 0.31859663128852844\n",
            "Epoch 1328/10000: L(Train): 0.34014928340911865; L(Test): 0.3178369700908661\n",
            "Epoch 1329/10000: L(Train): 0.3406609296798706; L(Test): 0.31742534041404724\n",
            "Epoch 1330/10000: L(Train): 0.325479120016098; L(Test): 0.318224161863327\n",
            "Epoch 1331/10000: L(Train): 0.34664276242256165; L(Test): 0.3179553747177124\n",
            "Epoch 1332/10000: L(Train): 0.33720627427101135; L(Test): 0.31779053807258606\n",
            "Epoch 1333/10000: L(Train): 0.33899617195129395; L(Test): 0.3188977539539337\n",
            "Epoch 1334/10000: L(Train): 0.3411201536655426; L(Test): 0.3186664581298828\n",
            "Epoch 1335/10000: L(Train): 0.3375012278556824; L(Test): 0.3191114068031311\n",
            "Epoch 1336/10000: L(Train): 0.3362438380718231; L(Test): 0.3177443742752075\n",
            "Epoch 1337/10000: L(Train): 0.33229517936706543; L(Test): 0.3178291618824005\n",
            "Epoch 1338/10000: L(Train): 0.3396965563297272; L(Test): 0.3181524872779846\n",
            "Epoch 1339/10000: L(Train): 0.34267497062683105; L(Test): 0.31788837909698486\n",
            "Epoch 1340/10000: L(Train): 0.34049156308174133; L(Test): 0.31805235147476196\n",
            "Epoch 1341/10000: L(Train): 0.3397253453731537; L(Test): 0.3167964816093445\n",
            "Epoch 1342/10000: L(Train): 0.3396323323249817; L(Test): 0.31699833273887634\n",
            "Epoch 1343/10000: L(Train): 0.3377285897731781; L(Test): 0.3184340000152588\n",
            "Epoch 1344/10000: L(Train): 0.3364376127719879; L(Test): 0.3178098499774933\n",
            "Epoch 1345/10000: L(Train): 0.34367555379867554; L(Test): 0.31790637969970703\n",
            "Epoch 1346/10000: L(Train): 0.34104645252227783; L(Test): 0.31853625178337097\n",
            "Epoch 1347/10000: L(Train): 0.3372440040111542; L(Test): 0.31874218583106995\n",
            "Epoch 1348/10000: L(Train): 0.3353402018547058; L(Test): 0.31840646266937256\n",
            "Epoch 1349/10000: L(Train): 0.34012943506240845; L(Test): 0.31847527623176575\n",
            "Epoch 1350/10000: L(Train): 0.3316251337528229; L(Test): 0.319132924079895\n",
            "Epoch 1351/10000: L(Train): 0.3356602191925049; L(Test): 0.3195285201072693\n",
            "Epoch 1352/10000: L(Train): 0.33479902148246765; L(Test): 0.3184499144554138\n",
            "Epoch 1353/10000: L(Train): 0.3341206908226013; L(Test): 0.31791630387306213\n",
            "Epoch 1354/10000: L(Train): 0.3413340151309967; L(Test): 0.3178110718727112\n",
            "Epoch 1355/10000: L(Train): 0.33818113803863525; L(Test): 0.3182239532470703\n",
            "Epoch 1356/10000: L(Train): 0.3324234187602997; L(Test): 0.3185078203678131\n",
            "Epoch 1357/10000: L(Train): 0.3372434377670288; L(Test): 0.3181707561016083\n",
            "Epoch 1358/10000: L(Train): 0.33413583040237427; L(Test): 0.3188343644142151\n",
            "Epoch 1359/10000: L(Train): 0.3358881175518036; L(Test): 0.31814202666282654\n",
            "Epoch 1360/10000: L(Train): 0.33952564001083374; L(Test): 0.31961044669151306\n",
            "Epoch 1361/10000: L(Train): 0.3464287519454956; L(Test): 0.3204526901245117\n",
            "Epoch 1362/10000: L(Train): 0.3485562801361084; L(Test): 0.31990811228752136\n",
            "Epoch 1363/10000: L(Train): 0.3393329381942749; L(Test): 0.31941571831703186\n",
            "Epoch 1364/10000: L(Train): 0.33129438757896423; L(Test): 0.3190268278121948\n",
            "Epoch 1365/10000: L(Train): 0.34023380279541016; L(Test): 0.31993842124938965\n",
            "Epoch 1366/10000: L(Train): 0.3369533121585846; L(Test): 0.3189026415348053\n",
            "Epoch 1367/10000: L(Train): 0.3355209529399872; L(Test): 0.31870409846305847\n",
            "Epoch 1368/10000: L(Train): 0.33916330337524414; L(Test): 0.3191412687301636\n",
            "Epoch 1369/10000: L(Train): 0.3374147415161133; L(Test): 0.3182315230369568\n",
            "Epoch 1370/10000: L(Train): 0.3412763178348541; L(Test): 0.3186662495136261\n",
            "Epoch 1371/10000: L(Train): 0.3321331739425659; L(Test): 0.31901827454566956\n",
            "Epoch 1372/10000: L(Train): 0.3491356372833252; L(Test): 0.31693029403686523\n",
            "Epoch 1373/10000: L(Train): 0.3345509171485901; L(Test): 0.31831860542297363\n",
            "Epoch 1374/10000: L(Train): 0.3429816961288452; L(Test): 0.31853559613227844\n",
            "Epoch 1375/10000: L(Train): 0.3389708399772644; L(Test): 0.3168315291404724\n",
            "Epoch 1376/10000: L(Train): 0.3414310812950134; L(Test): 0.31842854619026184\n",
            "Epoch 1377/10000: L(Train): 0.3490771949291229; L(Test): 0.31829944252967834\n",
            "Epoch 1378/10000: L(Train): 0.34111928939819336; L(Test): 0.3177947998046875\n",
            "Epoch 1379/10000: L(Train): 0.3361961245536804; L(Test): 0.31786271929740906\n",
            "Epoch 1380/10000: L(Train): 0.33156776428222656; L(Test): 0.3178393244743347\n",
            "Epoch 1381/10000: L(Train): 0.3284003734588623; L(Test): 0.3188560903072357\n",
            "Epoch 1382/10000: L(Train): 0.337196409702301; L(Test): 0.31789910793304443\n",
            "Epoch 1383/10000: L(Train): 0.3433646261692047; L(Test): 0.3173340857028961\n",
            "Epoch 1384/10000: L(Train): 0.3336889147758484; L(Test): 0.3184114694595337\n",
            "Epoch 1385/10000: L(Train): 0.3384298086166382; L(Test): 0.3176432251930237\n",
            "Epoch 1386/10000: L(Train): 0.34405264258384705; L(Test): 0.3180072605609894\n",
            "Epoch 1387/10000: L(Train): 0.33530354499816895; L(Test): 0.3186875879764557\n",
            "Epoch 1388/10000: L(Train): 0.3408814072608948; L(Test): 0.31800439953804016\n",
            "Epoch 1389/10000: L(Train): 0.33134356141090393; L(Test): 0.31801241636276245\n",
            "Epoch 1390/10000: L(Train): 0.3345058858394623; L(Test): 0.3175185024738312\n",
            "Epoch 1391/10000: L(Train): 0.33972692489624023; L(Test): 0.31807029247283936\n",
            "Epoch 1392/10000: L(Train): 0.34330984950065613; L(Test): 0.3172314763069153\n",
            "Epoch 1393/10000: L(Train): 0.32399824261665344; L(Test): 0.31683850288391113\n",
            "Epoch 1394/10000: L(Train): 0.34222155809402466; L(Test): 0.3167421221733093\n",
            "Epoch 1395/10000: L(Train): 0.33281996846199036; L(Test): 0.31692448258399963\n",
            "Epoch 1396/10000: L(Train): 0.3387378454208374; L(Test): 0.31659135222435\n",
            "Epoch 1397/10000: L(Train): 0.3273581266403198; L(Test): 0.31597840785980225\n",
            "Epoch 1398/10000: L(Train): 0.3271525800228119; L(Test): 0.3164633512496948\n",
            "Epoch 1399/10000: L(Train): 0.3376942574977875; L(Test): 0.31700900197029114\n",
            "Epoch 1400/10000: L(Train): 0.34013649821281433; L(Test): 0.31628885865211487\n",
            "Epoch 1401/10000: L(Train): 0.3403066098690033; L(Test): 0.3162396550178528\n",
            "Epoch 1402/10000: L(Train): 0.33439627289772034; L(Test): 0.3165299594402313\n",
            "Epoch 1403/10000: L(Train): 0.32723933458328247; L(Test): 0.3153453767299652\n",
            "Epoch 1404/10000: L(Train): 0.3261505663394928; L(Test): 0.3159641921520233\n",
            "Epoch 1405/10000: L(Train): 0.33291491866111755; L(Test): 0.31681013107299805\n",
            "Epoch 1406/10000: L(Train): 0.33688047528266907; L(Test): 0.3161073625087738\n",
            "Epoch 1407/10000: L(Train): 0.3385355472564697; L(Test): 0.3168582022190094\n",
            "Epoch 1408/10000: L(Train): 0.3239952325820923; L(Test): 0.3174360692501068\n",
            "Epoch 1409/10000: L(Train): 0.3291607201099396; L(Test): 0.31624141335487366\n",
            "Epoch 1410/10000: L(Train): 0.34711676836013794; L(Test): 0.315873920917511\n",
            "Epoch 1411/10000: L(Train): 0.33878350257873535; L(Test): 0.3159848749637604\n",
            "Epoch 1412/10000: L(Train): 0.34013497829437256; L(Test): 0.31561529636383057\n",
            "Epoch 1413/10000: L(Train): 0.3347431719303131; L(Test): 0.3162844181060791\n",
            "Epoch 1414/10000: L(Train): 0.34316518902778625; L(Test): 0.3165357708930969\n",
            "Epoch 1415/10000: L(Train): 0.34048861265182495; L(Test): 0.3163549304008484\n",
            "Epoch 1416/10000: L(Train): 0.33715513348579407; L(Test): 0.3166920840740204\n",
            "Epoch 1417/10000: L(Train): 0.33564141392707825; L(Test): 0.31649062037467957\n",
            "Epoch 1418/10000: L(Train): 0.3391006588935852; L(Test): 0.31662896275520325\n",
            "Epoch 1419/10000: L(Train): 0.32384583353996277; L(Test): 0.31681323051452637\n",
            "Epoch 1420/10000: L(Train): 0.3351871967315674; L(Test): 0.316292405128479\n",
            "Epoch 1421/10000: L(Train): 0.34505990147590637; L(Test): 0.3157988488674164\n",
            "Epoch 1422/10000: L(Train): 0.3308393955230713; L(Test): 0.31543460488319397\n",
            "Epoch 1423/10000: L(Train): 0.33363446593284607; L(Test): 0.31554532051086426\n",
            "Epoch 1424/10000: L(Train): 0.3388215899467468; L(Test): 0.31628844141960144\n",
            "Epoch 1425/10000: L(Train): 0.3370756208896637; L(Test): 0.315754234790802\n",
            "Epoch 1426/10000: L(Train): 0.33363664150238037; L(Test): 0.3156696557998657\n",
            "Epoch 1427/10000: L(Train): 0.3293094038963318; L(Test): 0.31595468521118164\n",
            "Epoch 1428/10000: L(Train): 0.3355569541454315; L(Test): 0.3151969611644745\n",
            "Epoch 1429/10000: L(Train): 0.3344112038612366; L(Test): 0.3158004879951477\n",
            "Epoch 1430/10000: L(Train): 0.34576216340065; L(Test): 0.31637462973594666\n",
            "Epoch 1431/10000: L(Train): 0.33116069436073303; L(Test): 0.3154168128967285\n",
            "Epoch 1432/10000: L(Train): 0.3355679214000702; L(Test): 0.3162400424480438\n",
            "Epoch 1433/10000: L(Train): 0.3290666937828064; L(Test): 0.3167687952518463\n",
            "Epoch 1434/10000: L(Train): 0.3332204222679138; L(Test): 0.31680551171302795\n",
            "Epoch 1435/10000: L(Train): 0.33734330534935; L(Test): 0.316394180059433\n",
            "Epoch 1436/10000: L(Train): 0.3367939293384552; L(Test): 0.3154323995113373\n",
            "Epoch 1437/10000: L(Train): 0.3320122957229614; L(Test): 0.3157578408718109\n",
            "Epoch 1438/10000: L(Train): 0.3462810218334198; L(Test): 0.3153752386569977\n",
            "Epoch 1439/10000: L(Train): 0.33109381794929504; L(Test): 0.31571638584136963\n",
            "Epoch 1440/10000: L(Train): 0.33648744225502014; L(Test): 0.31698015332221985\n",
            "Epoch 1441/10000: L(Train): 0.33539170026779175; L(Test): 0.3160804808139801\n",
            "Epoch 1442/10000: L(Train): 0.3391435742378235; L(Test): 0.3162212371826172\n",
            "Epoch 1443/10000: L(Train): 0.3313659131526947; L(Test): 0.3168286979198456\n",
            "Epoch 1444/10000: L(Train): 0.33433547616004944; L(Test): 0.3156334161758423\n",
            "Epoch 1445/10000: L(Train): 0.33386656641960144; L(Test): 0.31693726778030396\n",
            "Epoch 1446/10000: L(Train): 0.33383917808532715; L(Test): 0.3166191577911377\n",
            "Epoch 1447/10000: L(Train): 0.33537042140960693; L(Test): 0.3156479597091675\n",
            "Epoch 1448/10000: L(Train): 0.3355987071990967; L(Test): 0.31665095686912537\n",
            "Epoch 1449/10000: L(Train): 0.3334995210170746; L(Test): 0.3161730170249939\n",
            "Epoch 1450/10000: L(Train): 0.33308690786361694; L(Test): 0.31721651554107666\n",
            "Epoch 1451/10000: L(Train): 0.3342783451080322; L(Test): 0.3162311315536499\n",
            "Epoch 1452/10000: L(Train): 0.339810848236084; L(Test): 0.3178904950618744\n",
            "Epoch 1453/10000: L(Train): 0.3352789878845215; L(Test): 0.31838998198509216\n",
            "Epoch 1454/10000: L(Train): 0.34060052037239075; L(Test): 0.3181055784225464\n",
            "Epoch 1455/10000: L(Train): 0.34686341881752014; L(Test): 0.31875699758529663\n",
            "Epoch 1456/10000: L(Train): 0.3429449498653412; L(Test): 0.317626029253006\n",
            "Epoch 1457/10000: L(Train): 0.3370305895805359; L(Test): 0.3179989755153656\n",
            "Epoch 1458/10000: L(Train): 0.34335654973983765; L(Test): 0.3181518316268921\n",
            "Epoch 1459/10000: L(Train): 0.3315047323703766; L(Test): 0.3173142075538635\n",
            "Epoch 1460/10000: L(Train): 0.3284549415111542; L(Test): 0.3186255991458893\n",
            "Epoch 1461/10000: L(Train): 0.32761967182159424; L(Test): 0.31742602586746216\n",
            "Epoch 1462/10000: L(Train): 0.3344593644142151; L(Test): 0.31767135858535767\n",
            "Epoch 1463/10000: L(Train): 0.33492574095726013; L(Test): 0.31823647022247314\n",
            "Epoch 1464/10000: L(Train): 0.3393188714981079; L(Test): 0.3172021210193634\n",
            "Epoch 1465/10000: L(Train): 0.33844345808029175; L(Test): 0.31746339797973633\n",
            "Epoch 1466/10000: L(Train): 0.3308059871196747; L(Test): 0.31742361187934875\n",
            "Epoch 1467/10000: L(Train): 0.34290966391563416; L(Test): 0.31713467836380005\n",
            "Epoch 1468/10000: L(Train): 0.3352305293083191; L(Test): 0.31740519404411316\n",
            "Epoch 1469/10000: L(Train): 0.33949014544487; L(Test): 0.3166196644306183\n",
            "Epoch 1470/10000: L(Train): 0.33777058124542236; L(Test): 0.31679898500442505\n",
            "Epoch 1471/10000: L(Train): 0.3331746459007263; L(Test): 0.31761595606803894\n",
            "Epoch 1472/10000: L(Train): 0.34064966440200806; L(Test): 0.31669482588768005\n",
            "Epoch 1473/10000: L(Train): 0.3378335237503052; L(Test): 0.31676754355430603\n",
            "Epoch 1474/10000: L(Train): 0.33567431569099426; L(Test): 0.3176756501197815\n",
            "Epoch 1475/10000: L(Train): 0.3314037024974823; L(Test): 0.3165239691734314\n",
            "Epoch 1476/10000: L(Train): 0.3439139127731323; L(Test): 0.31664249300956726\n",
            "Epoch 1477/10000: L(Train): 0.33044302463531494; L(Test): 0.31635555624961853\n",
            "Epoch 1478/10000: L(Train): 0.3360217809677124; L(Test): 0.3162011504173279\n",
            "Epoch 1479/10000: L(Train): 0.33488866686820984; L(Test): 0.3179245889186859\n",
            "Epoch 1480/10000: L(Train): 0.3337104320526123; L(Test): 0.31760671734809875\n",
            "Epoch 1481/10000: L(Train): 0.3330148160457611; L(Test): 0.31646066904067993\n",
            "Epoch 1482/10000: L(Train): 0.3342861235141754; L(Test): 0.31676751375198364\n",
            "Epoch 1483/10000: L(Train): 0.33895057439804077; L(Test): 0.3163546621799469\n",
            "Epoch 1484/10000: L(Train): 0.338157057762146; L(Test): 0.3153820335865021\n",
            "Epoch 1485/10000: L(Train): 0.32582181692123413; L(Test): 0.3170451521873474\n",
            "Epoch 1486/10000: L(Train): 0.33502742648124695; L(Test): 0.31710368394851685\n",
            "Epoch 1487/10000: L(Train): 0.33533674478530884; L(Test): 0.31511273980140686\n",
            "Epoch 1488/10000: L(Train): 0.3344773054122925; L(Test): 0.31559208035469055\n",
            "Epoch 1489/10000: L(Train): 0.3291252851486206; L(Test): 0.3163085877895355\n",
            "Epoch 1490/10000: L(Train): 0.3324533700942993; L(Test): 0.3166521191596985\n",
            "Epoch 1491/10000: L(Train): 0.3423823416233063; L(Test): 0.3165847063064575\n",
            "Epoch 1492/10000: L(Train): 0.33841440081596375; L(Test): 0.3161605894565582\n",
            "Epoch 1493/10000: L(Train): 0.33439889550209045; L(Test): 0.31586915254592896\n",
            "Epoch 1494/10000: L(Train): 0.33430495858192444; L(Test): 0.3151266872882843\n",
            "Epoch 1495/10000: L(Train): 0.3341505229473114; L(Test): 0.3158697783946991\n",
            "Epoch 1496/10000: L(Train): 0.3427623510360718; L(Test): 0.31561219692230225\n",
            "Epoch 1497/10000: L(Train): 0.3341407775878906; L(Test): 0.31470444798469543\n",
            "Epoch 1498/10000: L(Train): 0.3327394425868988; L(Test): 0.3151595890522003\n",
            "Epoch 1499/10000: L(Train): 0.33655664324760437; L(Test): 0.31521862745285034\n",
            "Epoch 1500/10000: L(Train): 0.3397248685359955; L(Test): 0.3156619668006897\n",
            "Epoch 1501/10000: L(Train): 0.3360992968082428; L(Test): 0.31629934906959534\n",
            "Epoch 1502/10000: L(Train): 0.3343130350112915; L(Test): 0.31558987498283386\n",
            "Epoch 1503/10000: L(Train): 0.33195924758911133; L(Test): 0.3159124255180359\n",
            "Epoch 1504/10000: L(Train): 0.340181827545166; L(Test): 0.3163152039051056\n",
            "Epoch 1505/10000: L(Train): 0.3362390995025635; L(Test): 0.3161357343196869\n",
            "Epoch 1506/10000: L(Train): 0.337030291557312; L(Test): 0.31527748703956604\n",
            "Epoch 1507/10000: L(Train): 0.3240123987197876; L(Test): 0.3155166804790497\n",
            "Epoch 1508/10000: L(Train): 0.33780911564826965; L(Test): 0.3160322904586792\n",
            "Epoch 1509/10000: L(Train): 0.3287639915943146; L(Test): 0.31618985533714294\n",
            "Epoch 1510/10000: L(Train): 0.34331607818603516; L(Test): 0.31502917408943176\n",
            "Epoch 1511/10000: L(Train): 0.3369682729244232; L(Test): 0.3151538074016571\n",
            "Epoch 1512/10000: L(Train): 0.338360995054245; L(Test): 0.31622567772865295\n",
            "Epoch 1513/10000: L(Train): 0.3337464928627014; L(Test): 0.3156501352787018\n",
            "Epoch 1514/10000: L(Train): 0.342944860458374; L(Test): 0.3157977759838104\n",
            "Epoch 1515/10000: L(Train): 0.33484986424446106; L(Test): 0.3175843358039856\n",
            "Epoch 1516/10000: L(Train): 0.3377700746059418; L(Test): 0.31607285141944885\n",
            "Epoch 1517/10000: L(Train): 0.341106653213501; L(Test): 0.3158339560031891\n",
            "Epoch 1518/10000: L(Train): 0.33652952313423157; L(Test): 0.31625664234161377\n",
            "Epoch 1519/10000: L(Train): 0.3224051296710968; L(Test): 0.3161603808403015\n",
            "Epoch 1520/10000: L(Train): 0.33538562059402466; L(Test): 0.31855088472366333\n",
            "Epoch 1521/10000: L(Train): 0.326200395822525; L(Test): 0.31856927275657654\n",
            "Epoch 1522/10000: L(Train): 0.3409694731235504; L(Test): 0.3162820339202881\n",
            "Epoch 1523/10000: L(Train): 0.34189292788505554; L(Test): 0.3150140643119812\n",
            "Epoch 1524/10000: L(Train): 0.3281608521938324; L(Test): 0.31533023715019226\n",
            "Epoch 1525/10000: L(Train): 0.3429473042488098; L(Test): 0.31560757756233215\n",
            "Epoch 1526/10000: L(Train): 0.33161041140556335; L(Test): 0.3159042000770569\n",
            "Epoch 1527/10000: L(Train): 0.32909032702445984; L(Test): 0.31568366289138794\n",
            "Epoch 1528/10000: L(Train): 0.3416725695133209; L(Test): 0.3158218562602997\n",
            "Epoch 1529/10000: L(Train): 0.3431146442890167; L(Test): 0.3163437843322754\n",
            "Epoch 1530/10000: L(Train): 0.3414878845214844; L(Test): 0.31699123978614807\n",
            "Epoch 1531/10000: L(Train): 0.3267996907234192; L(Test): 0.3167273998260498\n",
            "Epoch 1532/10000: L(Train): 0.3263500928878784; L(Test): 0.31743451952934265\n",
            "Epoch 1533/10000: L(Train): 0.3376200199127197; L(Test): 0.3171943724155426\n",
            "Epoch 1534/10000: L(Train): 0.3382453918457031; L(Test): 0.3160652220249176\n",
            "Epoch 1535/10000: L(Train): 0.3389988839626312; L(Test): 0.316228985786438\n",
            "Epoch 1536/10000: L(Train): 0.3402824103832245; L(Test): 0.31602743268013\n",
            "Epoch 1537/10000: L(Train): 0.3434314429759979; L(Test): 0.31501051783561707\n",
            "Epoch 1538/10000: L(Train): 0.3344707190990448; L(Test): 0.3159688711166382\n",
            "Epoch 1539/10000: L(Train): 0.3354787528514862; L(Test): 0.31786513328552246\n",
            "Epoch 1540/10000: L(Train): 0.33792755007743835; L(Test): 0.3174591362476349\n",
            "Epoch 1541/10000: L(Train): 0.3315114676952362; L(Test): 0.31572988629341125\n",
            "Epoch 1542/10000: L(Train): 0.32762137055397034; L(Test): 0.3148253858089447\n",
            "Epoch 1543/10000: L(Train): 0.33474209904670715; L(Test): 0.31560856103897095\n",
            "Epoch 1544/10000: L(Train): 0.335254967212677; L(Test): 0.31588470935821533\n",
            "Epoch 1545/10000: L(Train): 0.33963069319725037; L(Test): 0.31507793068885803\n",
            "Epoch 1546/10000: L(Train): 0.33346250653266907; L(Test): 0.31527888774871826\n",
            "Epoch 1547/10000: L(Train): 0.3331610858440399; L(Test): 0.31485918164253235\n",
            "Epoch 1548/10000: L(Train): 0.32823067903518677; L(Test): 0.31542375683784485\n",
            "Epoch 1549/10000: L(Train): 0.33400630950927734; L(Test): 0.31599268317222595\n",
            "Epoch 1550/10000: L(Train): 0.32703739404678345; L(Test): 0.31587648391723633\n",
            "Epoch 1551/10000: L(Train): 0.3341560959815979; L(Test): 0.3151387870311737\n",
            "Epoch 1552/10000: L(Train): 0.3285326063632965; L(Test): 0.3144001066684723\n",
            "Epoch 1553/10000: L(Train): 0.32755154371261597; L(Test): 0.3140995502471924\n",
            "Epoch 1554/10000: L(Train): 0.3332078158855438; L(Test): 0.3147115111351013\n",
            "Epoch 1555/10000: L(Train): 0.33853399753570557; L(Test): 0.3142007887363434\n",
            "Epoch 1556/10000: L(Train): 0.34340357780456543; L(Test): 0.3135484755039215\n",
            "Epoch 1557/10000: L(Train): 0.3302566409111023; L(Test): 0.31409555673599243\n",
            "Epoch 1558/10000: L(Train): 0.3371800482273102; L(Test): 0.31367477774620056\n",
            "Epoch 1559/10000: L(Train): 0.3400377631187439; L(Test): 0.3130176365375519\n",
            "Epoch 1560/10000: L(Train): 0.3401530385017395; L(Test): 0.31309711933135986\n",
            "Epoch 1561/10000: L(Train): 0.3343918025493622; L(Test): 0.31387314200401306\n",
            "Epoch 1562/10000: L(Train): 0.33501461148262024; L(Test): 0.31344419717788696\n",
            "Epoch 1563/10000: L(Train): 0.33533957600593567; L(Test): 0.313708633184433\n",
            "Epoch 1564/10000: L(Train): 0.33254510164260864; L(Test): 0.3147376477718353\n",
            "Epoch 1565/10000: L(Train): 0.33517372608184814; L(Test): 0.3152026832103729\n",
            "Epoch 1566/10000: L(Train): 0.3266899585723877; L(Test): 0.3150176405906677\n",
            "Epoch 1567/10000: L(Train): 0.3310534358024597; L(Test): 0.31500568985939026\n",
            "Epoch 1568/10000: L(Train): 0.33496183156967163; L(Test): 0.31481119990348816\n",
            "Epoch 1569/10000: L(Train): 0.3338315188884735; L(Test): 0.31453973054885864\n",
            "Epoch 1570/10000: L(Train): 0.33138155937194824; L(Test): 0.31443747878074646\n",
            "Epoch 1571/10000: L(Train): 0.33289480209350586; L(Test): 0.31381756067276\n",
            "Epoch 1572/10000: L(Train): 0.32782459259033203; L(Test): 0.31356948614120483\n",
            "Epoch 1573/10000: L(Train): 0.3337447941303253; L(Test): 0.3139624297618866\n",
            "Epoch 1574/10000: L(Train): 0.3367505967617035; L(Test): 0.31419605016708374\n",
            "Epoch 1575/10000: L(Train): 0.3300473093986511; L(Test): 0.31394657492637634\n",
            "Epoch 1576/10000: L(Train): 0.3397931158542633; L(Test): 0.3139062523841858\n",
            "Epoch 1577/10000: L(Train): 0.34068983793258667; L(Test): 0.3133002519607544\n",
            "Epoch 1578/10000: L(Train): 0.3197275400161743; L(Test): 0.3141475319862366\n",
            "Epoch 1579/10000: L(Train): 0.32917025685310364; L(Test): 0.3163723051548004\n",
            "Epoch 1580/10000: L(Train): 0.34133586287498474; L(Test): 0.31381475925445557\n",
            "Epoch 1581/10000: L(Train): 0.32129332423210144; L(Test): 0.31433215737342834\n",
            "Epoch 1582/10000: L(Train): 0.3267401456832886; L(Test): 0.3158071041107178\n",
            "Epoch 1583/10000: L(Train): 0.3367655575275421; L(Test): 0.3152640163898468\n",
            "Epoch 1584/10000: L(Train): 0.3333853781223297; L(Test): 0.3141499161720276\n",
            "Epoch 1585/10000: L(Train): 0.33256974816322327; L(Test): 0.3141917288303375\n",
            "Epoch 1586/10000: L(Train): 0.33187195658683777; L(Test): 0.31358829140663147\n",
            "Epoch 1587/10000: L(Train): 0.3284575045108795; L(Test): 0.31427454948425293\n",
            "Epoch 1588/10000: L(Train): 0.32948538661003113; L(Test): 0.3140578269958496\n",
            "Epoch 1589/10000: L(Train): 0.33564862608909607; L(Test): 0.31482604146003723\n",
            "Epoch 1590/10000: L(Train): 0.33752307295799255; L(Test): 0.3158440589904785\n",
            "Epoch 1591/10000: L(Train): 0.3363548517227173; L(Test): 0.3151097297668457\n",
            "Epoch 1592/10000: L(Train): 0.3296377658843994; L(Test): 0.31445375084877014\n",
            "Epoch 1593/10000: L(Train): 0.3285457193851471; L(Test): 0.3144682049751282\n",
            "Epoch 1594/10000: L(Train): 0.3344258666038513; L(Test): 0.3138713538646698\n",
            "Epoch 1595/10000: L(Train): 0.3324347138404846; L(Test): 0.31366345286369324\n",
            "Epoch 1596/10000: L(Train): 0.3253782391548157; L(Test): 0.31453004479408264\n",
            "Epoch 1597/10000: L(Train): 0.33723893761634827; L(Test): 0.31356534361839294\n",
            "Epoch 1598/10000: L(Train): 0.33435502648353577; L(Test): 0.3138393461704254\n",
            "Epoch 1599/10000: L(Train): 0.3316989541053772; L(Test): 0.3136156499385834\n",
            "Epoch 1600/10000: L(Train): 0.3319094479084015; L(Test): 0.3140462338924408\n",
            "Epoch 1601/10000: L(Train): 0.3316992223262787; L(Test): 0.31421807408332825\n",
            "Epoch 1602/10000: L(Train): 0.3408578336238861; L(Test): 0.3146817088127136\n",
            "Epoch 1603/10000: L(Train): 0.3339681029319763; L(Test): 0.31456342339515686\n",
            "Epoch 1604/10000: L(Train): 0.3440167009830475; L(Test): 0.31378793716430664\n",
            "Epoch 1605/10000: L(Train): 0.33921775221824646; L(Test): 0.3140093982219696\n",
            "Epoch 1606/10000: L(Train): 0.33777323365211487; L(Test): 0.3148210644721985\n",
            "Epoch 1607/10000: L(Train): 0.33701521158218384; L(Test): 0.31382033228874207\n",
            "Epoch 1608/10000: L(Train): 0.32631978392601013; L(Test): 0.31341201066970825\n",
            "Epoch 1609/10000: L(Train): 0.3308848440647125; L(Test): 0.3138672113418579\n",
            "Epoch 1610/10000: L(Train): 0.3278198838233948; L(Test): 0.3132781386375427\n",
            "Epoch 1611/10000: L(Train): 0.3302322328090668; L(Test): 0.3134946823120117\n",
            "Epoch 1612/10000: L(Train): 0.334298312664032; L(Test): 0.3135165870189667\n",
            "Epoch 1613/10000: L(Train): 0.3394681513309479; L(Test): 0.31358709931373596\n",
            "Epoch 1614/10000: L(Train): 0.339530885219574; L(Test): 0.3145662248134613\n",
            "Epoch 1615/10000: L(Train): 0.3374224007129669; L(Test): 0.3156544864177704\n",
            "Epoch 1616/10000: L(Train): 0.3288952112197876; L(Test): 0.31601211428642273\n",
            "Epoch 1617/10000: L(Train): 0.34234726428985596; L(Test): 0.31570181250572205\n",
            "Epoch 1618/10000: L(Train): 0.3282456398010254; L(Test): 0.3151441216468811\n",
            "Epoch 1619/10000: L(Train): 0.3279798924922943; L(Test): 0.3150022029876709\n",
            "Epoch 1620/10000: L(Train): 0.3276546895503998; L(Test): 0.3146985173225403\n",
            "Epoch 1621/10000: L(Train): 0.33168694376945496; L(Test): 0.3160402476787567\n",
            "Epoch 1622/10000: L(Train): 0.34329742193222046; L(Test): 0.3170006275177002\n",
            "Epoch 1623/10000: L(Train): 0.32699254155158997; L(Test): 0.31718119978904724\n",
            "Epoch 1624/10000: L(Train): 0.33121457695961; L(Test): 0.3180006742477417\n",
            "Epoch 1625/10000: L(Train): 0.3403557240962982; L(Test): 0.3162626624107361\n",
            "Epoch 1626/10000: L(Train): 0.3292255997657776; L(Test): 0.3155057728290558\n",
            "Epoch 1627/10000: L(Train): 0.3256511092185974; L(Test): 0.3158373236656189\n",
            "Epoch 1628/10000: L(Train): 0.34346044063568115; L(Test): 0.3139544725418091\n",
            "Epoch 1629/10000: L(Train): 0.33493146300315857; L(Test): 0.31498992443084717\n",
            "Epoch 1630/10000: L(Train): 0.33516716957092285; L(Test): 0.31541481614112854\n",
            "Epoch 1631/10000: L(Train): 0.3174845278263092; L(Test): 0.31574922800064087\n",
            "Epoch 1632/10000: L(Train): 0.34083694219589233; L(Test): 0.3159603476524353\n",
            "Epoch 1633/10000: L(Train): 0.33239781856536865; L(Test): 0.3138057589530945\n",
            "Epoch 1634/10000: L(Train): 0.3346892297267914; L(Test): 0.3151280879974365\n",
            "Epoch 1635/10000: L(Train): 0.34802958369255066; L(Test): 0.31544259190559387\n",
            "Epoch 1636/10000: L(Train): 0.3410183787345886; L(Test): 0.31396734714508057\n",
            "Epoch 1637/10000: L(Train): 0.3169957995414734; L(Test): 0.3147156834602356\n",
            "Epoch 1638/10000: L(Train): 0.3358829915523529; L(Test): 0.31525352597236633\n",
            "Epoch 1639/10000: L(Train): 0.3394361436367035; L(Test): 0.3136814534664154\n",
            "Epoch 1640/10000: L(Train): 0.32524433732032776; L(Test): 0.314559668302536\n",
            "Epoch 1641/10000: L(Train): 0.34129440784454346; L(Test): 0.3152792155742645\n",
            "Epoch 1642/10000: L(Train): 0.3320266008377075; L(Test): 0.313681036233902\n",
            "Epoch 1643/10000: L(Train): 0.3350859582424164; L(Test): 0.31328144669532776\n",
            "Epoch 1644/10000: L(Train): 0.3318159878253937; L(Test): 0.31390318274497986\n",
            "Epoch 1645/10000: L(Train): 0.3295655846595764; L(Test): 0.31335964798927307\n",
            "Epoch 1646/10000: L(Train): 0.33112940192222595; L(Test): 0.3132265508174896\n",
            "Epoch 1647/10000: L(Train): 0.3311214745044708; L(Test): 0.31436777114868164\n",
            "Epoch 1648/10000: L(Train): 0.33385756611824036; L(Test): 0.3141544461250305\n",
            "Epoch 1649/10000: L(Train): 0.3305770754814148; L(Test): 0.31422659754753113\n",
            "Epoch 1650/10000: L(Train): 0.33163467049598694; L(Test): 0.31420472264289856\n",
            "Epoch 1651/10000: L(Train): 0.3429911434650421; L(Test): 0.31396040320396423\n",
            "Epoch 1652/10000: L(Train): 0.3315352201461792; L(Test): 0.31470751762390137\n",
            "Epoch 1653/10000: L(Train): 0.32997927069664; L(Test): 0.3144856095314026\n",
            "Epoch 1654/10000: L(Train): 0.33170825242996216; L(Test): 0.31372663378715515\n",
            "Epoch 1655/10000: L(Train): 0.3340741991996765; L(Test): 0.31376969814300537\n",
            "Epoch 1656/10000: L(Train): 0.33172282576560974; L(Test): 0.3132953941822052\n",
            "Epoch 1657/10000: L(Train): 0.3337287902832031; L(Test): 0.31481584906578064\n",
            "Epoch 1658/10000: L(Train): 0.33719778060913086; L(Test): 0.31483742594718933\n",
            "Epoch 1659/10000: L(Train): 0.3410479426383972; L(Test): 0.31431451439857483\n",
            "Epoch 1660/10000: L(Train): 0.32972589135169983; L(Test): 0.3150559663772583\n",
            "Epoch 1661/10000: L(Train): 0.3352372646331787; L(Test): 0.3139691948890686\n",
            "Epoch 1662/10000: L(Train): 0.3377745747566223; L(Test): 0.31341925263404846\n",
            "Epoch 1663/10000: L(Train): 0.34403738379478455; L(Test): 0.3145449757575989\n",
            "Epoch 1664/10000: L(Train): 0.3314514458179474; L(Test): 0.3130626082420349\n",
            "Epoch 1665/10000: L(Train): 0.33513736724853516; L(Test): 0.31293442845344543\n",
            "Epoch 1666/10000: L(Train): 0.3305969536304474; L(Test): 0.3134322166442871\n",
            "Epoch 1667/10000: L(Train): 0.3241913318634033; L(Test): 0.3136542737483978\n",
            "Epoch 1668/10000: L(Train): 0.3336757719516754; L(Test): 0.31324058771133423\n",
            "Epoch 1669/10000: L(Train): 0.33070454001426697; L(Test): 0.31221774220466614\n",
            "Epoch 1670/10000: L(Train): 0.3298850655555725; L(Test): 0.3125985860824585\n",
            "Epoch 1671/10000: L(Train): 0.3408266007900238; L(Test): 0.3136707544326782\n",
            "Epoch 1672/10000: L(Train): 0.34044578671455383; L(Test): 0.31324464082717896\n",
            "Epoch 1673/10000: L(Train): 0.3240811228752136; L(Test): 0.31297817826271057\n",
            "Epoch 1674/10000: L(Train): 0.33487656712532043; L(Test): 0.31369152665138245\n",
            "Epoch 1675/10000: L(Train): 0.3435012102127075; L(Test): 0.31338563561439514\n",
            "Epoch 1676/10000: L(Train): 0.33482256531715393; L(Test): 0.31344836950302124\n",
            "Epoch 1677/10000: L(Train): 0.3291514217853546; L(Test): 0.31358081102371216\n",
            "Epoch 1678/10000: L(Train): 0.3373238146305084; L(Test): 0.3131445348262787\n",
            "Epoch 1679/10000: L(Train): 0.32912421226501465; L(Test): 0.31357136368751526\n",
            "Epoch 1680/10000: L(Train): 0.34264498949050903; L(Test): 0.3125745952129364\n",
            "Epoch 1681/10000: L(Train): 0.3248254060745239; L(Test): 0.3132232427597046\n",
            "Epoch 1682/10000: L(Train): 0.33759942650794983; L(Test): 0.3130235970020294\n",
            "Epoch 1683/10000: L(Train): 0.3327246904373169; L(Test): 0.3132205307483673\n",
            "Epoch 1684/10000: L(Train): 0.3337644636631012; L(Test): 0.3146282136440277\n",
            "Epoch 1685/10000: L(Train): 0.32421934604644775; L(Test): 0.3149998188018799\n",
            "Epoch 1686/10000: L(Train): 0.3368763029575348; L(Test): 0.31398463249206543\n",
            "Epoch 1687/10000: L(Train): 0.3279140293598175; L(Test): 0.3145464360713959\n",
            "Epoch 1688/10000: L(Train): 0.330771803855896; L(Test): 0.3147720396518707\n",
            "Epoch 1689/10000: L(Train): 0.3438238501548767; L(Test): 0.31398335099220276\n",
            "Epoch 1690/10000: L(Train): 0.33272597193717957; L(Test): 0.31397750973701477\n",
            "Epoch 1691/10000: L(Train): 0.3307972252368927; L(Test): 0.31389138102531433\n",
            "Epoch 1692/10000: L(Train): 0.33528006076812744; L(Test): 0.31327128410339355\n",
            "Epoch 1693/10000: L(Train): 0.3221147656440735; L(Test): 0.3135577142238617\n",
            "Epoch 1694/10000: L(Train): 0.34856268763542175; L(Test): 0.31408825516700745\n",
            "Epoch 1695/10000: L(Train): 0.33041471242904663; L(Test): 0.31377434730529785\n",
            "Epoch 1696/10000: L(Train): 0.3285946547985077; L(Test): 0.3133043944835663\n",
            "Epoch 1697/10000: L(Train): 0.3362128734588623; L(Test): 0.31359872221946716\n",
            "Epoch 1698/10000: L(Train): 0.33959394693374634; L(Test): 0.31275513768196106\n",
            "Epoch 1699/10000: L(Train): 0.3333076238632202; L(Test): 0.3126716911792755\n",
            "Epoch 1700/10000: L(Train): 0.32650262117385864; L(Test): 0.3119925558567047\n",
            "Epoch 1701/10000: L(Train): 0.33521923422813416; L(Test): 0.3124837577342987\n",
            "Epoch 1702/10000: L(Train): 0.32454153895378113; L(Test): 0.31298062205314636\n",
            "Epoch 1703/10000: L(Train): 0.33472082018852234; L(Test): 0.31291019916534424\n",
            "Epoch 1704/10000: L(Train): 0.3281228840351105; L(Test): 0.3121394217014313\n",
            "Epoch 1705/10000: L(Train): 0.3345460891723633; L(Test): 0.3116370737552643\n",
            "Epoch 1706/10000: L(Train): 0.3251773416996002; L(Test): 0.3119221329689026\n",
            "Epoch 1707/10000: L(Train): 0.3401855528354645; L(Test): 0.3122638165950775\n",
            "Epoch 1708/10000: L(Train): 0.33029988408088684; L(Test): 0.3122274875640869\n",
            "Epoch 1709/10000: L(Train): 0.3456379771232605; L(Test): 0.311967670917511\n",
            "Epoch 1710/10000: L(Train): 0.33665430545806885; L(Test): 0.31223735213279724\n",
            "Epoch 1711/10000: L(Train): 0.33406785130500793; L(Test): 0.31274867057800293\n",
            "Epoch 1712/10000: L(Train): 0.32125261425971985; L(Test): 0.313196063041687\n",
            "Epoch 1713/10000: L(Train): 0.3360109329223633; L(Test): 0.31410282850265503\n",
            "Epoch 1714/10000: L(Train): 0.3408053517341614; L(Test): 0.31326964497566223\n",
            "Epoch 1715/10000: L(Train): 0.33547550439834595; L(Test): 0.3132770359516144\n",
            "Epoch 1716/10000: L(Train): 0.32808923721313477; L(Test): 0.31373974680900574\n",
            "Epoch 1717/10000: L(Train): 0.3420945703983307; L(Test): 0.3129504919052124\n",
            "Epoch 1718/10000: L(Train): 0.3338416516780853; L(Test): 0.3128294348716736\n",
            "Epoch 1719/10000: L(Train): 0.33730557560920715; L(Test): 0.31437167525291443\n",
            "Epoch 1720/10000: L(Train): 0.3351098597049713; L(Test): 0.31338322162628174\n",
            "Epoch 1721/10000: L(Train): 0.33076485991477966; L(Test): 0.3137165606021881\n",
            "Epoch 1722/10000: L(Train): 0.33224940299987793; L(Test): 0.31590425968170166\n",
            "Epoch 1723/10000: L(Train): 0.3365407884120941; L(Test): 0.31510129570961\n",
            "Epoch 1724/10000: L(Train): 0.33842605352401733; L(Test): 0.3158065676689148\n",
            "Epoch 1725/10000: L(Train): 0.33210062980651855; L(Test): 0.31616732478141785\n",
            "Epoch 1726/10000: L(Train): 0.3315979540348053; L(Test): 0.31564462184906006\n",
            "Epoch 1727/10000: L(Train): 0.3306993544101715; L(Test): 0.31343746185302734\n",
            "Epoch 1728/10000: L(Train): 0.3395580053329468; L(Test): 0.3142094910144806\n",
            "Epoch 1729/10000: L(Train): 0.3437296450138092; L(Test): 0.3139687478542328\n",
            "Epoch 1730/10000: L(Train): 0.34319549798965454; L(Test): 0.31292852759361267\n",
            "Epoch 1731/10000: L(Train): 0.3259830176830292; L(Test): 0.3144210875034332\n",
            "Epoch 1732/10000: L(Train): 0.33391040563583374; L(Test): 0.31412047147750854\n",
            "Epoch 1733/10000: L(Train): 0.3334771990776062; L(Test): 0.312553346157074\n",
            "Epoch 1734/10000: L(Train): 0.343736469745636; L(Test): 0.31269651651382446\n",
            "Epoch 1735/10000: L(Train): 0.3328697979450226; L(Test): 0.31340116262435913\n",
            "Epoch 1736/10000: L(Train): 0.3298744261264801; L(Test): 0.3132930099964142\n",
            "Epoch 1737/10000: L(Train): 0.3435544967651367; L(Test): 0.3137188255786896\n",
            "Epoch 1738/10000: L(Train): 0.334547221660614; L(Test): 0.31397753953933716\n",
            "Epoch 1739/10000: L(Train): 0.33296146988868713; L(Test): 0.3143352270126343\n",
            "Epoch 1740/10000: L(Train): 0.3387224078178406; L(Test): 0.31272611021995544\n",
            "Epoch 1741/10000: L(Train): 0.3397864103317261; L(Test): 0.31391260027885437\n",
            "Epoch 1742/10000: L(Train): 0.33561620116233826; L(Test): 0.314604252576828\n",
            "Epoch 1743/10000: L(Train): 0.3263483941555023; L(Test): 0.31289026141166687\n",
            "Epoch 1744/10000: L(Train): 0.34276828169822693; L(Test): 0.31279364228248596\n",
            "Epoch 1745/10000: L(Train): 0.3336884081363678; L(Test): 0.3150799870491028\n",
            "Epoch 1746/10000: L(Train): 0.34167537093162537; L(Test): 0.31447702646255493\n",
            "Epoch 1747/10000: L(Train): 0.3365624248981476; L(Test): 0.31295883655548096\n",
            "Epoch 1748/10000: L(Train): 0.32922688126564026; L(Test): 0.3138667643070221\n",
            "Epoch 1749/10000: L(Train): 0.3433035910129547; L(Test): 0.3135533034801483\n",
            "Epoch 1750/10000: L(Train): 0.3395997881889343; L(Test): 0.31258293986320496\n",
            "Epoch 1751/10000: L(Train): 0.3346172869205475; L(Test): 0.3122672736644745\n",
            "Epoch 1752/10000: L(Train): 0.33209028840065; L(Test): 0.31217071413993835\n",
            "Epoch 1753/10000: L(Train): 0.3318272531032562; L(Test): 0.3123525083065033\n",
            "Epoch 1754/10000: L(Train): 0.31838852167129517; L(Test): 0.31247857213020325\n",
            "Epoch 1755/10000: L(Train): 0.3345543146133423; L(Test): 0.31180131435394287\n",
            "Epoch 1756/10000: L(Train): 0.32844099402427673; L(Test): 0.31304579973220825\n",
            "Epoch 1757/10000: L(Train): 0.3435966372489929; L(Test): 0.31285735964775085\n",
            "Epoch 1758/10000: L(Train): 0.3256363272666931; L(Test): 0.3118247985839844\n",
            "Epoch 1759/10000: L(Train): 0.3506350815296173; L(Test): 0.31202232837677\n",
            "Epoch 1760/10000: L(Train): 0.3272380232810974; L(Test): 0.3122771978378296\n",
            "Epoch 1761/10000: L(Train): 0.3272342383861542; L(Test): 0.3114314675331116\n",
            "Epoch 1762/10000: L(Train): 0.3337192237377167; L(Test): 0.31147536635398865\n",
            "Epoch 1763/10000: L(Train): 0.3254207670688629; L(Test): 0.3117312788963318\n",
            "Epoch 1764/10000: L(Train): 0.3324759900569916; L(Test): 0.310994029045105\n",
            "Epoch 1765/10000: L(Train): 0.3327089548110962; L(Test): 0.31322184205055237\n",
            "Epoch 1766/10000: L(Train): 0.3353022038936615; L(Test): 0.312601238489151\n",
            "Epoch 1767/10000: L(Train): 0.3402477204799652; L(Test): 0.3102932274341583\n",
            "Epoch 1768/10000: L(Train): 0.3288971185684204; L(Test): 0.31180521845817566\n",
            "Epoch 1769/10000: L(Train): 0.3326495587825775; L(Test): 0.3124943971633911\n",
            "Epoch 1770/10000: L(Train): 0.32748648524284363; L(Test): 0.3117271661758423\n",
            "Epoch 1771/10000: L(Train): 0.3438158333301544; L(Test): 0.313175231218338\n",
            "Epoch 1772/10000: L(Train): 0.3312821388244629; L(Test): 0.31256237626075745\n",
            "Epoch 1773/10000: L(Train): 0.32654598355293274; L(Test): 0.311223566532135\n",
            "Epoch 1774/10000: L(Train): 0.32824069261550903; L(Test): 0.3129422068595886\n",
            "Epoch 1775/10000: L(Train): 0.32524973154067993; L(Test): 0.3115505576133728\n",
            "Epoch 1776/10000: L(Train): 0.33864423632621765; L(Test): 0.31254667043685913\n",
            "Epoch 1777/10000: L(Train): 0.3189831078052521; L(Test): 0.3135683834552765\n",
            "Epoch 1778/10000: L(Train): 0.33073389530181885; L(Test): 0.31161126494407654\n",
            "Epoch 1779/10000: L(Train): 0.3333360254764557; L(Test): 0.31158795952796936\n",
            "Epoch 1780/10000: L(Train): 0.32231906056404114; L(Test): 0.3125917613506317\n",
            "Epoch 1781/10000: L(Train): 0.33331167697906494; L(Test): 0.31205666065216064\n",
            "Epoch 1782/10000: L(Train): 0.3327752351760864; L(Test): 0.31227636337280273\n",
            "Epoch 1783/10000: L(Train): 0.3328517973423004; L(Test): 0.31318122148513794\n",
            "Epoch 1784/10000: L(Train): 0.32999151945114136; L(Test): 0.3125939667224884\n",
            "Epoch 1785/10000: L(Train): 0.34037843346595764; L(Test): 0.31150874495506287\n",
            "Epoch 1786/10000: L(Train): 0.33853188157081604; L(Test): 0.3128710687160492\n",
            "Epoch 1787/10000: L(Train): 0.3320370614528656; L(Test): 0.31167516112327576\n",
            "Epoch 1788/10000: L(Train): 0.33130672574043274; L(Test): 0.3119942843914032\n",
            "Epoch 1789/10000: L(Train): 0.32518357038497925; L(Test): 0.3128255307674408\n",
            "Epoch 1790/10000: L(Train): 0.33392465114593506; L(Test): 0.3123401403427124\n",
            "Epoch 1791/10000: L(Train): 0.3364555239677429; L(Test): 0.3113453984260559\n",
            "Epoch 1792/10000: L(Train): 0.3337956368923187; L(Test): 0.31166619062423706\n",
            "Epoch 1793/10000: L(Train): 0.3360058665275574; L(Test): 0.31152471899986267\n",
            "Epoch 1794/10000: L(Train): 0.3214958906173706; L(Test): 0.3111611604690552\n",
            "Epoch 1795/10000: L(Train): 0.3317568898200989; L(Test): 0.31066054105758667\n",
            "Epoch 1796/10000: L(Train): 0.3273580074310303; L(Test): 0.31081652641296387\n",
            "Epoch 1797/10000: L(Train): 0.33287861943244934; L(Test): 0.3115420639514923\n",
            "Epoch 1798/10000: L(Train): 0.33097273111343384; L(Test): 0.3122711181640625\n",
            "Epoch 1799/10000: L(Train): 0.3259500563144684; L(Test): 0.3126683533191681\n",
            "Epoch 1800/10000: L(Train): 0.3320409059524536; L(Test): 0.3125167191028595\n",
            "Epoch 1801/10000: L(Train): 0.33680665493011475; L(Test): 0.31273168325424194\n",
            "Epoch 1802/10000: L(Train): 0.3316420018672943; L(Test): 0.31294167041778564\n",
            "Epoch 1803/10000: L(Train): 0.33095359802246094; L(Test): 0.3125126361846924\n",
            "Epoch 1804/10000: L(Train): 0.3285645544528961; L(Test): 0.3129488229751587\n",
            "Epoch 1805/10000: L(Train): 0.3281089663505554; L(Test): 0.31325584650039673\n",
            "Epoch 1806/10000: L(Train): 0.3367796540260315; L(Test): 0.31299078464508057\n",
            "Epoch 1807/10000: L(Train): 0.33726170659065247; L(Test): 0.31133031845092773\n",
            "Epoch 1808/10000: L(Train): 0.338365375995636; L(Test): 0.3112780451774597\n",
            "Epoch 1809/10000: L(Train): 0.31819745898246765; L(Test): 0.31190356612205505\n",
            "Epoch 1810/10000: L(Train): 0.33110567927360535; L(Test): 0.3117164969444275\n",
            "Epoch 1811/10000: L(Train): 0.339913010597229; L(Test): 0.3107757568359375\n",
            "Epoch 1812/10000: L(Train): 0.3393479287624359; L(Test): 0.3104759156703949\n",
            "Epoch 1813/10000: L(Train): 0.3252484202384949; L(Test): 0.31182846426963806\n",
            "Epoch 1814/10000: L(Train): 0.3362608850002289; L(Test): 0.3119364380836487\n",
            "Epoch 1815/10000: L(Train): 0.3321143686771393; L(Test): 0.3113115131855011\n",
            "Epoch 1816/10000: L(Train): 0.33808842301368713; L(Test): 0.311043918132782\n",
            "Epoch 1817/10000: L(Train): 0.3336910009384155; L(Test): 0.3110257387161255\n",
            "Epoch 1818/10000: L(Train): 0.3326605558395386; L(Test): 0.3111651837825775\n",
            "Epoch 1819/10000: L(Train): 0.31644079089164734; L(Test): 0.3113093972206116\n",
            "Epoch 1820/10000: L(Train): 0.33669576048851013; L(Test): 0.3112812340259552\n",
            "Epoch 1821/10000: L(Train): 0.3355478346347809; L(Test): 0.31200605630874634\n",
            "Epoch 1822/10000: L(Train): 0.32645633816719055; L(Test): 0.3121001720428467\n",
            "Epoch 1823/10000: L(Train): 0.33289265632629395; L(Test): 0.3120776414871216\n",
            "Epoch 1824/10000: L(Train): 0.32427361607551575; L(Test): 0.31203293800354004\n",
            "Epoch 1825/10000: L(Train): 0.320526123046875; L(Test): 0.3116435110569\n",
            "Epoch 1826/10000: L(Train): 0.33754968643188477; L(Test): 0.31102994084358215\n",
            "Epoch 1827/10000: L(Train): 0.3330744504928589; L(Test): 0.3107994496822357\n",
            "Epoch 1828/10000: L(Train): 0.32853659987449646; L(Test): 0.3104760944843292\n",
            "Epoch 1829/10000: L(Train): 0.3392582833766937; L(Test): 0.311209499835968\n",
            "Epoch 1830/10000: L(Train): 0.33062249422073364; L(Test): 0.3125099241733551\n",
            "Epoch 1831/10000: L(Train): 0.32835790514945984; L(Test): 0.3153391182422638\n",
            "Epoch 1832/10000: L(Train): 0.33012738823890686; L(Test): 0.3133396506309509\n",
            "Epoch 1833/10000: L(Train): 0.33306077122688293; L(Test): 0.3158474266529083\n",
            "Epoch 1834/10000: L(Train): 0.32617685198783875; L(Test): 0.31625890731811523\n",
            "Epoch 1835/10000: L(Train): 0.34686002135276794; L(Test): 0.3151204586029053\n",
            "Epoch 1836/10000: L(Train): 0.33862897753715515; L(Test): 0.31559401750564575\n",
            "Epoch 1837/10000: L(Train): 0.3307729661464691; L(Test): 0.31661635637283325\n",
            "Epoch 1838/10000: L(Train): 0.3406079411506653; L(Test): 0.3152323067188263\n",
            "Epoch 1839/10000: L(Train): 0.3499310612678528; L(Test): 0.3148578405380249\n",
            "Epoch 1840/10000: L(Train): 0.3265681266784668; L(Test): 0.31414178013801575\n",
            "Epoch 1841/10000: L(Train): 0.3383829593658447; L(Test): 0.3135122060775757\n",
            "Epoch 1842/10000: L(Train): 0.33201271295547485; L(Test): 0.3152274191379547\n",
            "Epoch 1843/10000: L(Train): 0.332359254360199; L(Test): 0.31528955698013306\n",
            "Epoch 1844/10000: L(Train): 0.3333527445793152; L(Test): 0.3136817514896393\n",
            "Epoch 1845/10000: L(Train): 0.331901878118515; L(Test): 0.3139421343803406\n",
            "Epoch 1846/10000: L(Train): 0.3348850905895233; L(Test): 0.3140408992767334\n",
            "Epoch 1847/10000: L(Train): 0.3336072564125061; L(Test): 0.31416618824005127\n",
            "Epoch 1848/10000: L(Train): 0.3274880647659302; L(Test): 0.31389087438583374\n",
            "Epoch 1849/10000: L(Train): 0.3356534242630005; L(Test): 0.3133995532989502\n",
            "Epoch 1850/10000: L(Train): 0.3259427547454834; L(Test): 0.31346818804740906\n",
            "Epoch 1851/10000: L(Train): 0.3345249593257904; L(Test): 0.3129720985889435\n",
            "Epoch 1852/10000: L(Train): 0.33727753162384033; L(Test): 0.3117169439792633\n",
            "Epoch 1853/10000: L(Train): 0.3367093801498413; L(Test): 0.31190967559814453\n",
            "Epoch 1854/10000: L(Train): 0.3376796245574951; L(Test): 0.3120937645435333\n",
            "Epoch 1855/10000: L(Train): 0.32443177700042725; L(Test): 0.3119390904903412\n",
            "Epoch 1856/10000: L(Train): 0.3307598829269409; L(Test): 0.31203165650367737\n",
            "Epoch 1857/10000: L(Train): 0.3249341547489166; L(Test): 0.3128723204135895\n",
            "Epoch 1858/10000: L(Train): 0.3361796438694; L(Test): 0.3126945495605469\n",
            "Epoch 1859/10000: L(Train): 0.344974547624588; L(Test): 0.3116435706615448\n",
            "Epoch 1860/10000: L(Train): 0.33789291977882385; L(Test): 0.31158512830734253\n",
            "Epoch 1861/10000: L(Train): 0.33293822407722473; L(Test): 0.3113609850406647\n",
            "Epoch 1862/10000: L(Train): 0.33236780762672424; L(Test): 0.31215351819992065\n",
            "Epoch 1863/10000: L(Train): 0.32815316319465637; L(Test): 0.3121238648891449\n",
            "Epoch 1864/10000: L(Train): 0.33454227447509766; L(Test): 0.3122953772544861\n",
            "Epoch 1865/10000: L(Train): 0.330314576625824; L(Test): 0.31271740794181824\n",
            "Epoch 1866/10000: L(Train): 0.33143606781959534; L(Test): 0.311823308467865\n",
            "Epoch 1867/10000: L(Train): 0.3271583318710327; L(Test): 0.31099268794059753\n",
            "Epoch 1868/10000: L(Train): 0.33517786860466003; L(Test): 0.3116842806339264\n",
            "Epoch 1869/10000: L(Train): 0.3384073078632355; L(Test): 0.31166911125183105\n",
            "Epoch 1870/10000: L(Train): 0.33195844292640686; L(Test): 0.3115648329257965\n",
            "Epoch 1871/10000: L(Train): 0.33326175808906555; L(Test): 0.3118162751197815\n",
            "Epoch 1872/10000: L(Train): 0.3296574652194977; L(Test): 0.311589777469635\n",
            "Epoch 1873/10000: L(Train): 0.33023419976234436; L(Test): 0.3119465708732605\n",
            "Epoch 1874/10000: L(Train): 0.3229583501815796; L(Test): 0.31259027123451233\n",
            "Epoch 1875/10000: L(Train): 0.34017935395240784; L(Test): 0.3113633692264557\n",
            "Epoch 1876/10000: L(Train): 0.3335179388523102; L(Test): 0.3113137185573578\n",
            "Epoch 1877/10000: L(Train): 0.3234955966472626; L(Test): 0.311203271150589\n",
            "Epoch 1878/10000: L(Train): 0.33268338441848755; L(Test): 0.3113647401332855\n",
            "Epoch 1879/10000: L(Train): 0.3313945233821869; L(Test): 0.3127128481864929\n",
            "Epoch 1880/10000: L(Train): 0.33207565546035767; L(Test): 0.3122304677963257\n",
            "Epoch 1881/10000: L(Train): 0.3329433500766754; L(Test): 0.3109506666660309\n",
            "Epoch 1882/10000: L(Train): 0.33928877115249634; L(Test): 0.31248781085014343\n",
            "Epoch 1883/10000: L(Train): 0.33818942308425903; L(Test): 0.3124215006828308\n",
            "Epoch 1884/10000: L(Train): 0.3341042399406433; L(Test): 0.3125818967819214\n",
            "Epoch 1885/10000: L(Train): 0.34194454550743103; L(Test): 0.3122440576553345\n",
            "Epoch 1886/10000: L(Train): 0.34269195795059204; L(Test): 0.312397301197052\n",
            "Epoch 1887/10000: L(Train): 0.3318111300468445; L(Test): 0.3118767738342285\n",
            "Epoch 1888/10000: L(Train): 0.32889246940612793; L(Test): 0.3124125003814697\n",
            "Epoch 1889/10000: L(Train): 0.33277401328086853; L(Test): 0.3123934268951416\n",
            "Epoch 1890/10000: L(Train): 0.329952210187912; L(Test): 0.31151723861694336\n",
            "Epoch 1891/10000: L(Train): 0.3474142253398895; L(Test): 0.3124215304851532\n",
            "Epoch 1892/10000: L(Train): 0.33345329761505127; L(Test): 0.3125964403152466\n",
            "Epoch 1893/10000: L(Train): 0.33281636238098145; L(Test): 0.3109724223613739\n",
            "Epoch 1894/10000: L(Train): 0.32795843482017517; L(Test): 0.3118758797645569\n",
            "Epoch 1895/10000: L(Train): 0.3409329354763031; L(Test): 0.3113233745098114\n",
            "Epoch 1896/10000: L(Train): 0.32320496439933777; L(Test): 0.31078749895095825\n",
            "Epoch 1897/10000: L(Train): 0.3358564078807831; L(Test): 0.3120880424976349\n",
            "Epoch 1898/10000: L(Train): 0.3248901963233948; L(Test): 0.3114340901374817\n",
            "Epoch 1899/10000: L(Train): 0.32753148674964905; L(Test): 0.3112702965736389\n",
            "Epoch 1900/10000: L(Train): 0.33452022075653076; L(Test): 0.31229496002197266\n",
            "Epoch 1901/10000: L(Train): 0.33466365933418274; L(Test): 0.31139326095581055\n",
            "Epoch 1902/10000: L(Train): 0.33215630054473877; L(Test): 0.3108990490436554\n",
            "Epoch 1903/10000: L(Train): 0.3272756338119507; L(Test): 0.31050875782966614\n",
            "Epoch 1904/10000: L(Train): 0.3263152837753296; L(Test): 0.3107368052005768\n",
            "Epoch 1905/10000: L(Train): 0.3346484899520874; L(Test): 0.3111993670463562\n",
            "Epoch 1906/10000: L(Train): 0.3286112844944; L(Test): 0.3115290701389313\n",
            "Epoch 1907/10000: L(Train): 0.3325260579586029; L(Test): 0.3111412823200226\n",
            "Epoch 1908/10000: L(Train): 0.33222806453704834; L(Test): 0.3114049434661865\n",
            "Epoch 1909/10000: L(Train): 0.33168286085128784; L(Test): 0.31330230832099915\n",
            "Epoch 1910/10000: L(Train): 0.3339309096336365; L(Test): 0.3139185607433319\n",
            "Epoch 1911/10000: L(Train): 0.3368387818336487; L(Test): 0.31094855070114136\n",
            "Epoch 1912/10000: L(Train): 0.3376985192298889; L(Test): 0.31116244196891785\n",
            "Epoch 1913/10000: L(Train): 0.33014988899230957; L(Test): 0.3124226927757263\n",
            "Epoch 1914/10000: L(Train): 0.3359682261943817; L(Test): 0.3126932680606842\n",
            "Epoch 1915/10000: L(Train): 0.339729368686676; L(Test): 0.3121922016143799\n",
            "Epoch 1916/10000: L(Train): 0.33761799335479736; L(Test): 0.31244033575057983\n",
            "Epoch 1917/10000: L(Train): 0.3245948553085327; L(Test): 0.3133648931980133\n",
            "Epoch 1918/10000: L(Train): 0.334963321685791; L(Test): 0.31273213028907776\n",
            "Epoch 1919/10000: L(Train): 0.3276861906051636; L(Test): 0.3119863271713257\n",
            "Epoch 1920/10000: L(Train): 0.3379931151866913; L(Test): 0.311809241771698\n",
            "Epoch 1921/10000: L(Train): 0.33664029836654663; L(Test): 0.31242048740386963\n",
            "Epoch 1922/10000: L(Train): 0.32735398411750793; L(Test): 0.3124237358570099\n",
            "Epoch 1923/10000: L(Train): 0.32919248938560486; L(Test): 0.3130967319011688\n",
            "Epoch 1924/10000: L(Train): 0.3255648910999298; L(Test): 0.3129127621650696\n",
            "Epoch 1925/10000: L(Train): 0.3239533305168152; L(Test): 0.31146690249443054\n",
            "Epoch 1926/10000: L(Train): 0.3352760076522827; L(Test): 0.31108608841896057\n",
            "Epoch 1927/10000: L(Train): 0.339761346578598; L(Test): 0.31091228127479553\n",
            "Epoch 1928/10000: L(Train): 0.33004865050315857; L(Test): 0.3104071021080017\n",
            "Epoch 1929/10000: L(Train): 0.3282618820667267; L(Test): 0.3100185990333557\n",
            "Epoch 1930/10000: L(Train): 0.328624427318573; L(Test): 0.3106474280357361\n",
            "Epoch 1931/10000: L(Train): 0.3323657810688019; L(Test): 0.31128525733947754\n",
            "Epoch 1932/10000: L(Train): 0.3415270149707794; L(Test): 0.3111567497253418\n",
            "Epoch 1933/10000: L(Train): 0.3301854133605957; L(Test): 0.31099316477775574\n",
            "Epoch 1934/10000: L(Train): 0.32983097434043884; L(Test): 0.31101998686790466\n",
            "Epoch 1935/10000: L(Train): 0.33163803815841675; L(Test): 0.31033122539520264\n",
            "Epoch 1936/10000: L(Train): 0.33306169509887695; L(Test): 0.3098919987678528\n",
            "Epoch 1937/10000: L(Train): 0.328912615776062; L(Test): 0.3104284405708313\n",
            "Epoch 1938/10000: L(Train): 0.33655214309692383; L(Test): 0.3111591041088104\n",
            "Epoch 1939/10000: L(Train): 0.3230968415737152; L(Test): 0.31083983182907104\n",
            "Epoch 1940/10000: L(Train): 0.33418986201286316; L(Test): 0.30958670377731323\n",
            "Epoch 1941/10000: L(Train): 0.3266884982585907; L(Test): 0.3105049133300781\n",
            "Epoch 1942/10000: L(Train): 0.3271562159061432; L(Test): 0.31045153737068176\n",
            "Epoch 1943/10000: L(Train): 0.33937394618988037; L(Test): 0.3103474974632263\n",
            "Epoch 1944/10000: L(Train): 0.3309873342514038; L(Test): 0.31014731526374817\n",
            "Epoch 1945/10000: L(Train): 0.33578020334243774; L(Test): 0.30973270535469055\n",
            "Epoch 1946/10000: L(Train): 0.3336910009384155; L(Test): 0.31067436933517456\n",
            "Epoch 1947/10000: L(Train): 0.32123634219169617; L(Test): 0.31065812706947327\n",
            "Epoch 1948/10000: L(Train): 0.3366066515445709; L(Test): 0.31009379029273987\n",
            "Epoch 1949/10000: L(Train): 0.33805593848228455; L(Test): 0.3114987015724182\n",
            "Epoch 1950/10000: L(Train): 0.3225648105144501; L(Test): 0.3108266592025757\n",
            "Epoch 1951/10000: L(Train): 0.3323228359222412; L(Test): 0.3099114000797272\n",
            "Epoch 1952/10000: L(Train): 0.3274983763694763; L(Test): 0.31036144495010376\n",
            "Epoch 1953/10000: L(Train): 0.3357120752334595; L(Test): 0.31101134419441223\n",
            "Epoch 1954/10000: L(Train): 0.3336394429206848; L(Test): 0.31122130155563354\n",
            "Epoch 1955/10000: L(Train): 0.32976529002189636; L(Test): 0.3112865686416626\n",
            "Epoch 1956/10000: L(Train): 0.3348715901374817; L(Test): 0.3122080862522125\n",
            "Epoch 1957/10000: L(Train): 0.33128622174263; L(Test): 0.310580313205719\n",
            "Epoch 1958/10000: L(Train): 0.33085548877716064; L(Test): 0.3126852214336395\n",
            "Epoch 1959/10000: L(Train): 0.33414921164512634; L(Test): 0.3123883903026581\n",
            "Epoch 1960/10000: L(Train): 0.3383694887161255; L(Test): 0.3105603754520416\n",
            "Epoch 1961/10000: L(Train): 0.3348686695098877; L(Test): 0.31202423572540283\n",
            "Epoch 1962/10000: L(Train): 0.3404310345649719; L(Test): 0.31064921617507935\n",
            "Epoch 1963/10000: L(Train): 0.3330383002758026; L(Test): 0.31052544713020325\n",
            "Epoch 1964/10000: L(Train): 0.326198011636734; L(Test): 0.31115129590034485\n",
            "Epoch 1965/10000: L(Train): 0.3283692002296448; L(Test): 0.3098609447479248\n",
            "Epoch 1966/10000: L(Train): 0.34333324432373047; L(Test): 0.31066522002220154\n",
            "Epoch 1967/10000: L(Train): 0.3276228904724121; L(Test): 0.3108978569507599\n",
            "Epoch 1968/10000: L(Train): 0.3275260031223297; L(Test): 0.31097254157066345\n",
            "Epoch 1969/10000: L(Train): 0.33471620082855225; L(Test): 0.31196328997612\n",
            "Epoch 1970/10000: L(Train): 0.3320634961128235; L(Test): 0.3106083869934082\n",
            "Epoch 1971/10000: L(Train): 0.3235101103782654; L(Test): 0.3109900653362274\n",
            "Epoch 1972/10000: L(Train): 0.3295295238494873; L(Test): 0.3106532096862793\n",
            "Epoch 1973/10000: L(Train): 0.3329484164714813; L(Test): 0.31098806858062744\n",
            "Epoch 1974/10000: L(Train): 0.3413713872432709; L(Test): 0.3107786774635315\n",
            "Epoch 1975/10000: L(Train): 0.3345028758049011; L(Test): 0.3101879358291626\n",
            "Epoch 1976/10000: L(Train): 0.3374860882759094; L(Test): 0.3115208148956299\n",
            "Epoch 1977/10000: L(Train): 0.3281903862953186; L(Test): 0.3105928301811218\n",
            "Epoch 1978/10000: L(Train): 0.3257192075252533; L(Test): 0.3101635277271271\n",
            "Epoch 1979/10000: L(Train): 0.3339334726333618; L(Test): 0.31187984347343445\n",
            "Epoch 1980/10000: L(Train): 0.33487167954444885; L(Test): 0.3118534982204437\n",
            "Epoch 1981/10000: L(Train): 0.33238667249679565; L(Test): 0.31267714500427246\n",
            "Epoch 1982/10000: L(Train): 0.33356618881225586; L(Test): 0.3144832253456116\n",
            "Epoch 1983/10000: L(Train): 0.33925655484199524; L(Test): 0.3113008737564087\n",
            "Epoch 1984/10000: L(Train): 0.3349558115005493; L(Test): 0.31328314542770386\n",
            "Epoch 1985/10000: L(Train): 0.33680570125579834; L(Test): 0.31425273418426514\n",
            "Epoch 1986/10000: L(Train): 0.32854464650154114; L(Test): 0.3121488690376282\n",
            "Epoch 1987/10000: L(Train): 0.32945725321769714; L(Test): 0.31510302424430847\n",
            "Epoch 1988/10000: L(Train): 0.3426343500614166; L(Test): 0.31376394629478455\n",
            "Epoch 1989/10000: L(Train): 0.33115100860595703; L(Test): 0.3126681447029114\n",
            "Epoch 1990/10000: L(Train): 0.33579328656196594; L(Test): 0.3151460587978363\n",
            "Epoch 1991/10000: L(Train): 0.32586240768432617; L(Test): 0.3128516674041748\n",
            "Epoch 1992/10000: L(Train): 0.3344266712665558; L(Test): 0.3134775459766388\n",
            "Epoch 1993/10000: L(Train): 0.33282575011253357; L(Test): 0.3163546919822693\n",
            "Epoch 1994/10000: L(Train): 0.34676307439804077; L(Test): 0.31450986862182617\n",
            "Epoch 1995/10000: L(Train): 0.3339858651161194; L(Test): 0.3138943910598755\n",
            "Epoch 1996/10000: L(Train): 0.3353915512561798; L(Test): 0.3151085674762726\n",
            "Epoch 1997/10000: L(Train): 0.3370174169540405; L(Test): 0.3129863142967224\n",
            "Epoch 1998/10000: L(Train): 0.3379731774330139; L(Test): 0.31284627318382263\n",
            "Epoch 1999/10000: L(Train): 0.3251274526119232; L(Test): 0.3139599859714508\n",
            "Epoch 2000/10000: L(Train): 0.3345392942428589; L(Test): 0.31261739134788513\n",
            "Epoch 2001/10000: L(Train): 0.32906121015548706; L(Test): 0.3130897283554077\n",
            "Epoch 2002/10000: L(Train): 0.33651480078697205; L(Test): 0.31363779306411743\n",
            "Epoch 2003/10000: L(Train): 0.34308940172195435; L(Test): 0.31229910254478455\n",
            "Epoch 2004/10000: L(Train): 0.3363880217075348; L(Test): 0.31226664781570435\n",
            "Epoch 2005/10000: L(Train): 0.3292369842529297; L(Test): 0.31279438734054565\n",
            "Epoch 2006/10000: L(Train): 0.3228525221347809; L(Test): 0.31347033381462097\n",
            "Epoch 2007/10000: L(Train): 0.32644903659820557; L(Test): 0.3140166699886322\n",
            "Epoch 2008/10000: L(Train): 0.33522024750709534; L(Test): 0.3132360577583313\n",
            "Epoch 2009/10000: L(Train): 0.3371392786502838; L(Test): 0.31173262000083923\n",
            "Epoch 2010/10000: L(Train): 0.32878416776657104; L(Test): 0.3111938238143921\n",
            "Epoch 2011/10000: L(Train): 0.33277246356010437; L(Test): 0.31005173921585083\n",
            "Epoch 2012/10000: L(Train): 0.3280181586742401; L(Test): 0.31008824706077576\n",
            "Epoch 2013/10000: L(Train): 0.32469499111175537; L(Test): 0.31147500872612\n",
            "Epoch 2014/10000: L(Train): 0.33212706446647644; L(Test): 0.3105049729347229\n",
            "Epoch 2015/10000: L(Train): 0.32577070593833923; L(Test): 0.3102300763130188\n",
            "Epoch 2016/10000: L(Train): 0.3307878077030182; L(Test): 0.3105875253677368\n",
            "Epoch 2017/10000: L(Train): 0.33344900608062744; L(Test): 0.3097909986972809\n",
            "Epoch 2018/10000: L(Train): 0.3334103226661682; L(Test): 0.3102976381778717\n",
            "Epoch 2019/10000: L(Train): 0.3276940584182739; L(Test): 0.3102189600467682\n",
            "Epoch 2020/10000: L(Train): 0.33196166157722473; L(Test): 0.3100793957710266\n",
            "Epoch 2021/10000: L(Train): 0.3339652419090271; L(Test): 0.30983737111091614\n",
            "Epoch 2022/10000: L(Train): 0.33685922622680664; L(Test): 0.30919721722602844\n",
            "Epoch 2023/10000: L(Train): 0.33278337121009827; L(Test): 0.3092368245124817\n",
            "Epoch 2024/10000: L(Train): 0.33450910449028015; L(Test): 0.3093867003917694\n",
            "Epoch 2025/10000: L(Train): 0.3228699862957001; L(Test): 0.3095625042915344\n",
            "Epoch 2026/10000: L(Train): 0.332113116979599; L(Test): 0.3096078932285309\n",
            "Epoch 2027/10000: L(Train): 0.3271010220050812; L(Test): 0.310064435005188\n",
            "Epoch 2028/10000: L(Train): 0.3299518823623657; L(Test): 0.3093625009059906\n",
            "Epoch 2029/10000: L(Train): 0.33218422532081604; L(Test): 0.30990585684776306\n",
            "Epoch 2030/10000: L(Train): 0.33335667848587036; L(Test): 0.3092900216579437\n",
            "Epoch 2031/10000: L(Train): 0.32140105962753296; L(Test): 0.31017687916755676\n",
            "Epoch 2032/10000: L(Train): 0.32239222526550293; L(Test): 0.3104301393032074\n",
            "Epoch 2033/10000: L(Train): 0.3348713219165802; L(Test): 0.30929216742515564\n",
            "Epoch 2034/10000: L(Train): 0.3264092803001404; L(Test): 0.3101694583892822\n",
            "Epoch 2035/10000: L(Train): 0.3286283612251282; L(Test): 0.3101652264595032\n",
            "Epoch 2036/10000: L(Train): 0.330241858959198; L(Test): 0.3093496561050415\n",
            "Epoch 2037/10000: L(Train): 0.34096604585647583; L(Test): 0.3088615834712982\n",
            "Epoch 2038/10000: L(Train): 0.32872238755226135; L(Test): 0.30846238136291504\n",
            "Epoch 2039/10000: L(Train): 0.33641260862350464; L(Test): 0.30807751417160034\n",
            "Epoch 2040/10000: L(Train): 0.3281896412372589; L(Test): 0.30860334634780884\n",
            "Epoch 2041/10000: L(Train): 0.3306545615196228; L(Test): 0.30958831310272217\n",
            "Epoch 2042/10000: L(Train): 0.3240289092063904; L(Test): 0.3090444505214691\n",
            "Epoch 2043/10000: L(Train): 0.31545907258987427; L(Test): 0.3092538118362427\n",
            "Epoch 2044/10000: L(Train): 0.331150621175766; L(Test): 0.30877724289894104\n",
            "Epoch 2045/10000: L(Train): 0.3352820873260498; L(Test): 0.3077082335948944\n",
            "Epoch 2046/10000: L(Train): 0.33139768242836; L(Test): 0.30811506509780884\n",
            "Epoch 2047/10000: L(Train): 0.32836997509002686; L(Test): 0.3081759214401245\n",
            "Epoch 2048/10000: L(Train): 0.3239520192146301; L(Test): 0.3082839250564575\n",
            "Epoch 2049/10000: L(Train): 0.3311063349246979; L(Test): 0.30844250321388245\n",
            "Epoch 2050/10000: L(Train): 0.31748369336128235; L(Test): 0.3089756965637207\n",
            "Epoch 2051/10000: L(Train): 0.3223918080329895; L(Test): 0.3091624677181244\n",
            "Epoch 2052/10000: L(Train): 0.32917675375938416; L(Test): 0.3079341948032379\n",
            "Epoch 2053/10000: L(Train): 0.3281978964805603; L(Test): 0.3079029619693756\n",
            "Epoch 2054/10000: L(Train): 0.32569634914398193; L(Test): 0.30844229459762573\n",
            "Epoch 2055/10000: L(Train): 0.3281022310256958; L(Test): 0.30821898579597473\n",
            "Epoch 2056/10000: L(Train): 0.32943636178970337; L(Test): 0.3090689480304718\n",
            "Epoch 2057/10000: L(Train): 0.327358603477478; L(Test): 0.3092034161090851\n",
            "Epoch 2058/10000: L(Train): 0.32632550597190857; L(Test): 0.30916306376457214\n",
            "Epoch 2059/10000: L(Train): 0.3295629024505615; L(Test): 0.30873164534568787\n",
            "Epoch 2060/10000: L(Train): 0.3272874653339386; L(Test): 0.30896246433258057\n",
            "Epoch 2061/10000: L(Train): 0.32215359807014465; L(Test): 0.3099854588508606\n",
            "Epoch 2062/10000: L(Train): 0.32842302322387695; L(Test): 0.308805912733078\n",
            "Epoch 2063/10000: L(Train): 0.33097541332244873; L(Test): 0.30941441655158997\n",
            "Epoch 2064/10000: L(Train): 0.3272854685783386; L(Test): 0.30966997146606445\n",
            "Epoch 2065/10000: L(Train): 0.31468281149864197; L(Test): 0.3087480068206787\n",
            "Epoch 2066/10000: L(Train): 0.3384038805961609; L(Test): 0.3092305064201355\n",
            "Epoch 2067/10000: L(Train): 0.3297874331474304; L(Test): 0.31078028678894043\n",
            "Epoch 2068/10000: L(Train): 0.3285733759403229; L(Test): 0.3097721040248871\n",
            "Epoch 2069/10000: L(Train): 0.33557426929473877; L(Test): 0.31026288866996765\n",
            "Epoch 2070/10000: L(Train): 0.32487931847572327; L(Test): 0.31163930892944336\n",
            "Epoch 2071/10000: L(Train): 0.33669671416282654; L(Test): 0.3097270131111145\n",
            "Epoch 2072/10000: L(Train): 0.3273031711578369; L(Test): 0.3102964162826538\n",
            "Epoch 2073/10000: L(Train): 0.33203843235969543; L(Test): 0.3098788857460022\n",
            "Epoch 2074/10000: L(Train): 0.32125359773635864; L(Test): 0.30990445613861084\n",
            "Epoch 2075/10000: L(Train): 0.32680457830429077; L(Test): 0.31060856580734253\n",
            "Epoch 2076/10000: L(Train): 0.339742511510849; L(Test): 0.31065794825553894\n",
            "Epoch 2077/10000: L(Train): 0.32393643260002136; L(Test): 0.3108695149421692\n",
            "Epoch 2078/10000: L(Train): 0.3317989110946655; L(Test): 0.3102176785469055\n",
            "Epoch 2079/10000: L(Train): 0.3370424509048462; L(Test): 0.31092703342437744\n",
            "Epoch 2080/10000: L(Train): 0.33223915100097656; L(Test): 0.31077051162719727\n",
            "Epoch 2081/10000: L(Train): 0.32855769991874695; L(Test): 0.3106011748313904\n",
            "Epoch 2082/10000: L(Train): 0.3400774896144867; L(Test): 0.31131768226623535\n",
            "Epoch 2083/10000: L(Train): 0.3285570442676544; L(Test): 0.3106837272644043\n",
            "Epoch 2084/10000: L(Train): 0.33926114439964294; L(Test): 0.30987948179244995\n",
            "Epoch 2085/10000: L(Train): 0.334772527217865; L(Test): 0.3102238178253174\n",
            "Epoch 2086/10000: L(Train): 0.3366644084453583; L(Test): 0.30919939279556274\n",
            "Epoch 2087/10000: L(Train): 0.32455915212631226; L(Test): 0.31060341000556946\n",
            "Epoch 2088/10000: L(Train): 0.3339237570762634; L(Test): 0.311820924282074\n",
            "Epoch 2089/10000: L(Train): 0.3301553428173065; L(Test): 0.3094951808452606\n",
            "Epoch 2090/10000: L(Train): 0.3387211859226227; L(Test): 0.3104892075061798\n",
            "Epoch 2091/10000: L(Train): 0.3247336745262146; L(Test): 0.31214168667793274\n",
            "Epoch 2092/10000: L(Train): 0.3260326385498047; L(Test): 0.309928297996521\n",
            "Epoch 2093/10000: L(Train): 0.3339972496032715; L(Test): 0.3093454837799072\n",
            "Epoch 2094/10000: L(Train): 0.32107582688331604; L(Test): 0.3118453025817871\n",
            "Epoch 2095/10000: L(Train): 0.3375135064125061; L(Test): 0.3109005391597748\n",
            "Epoch 2096/10000: L(Train): 0.3324085474014282; L(Test): 0.312463641166687\n",
            "Epoch 2097/10000: L(Train): 0.32826873660087585; L(Test): 0.3133269250392914\n",
            "Epoch 2098/10000: L(Train): 0.32789015769958496; L(Test): 0.3111245930194855\n",
            "Epoch 2099/10000: L(Train): 0.3287612795829773; L(Test): 0.3109537959098816\n",
            "Epoch 2100/10000: L(Train): 0.33112481236457825; L(Test): 0.31099921464920044\n",
            "Epoch 2101/10000: L(Train): 0.32853636145591736; L(Test): 0.3105953633785248\n",
            "Epoch 2102/10000: L(Train): 0.327564001083374; L(Test): 0.3113231956958771\n",
            "Epoch 2103/10000: L(Train): 0.33483126759529114; L(Test): 0.31177660822868347\n",
            "Epoch 2104/10000: L(Train): 0.3279564380645752; L(Test): 0.31116440892219543\n",
            "Epoch 2105/10000: L(Train): 0.33450257778167725; L(Test): 0.3108305335044861\n",
            "Epoch 2106/10000: L(Train): 0.32536935806274414; L(Test): 0.3097086250782013\n",
            "Epoch 2107/10000: L(Train): 0.34321653842926025; L(Test): 0.3091293275356293\n",
            "Epoch 2108/10000: L(Train): 0.32965222001075745; L(Test): 0.30984821915626526\n",
            "Epoch 2109/10000: L(Train): 0.3287087678909302; L(Test): 0.3099474608898163\n",
            "Epoch 2110/10000: L(Train): 0.32035574316978455; L(Test): 0.30984947085380554\n",
            "Epoch 2111/10000: L(Train): 0.33293280005455017; L(Test): 0.309320330619812\n",
            "Epoch 2112/10000: L(Train): 0.329030841588974; L(Test): 0.30921801924705505\n",
            "Epoch 2113/10000: L(Train): 0.32729312777519226; L(Test): 0.3087199628353119\n",
            "Epoch 2114/10000: L(Train): 0.3336995244026184; L(Test): 0.3088519275188446\n",
            "Epoch 2115/10000: L(Train): 0.3294818699359894; L(Test): 0.3094436824321747\n",
            "Epoch 2116/10000: L(Train): 0.3283497393131256; L(Test): 0.3093903064727783\n",
            "Epoch 2117/10000: L(Train): 0.33577761054039; L(Test): 0.3085308372974396\n",
            "Epoch 2118/10000: L(Train): 0.3306022882461548; L(Test): 0.30817288160324097\n",
            "Epoch 2119/10000: L(Train): 0.3229258358478546; L(Test): 0.3085322976112366\n",
            "Epoch 2120/10000: L(Train): 0.33008965849876404; L(Test): 0.3102797567844391\n",
            "Epoch 2121/10000: L(Train): 0.3305042088031769; L(Test): 0.3099130094051361\n",
            "Epoch 2122/10000: L(Train): 0.3364938497543335; L(Test): 0.3089450001716614\n",
            "Epoch 2123/10000: L(Train): 0.33325275778770447; L(Test): 0.3084104359149933\n",
            "Epoch 2124/10000: L(Train): 0.33776864409446716; L(Test): 0.3083476722240448\n",
            "Epoch 2125/10000: L(Train): 0.3296276032924652; L(Test): 0.3079105317592621\n",
            "Epoch 2126/10000: L(Train): 0.33596375584602356; L(Test): 0.3092041611671448\n",
            "Epoch 2127/10000: L(Train): 0.3273329734802246; L(Test): 0.30908718705177307\n",
            "Epoch 2128/10000: L(Train): 0.33285415172576904; L(Test): 0.3078232705593109\n",
            "Epoch 2129/10000: L(Train): 0.32066839933395386; L(Test): 0.3090088367462158\n",
            "Epoch 2130/10000: L(Train): 0.3218926787376404; L(Test): 0.3088531196117401\n",
            "Epoch 2131/10000: L(Train): 0.32432255148887634; L(Test): 0.30828264355659485\n",
            "Epoch 2132/10000: L(Train): 0.3179410994052887; L(Test): 0.3080753982067108\n",
            "Epoch 2133/10000: L(Train): 0.3376554548740387; L(Test): 0.3076116442680359\n",
            "Epoch 2134/10000: L(Train): 0.3215217888355255; L(Test): 0.30742502212524414\n",
            "Epoch 2135/10000: L(Train): 0.3313470184803009; L(Test): 0.30865880846977234\n",
            "Epoch 2136/10000: L(Train): 0.32392823696136475; L(Test): 0.3096376359462738\n",
            "Epoch 2137/10000: L(Train): 0.333712100982666; L(Test): 0.3086715340614319\n",
            "Epoch 2138/10000: L(Train): 0.34523484110832214; L(Test): 0.3072358965873718\n",
            "Epoch 2139/10000: L(Train): 0.32633212208747864; L(Test): 0.3073902428150177\n",
            "Epoch 2140/10000: L(Train): 0.32988089323043823; L(Test): 0.30740293860435486\n",
            "Epoch 2141/10000: L(Train): 0.3276447355747223; L(Test): 0.30783355236053467\n",
            "Epoch 2142/10000: L(Train): 0.3261544704437256; L(Test): 0.30831995606422424\n",
            "Epoch 2143/10000: L(Train): 0.33062854409217834; L(Test): 0.3086347281932831\n",
            "Epoch 2144/10000: L(Train): 0.3233964443206787; L(Test): 0.3091681897640228\n",
            "Epoch 2145/10000: L(Train): 0.3343624472618103; L(Test): 0.30799582600593567\n",
            "Epoch 2146/10000: L(Train): 0.3335832953453064; L(Test): 0.30804678797721863\n",
            "Epoch 2147/10000: L(Train): 0.3334082365036011; L(Test): 0.30953261256217957\n",
            "Epoch 2148/10000: L(Train): 0.32695162296295166; L(Test): 0.3094671666622162\n",
            "Epoch 2149/10000: L(Train): 0.3308248817920685; L(Test): 0.30850932002067566\n",
            "Epoch 2150/10000: L(Train): 0.32825252413749695; L(Test): 0.3088654577732086\n",
            "Epoch 2151/10000: L(Train): 0.33542966842651367; L(Test): 0.30989447236061096\n",
            "Epoch 2152/10000: L(Train): 0.3287097215652466; L(Test): 0.31126928329467773\n",
            "Epoch 2153/10000: L(Train): 0.3279070258140564; L(Test): 0.3092444837093353\n",
            "Epoch 2154/10000: L(Train): 0.33478400111198425; L(Test): 0.3093244135379791\n",
            "Epoch 2155/10000: L(Train): 0.32996830344200134; L(Test): 0.3086881637573242\n",
            "Epoch 2156/10000: L(Train): 0.33551687002182007; L(Test): 0.308420330286026\n",
            "Epoch 2157/10000: L(Train): 0.3264770209789276; L(Test): 0.30881017446517944\n",
            "Epoch 2158/10000: L(Train): 0.3360486328601837; L(Test): 0.308159202337265\n",
            "Epoch 2159/10000: L(Train): 0.32788175344467163; L(Test): 0.30931755900382996\n",
            "Epoch 2160/10000: L(Train): 0.3339250087738037; L(Test): 0.30982282757759094\n",
            "Epoch 2161/10000: L(Train): 0.3241032063961029; L(Test): 0.3091009855270386\n",
            "Epoch 2162/10000: L(Train): 0.32419419288635254; L(Test): 0.3096317648887634\n",
            "Epoch 2163/10000: L(Train): 0.3364167809486389; L(Test): 0.3106689453125\n",
            "Epoch 2164/10000: L(Train): 0.3278217017650604; L(Test): 0.3093768060207367\n",
            "Epoch 2165/10000: L(Train): 0.3209288716316223; L(Test): 0.3084438741207123\n",
            "Epoch 2166/10000: L(Train): 0.3182438313961029; L(Test): 0.3091621994972229\n",
            "Epoch 2167/10000: L(Train): 0.3357451856136322; L(Test): 0.30864474177360535\n",
            "Epoch 2168/10000: L(Train): 0.32247573137283325; L(Test): 0.30823269486427307\n",
            "Epoch 2169/10000: L(Train): 0.3269159495830536; L(Test): 0.30886310338974\n",
            "Epoch 2170/10000: L(Train): 0.3300326466560364; L(Test): 0.3090539276599884\n",
            "Epoch 2171/10000: L(Train): 0.3326912224292755; L(Test): 0.30808836221694946\n",
            "Epoch 2172/10000: L(Train): 0.3224261999130249; L(Test): 0.3078070878982544\n",
            "Epoch 2173/10000: L(Train): 0.3299279808998108; L(Test): 0.30910956859588623\n",
            "Epoch 2174/10000: L(Train): 0.33007296919822693; L(Test): 0.3085886240005493\n",
            "Epoch 2175/10000: L(Train): 0.3322020471096039; L(Test): 0.3082521855831146\n",
            "Epoch 2176/10000: L(Train): 0.3276774287223816; L(Test): 0.30830568075180054\n",
            "Epoch 2177/10000: L(Train): 0.3229575753211975; L(Test): 0.3078519403934479\n",
            "Epoch 2178/10000: L(Train): 0.3258543908596039; L(Test): 0.3073940575122833\n",
            "Epoch 2179/10000: L(Train): 0.32531923055648804; L(Test): 0.30800604820251465\n",
            "Epoch 2180/10000: L(Train): 0.3384694457054138; L(Test): 0.3080650866031647\n",
            "Epoch 2181/10000: L(Train): 0.32935893535614014; L(Test): 0.30747905373573303\n",
            "Epoch 2182/10000: L(Train): 0.3315142095088959; L(Test): 0.3069974184036255\n",
            "Epoch 2183/10000: L(Train): 0.32663407921791077; L(Test): 0.3069010376930237\n",
            "Epoch 2184/10000: L(Train): 0.3297249376773834; L(Test): 0.3067878186702728\n",
            "Epoch 2185/10000: L(Train): 0.325621634721756; L(Test): 0.3063964247703552\n",
            "Epoch 2186/10000: L(Train): 0.3279191851615906; L(Test): 0.3063545823097229\n",
            "Epoch 2187/10000: L(Train): 0.3231368362903595; L(Test): 0.3068283796310425\n",
            "Epoch 2188/10000: L(Train): 0.32436633110046387; L(Test): 0.3069574236869812\n",
            "Epoch 2189/10000: L(Train): 0.3344631493091583; L(Test): 0.30687493085861206\n",
            "Epoch 2190/10000: L(Train): 0.32950982451438904; L(Test): 0.3070015013217926\n",
            "Epoch 2191/10000: L(Train): 0.32501211762428284; L(Test): 0.3070819675922394\n",
            "Epoch 2192/10000: L(Train): 0.31996479630470276; L(Test): 0.3072570562362671\n",
            "Epoch 2193/10000: L(Train): 0.33498525619506836; L(Test): 0.3068346381187439\n",
            "Epoch 2194/10000: L(Train): 0.3336513340473175; L(Test): 0.3067586421966553\n",
            "Epoch 2195/10000: L(Train): 0.3313435912132263; L(Test): 0.30663368105888367\n",
            "Epoch 2196/10000: L(Train): 0.3267709016799927; L(Test): 0.3058142364025116\n",
            "Epoch 2197/10000: L(Train): 0.33163341879844666; L(Test): 0.3059801459312439\n",
            "Epoch 2198/10000: L(Train): 0.32218948006629944; L(Test): 0.3065487742424011\n",
            "Epoch 2199/10000: L(Train): 0.32416945695877075; L(Test): 0.30679264664649963\n",
            "Epoch 2200/10000: L(Train): 0.33058232069015503; L(Test): 0.3073822855949402\n",
            "Epoch 2201/10000: L(Train): 0.32610851526260376; L(Test): 0.3070429265499115\n",
            "Epoch 2202/10000: L(Train): 0.33222275972366333; L(Test): 0.30627918243408203\n",
            "Epoch 2203/10000: L(Train): 0.31830301880836487; L(Test): 0.3063633441925049\n",
            "Epoch 2204/10000: L(Train): 0.32550010085105896; L(Test): 0.3070540428161621\n",
            "Epoch 2205/10000: L(Train): 0.32898208498954773; L(Test): 0.30700230598449707\n",
            "Epoch 2206/10000: L(Train): 0.32835668325424194; L(Test): 0.30648091435432434\n",
            "Epoch 2207/10000: L(Train): 0.3265954554080963; L(Test): 0.30645400285720825\n",
            "Epoch 2208/10000: L(Train): 0.3256469964981079; L(Test): 0.30671945214271545\n",
            "Epoch 2209/10000: L(Train): 0.33166831731796265; L(Test): 0.3073195517063141\n",
            "Epoch 2210/10000: L(Train): 0.32186058163642883; L(Test): 0.30730390548706055\n",
            "Epoch 2211/10000: L(Train): 0.32643312215805054; L(Test): 0.3067992627620697\n",
            "Epoch 2212/10000: L(Train): 0.3167898654937744; L(Test): 0.3062666356563568\n",
            "Epoch 2213/10000: L(Train): 0.3302033543586731; L(Test): 0.30661439895629883\n",
            "Epoch 2214/10000: L(Train): 0.3256915807723999; L(Test): 0.306363046169281\n",
            "Epoch 2215/10000: L(Train): 0.323989599943161; L(Test): 0.30629763007164\n",
            "Epoch 2216/10000: L(Train): 0.33811506628990173; L(Test): 0.3056802451610565\n",
            "Epoch 2217/10000: L(Train): 0.32823455333709717; L(Test): 0.30654284358024597\n",
            "Epoch 2218/10000: L(Train): 0.3277430236339569; L(Test): 0.30774185061454773\n",
            "Epoch 2219/10000: L(Train): 0.32834166288375854; L(Test): 0.30760809779167175\n",
            "Epoch 2220/10000: L(Train): 0.34338292479515076; L(Test): 0.3096654713153839\n",
            "Epoch 2221/10000: L(Train): 0.3392755687236786; L(Test): 0.30785462260246277\n",
            "Epoch 2222/10000: L(Train): 0.334212988615036; L(Test): 0.30714666843414307\n",
            "Epoch 2223/10000: L(Train): 0.33091312646865845; L(Test): 0.3082942068576813\n",
            "Epoch 2224/10000: L(Train): 0.3306332230567932; L(Test): 0.30708858370780945\n",
            "Epoch 2225/10000: L(Train): 0.33940786123275757; L(Test): 0.308750182390213\n",
            "Epoch 2226/10000: L(Train): 0.33211949467658997; L(Test): 0.3091481626033783\n",
            "Epoch 2227/10000: L(Train): 0.3323560655117035; L(Test): 0.30772578716278076\n",
            "Epoch 2228/10000: L(Train): 0.32134899497032166; L(Test): 0.30894339084625244\n",
            "Epoch 2229/10000: L(Train): 0.32715776562690735; L(Test): 0.30825376510620117\n",
            "Epoch 2230/10000: L(Train): 0.33273810148239136; L(Test): 0.30792975425720215\n",
            "Epoch 2231/10000: L(Train): 0.32922273874282837; L(Test): 0.30870264768600464\n",
            "Epoch 2232/10000: L(Train): 0.3219067454338074; L(Test): 0.3090905547142029\n",
            "Epoch 2233/10000: L(Train): 0.3334289491176605; L(Test): 0.3080001771450043\n",
            "Epoch 2234/10000: L(Train): 0.32978060841560364; L(Test): 0.30758631229400635\n",
            "Epoch 2235/10000: L(Train): 0.3334217667579651; L(Test): 0.3075943887233734\n",
            "Epoch 2236/10000: L(Train): 0.3333837389945984; L(Test): 0.3079361617565155\n",
            "Epoch 2237/10000: L(Train): 0.3284750282764435; L(Test): 0.3078003227710724\n",
            "Epoch 2238/10000: L(Train): 0.3263588845729828; L(Test): 0.3083305060863495\n",
            "Epoch 2239/10000: L(Train): 0.33176714181900024; L(Test): 0.3087969124317169\n",
            "Epoch 2240/10000: L(Train): 0.32970130443573; L(Test): 0.30852988362312317\n",
            "Epoch 2241/10000: L(Train): 0.3339475691318512; L(Test): 0.30859047174453735\n",
            "Epoch 2242/10000: L(Train): 0.3344845473766327; L(Test): 0.30854976177215576\n",
            "Epoch 2243/10000: L(Train): 0.3311895728111267; L(Test): 0.30733031034469604\n",
            "Epoch 2244/10000: L(Train): 0.3304579555988312; L(Test): 0.3081681430339813\n",
            "Epoch 2245/10000: L(Train): 0.3372381925582886; L(Test): 0.30749794840812683\n",
            "Epoch 2246/10000: L(Train): 0.3328413963317871; L(Test): 0.3064514100551605\n",
            "Epoch 2247/10000: L(Train): 0.3264882266521454; L(Test): 0.3071313202381134\n",
            "Epoch 2248/10000: L(Train): 0.33103764057159424; L(Test): 0.306767076253891\n",
            "Epoch 2249/10000: L(Train): 0.330728143453598; L(Test): 0.305879682302475\n",
            "Epoch 2250/10000: L(Train): 0.32846224308013916; L(Test): 0.3072955906391144\n",
            "Epoch 2251/10000: L(Train): 0.33487263321876526; L(Test): 0.30715876817703247\n",
            "Epoch 2252/10000: L(Train): 0.3253896236419678; L(Test): 0.30717039108276367\n",
            "Epoch 2253/10000: L(Train): 0.33278709650039673; L(Test): 0.30879855155944824\n",
            "Epoch 2254/10000: L(Train): 0.3270798921585083; L(Test): 0.3102622628211975\n",
            "Epoch 2255/10000: L(Train): 0.3331602215766907; L(Test): 0.3081313371658325\n",
            "Epoch 2256/10000: L(Train): 0.3331621289253235; L(Test): 0.3076781630516052\n",
            "Epoch 2257/10000: L(Train): 0.33014175295829773; L(Test): 0.3087003827095032\n",
            "Epoch 2258/10000: L(Train): 0.34220802783966064; L(Test): 0.30759698152542114\n",
            "Epoch 2259/10000: L(Train): 0.32914066314697266; L(Test): 0.30952250957489014\n",
            "Epoch 2260/10000: L(Train): 0.3350553512573242; L(Test): 0.308676153421402\n",
            "Epoch 2261/10000: L(Train): 0.32694000005722046; L(Test): 0.3083631694316864\n",
            "Epoch 2262/10000: L(Train): 0.33505967259407043; L(Test): 0.3090534508228302\n",
            "Epoch 2263/10000: L(Train): 0.32595738768577576; L(Test): 0.30929240584373474\n",
            "Epoch 2264/10000: L(Train): 0.3231029808521271; L(Test): 0.3087066411972046\n",
            "Epoch 2265/10000: L(Train): 0.3321666419506073; L(Test): 0.3097640872001648\n",
            "Epoch 2266/10000: L(Train): 0.32280609011650085; L(Test): 0.30988258123397827\n",
            "Epoch 2267/10000: L(Train): 0.33286336064338684; L(Test): 0.308763712644577\n",
            "Epoch 2268/10000: L(Train): 0.3302246034145355; L(Test): 0.3094860911369324\n",
            "Epoch 2269/10000: L(Train): 0.32971662282943726; L(Test): 0.3097785413265228\n",
            "Epoch 2270/10000: L(Train): 0.33648738265037537; L(Test): 0.30898815393447876\n",
            "Epoch 2271/10000: L(Train): 0.32747140526771545; L(Test): 0.30742502212524414\n",
            "Epoch 2272/10000: L(Train): 0.32381272315979004; L(Test): 0.3066270053386688\n",
            "Epoch 2273/10000: L(Train): 0.3298718333244324; L(Test): 0.30795422196388245\n",
            "Epoch 2274/10000: L(Train): 0.33139437437057495; L(Test): 0.3079560697078705\n",
            "Epoch 2275/10000: L(Train): 0.3289874494075775; L(Test): 0.3083552420139313\n",
            "Epoch 2276/10000: L(Train): 0.3296532928943634; L(Test): 0.309296578168869\n",
            "Epoch 2277/10000: L(Train): 0.3296659290790558; L(Test): 0.3083576261997223\n",
            "Epoch 2278/10000: L(Train): 0.3322901427745819; L(Test): 0.3080853223800659\n",
            "Epoch 2279/10000: L(Train): 0.33433428406715393; L(Test): 0.30879291892051697\n",
            "Epoch 2280/10000: L(Train): 0.33521389961242676; L(Test): 0.30887773633003235\n",
            "Epoch 2281/10000: L(Train): 0.3238944113254547; L(Test): 0.309181809425354\n",
            "Epoch 2282/10000: L(Train): 0.32564619183540344; L(Test): 0.307660847902298\n",
            "Epoch 2283/10000: L(Train): 0.32267138361930847; L(Test): 0.30697202682495117\n",
            "Epoch 2284/10000: L(Train): 0.3300890624523163; L(Test): 0.30747199058532715\n",
            "Epoch 2285/10000: L(Train): 0.31499147415161133; L(Test): 0.3084990978240967\n",
            "Epoch 2286/10000: L(Train): 0.3206827640533447; L(Test): 0.307826966047287\n",
            "Epoch 2287/10000: L(Train): 0.331229031085968; L(Test): 0.3072074055671692\n",
            "Epoch 2288/10000: L(Train): 0.3241708278656006; L(Test): 0.3071475923061371\n",
            "Epoch 2289/10000: L(Train): 0.3316836953163147; L(Test): 0.30646079778671265\n",
            "Epoch 2290/10000: L(Train): 0.32910972833633423; L(Test): 0.30597060918807983\n",
            "Epoch 2291/10000: L(Train): 0.3239341974258423; L(Test): 0.30723872780799866\n",
            "Epoch 2292/10000: L(Train): 0.33164098858833313; L(Test): 0.3074935972690582\n",
            "Epoch 2293/10000: L(Train): 0.33039382100105286; L(Test): 0.30654391646385193\n",
            "Epoch 2294/10000: L(Train): 0.33380958437919617; L(Test): 0.306437611579895\n",
            "Epoch 2295/10000: L(Train): 0.3223337233066559; L(Test): 0.30605438351631165\n",
            "Epoch 2296/10000: L(Train): 0.33592522144317627; L(Test): 0.30626946687698364\n",
            "Epoch 2297/10000: L(Train): 0.33266329765319824; L(Test): 0.30675575137138367\n",
            "Epoch 2298/10000: L(Train): 0.3383282423019409; L(Test): 0.3062164783477783\n",
            "Epoch 2299/10000: L(Train): 0.3339320123195648; L(Test): 0.3065277338027954\n",
            "Epoch 2300/10000: L(Train): 0.33679696917533875; L(Test): 0.3061234951019287\n",
            "Epoch 2301/10000: L(Train): 0.32728615403175354; L(Test): 0.3056594133377075\n",
            "Epoch 2302/10000: L(Train): 0.31204304099082947; L(Test): 0.30608588457107544\n",
            "Epoch 2303/10000: L(Train): 0.3312399387359619; L(Test): 0.3066198229789734\n",
            "Epoch 2304/10000: L(Train): 0.32220789790153503; L(Test): 0.3066602647304535\n",
            "Epoch 2305/10000: L(Train): 0.33512401580810547; L(Test): 0.30630624294281006\n",
            "Epoch 2306/10000: L(Train): 0.33326807618141174; L(Test): 0.3061620593070984\n",
            "Epoch 2307/10000: L(Train): 0.31891387701034546; L(Test): 0.3064388334751129\n",
            "Epoch 2308/10000: L(Train): 0.31966879963874817; L(Test): 0.30684801936149597\n",
            "Epoch 2309/10000: L(Train): 0.34037771821022034; L(Test): 0.30617690086364746\n",
            "Epoch 2310/10000: L(Train): 0.32543712854385376; L(Test): 0.30528348684310913\n",
            "Epoch 2311/10000: L(Train): 0.33147478103637695; L(Test): 0.30588433146476746\n",
            "Epoch 2312/10000: L(Train): 0.3334503471851349; L(Test): 0.30549100041389465\n",
            "Epoch 2313/10000: L(Train): 0.32691049575805664; L(Test): 0.30645689368247986\n",
            "Epoch 2314/10000: L(Train): 0.33115455508232117; L(Test): 0.3056526482105255\n",
            "Epoch 2315/10000: L(Train): 0.32669714093208313; L(Test): 0.308006227016449\n",
            "Epoch 2316/10000: L(Train): 0.3226492702960968; L(Test): 0.3104114532470703\n",
            "Epoch 2317/10000: L(Train): 0.3303746283054352; L(Test): 0.31025633215904236\n",
            "Epoch 2318/10000: L(Train): 0.323934942483902; L(Test): 0.31151390075683594\n",
            "Epoch 2319/10000: L(Train): 0.3385255038738251; L(Test): 0.31020402908325195\n",
            "Epoch 2320/10000: L(Train): 0.33477258682250977; L(Test): 0.30860304832458496\n",
            "Epoch 2321/10000: L(Train): 0.3258666396141052; L(Test): 0.3096437454223633\n",
            "Epoch 2322/10000: L(Train): 0.32916882634162903; L(Test): 0.3092978894710541\n",
            "Epoch 2323/10000: L(Train): 0.3351512849330902; L(Test): 0.3095241189002991\n",
            "Epoch 2324/10000: L(Train): 0.33004382252693176; L(Test): 0.31031110882759094\n",
            "Epoch 2325/10000: L(Train): 0.32543250918388367; L(Test): 0.309247761964798\n",
            "Epoch 2326/10000: L(Train): 0.3256753981113434; L(Test): 0.30798712372779846\n",
            "Epoch 2327/10000: L(Train): 0.3331051766872406; L(Test): 0.30760854482650757\n",
            "Epoch 2328/10000: L(Train): 0.33667564392089844; L(Test): 0.3076375722885132\n",
            "Epoch 2329/10000: L(Train): 0.33058127760887146; L(Test): 0.3075948655605316\n",
            "Epoch 2330/10000: L(Train): 0.3271893262863159; L(Test): 0.30749255418777466\n",
            "Epoch 2331/10000: L(Train): 0.3219117522239685; L(Test): 0.3077666759490967\n",
            "Epoch 2332/10000: L(Train): 0.3275873363018036; L(Test): 0.3073388636112213\n",
            "Epoch 2333/10000: L(Train): 0.32750388979911804; L(Test): 0.3069729208946228\n",
            "Epoch 2334/10000: L(Train): 0.32865312695503235; L(Test): 0.3065814673900604\n",
            "Epoch 2335/10000: L(Train): 0.3323040306568146; L(Test): 0.3067981004714966\n",
            "Epoch 2336/10000: L(Train): 0.32232144474983215; L(Test): 0.3063751757144928\n",
            "Epoch 2337/10000: L(Train): 0.33574986457824707; L(Test): 0.305889368057251\n",
            "Epoch 2338/10000: L(Train): 0.3247334361076355; L(Test): 0.30680468678474426\n",
            "Epoch 2339/10000: L(Train): 0.33494505286216736; L(Test): 0.306735098361969\n",
            "Epoch 2340/10000: L(Train): 0.31940749287605286; L(Test): 0.30608418583869934\n",
            "Epoch 2341/10000: L(Train): 0.3354303538799286; L(Test): 0.3065603971481323\n",
            "Epoch 2342/10000: L(Train): 0.3211279511451721; L(Test): 0.30716413259506226\n",
            "Epoch 2343/10000: L(Train): 0.3303711414337158; L(Test): 0.3064460754394531\n",
            "Epoch 2344/10000: L(Train): 0.3247310519218445; L(Test): 0.306538462638855\n",
            "Epoch 2345/10000: L(Train): 0.3197723925113678; L(Test): 0.3072105348110199\n",
            "Epoch 2346/10000: L(Train): 0.32950228452682495; L(Test): 0.3081968128681183\n",
            "Epoch 2347/10000: L(Train): 0.3355112373828888; L(Test): 0.30748432874679565\n",
            "Epoch 2348/10000: L(Train): 0.3332451581954956; L(Test): 0.30830714106559753\n",
            "Epoch 2349/10000: L(Train): 0.32703331112861633; L(Test): 0.306858092546463\n",
            "Epoch 2350/10000: L(Train): 0.3365834355354309; L(Test): 0.30667564272880554\n",
            "Epoch 2351/10000: L(Train): 0.3298179507255554; L(Test): 0.30767229199409485\n",
            "Epoch 2352/10000: L(Train): 0.33451372385025024; L(Test): 0.30706170201301575\n",
            "Epoch 2353/10000: L(Train): 0.3199979364871979; L(Test): 0.30623456835746765\n",
            "Epoch 2354/10000: L(Train): 0.32442909479141235; L(Test): 0.3071669340133667\n",
            "Epoch 2355/10000: L(Train): 0.3248976171016693; L(Test): 0.3069666624069214\n",
            "Epoch 2356/10000: L(Train): 0.34231990575790405; L(Test): 0.30705389380455017\n",
            "Epoch 2357/10000: L(Train): 0.327251136302948; L(Test): 0.30759197473526\n",
            "Epoch 2358/10000: L(Train): 0.3298003673553467; L(Test): 0.30717578530311584\n",
            "Epoch 2359/10000: L(Train): 0.32560887932777405; L(Test): 0.3062114119529724\n",
            "Epoch 2360/10000: L(Train): 0.3175680935382843; L(Test): 0.3070371150970459\n",
            "Epoch 2361/10000: L(Train): 0.3285467326641083; L(Test): 0.30815163254737854\n",
            "Epoch 2362/10000: L(Train): 0.32692062854766846; L(Test): 0.30934467911720276\n",
            "Epoch 2363/10000: L(Train): 0.33685746788978577; L(Test): 0.30867549777030945\n",
            "Epoch 2364/10000: L(Train): 0.33309462666511536; L(Test): 0.3071877658367157\n",
            "Epoch 2365/10000: L(Train): 0.3337763249874115; L(Test): 0.3073112964630127\n",
            "Epoch 2366/10000: L(Train): 0.33054065704345703; L(Test): 0.30719807744026184\n",
            "Epoch 2367/10000: L(Train): 0.3260858952999115; L(Test): 0.30618396401405334\n",
            "Epoch 2368/10000: L(Train): 0.3266927897930145; L(Test): 0.3054972290992737\n",
            "Epoch 2369/10000: L(Train): 0.32599321007728577; L(Test): 0.30582335591316223\n",
            "Epoch 2370/10000: L(Train): 0.3215612769126892; L(Test): 0.30608469247817993\n",
            "Epoch 2371/10000: L(Train): 0.33351075649261475; L(Test): 0.3064867854118347\n",
            "Epoch 2372/10000: L(Train): 0.3318391442298889; L(Test): 0.30575498938560486\n",
            "Epoch 2373/10000: L(Train): 0.33037444949150085; L(Test): 0.3058300018310547\n",
            "Epoch 2374/10000: L(Train): 0.33072778582572937; L(Test): 0.306408554315567\n",
            "Epoch 2375/10000: L(Train): 0.32850953936576843; L(Test): 0.3057205379009247\n",
            "Epoch 2376/10000: L(Train): 0.3235495984554291; L(Test): 0.3055610656738281\n",
            "Epoch 2377/10000: L(Train): 0.32583537697792053; L(Test): 0.30597397685050964\n",
            "Epoch 2378/10000: L(Train): 0.3193967938423157; L(Test): 0.30615487694740295\n",
            "Epoch 2379/10000: L(Train): 0.33318498730659485; L(Test): 0.30657532811164856\n",
            "Epoch 2380/10000: L(Train): 0.3286903500556946; L(Test): 0.3070485293865204\n",
            "Epoch 2381/10000: L(Train): 0.3290543258190155; L(Test): 0.30644506216049194\n",
            "Epoch 2382/10000: L(Train): 0.32206061482429504; L(Test): 0.3060503304004669\n",
            "Epoch 2383/10000: L(Train): 0.32578039169311523; L(Test): 0.3061637878417969\n",
            "Epoch 2384/10000: L(Train): 0.3335016965866089; L(Test): 0.3060052692890167\n",
            "Epoch 2385/10000: L(Train): 0.3315180838108063; L(Test): 0.3063584566116333\n",
            "Epoch 2386/10000: L(Train): 0.3225802779197693; L(Test): 0.3060910403728485\n",
            "Epoch 2387/10000: L(Train): 0.3327951729297638; L(Test): 0.3058162331581116\n",
            "Epoch 2388/10000: L(Train): 0.32900139689445496; L(Test): 0.3052982687950134\n",
            "Epoch 2389/10000: L(Train): 0.33348143100738525; L(Test): 0.3045746684074402\n",
            "Epoch 2390/10000: L(Train): 0.31826332211494446; L(Test): 0.306496262550354\n",
            "Epoch 2391/10000: L(Train): 0.32373589277267456; L(Test): 0.3069247901439667\n",
            "Epoch 2392/10000: L(Train): 0.3289693295955658; L(Test): 0.3063286542892456\n",
            "Epoch 2393/10000: L(Train): 0.33057111501693726; L(Test): 0.3054715096950531\n",
            "Epoch 2394/10000: L(Train): 0.332392156124115; L(Test): 0.30489933490753174\n",
            "Epoch 2395/10000: L(Train): 0.32848742604255676; L(Test): 0.30450674891471863\n",
            "Epoch 2396/10000: L(Train): 0.33415091037750244; L(Test): 0.30555394291877747\n",
            "Epoch 2397/10000: L(Train): 0.3254132568836212; L(Test): 0.3061122000217438\n",
            "Epoch 2398/10000: L(Train): 0.32563138008117676; L(Test): 0.3060227334499359\n",
            "Epoch 2399/10000: L(Train): 0.3273952007293701; L(Test): 0.30579695105552673\n",
            "Epoch 2400/10000: L(Train): 0.3279704749584198; L(Test): 0.3057057857513428\n",
            "Epoch 2401/10000: L(Train): 0.32819506525993347; L(Test): 0.30649086833000183\n",
            "Epoch 2402/10000: L(Train): 0.32785528898239136; L(Test): 0.3058047592639923\n",
            "Epoch 2403/10000: L(Train): 0.32669204473495483; L(Test): 0.30561527609825134\n",
            "Epoch 2404/10000: L(Train): 0.32705220580101013; L(Test): 0.3054050803184509\n",
            "Epoch 2405/10000: L(Train): 0.3284533619880676; L(Test): 0.3052734434604645\n",
            "Epoch 2406/10000: L(Train): 0.33240580558776855; L(Test): 0.30583813786506653\n",
            "Epoch 2407/10000: L(Train): 0.3345203101634979; L(Test): 0.30541273951530457\n",
            "Epoch 2408/10000: L(Train): 0.3344426453113556; L(Test): 0.3055087625980377\n",
            "Epoch 2409/10000: L(Train): 0.32676592469215393; L(Test): 0.30549779534339905\n",
            "Epoch 2410/10000: L(Train): 0.3345329165458679; L(Test): 0.3057738244533539\n",
            "Epoch 2411/10000: L(Train): 0.3266204595565796; L(Test): 0.3061554431915283\n",
            "Epoch 2412/10000: L(Train): 0.326983243227005; L(Test): 0.3063929080963135\n",
            "Epoch 2413/10000: L(Train): 0.33159440755844116; L(Test): 0.30553552508354187\n",
            "Epoch 2414/10000: L(Train): 0.3333475589752197; L(Test): 0.3052487075328827\n",
            "Epoch 2415/10000: L(Train): 0.33154141902923584; L(Test): 0.30580592155456543\n",
            "Epoch 2416/10000: L(Train): 0.32957643270492554; L(Test): 0.30520954728126526\n",
            "Epoch 2417/10000: L(Train): 0.32880091667175293; L(Test): 0.3046513497829437\n",
            "Epoch 2418/10000: L(Train): 0.3225114345550537; L(Test): 0.304666668176651\n",
            "Epoch 2419/10000: L(Train): 0.32891616225242615; L(Test): 0.3057457208633423\n",
            "Epoch 2420/10000: L(Train): 0.3162323236465454; L(Test): 0.3061816990375519\n",
            "Epoch 2421/10000: L(Train): 0.32828599214553833; L(Test): 0.3056178092956543\n",
            "Epoch 2422/10000: L(Train): 0.32897356152534485; L(Test): 0.30497801303863525\n",
            "Epoch 2423/10000: L(Train): 0.33274975419044495; L(Test): 0.3053613305091858\n",
            "Epoch 2424/10000: L(Train): 0.3307502269744873; L(Test): 0.30523383617401123\n",
            "Epoch 2425/10000: L(Train): 0.31970155239105225; L(Test): 0.30501875281333923\n",
            "Epoch 2426/10000: L(Train): 0.32760581374168396; L(Test): 0.30541273951530457\n",
            "Epoch 2427/10000: L(Train): 0.319387823343277; L(Test): 0.306047260761261\n",
            "Epoch 2428/10000: L(Train): 0.3249308466911316; L(Test): 0.30578458309173584\n",
            "Epoch 2429/10000: L(Train): 0.33623793721199036; L(Test): 0.30561164021492004\n",
            "Epoch 2430/10000: L(Train): 0.32671868801116943; L(Test): 0.30627232789993286\n",
            "Epoch 2431/10000: L(Train): 0.3201732933521271; L(Test): 0.30565980076789856\n",
            "Epoch 2432/10000: L(Train): 0.33088767528533936; L(Test): 0.30646079778671265\n",
            "Epoch 2433/10000: L(Train): 0.32050788402557373; L(Test): 0.3053489625453949\n",
            "Epoch 2434/10000: L(Train): 0.3221713602542877; L(Test): 0.3050556778907776\n",
            "Epoch 2435/10000: L(Train): 0.3300463855266571; L(Test): 0.3051060736179352\n",
            "Epoch 2436/10000: L(Train): 0.3263286054134369; L(Test): 0.30672597885131836\n",
            "Epoch 2437/10000: L(Train): 0.32916519045829773; L(Test): 0.3061257004737854\n",
            "Epoch 2438/10000: L(Train): 0.3258282542228699; L(Test): 0.30557718873023987\n",
            "Epoch 2439/10000: L(Train): 0.33125734329223633; L(Test): 0.30551427602767944\n",
            "Epoch 2440/10000: L(Train): 0.325465589761734; L(Test): 0.30541715025901794\n",
            "Epoch 2441/10000: L(Train): 0.3227708637714386; L(Test): 0.30523303151130676\n",
            "Epoch 2442/10000: L(Train): 0.3280656635761261; L(Test): 0.30581963062286377\n",
            "Epoch 2443/10000: L(Train): 0.32957449555397034; L(Test): 0.30587872862815857\n",
            "Epoch 2444/10000: L(Train): 0.3329155445098877; L(Test): 0.305656373500824\n",
            "Epoch 2445/10000: L(Train): 0.32521939277648926; L(Test): 0.30576616525650024\n",
            "Epoch 2446/10000: L(Train): 0.3177943229675293; L(Test): 0.30627959966659546\n",
            "Epoch 2447/10000: L(Train): 0.3267710506916046; L(Test): 0.3072377145290375\n",
            "Epoch 2448/10000: L(Train): 0.3275812864303589; L(Test): 0.3068035840988159\n",
            "Epoch 2449/10000: L(Train): 0.33142054080963135; L(Test): 0.3061336874961853\n",
            "Epoch 2450/10000: L(Train): 0.3268980383872986; L(Test): 0.3059435784816742\n",
            "Epoch 2451/10000: L(Train): 0.3338872492313385; L(Test): 0.30511996150016785\n",
            "Epoch 2452/10000: L(Train): 0.3231067359447479; L(Test): 0.30696067214012146\n",
            "Epoch 2453/10000: L(Train): 0.3277492821216583; L(Test): 0.3067654073238373\n",
            "Epoch 2454/10000: L(Train): 0.3242655396461487; L(Test): 0.3054136633872986\n",
            "Epoch 2455/10000: L(Train): 0.31431838870048523; L(Test): 0.3053065836429596\n",
            "Epoch 2456/10000: L(Train): 0.32865655422210693; L(Test): 0.3065357804298401\n",
            "Epoch 2457/10000: L(Train): 0.33378493785858154; L(Test): 0.30661800503730774\n",
            "Epoch 2458/10000: L(Train): 0.3222976624965668; L(Test): 0.30688419938087463\n",
            "Epoch 2459/10000: L(Train): 0.3297601342201233; L(Test): 0.3078981041908264\n",
            "Epoch 2460/10000: L(Train): 0.3340114653110504; L(Test): 0.3079797029495239\n",
            "Epoch 2461/10000: L(Train): 0.3299409747123718; L(Test): 0.30728694796562195\n",
            "Epoch 2462/10000: L(Train): 0.322261780500412; L(Test): 0.30741772055625916\n",
            "Epoch 2463/10000: L(Train): 0.3284558355808258; L(Test): 0.3076733350753784\n",
            "Epoch 2464/10000: L(Train): 0.33173316717147827; L(Test): 0.3052463233470917\n",
            "Epoch 2465/10000: L(Train): 0.32268399000167847; L(Test): 0.3056650757789612\n",
            "Epoch 2466/10000: L(Train): 0.3349054455757141; L(Test): 0.30576804280281067\n",
            "Epoch 2467/10000: L(Train): 0.33380618691444397; L(Test): 0.3059975802898407\n",
            "Epoch 2468/10000: L(Train): 0.3353278636932373; L(Test): 0.30603793263435364\n",
            "Epoch 2469/10000: L(Train): 0.3322644531726837; L(Test): 0.30493757128715515\n",
            "Epoch 2470/10000: L(Train): 0.32146093249320984; L(Test): 0.3065691292285919\n",
            "Epoch 2471/10000: L(Train): 0.3267783224582672; L(Test): 0.30555254220962524\n",
            "Epoch 2472/10000: L(Train): 0.3221960663795471; L(Test): 0.30709341168403625\n",
            "Epoch 2473/10000: L(Train): 0.33351847529411316; L(Test): 0.30629369616508484\n",
            "Epoch 2474/10000: L(Train): 0.33246755599975586; L(Test): 0.306876003742218\n",
            "Epoch 2475/10000: L(Train): 0.32443663477897644; L(Test): 0.30740445852279663\n",
            "Epoch 2476/10000: L(Train): 0.3248611092567444; L(Test): 0.30782926082611084\n",
            "Epoch 2477/10000: L(Train): 0.3327692449092865; L(Test): 0.30691999197006226\n",
            "Epoch 2478/10000: L(Train): 0.3294091522693634; L(Test): 0.3064768612384796\n",
            "Epoch 2479/10000: L(Train): 0.3240338861942291; L(Test): 0.30694982409477234\n",
            "Epoch 2480/10000: L(Train): 0.3231542408466339; L(Test): 0.30613812804222107\n",
            "Epoch 2481/10000: L(Train): 0.32991230487823486; L(Test): 0.3067556321620941\n",
            "Epoch 2482/10000: L(Train): 0.3194800019264221; L(Test): 0.3074732720851898\n",
            "Epoch 2483/10000: L(Train): 0.3294508457183838; L(Test): 0.30642810463905334\n",
            "Epoch 2484/10000: L(Train): 0.32681697607040405; L(Test): 0.3054194748401642\n",
            "Epoch 2485/10000: L(Train): 0.32913073897361755; L(Test): 0.30524197220802307\n",
            "Epoch 2486/10000: L(Train): 0.32520076632499695; L(Test): 0.30560705065727234\n",
            "Epoch 2487/10000: L(Train): 0.3343939185142517; L(Test): 0.3063631057739258\n",
            "Epoch 2488/10000: L(Train): 0.32266220450401306; L(Test): 0.305908203125\n",
            "Epoch 2489/10000: L(Train): 0.33141788840293884; L(Test): 0.30589210987091064\n",
            "Epoch 2490/10000: L(Train): 0.3289569318294525; L(Test): 0.30697816610336304\n",
            "Epoch 2491/10000: L(Train): 0.32858067750930786; L(Test): 0.30533963441848755\n",
            "Epoch 2492/10000: L(Train): 0.3465229570865631; L(Test): 0.30613061785697937\n",
            "Epoch 2493/10000: L(Train): 0.3344663977622986; L(Test): 0.30668380856513977\n",
            "Epoch 2494/10000: L(Train): 0.3289821445941925; L(Test): 0.306766539812088\n",
            "Epoch 2495/10000: L(Train): 0.34021225571632385; L(Test): 0.30575132369995117\n",
            "Epoch 2496/10000: L(Train): 0.331002414226532; L(Test): 0.3059440851211548\n",
            "Epoch 2497/10000: L(Train): 0.32066091895103455; L(Test): 0.3064939081668854\n",
            "Epoch 2498/10000: L(Train): 0.32210466265678406; L(Test): 0.3074570298194885\n",
            "Epoch 2499/10000: L(Train): 0.3345300853252411; L(Test): 0.3075869381427765\n",
            "Epoch 2500/10000: L(Train): 0.33264732360839844; L(Test): 0.3055170476436615\n",
            "Epoch 2501/10000: L(Train): 0.32037290930747986; L(Test): 0.30649876594543457\n",
            "Epoch 2502/10000: L(Train): 0.32706335186958313; L(Test): 0.3079371750354767\n",
            "Epoch 2503/10000: L(Train): 0.32618775963783264; L(Test): 0.3074815273284912\n",
            "Epoch 2504/10000: L(Train): 0.3297450840473175; L(Test): 0.3076930642127991\n",
            "Epoch 2505/10000: L(Train): 0.32785558700561523; L(Test): 0.30678969621658325\n",
            "Epoch 2506/10000: L(Train): 0.3170962929725647; L(Test): 0.30543145537376404\n",
            "Epoch 2507/10000: L(Train): 0.3149334788322449; L(Test): 0.3057360053062439\n",
            "Epoch 2508/10000: L(Train): 0.3332822620868683; L(Test): 0.30681875348091125\n",
            "Epoch 2509/10000: L(Train): 0.3278026878833771; L(Test): 0.30577242374420166\n",
            "Epoch 2510/10000: L(Train): 0.3217284083366394; L(Test): 0.30478593707084656\n",
            "Epoch 2511/10000: L(Train): 0.3261657655239105; L(Test): 0.3055928647518158\n",
            "Epoch 2512/10000: L(Train): 0.31670743227005005; L(Test): 0.30534449219703674\n",
            "Epoch 2513/10000: L(Train): 0.3346540331840515; L(Test): 0.3045724332332611\n",
            "Epoch 2514/10000: L(Train): 0.3232066035270691; L(Test): 0.3051121234893799\n",
            "Epoch 2515/10000: L(Train): 0.3280263841152191; L(Test): 0.3054634630680084\n",
            "Epoch 2516/10000: L(Train): 0.33033114671707153; L(Test): 0.30433619022369385\n",
            "Epoch 2517/10000: L(Train): 0.3280889391899109; L(Test): 0.30473792552948\n",
            "Epoch 2518/10000: L(Train): 0.3257208466529846; L(Test): 0.3050616383552551\n",
            "Epoch 2519/10000: L(Train): 0.333497017621994; L(Test): 0.30548039078712463\n",
            "Epoch 2520/10000: L(Train): 0.3315294682979584; L(Test): 0.3044945299625397\n",
            "Epoch 2521/10000: L(Train): 0.3279053568840027; L(Test): 0.30472227931022644\n",
            "Epoch 2522/10000: L(Train): 0.3253541886806488; L(Test): 0.30534985661506653\n",
            "Epoch 2523/10000: L(Train): 0.3208863437175751; L(Test): 0.3048938810825348\n",
            "Epoch 2524/10000: L(Train): 0.310580849647522; L(Test): 0.30463728308677673\n",
            "Epoch 2525/10000: L(Train): 0.329256534576416; L(Test): 0.30389726161956787\n",
            "Epoch 2526/10000: L(Train): 0.32911768555641174; L(Test): 0.30492183566093445\n",
            "Epoch 2527/10000: L(Train): 0.32966554164886475; L(Test): 0.3050984740257263\n",
            "Epoch 2528/10000: L(Train): 0.32382434606552124; L(Test): 0.30488449335098267\n",
            "Epoch 2529/10000: L(Train): 0.32376328110694885; L(Test): 0.3067052662372589\n",
            "Epoch 2530/10000: L(Train): 0.32964128255844116; L(Test): 0.3060134947299957\n",
            "Epoch 2531/10000: L(Train): 0.321391761302948; L(Test): 0.30565145611763\n",
            "Epoch 2532/10000: L(Train): 0.32540804147720337; L(Test): 0.30475834012031555\n",
            "Epoch 2533/10000: L(Train): 0.33753687143325806; L(Test): 0.30413052439689636\n",
            "Epoch 2534/10000: L(Train): 0.329623281955719; L(Test): 0.30446943640708923\n",
            "Epoch 2535/10000: L(Train): 0.32593604922294617; L(Test): 0.304571270942688\n",
            "Epoch 2536/10000: L(Train): 0.3298414647579193; L(Test): 0.30380555987358093\n",
            "Epoch 2537/10000: L(Train): 0.33587390184402466; L(Test): 0.3047618269920349\n",
            "Epoch 2538/10000: L(Train): 0.32334113121032715; L(Test): 0.3043864071369171\n",
            "Epoch 2539/10000: L(Train): 0.32881078124046326; L(Test): 0.3040473461151123\n",
            "Epoch 2540/10000: L(Train): 0.32341110706329346; L(Test): 0.304635614156723\n",
            "Epoch 2541/10000: L(Train): 0.3237280249595642; L(Test): 0.3055295944213867\n",
            "Epoch 2542/10000: L(Train): 0.32929837703704834; L(Test): 0.30429980158805847\n",
            "Epoch 2543/10000: L(Train): 0.32690107822418213; L(Test): 0.3047315776348114\n",
            "Epoch 2544/10000: L(Train): 0.3172052204608917; L(Test): 0.30433037877082825\n",
            "Epoch 2545/10000: L(Train): 0.3256974220275879; L(Test): 0.30468904972076416\n",
            "Epoch 2546/10000: L(Train): 0.32088226079940796; L(Test): 0.3060912489891052\n",
            "Epoch 2547/10000: L(Train): 0.32768306136131287; L(Test): 0.30517780780792236\n",
            "Epoch 2548/10000: L(Train): 0.3131144344806671; L(Test): 0.30481886863708496\n",
            "Epoch 2549/10000: L(Train): 0.32164010405540466; L(Test): 0.30513861775398254\n",
            "Epoch 2550/10000: L(Train): 0.3231339156627655; L(Test): 0.30494746565818787\n",
            "Epoch 2551/10000: L(Train): 0.32786089181900024; L(Test): 0.3051460385322571\n",
            "Epoch 2552/10000: L(Train): 0.32647961378097534; L(Test): 0.3047650456428528\n",
            "Epoch 2553/10000: L(Train): 0.32405245304107666; L(Test): 0.30403757095336914\n",
            "Epoch 2554/10000: L(Train): 0.33359697461128235; L(Test): 0.30450060963630676\n",
            "Epoch 2555/10000: L(Train): 0.32052677869796753; L(Test): 0.30527350306510925\n",
            "Epoch 2556/10000: L(Train): 0.32917162775993347; L(Test): 0.3049832582473755\n",
            "Epoch 2557/10000: L(Train): 0.330062597990036; L(Test): 0.30548611283302307\n",
            "Epoch 2558/10000: L(Train): 0.32657337188720703; L(Test): 0.3056589961051941\n",
            "Epoch 2559/10000: L(Train): 0.3280127942562103; L(Test): 0.3073340654373169\n",
            "Epoch 2560/10000: L(Train): 0.3339512050151825; L(Test): 0.30853724479675293\n",
            "Epoch 2561/10000: L(Train): 0.3342808783054352; L(Test): 0.3069608807563782\n",
            "Epoch 2562/10000: L(Train): 0.32097336649894714; L(Test): 0.30797478556632996\n",
            "Epoch 2563/10000: L(Train): 0.3388655483722687; L(Test): 0.3082486391067505\n",
            "Epoch 2564/10000: L(Train): 0.32743725180625916; L(Test): 0.3073437511920929\n",
            "Epoch 2565/10000: L(Train): 0.3321358859539032; L(Test): 0.3078327476978302\n",
            "Epoch 2566/10000: L(Train): 0.32476478815078735; L(Test): 0.3075559735298157\n",
            "Epoch 2567/10000: L(Train): 0.33443012833595276; L(Test): 0.30730199813842773\n",
            "Epoch 2568/10000: L(Train): 0.3235601782798767; L(Test): 0.30677908658981323\n",
            "Epoch 2569/10000: L(Train): 0.32958078384399414; L(Test): 0.3076985478401184\n",
            "Epoch 2570/10000: L(Train): 0.31741994619369507; L(Test): 0.3077377378940582\n",
            "Epoch 2571/10000: L(Train): 0.33708855509757996; L(Test): 0.3063979148864746\n",
            "Epoch 2572/10000: L(Train): 0.3325357139110565; L(Test): 0.30576884746551514\n",
            "Epoch 2573/10000: L(Train): 0.3184843957424164; L(Test): 0.30632027983665466\n",
            "Epoch 2574/10000: L(Train): 0.3372842073440552; L(Test): 0.3064296245574951\n",
            "Epoch 2575/10000: L(Train): 0.31887882947921753; L(Test): 0.30653116106987\n",
            "Epoch 2576/10000: L(Train): 0.3291286528110504; L(Test): 0.3063102960586548\n",
            "Epoch 2577/10000: L(Train): 0.33585283160209656; L(Test): 0.3046784996986389\n",
            "Epoch 2578/10000: L(Train): 0.325595498085022; L(Test): 0.30458685755729675\n",
            "Epoch 2579/10000: L(Train): 0.3193526566028595; L(Test): 0.3051508069038391\n",
            "Epoch 2580/10000: L(Train): 0.3195597529411316; L(Test): 0.3054457902908325\n",
            "Epoch 2581/10000: L(Train): 0.32728061079978943; L(Test): 0.30651357769966125\n",
            "Epoch 2582/10000: L(Train): 0.335582435131073; L(Test): 0.3062109649181366\n",
            "Epoch 2583/10000: L(Train): 0.3345971405506134; L(Test): 0.3069000840187073\n",
            "Epoch 2584/10000: L(Train): 0.3292537331581116; L(Test): 0.30613455176353455\n",
            "Epoch 2585/10000: L(Train): 0.321235716342926; L(Test): 0.3050976097583771\n",
            "Epoch 2586/10000: L(Train): 0.3226841986179352; L(Test): 0.30537131428718567\n",
            "Epoch 2587/10000: L(Train): 0.3260256052017212; L(Test): 0.30575844645500183\n",
            "Epoch 2588/10000: L(Train): 0.3312029242515564; L(Test): 0.3054608106613159\n",
            "Epoch 2589/10000: L(Train): 0.3349989950656891; L(Test): 0.30563247203826904\n",
            "Epoch 2590/10000: L(Train): 0.32505500316619873; L(Test): 0.3061825633049011\n",
            "Epoch 2591/10000: L(Train): 0.316799521446228; L(Test): 0.30586472153663635\n",
            "Epoch 2592/10000: L(Train): 0.33816343545913696; L(Test): 0.30559638142585754\n",
            "Epoch 2593/10000: L(Train): 0.3258760869503021; L(Test): 0.306041419506073\n",
            "Epoch 2594/10000: L(Train): 0.3192557096481323; L(Test): 0.30725178122520447\n",
            "Epoch 2595/10000: L(Train): 0.3249291181564331; L(Test): 0.3061491847038269\n",
            "Epoch 2596/10000: L(Train): 0.33142757415771484; L(Test): 0.3064848780632019\n",
            "Epoch 2597/10000: L(Train): 0.3296486735343933; L(Test): 0.3066040575504303\n",
            "Epoch 2598/10000: L(Train): 0.3280141353607178; L(Test): 0.30529776215553284\n",
            "Epoch 2599/10000: L(Train): 0.31317296624183655; L(Test): 0.30595800280570984\n",
            "Epoch 2600/10000: L(Train): 0.32915082573890686; L(Test): 0.3051931858062744\n",
            "Epoch 2601/10000: L(Train): 0.33181753754615784; L(Test): 0.30477049946784973\n",
            "Epoch 2602/10000: L(Train): 0.32984110713005066; L(Test): 0.3054644465446472\n",
            "Epoch 2603/10000: L(Train): 0.32723501324653625; L(Test): 0.3055957853794098\n",
            "Epoch 2604/10000: L(Train): 0.32513195276260376; L(Test): 0.30619102716445923\n",
            "Epoch 2605/10000: L(Train): 0.32163122296333313; L(Test): 0.3071659207344055\n",
            "Epoch 2606/10000: L(Train): 0.3282453417778015; L(Test): 0.3060699701309204\n",
            "Epoch 2607/10000: L(Train): 0.3204486072063446; L(Test): 0.3053489625453949\n",
            "Epoch 2608/10000: L(Train): 0.31540289521217346; L(Test): 0.3059427738189697\n",
            "Epoch 2609/10000: L(Train): 0.3284449577331543; L(Test): 0.3057129383087158\n",
            "Epoch 2610/10000: L(Train): 0.33343076705932617; L(Test): 0.30479708313941956\n",
            "Epoch 2611/10000: L(Train): 0.32556718587875366; L(Test): 0.3049442768096924\n",
            "Epoch 2612/10000: L(Train): 0.331926167011261; L(Test): 0.30459967255592346\n",
            "Epoch 2613/10000: L(Train): 0.32687318325042725; L(Test): 0.30513307452201843\n",
            "Epoch 2614/10000: L(Train): 0.32657018303871155; L(Test): 0.3060961067676544\n",
            "Epoch 2615/10000: L(Train): 0.33033761382102966; L(Test): 0.30568596720695496\n",
            "Epoch 2616/10000: L(Train): 0.32889941334724426; L(Test): 0.3060772120952606\n",
            "Epoch 2617/10000: L(Train): 0.33231765031814575; L(Test): 0.30637088418006897\n",
            "Epoch 2618/10000: L(Train): 0.3323194086551666; L(Test): 0.305398553609848\n",
            "Epoch 2619/10000: L(Train): 0.32529881596565247; L(Test): 0.3057372570037842\n",
            "Epoch 2620/10000: L(Train): 0.33411094546318054; L(Test): 0.30555054545402527\n",
            "Epoch 2621/10000: L(Train): 0.3158313035964966; L(Test): 0.3047938048839569\n",
            "Epoch 2622/10000: L(Train): 0.3279878497123718; L(Test): 0.30455753207206726\n",
            "Epoch 2623/10000: L(Train): 0.3285099267959595; L(Test): 0.30487337708473206\n",
            "Epoch 2624/10000: L(Train): 0.3297886252403259; L(Test): 0.30528756976127625\n",
            "Epoch 2625/10000: L(Train): 0.3344782888889313; L(Test): 0.3053475320339203\n",
            "Epoch 2626/10000: L(Train): 0.3259722590446472; L(Test): 0.30481013655662537\n",
            "Epoch 2627/10000: L(Train): 0.3362048864364624; L(Test): 0.30440646409988403\n",
            "Epoch 2628/10000: L(Train): 0.3309202194213867; L(Test): 0.30507922172546387\n",
            "Epoch 2629/10000: L(Train): 0.32187435030937195; L(Test): 0.3057149648666382\n",
            "Epoch 2630/10000: L(Train): 0.3262113630771637; L(Test): 0.30556443333625793\n",
            "Epoch 2631/10000: L(Train): 0.32942771911621094; L(Test): 0.3051908314228058\n",
            "Epoch 2632/10000: L(Train): 0.32380566000938416; L(Test): 0.3063209652900696\n",
            "Epoch 2633/10000: L(Train): 0.3362961709499359; L(Test): 0.30637675523757935\n",
            "Epoch 2634/10000: L(Train): 0.32946398854255676; L(Test): 0.30527985095977783\n",
            "Epoch 2635/10000: L(Train): 0.3281500041484833; L(Test): 0.3061518371105194\n",
            "Epoch 2636/10000: L(Train): 0.33255863189697266; L(Test): 0.3071170449256897\n",
            "Epoch 2637/10000: L(Train): 0.3320528268814087; L(Test): 0.30613818764686584\n",
            "Epoch 2638/10000: L(Train): 0.3352433741092682; L(Test): 0.30515536665916443\n",
            "Epoch 2639/10000: L(Train): 0.3303404748439789; L(Test): 0.3060970604419708\n",
            "Epoch 2640/10000: L(Train): 0.3317849338054657; L(Test): 0.30602896213531494\n",
            "Epoch 2641/10000: L(Train): 0.3316271901130676; L(Test): 0.3057667016983032\n",
            "Epoch 2642/10000: L(Train): 0.31800198554992676; L(Test): 0.30642271041870117\n",
            "Epoch 2643/10000: L(Train): 0.32219797372817993; L(Test): 0.3071349859237671\n",
            "Epoch 2644/10000: L(Train): 0.32159850001335144; L(Test): 0.3063964247703552\n",
            "Epoch 2645/10000: L(Train): 0.3235604166984558; L(Test): 0.30566319823265076\n",
            "Epoch 2646/10000: L(Train): 0.32603123784065247; L(Test): 0.3068526089191437\n",
            "Epoch 2647/10000: L(Train): 0.32866403460502625; L(Test): 0.30746591091156006\n",
            "Epoch 2648/10000: L(Train): 0.3253532350063324; L(Test): 0.30573901534080505\n",
            "Epoch 2649/10000: L(Train): 0.32914215326309204; L(Test): 0.3057844638824463\n",
            "Epoch 2650/10000: L(Train): 0.3252917528152466; L(Test): 0.3062084913253784\n",
            "Epoch 2651/10000: L(Train): 0.3306823670864105; L(Test): 0.30524134635925293\n",
            "Epoch 2652/10000: L(Train): 0.32333099842071533; L(Test): 0.3050006628036499\n",
            "Epoch 2653/10000: L(Train): 0.32770445942878723; L(Test): 0.3054874837398529\n",
            "Epoch 2654/10000: L(Train): 0.3273928761482239; L(Test): 0.30570921301841736\n",
            "Epoch 2655/10000: L(Train): 0.3232184946537018; L(Test): 0.3053087890148163\n",
            "Epoch 2656/10000: L(Train): 0.32529133558273315; L(Test): 0.3050990104675293\n",
            "Epoch 2657/10000: L(Train): 0.3311024010181427; L(Test): 0.30539992451667786\n",
            "Epoch 2658/10000: L(Train): 0.33001771569252014; L(Test): 0.3052307963371277\n",
            "Epoch 2659/10000: L(Train): 0.3306003510951996; L(Test): 0.30519890785217285\n",
            "Epoch 2660/10000: L(Train): 0.3294428288936615; L(Test): 0.305302232503891\n",
            "Epoch 2661/10000: L(Train): 0.3248470723628998; L(Test): 0.30497878789901733\n",
            "Epoch 2662/10000: L(Train): 0.3289887309074402; L(Test): 0.3045453727245331\n",
            "Epoch 2663/10000: L(Train): 0.32400304079055786; L(Test): 0.30471986532211304\n",
            "Epoch 2664/10000: L(Train): 0.3240939676761627; L(Test): 0.3048703968524933\n",
            "Epoch 2665/10000: L(Train): 0.32640793919563293; L(Test): 0.3048940598964691\n",
            "Epoch 2666/10000: L(Train): 0.31515058875083923; L(Test): 0.3042183220386505\n",
            "Epoch 2667/10000: L(Train): 0.3226951062679291; L(Test): 0.30363568663597107\n",
            "Epoch 2668/10000: L(Train): 0.3179425001144409; L(Test): 0.3036769926548004\n",
            "Epoch 2669/10000: L(Train): 0.3308582901954651; L(Test): 0.30458882451057434\n",
            "Epoch 2670/10000: L(Train): 0.3316609263420105; L(Test): 0.30465707182884216\n",
            "Epoch 2671/10000: L(Train): 0.32951879501342773; L(Test): 0.3039724826812744\n",
            "Epoch 2672/10000: L(Train): 0.3231066167354584; L(Test): 0.30515196919441223\n",
            "Epoch 2673/10000: L(Train): 0.3401702642440796; L(Test): 0.3046773672103882\n",
            "Epoch 2674/10000: L(Train): 0.3245762884616852; L(Test): 0.30447274446487427\n",
            "Epoch 2675/10000: L(Train): 0.3323897123336792; L(Test): 0.30440038442611694\n",
            "Epoch 2676/10000: L(Train): 0.3298017978668213; L(Test): 0.30448561906814575\n",
            "Epoch 2677/10000: L(Train): 0.32741883397102356; L(Test): 0.304234117269516\n",
            "Epoch 2678/10000: L(Train): 0.32653456926345825; L(Test): 0.3041389584541321\n",
            "Epoch 2679/10000: L(Train): 0.32906144857406616; L(Test): 0.3052002787590027\n",
            "Epoch 2680/10000: L(Train): 0.32987266778945923; L(Test): 0.30453214049339294\n",
            "Epoch 2681/10000: L(Train): 0.3256743550300598; L(Test): 0.30594515800476074\n",
            "Epoch 2682/10000: L(Train): 0.3235458433628082; L(Test): 0.30612969398498535\n",
            "Epoch 2683/10000: L(Train): 0.3264826238155365; L(Test): 0.30513882637023926\n",
            "Epoch 2684/10000: L(Train): 0.33543479442596436; L(Test): 0.3060552179813385\n",
            "Epoch 2685/10000: L(Train): 0.33213087916374207; L(Test): 0.3055168092250824\n",
            "Epoch 2686/10000: L(Train): 0.32248935103416443; L(Test): 0.3059655725955963\n",
            "Epoch 2687/10000: L(Train): 0.3253653049468994; L(Test): 0.3049207031726837\n",
            "Epoch 2688/10000: L(Train): 0.3313996493816376; L(Test): 0.30403396487236023\n",
            "Epoch 2689/10000: L(Train): 0.3236299753189087; L(Test): 0.3056861162185669\n",
            "Epoch 2690/10000: L(Train): 0.31993624567985535; L(Test): 0.30614399909973145\n",
            "Epoch 2691/10000: L(Train): 0.32507941126823425; L(Test): 0.30547264218330383\n",
            "Epoch 2692/10000: L(Train): 0.3219204246997833; L(Test): 0.30615106225013733\n",
            "Epoch 2693/10000: L(Train): 0.3175535202026367; L(Test): 0.3053188621997833\n",
            "Epoch 2694/10000: L(Train): 0.33769872784614563; L(Test): 0.3046242296695709\n",
            "Epoch 2695/10000: L(Train): 0.33467158675193787; L(Test): 0.30518972873687744\n",
            "Epoch 2696/10000: L(Train): 0.3181988298892975; L(Test): 0.30674853920936584\n",
            "Epoch 2697/10000: L(Train): 0.32801753282546997; L(Test): 0.3066343665122986\n",
            "Epoch 2698/10000: L(Train): 0.33170029520988464; L(Test): 0.30477002263069153\n",
            "Epoch 2699/10000: L(Train): 0.3221265375614166; L(Test): 0.3054487705230713\n",
            "Epoch 2700/10000: L(Train): 0.33342263102531433; L(Test): 0.30650463700294495\n",
            "Epoch 2701/10000: L(Train): 0.32426026463508606; L(Test): 0.306367427110672\n",
            "Epoch 2702/10000: L(Train): 0.33556562662124634; L(Test): 0.3056395649909973\n",
            "Epoch 2703/10000: L(Train): 0.339774489402771; L(Test): 0.30701935291290283\n",
            "Epoch 2704/10000: L(Train): 0.32346922159194946; L(Test): 0.3057398796081543\n",
            "Epoch 2705/10000: L(Train): 0.3269466459751129; L(Test): 0.30418238043785095\n",
            "Epoch 2706/10000: L(Train): 0.3254694640636444; L(Test): 0.30599045753479004\n",
            "Epoch 2707/10000: L(Train): 0.33675065636634827; L(Test): 0.30592817068099976\n",
            "Epoch 2708/10000: L(Train): 0.32049182057380676; L(Test): 0.3054279386997223\n",
            "Epoch 2709/10000: L(Train): 0.33523035049438477; L(Test): 0.30553171038627625\n",
            "Epoch 2710/10000: L(Train): 0.32586321234703064; L(Test): 0.30500367283821106\n",
            "Epoch 2711/10000: L(Train): 0.332141637802124; L(Test): 0.3045410215854645\n",
            "Epoch 2712/10000: L(Train): 0.326959490776062; L(Test): 0.30468153953552246\n",
            "Epoch 2713/10000: L(Train): 0.3200940191745758; L(Test): 0.3052746057510376\n",
            "Epoch 2714/10000: L(Train): 0.33130353689193726; L(Test): 0.304691344499588\n",
            "Epoch 2715/10000: L(Train): 0.3298226594924927; L(Test): 0.3045690953731537\n",
            "Epoch 2716/10000: L(Train): 0.32697793841362; L(Test): 0.3035059869289398\n",
            "Epoch 2717/10000: L(Train): 0.33356714248657227; L(Test): 0.3047870397567749\n",
            "Epoch 2718/10000: L(Train): 0.3242318332195282; L(Test): 0.30563482642173767\n",
            "Epoch 2719/10000: L(Train): 0.33270734548568726; L(Test): 0.3042621910572052\n",
            "Epoch 2720/10000: L(Train): 0.32371971011161804; L(Test): 0.30469417572021484\n",
            "Epoch 2721/10000: L(Train): 0.331466943025589; L(Test): 0.30536192655563354\n",
            "Epoch 2722/10000: L(Train): 0.3246017396450043; L(Test): 0.3038697838783264\n",
            "Epoch 2723/10000: L(Train): 0.32280758023262024; L(Test): 0.3040224313735962\n",
            "Epoch 2724/10000: L(Train): 0.3323061466217041; L(Test): 0.3058253824710846\n",
            "Epoch 2725/10000: L(Train): 0.32981932163238525; L(Test): 0.3046150505542755\n",
            "Epoch 2726/10000: L(Train): 0.3279247581958771; L(Test): 0.3039439618587494\n",
            "Epoch 2727/10000: L(Train): 0.3293316066265106; L(Test): 0.30509060621261597\n",
            "Epoch 2728/10000: L(Train): 0.3287988007068634; L(Test): 0.30545860528945923\n",
            "Epoch 2729/10000: L(Train): 0.32457199692726135; L(Test): 0.30490365624427795\n",
            "Epoch 2730/10000: L(Train): 0.3305700421333313; L(Test): 0.30537712574005127\n",
            "Epoch 2731/10000: L(Train): 0.33075496554374695; L(Test): 0.3050917983055115\n",
            "Epoch 2732/10000: L(Train): 0.3273369073867798; L(Test): 0.304069459438324\n",
            "Epoch 2733/10000: L(Train): 0.33076462149620056; L(Test): 0.30415645241737366\n",
            "Epoch 2734/10000: L(Train): 0.31406840682029724; L(Test): 0.3047543466091156\n",
            "Epoch 2735/10000: L(Train): 0.3282102346420288; L(Test): 0.3043670952320099\n",
            "Epoch 2736/10000: L(Train): 0.32215067744255066; L(Test): 0.30513477325439453\n",
            "Epoch 2737/10000: L(Train): 0.31968796253204346; L(Test): 0.30496639013290405\n",
            "Epoch 2738/10000: L(Train): 0.3313412070274353; L(Test): 0.30327510833740234\n",
            "Epoch 2739/10000: L(Train): 0.32573795318603516; L(Test): 0.3033062517642975\n",
            "Epoch 2740/10000: L(Train): 0.32564446330070496; L(Test): 0.30342015624046326\n",
            "Epoch 2741/10000: L(Train): 0.3284015953540802; L(Test): 0.30372628569602966\n",
            "Epoch 2742/10000: L(Train): 0.3312549293041229; L(Test): 0.30463892221450806\n",
            "Epoch 2743/10000: L(Train): 0.33199343085289; L(Test): 0.3038902282714844\n",
            "Epoch 2744/10000: L(Train): 0.3310888707637787; L(Test): 0.305045485496521\n",
            "Epoch 2745/10000: L(Train): 0.32548367977142334; L(Test): 0.30386659502983093\n",
            "Epoch 2746/10000: L(Train): 0.321223646402359; L(Test): 0.30417710542678833\n",
            "Epoch 2747/10000: L(Train): 0.3286030888557434; L(Test): 0.3046266734600067\n",
            "Epoch 2748/10000: L(Train): 0.3167523443698883; L(Test): 0.3034544587135315\n",
            "Epoch 2749/10000: L(Train): 0.3279936909675598; L(Test): 0.3033922612667084\n",
            "Epoch 2750/10000: L(Train): 0.32702621817588806; L(Test): 0.30411648750305176\n",
            "Epoch 2751/10000: L(Train): 0.3232550024986267; L(Test): 0.30486130714416504\n",
            "Epoch 2752/10000: L(Train): 0.33083298802375793; L(Test): 0.3064727187156677\n",
            "Epoch 2753/10000: L(Train): 0.33180898427963257; L(Test): 0.3039402663707733\n",
            "Epoch 2754/10000: L(Train): 0.32167303562164307; L(Test): 0.30357450246810913\n",
            "Epoch 2755/10000: L(Train): 0.32953980565071106; L(Test): 0.3050670325756073\n",
            "Epoch 2756/10000: L(Train): 0.3416217267513275; L(Test): 0.30436551570892334\n",
            "Epoch 2757/10000: L(Train): 0.3312472403049469; L(Test): 0.30491575598716736\n",
            "Epoch 2758/10000: L(Train): 0.32505232095718384; L(Test): 0.3057366907596588\n",
            "Epoch 2759/10000: L(Train): 0.32034847140312195; L(Test): 0.3041976988315582\n",
            "Epoch 2760/10000: L(Train): 0.32829731702804565; L(Test): 0.3048431873321533\n",
            "Epoch 2761/10000: L(Train): 0.3225245177745819; L(Test): 0.30454954504966736\n",
            "Epoch 2762/10000: L(Train): 0.3289068937301636; L(Test): 0.303800493478775\n",
            "Epoch 2763/10000: L(Train): 0.3246259391307831; L(Test): 0.30436986684799194\n",
            "Epoch 2764/10000: L(Train): 0.32097071409225464; L(Test): 0.3049197793006897\n",
            "Epoch 2765/10000: L(Train): 0.32630038261413574; L(Test): 0.30438944697380066\n",
            "Epoch 2766/10000: L(Train): 0.3218868672847748; L(Test): 0.30494019389152527\n",
            "Epoch 2767/10000: L(Train): 0.3255566358566284; L(Test): 0.30606672167778015\n",
            "Epoch 2768/10000: L(Train): 0.3349134624004364; L(Test): 0.30477774143218994\n",
            "Epoch 2769/10000: L(Train): 0.3291137218475342; L(Test): 0.3033809959888458\n",
            "Epoch 2770/10000: L(Train): 0.3247862756252289; L(Test): 0.30456116795539856\n",
            "Epoch 2771/10000: L(Train): 0.31770122051239014; L(Test): 0.3056260645389557\n",
            "Epoch 2772/10000: L(Train): 0.3259223699569702; L(Test): 0.3052965998649597\n",
            "Epoch 2773/10000: L(Train): 0.3317139148712158; L(Test): 0.3053690195083618\n",
            "Epoch 2774/10000: L(Train): 0.32500675320625305; L(Test): 0.3060300052165985\n",
            "Epoch 2775/10000: L(Train): 0.3239813446998596; L(Test): 0.305258572101593\n",
            "Epoch 2776/10000: L(Train): 0.32560622692108154; L(Test): 0.30418047308921814\n",
            "Epoch 2777/10000: L(Train): 0.3308810293674469; L(Test): 0.3040805459022522\n",
            "Epoch 2778/10000: L(Train): 0.32730185985565186; L(Test): 0.30417537689208984\n",
            "Epoch 2779/10000: L(Train): 0.3180443048477173; L(Test): 0.30358561873435974\n",
            "Epoch 2780/10000: L(Train): 0.31840941309928894; L(Test): 0.3039729595184326\n",
            "Epoch 2781/10000: L(Train): 0.3297506272792816; L(Test): 0.304196834564209\n",
            "Epoch 2782/10000: L(Train): 0.3212304413318634; L(Test): 0.3046845495700836\n",
            "Epoch 2783/10000: L(Train): 0.32531145215034485; L(Test): 0.3046329915523529\n",
            "Epoch 2784/10000: L(Train): 0.33332860469818115; L(Test): 0.3043993413448334\n",
            "Epoch 2785/10000: L(Train): 0.32248595356941223; L(Test): 0.3038512170314789\n",
            "Epoch 2786/10000: L(Train): 0.3194902539253235; L(Test): 0.30310675501823425\n",
            "Epoch 2787/10000: L(Train): 0.33846503496170044; L(Test): 0.30290114879608154\n",
            "Epoch 2788/10000: L(Train): 0.3154177963733673; L(Test): 0.3029966354370117\n",
            "Epoch 2789/10000: L(Train): 0.31570062041282654; L(Test): 0.3039083778858185\n",
            "Epoch 2790/10000: L(Train): 0.3270869553089142; L(Test): 0.30333632230758667\n",
            "Epoch 2791/10000: L(Train): 0.32304084300994873; L(Test): 0.30358800292015076\n",
            "Epoch 2792/10000: L(Train): 0.3301161527633667; L(Test): 0.3034944534301758\n",
            "Epoch 2793/10000: L(Train): 0.32990145683288574; L(Test): 0.30246084928512573\n",
            "Epoch 2794/10000: L(Train): 0.3216908574104309; L(Test): 0.3026212751865387\n",
            "Epoch 2795/10000: L(Train): 0.3317025899887085; L(Test): 0.30362123250961304\n",
            "Epoch 2796/10000: L(Train): 0.3339802324771881; L(Test): 0.30317002534866333\n",
            "Epoch 2797/10000: L(Train): 0.32281696796417236; L(Test): 0.3024975061416626\n",
            "Epoch 2798/10000: L(Train): 0.32759329676628113; L(Test): 0.3030722737312317\n",
            "Epoch 2799/10000: L(Train): 0.3225708603858948; L(Test): 0.3026696443557739\n",
            "Epoch 2800/10000: L(Train): 0.32327133417129517; L(Test): 0.3031098246574402\n",
            "Epoch 2801/10000: L(Train): 0.32564255595207214; L(Test): 0.3029076159000397\n",
            "Epoch 2802/10000: L(Train): 0.316394180059433; L(Test): 0.30254483222961426\n",
            "Epoch 2803/10000: L(Train): 0.3319648504257202; L(Test): 0.3033159375190735\n",
            "Epoch 2804/10000: L(Train): 0.3267829418182373; L(Test): 0.3032075762748718\n",
            "Epoch 2805/10000: L(Train): 0.31485801935195923; L(Test): 0.30325847864151\n",
            "Epoch 2806/10000: L(Train): 0.3291265070438385; L(Test): 0.30346155166625977\n",
            "Epoch 2807/10000: L(Train): 0.3313237428665161; L(Test): 0.30273881554603577\n",
            "Epoch 2808/10000: L(Train): 0.327415406703949; L(Test): 0.30306875705718994\n",
            "Epoch 2809/10000: L(Train): 0.3279103934764862; L(Test): 0.30261746048927307\n",
            "Epoch 2810/10000: L(Train): 0.3168736696243286; L(Test): 0.30297136306762695\n",
            "Epoch 2811/10000: L(Train): 0.33425039052963257; L(Test): 0.3037398159503937\n",
            "Epoch 2812/10000: L(Train): 0.3251741826534271; L(Test): 0.3041130304336548\n",
            "Epoch 2813/10000: L(Train): 0.3321734666824341; L(Test): 0.3053431212902069\n",
            "Epoch 2814/10000: L(Train): 0.32206377387046814; L(Test): 0.3037274479866028\n",
            "Epoch 2815/10000: L(Train): 0.3216547667980194; L(Test): 0.30594712495803833\n",
            "Epoch 2816/10000: L(Train): 0.33085736632347107; L(Test): 0.30417707562446594\n",
            "Epoch 2817/10000: L(Train): 0.33030998706817627; L(Test): 0.30293774604797363\n",
            "Epoch 2818/10000: L(Train): 0.32083195447921753; L(Test): 0.30464106798171997\n",
            "Epoch 2819/10000: L(Train): 0.3286574184894562; L(Test): 0.30424052476882935\n",
            "Epoch 2820/10000: L(Train): 0.325082927942276; L(Test): 0.30431419610977173\n",
            "Epoch 2821/10000: L(Train): 0.3144013583660126; L(Test): 0.30597907304763794\n",
            "Epoch 2822/10000: L(Train): 0.32585006952285767; L(Test): 0.3043062388896942\n",
            "Epoch 2823/10000: L(Train): 0.31369614601135254; L(Test): 0.30390462279319763\n",
            "Epoch 2824/10000: L(Train): 0.321196049451828; L(Test): 0.30342158675193787\n",
            "Epoch 2825/10000: L(Train): 0.327970951795578; L(Test): 0.30298417806625366\n",
            "Epoch 2826/10000: L(Train): 0.32856011390686035; L(Test): 0.30397653579711914\n",
            "Epoch 2827/10000: L(Train): 0.32346436381340027; L(Test): 0.3041296601295471\n",
            "Epoch 2828/10000: L(Train): 0.326375812292099; L(Test): 0.3034346103668213\n",
            "Epoch 2829/10000: L(Train): 0.3329963982105255; L(Test): 0.3036855161190033\n",
            "Epoch 2830/10000: L(Train): 0.330578476190567; L(Test): 0.30380815267562866\n",
            "Epoch 2831/10000: L(Train): 0.3172708749771118; L(Test): 0.3033365309238434\n",
            "Epoch 2832/10000: L(Train): 0.32042935490608215; L(Test): 0.30315670371055603\n",
            "Epoch 2833/10000: L(Train): 0.32930418848991394; L(Test): 0.30198007822036743\n",
            "Epoch 2834/10000: L(Train): 0.31984657049179077; L(Test): 0.30236777663230896\n",
            "Epoch 2835/10000: L(Train): 0.34111079573631287; L(Test): 0.3023172616958618\n",
            "Epoch 2836/10000: L(Train): 0.3235718905925751; L(Test): 0.3016240894794464\n",
            "Epoch 2837/10000: L(Train): 0.32275497913360596; L(Test): 0.30292534828186035\n",
            "Epoch 2838/10000: L(Train): 0.318615585565567; L(Test): 0.30349522829055786\n",
            "Epoch 2839/10000: L(Train): 0.3310821056365967; L(Test): 0.3025199770927429\n",
            "Epoch 2840/10000: L(Train): 0.3248710036277771; L(Test): 0.30274340510368347\n",
            "Epoch 2841/10000: L(Train): 0.3257049024105072; L(Test): 0.3021375834941864\n",
            "Epoch 2842/10000: L(Train): 0.32732266187667847; L(Test): 0.3021923303604126\n",
            "Epoch 2843/10000: L(Train): 0.3219681978225708; L(Test): 0.30343109369277954\n",
            "Epoch 2844/10000: L(Train): 0.3366352617740631; L(Test): 0.3040637671947479\n",
            "Epoch 2845/10000: L(Train): 0.33061283826828003; L(Test): 0.3047962188720703\n",
            "Epoch 2846/10000: L(Train): 0.32858511805534363; L(Test): 0.30436405539512634\n",
            "Epoch 2847/10000: L(Train): 0.3254077136516571; L(Test): 0.304414838552475\n",
            "Epoch 2848/10000: L(Train): 0.3255622982978821; L(Test): 0.30646154284477234\n",
            "Epoch 2849/10000: L(Train): 0.32468122243881226; L(Test): 0.30555036664009094\n",
            "Epoch 2850/10000: L(Train): 0.31980815529823303; L(Test): 0.30473965406417847\n",
            "Epoch 2851/10000: L(Train): 0.33037957549095154; L(Test): 0.30579257011413574\n",
            "Epoch 2852/10000: L(Train): 0.3329024910926819; L(Test): 0.30503615736961365\n",
            "Epoch 2853/10000: L(Train): 0.32485029101371765; L(Test): 0.3045690655708313\n",
            "Epoch 2854/10000: L(Train): 0.3311477303504944; L(Test): 0.3051539361476898\n",
            "Epoch 2855/10000: L(Train): 0.32678917050361633; L(Test): 0.30543819069862366\n",
            "Epoch 2856/10000: L(Train): 0.32572105526924133; L(Test): 0.30475857853889465\n",
            "Epoch 2857/10000: L(Train): 0.3234966993331909; L(Test): 0.30433154106140137\n",
            "Epoch 2858/10000: L(Train): 0.31903275847435; L(Test): 0.30414867401123047\n",
            "Epoch 2859/10000: L(Train): 0.3300451934337616; L(Test): 0.30432963371276855\n",
            "Epoch 2860/10000: L(Train): 0.3323958218097687; L(Test): 0.30604860186576843\n",
            "Epoch 2861/10000: L(Train): 0.32246091961860657; L(Test): 0.3044087886810303\n",
            "Epoch 2862/10000: L(Train): 0.3276423513889313; L(Test): 0.3040405511856079\n",
            "Epoch 2863/10000: L(Train): 0.32571324706077576; L(Test): 0.3045407235622406\n",
            "Epoch 2864/10000: L(Train): 0.3192705810070038; L(Test): 0.30444031953811646\n",
            "Epoch 2865/10000: L(Train): 0.32214999198913574; L(Test): 0.3054594397544861\n",
            "Epoch 2866/10000: L(Train): 0.3305854797363281; L(Test): 0.3049316108226776\n",
            "Epoch 2867/10000: L(Train): 0.33024463057518005; L(Test): 0.30452626943588257\n",
            "Epoch 2868/10000: L(Train): 0.3247664272785187; L(Test): 0.3050251603126526\n",
            "Epoch 2869/10000: L(Train): 0.32222244143486023; L(Test): 0.3041175603866577\n",
            "Epoch 2870/10000: L(Train): 0.325174480676651; L(Test): 0.3035832941532135\n",
            "Epoch 2871/10000: L(Train): 0.3249083459377289; L(Test): 0.3039005398750305\n",
            "Epoch 2872/10000: L(Train): 0.33935829997062683; L(Test): 0.30275341868400574\n",
            "Epoch 2873/10000: L(Train): 0.3257242739200592; L(Test): 0.3025168180465698\n",
            "Epoch 2874/10000: L(Train): 0.32328733801841736; L(Test): 0.3033636808395386\n",
            "Epoch 2875/10000: L(Train): 0.32956981658935547; L(Test): 0.30326417088508606\n",
            "Epoch 2876/10000: L(Train): 0.3303113579750061; L(Test): 0.30325847864151\n",
            "Epoch 2877/10000: L(Train): 0.31879860162734985; L(Test): 0.30369436740875244\n",
            "Epoch 2878/10000: L(Train): 0.326313316822052; L(Test): 0.3046529293060303\n",
            "Epoch 2879/10000: L(Train): 0.32875508069992065; L(Test): 0.3049711287021637\n",
            "Epoch 2880/10000: L(Train): 0.32808321714401245; L(Test): 0.3040931820869446\n",
            "Epoch 2881/10000: L(Train): 0.33140048384666443; L(Test): 0.30344870686531067\n",
            "Epoch 2882/10000: L(Train): 0.3306235074996948; L(Test): 0.3037646412849426\n",
            "Epoch 2883/10000: L(Train): 0.32369714975357056; L(Test): 0.3032192289829254\n",
            "Epoch 2884/10000: L(Train): 0.3201442360877991; L(Test): 0.3046082854270935\n",
            "Epoch 2885/10000: L(Train): 0.3259776532649994; L(Test): 0.304231733083725\n",
            "Epoch 2886/10000: L(Train): 0.32077690958976746; L(Test): 0.3030543029308319\n",
            "Epoch 2887/10000: L(Train): 0.32606714963912964; L(Test): 0.3038839101791382\n",
            "Epoch 2888/10000: L(Train): 0.32196328043937683; L(Test): 0.3048709034919739\n",
            "Epoch 2889/10000: L(Train): 0.32270386815071106; L(Test): 0.30482327938079834\n",
            "Epoch 2890/10000: L(Train): 0.33308929204940796; L(Test): 0.3050956428050995\n",
            "Epoch 2891/10000: L(Train): 0.33406612277030945; L(Test): 0.3053293526172638\n",
            "Epoch 2892/10000: L(Train): 0.3225698173046112; L(Test): 0.30467653274536133\n",
            "Epoch 2893/10000: L(Train): 0.3235571086406708; L(Test): 0.30366867780685425\n",
            "Epoch 2894/10000: L(Train): 0.3164604604244232; L(Test): 0.3038521111011505\n",
            "Epoch 2895/10000: L(Train): 0.3339668810367584; L(Test): 0.30489999055862427\n",
            "Epoch 2896/10000: L(Train): 0.3269205391407013; L(Test): 0.3062458038330078\n",
            "Epoch 2897/10000: L(Train): 0.33344537019729614; L(Test): 0.30705392360687256\n",
            "Epoch 2898/10000: L(Train): 0.33400192856788635; L(Test): 0.3059065341949463\n",
            "Epoch 2899/10000: L(Train): 0.32406193017959595; L(Test): 0.30518290400505066\n",
            "Epoch 2900/10000: L(Train): 0.32691362500190735; L(Test): 0.30608317255973816\n",
            "Epoch 2901/10000: L(Train): 0.32630613446235657; L(Test): 0.30654942989349365\n",
            "Epoch 2902/10000: L(Train): 0.318021297454834; L(Test): 0.30621984601020813\n",
            "Epoch 2903/10000: L(Train): 0.33652982115745544; L(Test): 0.3055609464645386\n",
            "Epoch 2904/10000: L(Train): 0.32714709639549255; L(Test): 0.30493876338005066\n",
            "Epoch 2905/10000: L(Train): 0.3279986083507538; L(Test): 0.3049335479736328\n",
            "Epoch 2906/10000: L(Train): 0.32074791193008423; L(Test): 0.305824339389801\n",
            "Epoch 2907/10000: L(Train): 0.3247232735157013; L(Test): 0.3063541352748871\n",
            "Epoch 2908/10000: L(Train): 0.3310742974281311; L(Test): 0.30557113885879517\n",
            "Epoch 2909/10000: L(Train): 0.3318750262260437; L(Test): 0.30399298667907715\n",
            "Epoch 2910/10000: L(Train): 0.31430116295814514; L(Test): 0.3034047484397888\n",
            "Epoch 2911/10000: L(Train): 0.3169882297515869; L(Test): 0.30409881472587585\n",
            "Epoch 2912/10000: L(Train): 0.32227078080177307; L(Test): 0.30432477593421936\n",
            "Epoch 2913/10000: L(Train): 0.334579735994339; L(Test): 0.3039999306201935\n",
            "Epoch 2914/10000: L(Train): 0.322469562292099; L(Test): 0.30365291237831116\n",
            "Epoch 2915/10000: L(Train): 0.3252502381801605; L(Test): 0.30427780747413635\n",
            "Epoch 2916/10000: L(Train): 0.3224860429763794; L(Test): 0.30392929911613464\n",
            "Epoch 2917/10000: L(Train): 0.3261040449142456; L(Test): 0.3031933605670929\n",
            "Epoch 2918/10000: L(Train): 0.32795628905296326; L(Test): 0.30352064967155457\n",
            "Epoch 2919/10000: L(Train): 0.3228479027748108; L(Test): 0.3034554719924927\n",
            "Epoch 2920/10000: L(Train): 0.32703739404678345; L(Test): 0.30334553122520447\n",
            "Epoch 2921/10000: L(Train): 0.33338621258735657; L(Test): 0.3039010465145111\n",
            "Epoch 2922/10000: L(Train): 0.32870054244995117; L(Test): 0.3031979501247406\n",
            "Epoch 2923/10000: L(Train): 0.3132665753364563; L(Test): 0.3028310239315033\n",
            "Epoch 2924/10000: L(Train): 0.32342779636383057; L(Test): 0.3029439151287079\n",
            "Epoch 2925/10000: L(Train): 0.32252827286720276; L(Test): 0.3023322522640228\n",
            "Epoch 2926/10000: L(Train): 0.32195740938186646; L(Test): 0.30285996198654175\n",
            "Epoch 2927/10000: L(Train): 0.32612529397010803; L(Test): 0.30371466279029846\n",
            "Epoch 2928/10000: L(Train): 0.32569265365600586; L(Test): 0.303079754114151\n",
            "Epoch 2929/10000: L(Train): 0.32854321599006653; L(Test): 0.3029783368110657\n",
            "Epoch 2930/10000: L(Train): 0.32277920842170715; L(Test): 0.30266931653022766\n",
            "Epoch 2931/10000: L(Train): 0.3228060007095337; L(Test): 0.30236315727233887\n",
            "Epoch 2932/10000: L(Train): 0.3258165419101715; L(Test): 0.30332624912261963\n",
            "Epoch 2933/10000: L(Train): 0.33100026845932007; L(Test): 0.303326278924942\n",
            "Epoch 2934/10000: L(Train): 0.3291163444519043; L(Test): 0.30529680848121643\n",
            "Epoch 2935/10000: L(Train): 0.3289394676685333; L(Test): 0.30811917781829834\n",
            "Epoch 2936/10000: L(Train): 0.3371194899082184; L(Test): 0.30632340908050537\n",
            "Epoch 2937/10000: L(Train): 0.32866957783699036; L(Test): 0.3067667782306671\n",
            "Epoch 2938/10000: L(Train): 0.3386746942996979; L(Test): 0.30657875537872314\n",
            "Epoch 2939/10000: L(Train): 0.3311798572540283; L(Test): 0.306486040353775\n",
            "Epoch 2940/10000: L(Train): 0.3356282413005829; L(Test): 0.30773767828941345\n",
            "Epoch 2941/10000: L(Train): 0.3387436866760254; L(Test): 0.30736514925956726\n",
            "Epoch 2942/10000: L(Train): 0.33221668004989624; L(Test): 0.30695366859436035\n",
            "Epoch 2943/10000: L(Train): 0.3279746472835541; L(Test): 0.3080049753189087\n",
            "Epoch 2944/10000: L(Train): 0.3335464596748352; L(Test): 0.3086778223514557\n",
            "Epoch 2945/10000: L(Train): 0.3265373110771179; L(Test): 0.3070535361766815\n",
            "Epoch 2946/10000: L(Train): 0.31446561217308044; L(Test): 0.30635562539100647\n",
            "Epoch 2947/10000: L(Train): 0.3368912637233734; L(Test): 0.3060586750507355\n",
            "Epoch 2948/10000: L(Train): 0.3338013291358948; L(Test): 0.30600693821907043\n",
            "Epoch 2949/10000: L(Train): 0.3322302997112274; L(Test): 0.3062615692615509\n",
            "Epoch 2950/10000: L(Train): 0.3336809575557709; L(Test): 0.30562517046928406\n",
            "Epoch 2951/10000: L(Train): 0.33350682258605957; L(Test): 0.3048967719078064\n",
            "Epoch 2952/10000: L(Train): 0.32971999049186707; L(Test): 0.30563417077064514\n",
            "Epoch 2953/10000: L(Train): 0.33658647537231445; L(Test): 0.30538055300712585\n",
            "Epoch 2954/10000: L(Train): 0.3250891864299774; L(Test): 0.30434364080429077\n",
            "Epoch 2955/10000: L(Train): 0.3215627074241638; L(Test): 0.3043346405029297\n",
            "Epoch 2956/10000: L(Train): 0.32375919818878174; L(Test): 0.30486634373664856\n",
            "Epoch 2957/10000: L(Train): 0.32805681228637695; L(Test): 0.30446726083755493\n",
            "Epoch 2958/10000: L(Train): 0.3283548951148987; L(Test): 0.30400216579437256\n",
            "Epoch 2959/10000: L(Train): 0.32855287194252014; L(Test): 0.3039534389972687\n",
            "Epoch 2960/10000: L(Train): 0.32383087277412415; L(Test): 0.30406397581100464\n",
            "Epoch 2961/10000: L(Train): 0.3187224268913269; L(Test): 0.3045436441898346\n",
            "Epoch 2962/10000: L(Train): 0.33257707953453064; L(Test): 0.30358895659446716\n",
            "Epoch 2963/10000: L(Train): 0.32944387197494507; L(Test): 0.3027668297290802\n",
            "Epoch 2964/10000: L(Train): 0.331800639629364; L(Test): 0.30396878719329834\n",
            "Epoch 2965/10000: L(Train): 0.3313891291618347; L(Test): 0.3036065101623535\n",
            "Epoch 2966/10000: L(Train): 0.32699355483055115; L(Test): 0.3025432527065277\n",
            "Epoch 2967/10000: L(Train): 0.32286617159843445; L(Test): 0.30476242303848267\n",
            "Epoch 2968/10000: L(Train): 0.3205735385417938; L(Test): 0.3039047420024872\n",
            "Epoch 2969/10000: L(Train): 0.3215065896511078; L(Test): 0.3034786283969879\n",
            "Epoch 2970/10000: L(Train): 0.33212488889694214; L(Test): 0.3048585057258606\n",
            "Epoch 2971/10000: L(Train): 0.3186611235141754; L(Test): 0.3040704131126404\n",
            "Epoch 2972/10000: L(Train): 0.3252764940261841; L(Test): 0.30242466926574707\n",
            "Epoch 2973/10000: L(Train): 0.31890588998794556; L(Test): 0.3036414384841919\n",
            "Epoch 2974/10000: L(Train): 0.3309824466705322; L(Test): 0.30431067943573\n",
            "Epoch 2975/10000: L(Train): 0.31457942724227905; L(Test): 0.30328696966171265\n",
            "Epoch 2976/10000: L(Train): 0.3207618296146393; L(Test): 0.3046905994415283\n",
            "Epoch 2977/10000: L(Train): 0.3253374993801117; L(Test): 0.3044882118701935\n",
            "Epoch 2978/10000: L(Train): 0.3287348747253418; L(Test): 0.30404582619667053\n",
            "Epoch 2979/10000: L(Train): 0.32232213020324707; L(Test): 0.30551764369010925\n",
            "Epoch 2980/10000: L(Train): 0.33038774132728577; L(Test): 0.3037506341934204\n",
            "Epoch 2981/10000: L(Train): 0.3259662091732025; L(Test): 0.3031063973903656\n",
            "Epoch 2982/10000: L(Train): 0.3232223391532898; L(Test): 0.30348554253578186\n",
            "Epoch 2983/10000: L(Train): 0.3230220675468445; L(Test): 0.30339714884757996\n",
            "Epoch 2984/10000: L(Train): 0.32232221961021423; L(Test): 0.3026390075683594\n",
            "Epoch 2985/10000: L(Train): 0.32892096042633057; L(Test): 0.3032216727733612\n",
            "Epoch 2986/10000: L(Train): 0.3295871913433075; L(Test): 0.3034648299217224\n",
            "Epoch 2987/10000: L(Train): 0.3210265040397644; L(Test): 0.30316880345344543\n",
            "Epoch 2988/10000: L(Train): 0.3248933255672455; L(Test): 0.30345991253852844\n",
            "Epoch 2989/10000: L(Train): 0.3323265612125397; L(Test): 0.30321744084358215\n",
            "Epoch 2990/10000: L(Train): 0.31669941544532776; L(Test): 0.3030396103858948\n",
            "Epoch 2991/10000: L(Train): 0.321660578250885; L(Test): 0.30298736691474915\n",
            "Epoch 2992/10000: L(Train): 0.3322303295135498; L(Test): 0.3029957115650177\n",
            "Epoch 2993/10000: L(Train): 0.3241403102874756; L(Test): 0.30356112122535706\n",
            "Epoch 2994/10000: L(Train): 0.3316492736339569; L(Test): 0.3027004301548004\n",
            "Epoch 2995/10000: L(Train): 0.324759840965271; L(Test): 0.30237624049186707\n",
            "Epoch 2996/10000: L(Train): 0.32279300689697266; L(Test): 0.302377849817276\n",
            "Epoch 2997/10000: L(Train): 0.3229919373989105; L(Test): 0.30260491371154785\n",
            "Epoch 2998/10000: L(Train): 0.3215970993041992; L(Test): 0.303377240896225\n",
            "Epoch 2999/10000: L(Train): 0.32892340421676636; L(Test): 0.30421188473701477\n",
            "Epoch 3000/10000: L(Train): 0.33894082903862; L(Test): 0.30443522334098816\n",
            "Epoch 3001/10000: L(Train): 0.3222621977329254; L(Test): 0.3039514124393463\n",
            "Epoch 3002/10000: L(Train): 0.31541702151298523; L(Test): 0.3037622570991516\n",
            "Epoch 3003/10000: L(Train): 0.3156922459602356; L(Test): 0.3045152723789215\n",
            "Epoch 3004/10000: L(Train): 0.3257349729537964; L(Test): 0.30520135164260864\n",
            "Epoch 3005/10000: L(Train): 0.3286900222301483; L(Test): 0.30385488271713257\n",
            "Epoch 3006/10000: L(Train): 0.3253275156021118; L(Test): 0.302789568901062\n",
            "Epoch 3007/10000: L(Train): 0.32268160581588745; L(Test): 0.3032676577568054\n",
            "Epoch 3008/10000: L(Train): 0.31966832280158997; L(Test): 0.3042311668395996\n",
            "Epoch 3009/10000: L(Train): 0.3280668258666992; L(Test): 0.3048873543739319\n",
            "Epoch 3010/10000: L(Train): 0.3340916335582733; L(Test): 0.3046110272407532\n",
            "Epoch 3011/10000: L(Train): 0.3196220099925995; L(Test): 0.3037743866443634\n",
            "Epoch 3012/10000: L(Train): 0.3250740170478821; L(Test): 0.30373501777648926\n",
            "Epoch 3013/10000: L(Train): 0.32828396558761597; L(Test): 0.30424216389656067\n",
            "Epoch 3014/10000: L(Train): 0.32292696833610535; L(Test): 0.3037235736846924\n",
            "Epoch 3015/10000: L(Train): 0.3268616795539856; L(Test): 0.30282148718833923\n",
            "Epoch 3016/10000: L(Train): 0.32679060101509094; L(Test): 0.3035130202770233\n",
            "Epoch 3017/10000: L(Train): 0.3376828730106354; L(Test): 0.3034251928329468\n",
            "Epoch 3018/10000: L(Train): 0.32753652334213257; L(Test): 0.30347904562950134\n",
            "Epoch 3019/10000: L(Train): 0.33238503336906433; L(Test): 0.3040774464607239\n",
            "Epoch 3020/10000: L(Train): 0.33174991607666016; L(Test): 0.30514517426490784\n",
            "Epoch 3021/10000: L(Train): 0.3221995234489441; L(Test): 0.3041326403617859\n",
            "Epoch 3022/10000: L(Train): 0.31939056515693665; L(Test): 0.3035275936126709\n",
            "Epoch 3023/10000: L(Train): 0.32907041907310486; L(Test): 0.3037432134151459\n",
            "Epoch 3024/10000: L(Train): 0.3242194652557373; L(Test): 0.3038310408592224\n",
            "Epoch 3025/10000: L(Train): 0.3250834345817566; L(Test): 0.3040260672569275\n",
            "Epoch 3026/10000: L(Train): 0.31943655014038086; L(Test): 0.3038908839225769\n",
            "Epoch 3027/10000: L(Train): 0.33019885420799255; L(Test): 0.30210962891578674\n",
            "Epoch 3028/10000: L(Train): 0.3268551230430603; L(Test): 0.30210891366004944\n",
            "Epoch 3029/10000: L(Train): 0.3188591003417969; L(Test): 0.3026621341705322\n",
            "Epoch 3030/10000: L(Train): 0.3274780809879303; L(Test): 0.3023591935634613\n",
            "Epoch 3031/10000: L(Train): 0.3281066417694092; L(Test): 0.30207177996635437\n",
            "Epoch 3032/10000: L(Train): 0.3200661540031433; L(Test): 0.302592933177948\n",
            "Epoch 3033/10000: L(Train): 0.32700902223587036; L(Test): 0.3020944893360138\n",
            "Epoch 3034/10000: L(Train): 0.33038100600242615; L(Test): 0.30186352133750916\n",
            "Epoch 3035/10000: L(Train): 0.3203986883163452; L(Test): 0.30246731638908386\n",
            "Epoch 3036/10000: L(Train): 0.32748228311538696; L(Test): 0.30204087495803833\n",
            "Epoch 3037/10000: L(Train): 0.31927689909935; L(Test): 0.3014397621154785\n",
            "Epoch 3038/10000: L(Train): 0.3303855359554291; L(Test): 0.3015219271183014\n",
            "Epoch 3039/10000: L(Train): 0.32418400049209595; L(Test): 0.3016206920146942\n",
            "Epoch 3040/10000: L(Train): 0.3187141716480255; L(Test): 0.30251121520996094\n",
            "Epoch 3041/10000: L(Train): 0.33268851041793823; L(Test): 0.3009646534919739\n",
            "Epoch 3042/10000: L(Train): 0.3140218257904053; L(Test): 0.3006735146045685\n",
            "Epoch 3043/10000: L(Train): 0.3289778530597687; L(Test): 0.3016219735145569\n",
            "Epoch 3044/10000: L(Train): 0.3210757374763489; L(Test): 0.3019672930240631\n",
            "Epoch 3045/10000: L(Train): 0.3313907980918884; L(Test): 0.3024992346763611\n",
            "Epoch 3046/10000: L(Train): 0.3322603702545166; L(Test): 0.30206143856048584\n",
            "Epoch 3047/10000: L(Train): 0.3137770891189575; L(Test): 0.3021741211414337\n",
            "Epoch 3048/10000: L(Train): 0.3256469964981079; L(Test): 0.30205202102661133\n",
            "Epoch 3049/10000: L(Train): 0.3266940414905548; L(Test): 0.30244317650794983\n",
            "Epoch 3050/10000: L(Train): 0.3303668797016144; L(Test): 0.302188515663147\n",
            "Epoch 3051/10000: L(Train): 0.31498613953590393; L(Test): 0.30223897099494934\n",
            "Epoch 3052/10000: L(Train): 0.3233695924282074; L(Test): 0.3024509847164154\n",
            "Epoch 3053/10000: L(Train): 0.3236752450466156; L(Test): 0.3007931113243103\n",
            "Epoch 3054/10000: L(Train): 0.3277609348297119; L(Test): 0.30090785026550293\n",
            "Epoch 3055/10000: L(Train): 0.3245476186275482; L(Test): 0.30198532342910767\n",
            "Epoch 3056/10000: L(Train): 0.32940536737442017; L(Test): 0.30315926671028137\n",
            "Epoch 3057/10000: L(Train): 0.3284372389316559; L(Test): 0.30329418182373047\n",
            "Epoch 3058/10000: L(Train): 0.3254547417163849; L(Test): 0.3021155893802643\n",
            "Epoch 3059/10000: L(Train): 0.3266640901565552; L(Test): 0.3015833795070648\n",
            "Epoch 3060/10000: L(Train): 0.313078373670578; L(Test): 0.3020552098751068\n",
            "Epoch 3061/10000: L(Train): 0.3234463036060333; L(Test): 0.3037734627723694\n",
            "Epoch 3062/10000: L(Train): 0.33586084842681885; L(Test): 0.3047087490558624\n",
            "Epoch 3063/10000: L(Train): 0.3187963664531708; L(Test): 0.305848628282547\n",
            "Epoch 3064/10000: L(Train): 0.33337363600730896; L(Test): 0.3059612810611725\n",
            "Epoch 3065/10000: L(Train): 0.32437601685523987; L(Test): 0.3069743514060974\n",
            "Epoch 3066/10000: L(Train): 0.3279626965522766; L(Test): 0.30689671635627747\n",
            "Epoch 3067/10000: L(Train): 0.3266414403915405; L(Test): 0.3072274625301361\n",
            "Epoch 3068/10000: L(Train): 0.32279282808303833; L(Test): 0.30796870589256287\n",
            "Epoch 3069/10000: L(Train): 0.32873067259788513; L(Test): 0.3063337504863739\n",
            "Epoch 3070/10000: L(Train): 0.3260800838470459; L(Test): 0.3066003918647766\n",
            "Epoch 3071/10000: L(Train): 0.3263494074344635; L(Test): 0.30703112483024597\n",
            "Epoch 3072/10000: L(Train): 0.3316437304019928; L(Test): 0.30653029680252075\n",
            "Epoch 3073/10000: L(Train): 0.33485928177833557; L(Test): 0.3078608214855194\n",
            "Epoch 3074/10000: L(Train): 0.32444143295288086; L(Test): 0.3098175525665283\n",
            "Epoch 3075/10000: L(Train): 0.3350364565849304; L(Test): 0.3065211772918701\n",
            "Epoch 3076/10000: L(Train): 0.3238788843154907; L(Test): 0.30640465021133423\n",
            "Epoch 3077/10000: L(Train): 0.3198279142379761; L(Test): 0.3068769872188568\n",
            "Epoch 3078/10000: L(Train): 0.3271067142486572; L(Test): 0.30595827102661133\n",
            "Epoch 3079/10000: L(Train): 0.3241763710975647; L(Test): 0.3060961067676544\n",
            "Epoch 3080/10000: L(Train): 0.32270175218582153; L(Test): 0.3079183101654053\n",
            "Epoch 3081/10000: L(Train): 0.32361528277397156; L(Test): 0.3073757588863373\n",
            "Epoch 3082/10000: L(Train): 0.3327901363372803; L(Test): 0.30505701899528503\n",
            "Epoch 3083/10000: L(Train): 0.32573455572128296; L(Test): 0.3057834804058075\n",
            "Epoch 3084/10000: L(Train): 0.3264806866645813; L(Test): 0.3058975338935852\n",
            "Epoch 3085/10000: L(Train): 0.33082443475723267; L(Test): 0.30543169379234314\n",
            "Epoch 3086/10000: L(Train): 0.3331775367259979; L(Test): 0.306485116481781\n",
            "Epoch 3087/10000: L(Train): 0.3243713080883026; L(Test): 0.30657508969306946\n",
            "Epoch 3088/10000: L(Train): 0.32017284631729126; L(Test): 0.3052755892276764\n",
            "Epoch 3089/10000: L(Train): 0.32973065972328186; L(Test): 0.3050262928009033\n",
            "Epoch 3090/10000: L(Train): 0.32512232661247253; L(Test): 0.30584779381752014\n",
            "Epoch 3091/10000: L(Train): 0.3173891305923462; L(Test): 0.3060682415962219\n",
            "Epoch 3092/10000: L(Train): 0.3294536769390106; L(Test): 0.304290771484375\n",
            "Epoch 3093/10000: L(Train): 0.3275834918022156; L(Test): 0.3052935302257538\n",
            "Epoch 3094/10000: L(Train): 0.32450205087661743; L(Test): 0.30517011880874634\n",
            "Epoch 3095/10000: L(Train): 0.32121285796165466; L(Test): 0.3052347004413605\n",
            "Epoch 3096/10000: L(Train): 0.3364425003528595; L(Test): 0.3055289387702942\n",
            "Epoch 3097/10000: L(Train): 0.32699158787727356; L(Test): 0.3053496181964874\n",
            "Epoch 3098/10000: L(Train): 0.32814332842826843; L(Test): 0.30513572692871094\n",
            "Epoch 3099/10000: L(Train): 0.3255600035190582; L(Test): 0.3051876127719879\n",
            "Epoch 3100/10000: L(Train): 0.3273748755455017; L(Test): 0.3049074411392212\n",
            "Epoch 3101/10000: L(Train): 0.32931193709373474; L(Test): 0.3054567575454712\n",
            "Epoch 3102/10000: L(Train): 0.326704204082489; L(Test): 0.30533304810523987\n",
            "Epoch 3103/10000: L(Train): 0.32381513714790344; L(Test): 0.30372002720832825\n",
            "Epoch 3104/10000: L(Train): 0.32916298508644104; L(Test): 0.30364280939102173\n",
            "Epoch 3105/10000: L(Train): 0.33127328753471375; L(Test): 0.30320993065834045\n",
            "Epoch 3106/10000: L(Train): 0.3226996958255768; L(Test): 0.30384930968284607\n",
            "Epoch 3107/10000: L(Train): 0.33745646476745605; L(Test): 0.3041974604129791\n",
            "Epoch 3108/10000: L(Train): 0.33084985613822937; L(Test): 0.30372002720832825\n",
            "Epoch 3109/10000: L(Train): 0.3199306130409241; L(Test): 0.30353885889053345\n",
            "Epoch 3110/10000: L(Train): 0.32369309663772583; L(Test): 0.3029438555240631\n",
            "Epoch 3111/10000: L(Train): 0.3256417214870453; L(Test): 0.30301618576049805\n",
            "Epoch 3112/10000: L(Train): 0.3274661600589752; L(Test): 0.30350133776664734\n",
            "Epoch 3113/10000: L(Train): 0.3263195753097534; L(Test): 0.3028016686439514\n",
            "Epoch 3114/10000: L(Train): 0.3252851366996765; L(Test): 0.3021051585674286\n",
            "Epoch 3115/10000: L(Train): 0.31928882002830505; L(Test): 0.30219516158103943\n",
            "Epoch 3116/10000: L(Train): 0.33178284764289856; L(Test): 0.30173030495643616\n",
            "Epoch 3117/10000: L(Train): 0.3144918382167816; L(Test): 0.3023240566253662\n",
            "Epoch 3118/10000: L(Train): 0.32343360781669617; L(Test): 0.30241870880126953\n",
            "Epoch 3119/10000: L(Train): 0.3249984681606293; L(Test): 0.30208733677864075\n",
            "Epoch 3120/10000: L(Train): 0.3173519968986511; L(Test): 0.30200493335723877\n",
            "Epoch 3121/10000: L(Train): 0.32182684540748596; L(Test): 0.3022170662879944\n",
            "Epoch 3122/10000: L(Train): 0.32531288266181946; L(Test): 0.3020195960998535\n",
            "Epoch 3123/10000: L(Train): 0.3164004981517792; L(Test): 0.3031274676322937\n",
            "Epoch 3124/10000: L(Train): 0.323061466217041; L(Test): 0.3035416007041931\n",
            "Epoch 3125/10000: L(Train): 0.3248036801815033; L(Test): 0.3028682768344879\n",
            "Epoch 3126/10000: L(Train): 0.3233887255191803; L(Test): 0.30347758531570435\n",
            "Epoch 3127/10000: L(Train): 0.32576024532318115; L(Test): 0.30086296796798706\n",
            "Epoch 3128/10000: L(Train): 0.31693145632743835; L(Test): 0.30178919434547424\n",
            "Epoch 3129/10000: L(Train): 0.3246513307094574; L(Test): 0.30218198895454407\n",
            "Epoch 3130/10000: L(Train): 0.33354997634887695; L(Test): 0.30186793208122253\n",
            "Epoch 3131/10000: L(Train): 0.32785725593566895; L(Test): 0.30163899064064026\n",
            "Epoch 3132/10000: L(Train): 0.32415571808815; L(Test): 0.3026159107685089\n",
            "Epoch 3133/10000: L(Train): 0.32201096415519714; L(Test): 0.3027229905128479\n",
            "Epoch 3134/10000: L(Train): 0.31799161434173584; L(Test): 0.30223193764686584\n",
            "Epoch 3135/10000: L(Train): 0.3338332176208496; L(Test): 0.30096715688705444\n",
            "Epoch 3136/10000: L(Train): 0.32911816239356995; L(Test): 0.3011360764503479\n",
            "Epoch 3137/10000: L(Train): 0.3196274936199188; L(Test): 0.30187147855758667\n",
            "Epoch 3138/10000: L(Train): 0.3300023078918457; L(Test): 0.3022301495075226\n",
            "Epoch 3139/10000: L(Train): 0.32152417302131653; L(Test): 0.30247965455055237\n",
            "Epoch 3140/10000: L(Train): 0.32997462153434753; L(Test): 0.30325570702552795\n",
            "Epoch 3141/10000: L(Train): 0.3210445046424866; L(Test): 0.30234989523887634\n",
            "Epoch 3142/10000: L(Train): 0.3176726698875427; L(Test): 0.30154916644096375\n",
            "Epoch 3143/10000: L(Train): 0.31872662901878357; L(Test): 0.30160078406333923\n",
            "Epoch 3144/10000: L(Train): 0.32399189472198486; L(Test): 0.3012860417366028\n",
            "Epoch 3145/10000: L(Train): 0.3257260024547577; L(Test): 0.30068105459213257\n",
            "Epoch 3146/10000: L(Train): 0.3154277801513672; L(Test): 0.30153825879096985\n",
            "Epoch 3147/10000: L(Train): 0.3248060643672943; L(Test): 0.30195125937461853\n",
            "Epoch 3148/10000: L(Train): 0.31842878460884094; L(Test): 0.3013651967048645\n",
            "Epoch 3149/10000: L(Train): 0.3284514844417572; L(Test): 0.30102095007896423\n",
            "Epoch 3150/10000: L(Train): 0.32792261242866516; L(Test): 0.3008354604244232\n",
            "Epoch 3151/10000: L(Train): 0.32937857508659363; L(Test): 0.30105164647102356\n",
            "Epoch 3152/10000: L(Train): 0.33039161562919617; L(Test): 0.3013244867324829\n",
            "Epoch 3153/10000: L(Train): 0.32169705629348755; L(Test): 0.30145686864852905\n",
            "Epoch 3154/10000: L(Train): 0.3154010474681854; L(Test): 0.30143412947654724\n",
            "Epoch 3155/10000: L(Train): 0.3319493532180786; L(Test): 0.3008922338485718\n",
            "Epoch 3156/10000: L(Train): 0.32872241735458374; L(Test): 0.30104920268058777\n",
            "Epoch 3157/10000: L(Train): 0.3284326195716858; L(Test): 0.3005313575267792\n",
            "Epoch 3158/10000: L(Train): 0.32277488708496094; L(Test): 0.3009470999240875\n",
            "Epoch 3159/10000: L(Train): 0.3228321969509125; L(Test): 0.3020997941493988\n",
            "Epoch 3160/10000: L(Train): 0.33045679330825806; L(Test): 0.30102428793907166\n",
            "Epoch 3161/10000: L(Train): 0.3205873966217041; L(Test): 0.30103638768196106\n",
            "Epoch 3162/10000: L(Train): 0.32476112246513367; L(Test): 0.30052876472473145\n",
            "Epoch 3163/10000: L(Train): 0.33541569113731384; L(Test): 0.3009242117404938\n",
            "Epoch 3164/10000: L(Train): 0.3218357563018799; L(Test): 0.30258414149284363\n",
            "Epoch 3165/10000: L(Train): 0.3215598165988922; L(Test): 0.3032740354537964\n",
            "Epoch 3166/10000: L(Train): 0.32794108986854553; L(Test): 0.30229732394218445\n",
            "Epoch 3167/10000: L(Train): 0.32623985409736633; L(Test): 0.30229097604751587\n",
            "Epoch 3168/10000: L(Train): 0.32932114601135254; L(Test): 0.3027195930480957\n",
            "Epoch 3169/10000: L(Train): 0.33247241377830505; L(Test): 0.30191171169281006\n",
            "Epoch 3170/10000: L(Train): 0.3248692750930786; L(Test): 0.3021480441093445\n",
            "Epoch 3171/10000: L(Train): 0.3163899779319763; L(Test): 0.3022300601005554\n",
            "Epoch 3172/10000: L(Train): 0.32612183690071106; L(Test): 0.30205899477005005\n",
            "Epoch 3173/10000: L(Train): 0.3395872116088867; L(Test): 0.30196413397789\n",
            "Epoch 3174/10000: L(Train): 0.3299700617790222; L(Test): 0.3023953139781952\n",
            "Epoch 3175/10000: L(Train): 0.31820204854011536; L(Test): 0.30290311574935913\n",
            "Epoch 3176/10000: L(Train): 0.32690757513046265; L(Test): 0.3046100437641144\n",
            "Epoch 3177/10000: L(Train): 0.31765216588974; L(Test): 0.3025650978088379\n",
            "Epoch 3178/10000: L(Train): 0.31241023540496826; L(Test): 0.30250126123428345\n",
            "Epoch 3179/10000: L(Train): 0.31189095973968506; L(Test): 0.3026280105113983\n",
            "Epoch 3180/10000: L(Train): 0.33463430404663086; L(Test): 0.3026883900165558\n",
            "Epoch 3181/10000: L(Train): 0.3160615861415863; L(Test): 0.30394068360328674\n",
            "Epoch 3182/10000: L(Train): 0.31741029024124146; L(Test): 0.3044319450855255\n",
            "Epoch 3183/10000: L(Train): 0.3299408257007599; L(Test): 0.3027789294719696\n",
            "Epoch 3184/10000: L(Train): 0.33887815475463867; L(Test): 0.30331510305404663\n",
            "Epoch 3185/10000: L(Train): 0.31976449489593506; L(Test): 0.30275261402130127\n",
            "Epoch 3186/10000: L(Train): 0.3246096968650818; L(Test): 0.30354195833206177\n",
            "Epoch 3187/10000: L(Train): 0.32969388365745544; L(Test): 0.30349332094192505\n",
            "Epoch 3188/10000: L(Train): 0.3280404508113861; L(Test): 0.3020522892475128\n",
            "Epoch 3189/10000: L(Train): 0.3225264549255371; L(Test): 0.3022528290748596\n",
            "Epoch 3190/10000: L(Train): 0.3214972913265228; L(Test): 0.30319151282310486\n",
            "Epoch 3191/10000: L(Train): 0.33666151762008667; L(Test): 0.3033754229545593\n",
            "Epoch 3192/10000: L(Train): 0.33073481917381287; L(Test): 0.30327731370925903\n",
            "Epoch 3193/10000: L(Train): 0.3263753354549408; L(Test): 0.3025456964969635\n",
            "Epoch 3194/10000: L(Train): 0.3346658945083618; L(Test): 0.3026224672794342\n",
            "Epoch 3195/10000: L(Train): 0.3252739906311035; L(Test): 0.3023279309272766\n",
            "Epoch 3196/10000: L(Train): 0.314256489276886; L(Test): 0.3018321692943573\n",
            "Epoch 3197/10000: L(Train): 0.32830101251602173; L(Test): 0.3018038272857666\n",
            "Epoch 3198/10000: L(Train): 0.3203471601009369; L(Test): 0.30119943618774414\n",
            "Epoch 3199/10000: L(Train): 0.3276647925376892; L(Test): 0.300590842962265\n",
            "Epoch 3200/10000: L(Train): 0.32124263048171997; L(Test): 0.3019344210624695\n",
            "Epoch 3201/10000: L(Train): 0.3216899037361145; L(Test): 0.3012963533401489\n",
            "Epoch 3202/10000: L(Train): 0.328859806060791; L(Test): 0.3010631799697876\n",
            "Epoch 3203/10000: L(Train): 0.3206363320350647; L(Test): 0.3011532723903656\n",
            "Epoch 3204/10000: L(Train): 0.3332866430282593; L(Test): 0.3004632294178009\n",
            "Epoch 3205/10000: L(Train): 0.3256070613861084; L(Test): 0.30041977763175964\n",
            "Epoch 3206/10000: L(Train): 0.324302077293396; L(Test): 0.30090540647506714\n",
            "Epoch 3207/10000: L(Train): 0.31871268153190613; L(Test): 0.3021966814994812\n",
            "Epoch 3208/10000: L(Train): 0.32015857100486755; L(Test): 0.3019998073577881\n",
            "Epoch 3209/10000: L(Train): 0.3301195800304413; L(Test): 0.30064526200294495\n",
            "Epoch 3210/10000: L(Train): 0.32232406735420227; L(Test): 0.3013231158256531\n",
            "Epoch 3211/10000: L(Train): 0.3327948749065399; L(Test): 0.30074623227119446\n",
            "Epoch 3212/10000: L(Train): 0.3169381618499756; L(Test): 0.30224668979644775\n",
            "Epoch 3213/10000: L(Train): 0.32710883021354675; L(Test): 0.3026329576969147\n",
            "Epoch 3214/10000: L(Train): 0.3366444706916809; L(Test): 0.3019910156726837\n",
            "Epoch 3215/10000: L(Train): 0.3174930810928345; L(Test): 0.3013751208782196\n",
            "Epoch 3216/10000: L(Train): 0.3256462514400482; L(Test): 0.3019551932811737\n",
            "Epoch 3217/10000: L(Train): 0.33459553122520447; L(Test): 0.30254286527633667\n",
            "Epoch 3218/10000: L(Train): 0.3282671272754669; L(Test): 0.3026370704174042\n",
            "Epoch 3219/10000: L(Train): 0.31394684314727783; L(Test): 0.3032838702201843\n",
            "Epoch 3220/10000: L(Train): 0.3200937509536743; L(Test): 0.3036254942417145\n",
            "Epoch 3221/10000: L(Train): 0.31888630986213684; L(Test): 0.3036305010318756\n",
            "Epoch 3222/10000: L(Train): 0.3180541396141052; L(Test): 0.30366212129592896\n",
            "Epoch 3223/10000: L(Train): 0.3138483464717865; L(Test): 0.30327144265174866\n",
            "Epoch 3224/10000: L(Train): 0.32571691274642944; L(Test): 0.30244123935699463\n",
            "Epoch 3225/10000: L(Train): 0.3303438127040863; L(Test): 0.30190131068229675\n",
            "Epoch 3226/10000: L(Train): 0.3329841196537018; L(Test): 0.3025224506855011\n",
            "Epoch 3227/10000: L(Train): 0.3393895924091339; L(Test): 0.30214908719062805\n",
            "Epoch 3228/10000: L(Train): 0.330068975687027; L(Test): 0.30211782455444336\n",
            "Epoch 3229/10000: L(Train): 0.3335135281085968; L(Test): 0.30207908153533936\n",
            "Epoch 3230/10000: L(Train): 0.3330737054347992; L(Test): 0.30254054069519043\n",
            "Epoch 3231/10000: L(Train): 0.31913912296295166; L(Test): 0.30378738045692444\n",
            "Epoch 3232/10000: L(Train): 0.3298795521259308; L(Test): 0.3033853769302368\n",
            "Epoch 3233/10000: L(Train): 0.3265143036842346; L(Test): 0.3021497428417206\n",
            "Epoch 3234/10000: L(Train): 0.3331177234649658; L(Test): 0.30250391364097595\n",
            "Epoch 3235/10000: L(Train): 0.3216733932495117; L(Test): 0.30424320697784424\n",
            "Epoch 3236/10000: L(Train): 0.32858794927597046; L(Test): 0.3051086664199829\n",
            "Epoch 3237/10000: L(Train): 0.3281921446323395; L(Test): 0.30281662940979004\n",
            "Epoch 3238/10000: L(Train): 0.3167344629764557; L(Test): 0.3019528090953827\n",
            "Epoch 3239/10000: L(Train): 0.32678407430648804; L(Test): 0.3040613830089569\n",
            "Epoch 3240/10000: L(Train): 0.3258712589740753; L(Test): 0.30240556597709656\n",
            "Epoch 3241/10000: L(Train): 0.32307320833206177; L(Test): 0.30165156722068787\n",
            "Epoch 3242/10000: L(Train): 0.3224771022796631; L(Test): 0.30293989181518555\n",
            "Epoch 3243/10000: L(Train): 0.32265397906303406; L(Test): 0.30403804779052734\n",
            "Epoch 3244/10000: L(Train): 0.3281853497028351; L(Test): 0.30259305238723755\n",
            "Epoch 3245/10000: L(Train): 0.3229389190673828; L(Test): 0.30249089002609253\n",
            "Epoch 3246/10000: L(Train): 0.33488336205482483; L(Test): 0.30472955107688904\n",
            "Epoch 3247/10000: L(Train): 0.3228078782558441; L(Test): 0.3048957884311676\n",
            "Epoch 3248/10000: L(Train): 0.32324495911598206; L(Test): 0.3027466833591461\n",
            "Epoch 3249/10000: L(Train): 0.32544374465942383; L(Test): 0.30219098925590515\n",
            "Epoch 3250/10000: L(Train): 0.32657355070114136; L(Test): 0.30282464623451233\n",
            "Epoch 3251/10000: L(Train): 0.33836543560028076; L(Test): 0.30401015281677246\n",
            "Epoch 3252/10000: L(Train): 0.33519962430000305; L(Test): 0.3043583035469055\n",
            "Epoch 3253/10000: L(Train): 0.32347723841667175; L(Test): 0.3044589161872864\n",
            "Epoch 3254/10000: L(Train): 0.32528820633888245; L(Test): 0.30526307225227356\n",
            "Epoch 3255/10000: L(Train): 0.32893308997154236; L(Test): 0.3056480288505554\n",
            "Epoch 3256/10000: L(Train): 0.3204132616519928; L(Test): 0.3060188591480255\n",
            "Epoch 3257/10000: L(Train): 0.3247990906238556; L(Test): 0.30605125427246094\n",
            "Epoch 3258/10000: L(Train): 0.33429399132728577; L(Test): 0.30488091707229614\n",
            "Epoch 3259/10000: L(Train): 0.3203604519367218; L(Test): 0.3053217828273773\n",
            "Epoch 3260/10000: L(Train): 0.3293132781982422; L(Test): 0.304613322019577\n",
            "Epoch 3261/10000: L(Train): 0.3220542371273041; L(Test): 0.30444297194480896\n",
            "Epoch 3262/10000: L(Train): 0.3208772838115692; L(Test): 0.3043696880340576\n",
            "Epoch 3263/10000: L(Train): 0.3328112065792084; L(Test): 0.30543196201324463\n",
            "Epoch 3264/10000: L(Train): 0.3203454613685608; L(Test): 0.305794894695282\n",
            "Epoch 3265/10000: L(Train): 0.3252994120121002; L(Test): 0.3033323287963867\n",
            "Epoch 3266/10000: L(Train): 0.32316073775291443; L(Test): 0.3029719889163971\n",
            "Epoch 3267/10000: L(Train): 0.33027154207229614; L(Test): 0.30341023206710815\n",
            "Epoch 3268/10000: L(Train): 0.3303512930870056; L(Test): 0.30469274520874023\n",
            "Epoch 3269/10000: L(Train): 0.3393522799015045; L(Test): 0.3053717613220215\n",
            "Epoch 3270/10000: L(Train): 0.33075806498527527; L(Test): 0.3028983175754547\n",
            "Epoch 3271/10000: L(Train): 0.32622331380844116; L(Test): 0.3041059672832489\n",
            "Epoch 3272/10000: L(Train): 0.32629573345184326; L(Test): 0.306347131729126\n",
            "Epoch 3273/10000: L(Train): 0.3380921483039856; L(Test): 0.3056127727031708\n",
            "Epoch 3274/10000: L(Train): 0.32260262966156006; L(Test): 0.30379167199134827\n",
            "Epoch 3275/10000: L(Train): 0.3204670250415802; L(Test): 0.30353665351867676\n",
            "Epoch 3276/10000: L(Train): 0.3194234073162079; L(Test): 0.30396780371665955\n",
            "Epoch 3277/10000: L(Train): 0.31880688667297363; L(Test): 0.30365803837776184\n",
            "Epoch 3278/10000: L(Train): 0.3135074973106384; L(Test): 0.3048498034477234\n",
            "Epoch 3279/10000: L(Train): 0.3176026940345764; L(Test): 0.30477362871170044\n",
            "Epoch 3280/10000: L(Train): 0.32284092903137207; L(Test): 0.30319398641586304\n",
            "Epoch 3281/10000: L(Train): 0.32145246863365173; L(Test): 0.3041399121284485\n",
            "Epoch 3282/10000: L(Train): 0.3266744315624237; L(Test): 0.3055061399936676\n",
            "Epoch 3283/10000: L(Train): 0.33294209837913513; L(Test): 0.3041456639766693\n",
            "Epoch 3284/10000: L(Train): 0.32172277569770813; L(Test): 0.3035123944282532\n",
            "Epoch 3285/10000: L(Train): 0.33137309551239014; L(Test): 0.30346935987472534\n",
            "Epoch 3286/10000: L(Train): 0.33013802766799927; L(Test): 0.30313947796821594\n",
            "Epoch 3287/10000: L(Train): 0.32765886187553406; L(Test): 0.30300962924957275\n",
            "Epoch 3288/10000: L(Train): 0.32204410433769226; L(Test): 0.3031352758407593\n",
            "Epoch 3289/10000: L(Train): 0.32654982805252075; L(Test): 0.3028962016105652\n",
            "Epoch 3290/10000: L(Train): 0.32521986961364746; L(Test): 0.30178147554397583\n",
            "Epoch 3291/10000: L(Train): 0.3297165632247925; L(Test): 0.30123409628868103\n",
            "Epoch 3292/10000: L(Train): 0.3265205919742584; L(Test): 0.30103784799575806\n",
            "Epoch 3293/10000: L(Train): 0.3282336890697479; L(Test): 0.3009886145591736\n",
            "Epoch 3294/10000: L(Train): 0.3230225741863251; L(Test): 0.30107182264328003\n",
            "Epoch 3295/10000: L(Train): 0.3282751441001892; L(Test): 0.3008824288845062\n",
            "Epoch 3296/10000: L(Train): 0.327033668756485; L(Test): 0.3025127947330475\n",
            "Epoch 3297/10000: L(Train): 0.32870718836784363; L(Test): 0.3018173277378082\n",
            "Epoch 3298/10000: L(Train): 0.3299320340156555; L(Test): 0.30075010657310486\n",
            "Epoch 3299/10000: L(Train): 0.3312053084373474; L(Test): 0.3014816641807556\n",
            "Epoch 3300/10000: L(Train): 0.32725587487220764; L(Test): 0.3006077706813812\n",
            "Epoch 3301/10000: L(Train): 0.3179965317249298; L(Test): 0.3007471561431885\n",
            "Epoch 3302/10000: L(Train): 0.31834498047828674; L(Test): 0.301702082157135\n",
            "Epoch 3303/10000: L(Train): 0.32775387167930603; L(Test): 0.301290363073349\n",
            "Epoch 3304/10000: L(Train): 0.3250732123851776; L(Test): 0.3017151355743408\n",
            "Epoch 3305/10000: L(Train): 0.33584699034690857; L(Test): 0.30143892765045166\n",
            "Epoch 3306/10000: L(Train): 0.33230626583099365; L(Test): 0.30110788345336914\n",
            "Epoch 3307/10000: L(Train): 0.32051554322242737; L(Test): 0.30092912912368774\n",
            "Epoch 3308/10000: L(Train): 0.32221469283103943; L(Test): 0.30104073882102966\n",
            "Epoch 3309/10000: L(Train): 0.3259018063545227; L(Test): 0.3011643886566162\n",
            "Epoch 3310/10000: L(Train): 0.32287341356277466; L(Test): 0.3014305531978607\n",
            "Epoch 3311/10000: L(Train): 0.3219439685344696; L(Test): 0.3015584349632263\n",
            "Epoch 3312/10000: L(Train): 0.32482025027275085; L(Test): 0.3016871213912964\n",
            "Epoch 3313/10000: L(Train): 0.323244571685791; L(Test): 0.3018546998500824\n",
            "Epoch 3314/10000: L(Train): 0.32951319217681885; L(Test): 0.3019391596317291\n",
            "Epoch 3315/10000: L(Train): 0.3232412040233612; L(Test): 0.3019946217536926\n",
            "Epoch 3316/10000: L(Train): 0.33023884892463684; L(Test): 0.3014239966869354\n",
            "Epoch 3317/10000: L(Train): 0.3169231414794922; L(Test): 0.3019108474254608\n",
            "Epoch 3318/10000: L(Train): 0.3299539387226105; L(Test): 0.30167877674102783\n",
            "Epoch 3319/10000: L(Train): 0.3235234320163727; L(Test): 0.3012661635875702\n",
            "Epoch 3320/10000: L(Train): 0.3268989622592926; L(Test): 0.3014150857925415\n",
            "Epoch 3321/10000: L(Train): 0.3279973864555359; L(Test): 0.3014853000640869\n",
            "Epoch 3322/10000: L(Train): 0.32299208641052246; L(Test): 0.3013976812362671\n",
            "Epoch 3323/10000: L(Train): 0.3229532539844513; L(Test): 0.3010500371456146\n",
            "Epoch 3324/10000: L(Train): 0.32405370473861694; L(Test): 0.3009103238582611\n",
            "Epoch 3325/10000: L(Train): 0.324482262134552; L(Test): 0.3011896014213562\n",
            "Epoch 3326/10000: L(Train): 0.32744115591049194; L(Test): 0.3015231192111969\n",
            "Epoch 3327/10000: L(Train): 0.322331041097641; L(Test): 0.3016071915626526\n",
            "Epoch 3328/10000: L(Train): 0.3170776665210724; L(Test): 0.30126139521598816\n",
            "Epoch 3329/10000: L(Train): 0.3215031623840332; L(Test): 0.3019176721572876\n",
            "Epoch 3330/10000: L(Train): 0.32324257493019104; L(Test): 0.3017716407775879\n",
            "Epoch 3331/10000: L(Train): 0.33420613408088684; L(Test): 0.30147576332092285\n",
            "Epoch 3332/10000: L(Train): 0.31988778710365295; L(Test): 0.30154427886009216\n",
            "Epoch 3333/10000: L(Train): 0.3293970227241516; L(Test): 0.3015289306640625\n",
            "Epoch 3334/10000: L(Train): 0.3251483738422394; L(Test): 0.3013623058795929\n",
            "Epoch 3335/10000: L(Train): 0.32300788164138794; L(Test): 0.30142101645469666\n",
            "Epoch 3336/10000: L(Train): 0.31736403703689575; L(Test): 0.3011322021484375\n",
            "Epoch 3337/10000: L(Train): 0.32972174882888794; L(Test): 0.3005179762840271\n",
            "Epoch 3338/10000: L(Train): 0.3296584486961365; L(Test): 0.3004252016544342\n",
            "Epoch 3339/10000: L(Train): 0.3232651650905609; L(Test): 0.30096563696861267\n",
            "Epoch 3340/10000: L(Train): 0.3263596296310425; L(Test): 0.30164533853530884\n",
            "Epoch 3341/10000: L(Train): 0.3272936940193176; L(Test): 0.3022578954696655\n",
            "Epoch 3342/10000: L(Train): 0.32173749804496765; L(Test): 0.3021913766860962\n",
            "Epoch 3343/10000: L(Train): 0.33355769515037537; L(Test): 0.3016764521598816\n",
            "Epoch 3344/10000: L(Train): 0.3240918815135956; L(Test): 0.302200585603714\n",
            "Epoch 3345/10000: L(Train): 0.322706401348114; L(Test): 0.30234473943710327\n",
            "Epoch 3346/10000: L(Train): 0.31879279017448425; L(Test): 0.30204126238822937\n",
            "Epoch 3347/10000: L(Train): 0.32435235381126404; L(Test): 0.30147862434387207\n",
            "Epoch 3348/10000: L(Train): 0.3276621401309967; L(Test): 0.30159199237823486\n",
            "Epoch 3349/10000: L(Train): 0.34066084027290344; L(Test): 0.301192969083786\n",
            "Epoch 3350/10000: L(Train): 0.31521809101104736; L(Test): 0.3020571172237396\n",
            "Epoch 3351/10000: L(Train): 0.3258959949016571; L(Test): 0.3027978539466858\n",
            "Epoch 3352/10000: L(Train): 0.3263474106788635; L(Test): 0.30262571573257446\n",
            "Epoch 3353/10000: L(Train): 0.3234594464302063; L(Test): 0.301242470741272\n",
            "Epoch 3354/10000: L(Train): 0.32980623841285706; L(Test): 0.3030090630054474\n",
            "Epoch 3355/10000: L(Train): 0.3233013153076172; L(Test): 0.30242273211479187\n",
            "Epoch 3356/10000: L(Train): 0.3244593143463135; L(Test): 0.3020390570163727\n",
            "Epoch 3357/10000: L(Train): 0.32119035720825195; L(Test): 0.3036378026008606\n",
            "Epoch 3358/10000: L(Train): 0.33174219727516174; L(Test): 0.3021358251571655\n",
            "Epoch 3359/10000: L(Train): 0.3215685188770294; L(Test): 0.30155646800994873\n",
            "Epoch 3360/10000: L(Train): 0.3370265066623688; L(Test): 0.3021881580352783\n",
            "Epoch 3361/10000: L(Train): 0.33438563346862793; L(Test): 0.30162835121154785\n",
            "Epoch 3362/10000: L(Train): 0.3173832297325134; L(Test): 0.3022922873497009\n",
            "Epoch 3363/10000: L(Train): 0.3248204290866852; L(Test): 0.30212894082069397\n",
            "Epoch 3364/10000: L(Train): 0.32889240980148315; L(Test): 0.3016059398651123\n",
            "Epoch 3365/10000: L(Train): 0.3293812870979309; L(Test): 0.30158597230911255\n",
            "Epoch 3366/10000: L(Train): 0.32514330744743347; L(Test): 0.30248263478279114\n",
            "Epoch 3367/10000: L(Train): 0.32873913645744324; L(Test): 0.3024689555168152\n",
            "Epoch 3368/10000: L(Train): 0.3169589936733246; L(Test): 0.30205821990966797\n",
            "Epoch 3369/10000: L(Train): 0.32345855236053467; L(Test): 0.3018861711025238\n",
            "Epoch 3370/10000: L(Train): 0.3229617476463318; L(Test): 0.3036617040634155\n",
            "Epoch 3371/10000: L(Train): 0.33216041326522827; L(Test): 0.30335286259651184\n",
            "Epoch 3372/10000: L(Train): 0.3246626853942871; L(Test): 0.3021879196166992\n",
            "Epoch 3373/10000: L(Train): 0.3205586075782776; L(Test): 0.3024255037307739\n",
            "Epoch 3374/10000: L(Train): 0.3229857385158539; L(Test): 0.3017798960208893\n",
            "Epoch 3375/10000: L(Train): 0.3217119574546814; L(Test): 0.3012257516384125\n",
            "Epoch 3376/10000: L(Train): 0.3271353840827942; L(Test): 0.3017514944076538\n",
            "Epoch 3377/10000: L(Train): 0.3225252032279968; L(Test): 0.3012007474899292\n",
            "Epoch 3378/10000: L(Train): 0.3253277540206909; L(Test): 0.3022017776966095\n",
            "Epoch 3379/10000: L(Train): 0.3303489089012146; L(Test): 0.30398067831993103\n",
            "Epoch 3380/10000: L(Train): 0.3286340534687042; L(Test): 0.3026193082332611\n",
            "Epoch 3381/10000: L(Train): 0.3261377215385437; L(Test): 0.3029828667640686\n",
            "Epoch 3382/10000: L(Train): 0.32522332668304443; L(Test): 0.3024943172931671\n",
            "Epoch 3383/10000: L(Train): 0.3197842836380005; L(Test): 0.3011028468608856\n",
            "Epoch 3384/10000: L(Train): 0.3238258361816406; L(Test): 0.3030599057674408\n",
            "Epoch 3385/10000: L(Train): 0.3306306004524231; L(Test): 0.3020135760307312\n",
            "Epoch 3386/10000: L(Train): 0.3249371349811554; L(Test): 0.30142298340797424\n",
            "Epoch 3387/10000: L(Train): 0.33126360177993774; L(Test): 0.3017866909503937\n",
            "Epoch 3388/10000: L(Train): 0.3297308683395386; L(Test): 0.3010452389717102\n",
            "Epoch 3389/10000: L(Train): 0.3303776681423187; L(Test): 0.3019796311855316\n",
            "Epoch 3390/10000: L(Train): 0.3210206627845764; L(Test): 0.3014994263648987\n",
            "Epoch 3391/10000: L(Train): 0.3229907751083374; L(Test): 0.301299124956131\n",
            "Epoch 3392/10000: L(Train): 0.3177112638950348; L(Test): 0.30231621861457825\n",
            "Epoch 3393/10000: L(Train): 0.3206251859664917; L(Test): 0.30235910415649414\n",
            "Epoch 3394/10000: L(Train): 0.32019174098968506; L(Test): 0.30160439014434814\n",
            "Epoch 3395/10000: L(Train): 0.3202095925807953; L(Test): 0.30204540491104126\n",
            "Epoch 3396/10000: L(Train): 0.32895684242248535; L(Test): 0.3020032048225403\n",
            "Epoch 3397/10000: L(Train): 0.33602088689804077; L(Test): 0.3015110492706299\n",
            "Epoch 3398/10000: L(Train): 0.32491758465766907; L(Test): 0.30125656723976135\n",
            "Epoch 3399/10000: L(Train): 0.3221112787723541; L(Test): 0.30128657817840576\n",
            "Epoch 3400/10000: L(Train): 0.317554771900177; L(Test): 0.3011108636856079\n",
            "Epoch 3401/10000: L(Train): 0.3210029602050781; L(Test): 0.3018897771835327\n",
            "Epoch 3402/10000: L(Train): 0.32529401779174805; L(Test): 0.3022528290748596\n",
            "Epoch 3403/10000: L(Train): 0.3266160786151886; L(Test): 0.3015691936016083\n",
            "Epoch 3404/10000: L(Train): 0.3246771991252899; L(Test): 0.3006811738014221\n",
            "Epoch 3405/10000: L(Train): 0.3228204846382141; L(Test): 0.30025047063827515\n",
            "Epoch 3406/10000: L(Train): 0.32468029856681824; L(Test): 0.30011317133903503\n",
            "Epoch 3407/10000: L(Train): 0.3210830092430115; L(Test): 0.30077654123306274\n",
            "Epoch 3408/10000: L(Train): 0.33241963386535645; L(Test): 0.300517737865448\n",
            "Epoch 3409/10000: L(Train): 0.31801220774650574; L(Test): 0.29991239309310913\n",
            "Epoch 3410/10000: L(Train): 0.32141605019569397; L(Test): 0.3010045886039734\n",
            "Epoch 3411/10000: L(Train): 0.31887301802635193; L(Test): 0.3017691373825073\n",
            "Epoch 3412/10000: L(Train): 0.3228919506072998; L(Test): 0.30186572670936584\n",
            "Epoch 3413/10000: L(Train): 0.3211454153060913; L(Test): 0.3015720546245575\n",
            "Epoch 3414/10000: L(Train): 0.32394900918006897; L(Test): 0.3014976382255554\n",
            "Epoch 3415/10000: L(Train): 0.32928144931793213; L(Test): 0.30034372210502625\n",
            "Epoch 3416/10000: L(Train): 0.3275737464427948; L(Test): 0.29978835582733154\n",
            "Epoch 3417/10000: L(Train): 0.325308233499527; L(Test): 0.30040034651756287\n",
            "Epoch 3418/10000: L(Train): 0.3218066394329071; L(Test): 0.3011971712112427\n",
            "Epoch 3419/10000: L(Train): 0.3288625180721283; L(Test): 0.30089855194091797\n",
            "Epoch 3420/10000: L(Train): 0.32226163148880005; L(Test): 0.3007913827896118\n",
            "Epoch 3421/10000: L(Train): 0.3196137547492981; L(Test): 0.30062904953956604\n",
            "Epoch 3422/10000: L(Train): 0.33170780539512634; L(Test): 0.30070847272872925\n",
            "Epoch 3423/10000: L(Train): 0.32724645733833313; L(Test): 0.301007479429245\n",
            "Epoch 3424/10000: L(Train): 0.3391668200492859; L(Test): 0.30068454146385193\n",
            "Epoch 3425/10000: L(Train): 0.32121726870536804; L(Test): 0.3031536042690277\n",
            "Epoch 3426/10000: L(Train): 0.3252162039279938; L(Test): 0.3049514591693878\n",
            "Epoch 3427/10000: L(Train): 0.3252023756504059; L(Test): 0.3030417859554291\n",
            "Epoch 3428/10000: L(Train): 0.31759703159332275; L(Test): 0.3025401532649994\n",
            "Epoch 3429/10000: L(Train): 0.3213406801223755; L(Test): 0.3028905689716339\n",
            "Epoch 3430/10000: L(Train): 0.32956257462501526; L(Test): 0.30288827419281006\n",
            "Epoch 3431/10000: L(Train): 0.3257848024368286; L(Test): 0.3020913600921631\n",
            "Epoch 3432/10000: L(Train): 0.32337164878845215; L(Test): 0.3026352822780609\n",
            "Epoch 3433/10000: L(Train): 0.3172623813152313; L(Test): 0.30442318320274353\n",
            "Epoch 3434/10000: L(Train): 0.3255421817302704; L(Test): 0.30447107553482056\n",
            "Epoch 3435/10000: L(Train): 0.32895955443382263; L(Test): 0.30373263359069824\n",
            "Epoch 3436/10000: L(Train): 0.32776889204978943; L(Test): 0.30235758423805237\n",
            "Epoch 3437/10000: L(Train): 0.3271161913871765; L(Test): 0.3024614155292511\n",
            "Epoch 3438/10000: L(Train): 0.32888540625572205; L(Test): 0.30229923129081726\n",
            "Epoch 3439/10000: L(Train): 0.3222365975379944; L(Test): 0.3029470145702362\n",
            "Epoch 3440/10000: L(Train): 0.3250485360622406; L(Test): 0.3031415641307831\n",
            "Epoch 3441/10000: L(Train): 0.32068413496017456; L(Test): 0.303963303565979\n",
            "Epoch 3442/10000: L(Train): 0.3236573338508606; L(Test): 0.3038954734802246\n",
            "Epoch 3443/10000: L(Train): 0.3209218382835388; L(Test): 0.30285152792930603\n",
            "Epoch 3444/10000: L(Train): 0.3281646966934204; L(Test): 0.3018971383571625\n",
            "Epoch 3445/10000: L(Train): 0.32423415780067444; L(Test): 0.301832377910614\n",
            "Epoch 3446/10000: L(Train): 0.32534700632095337; L(Test): 0.3026890158653259\n",
            "Epoch 3447/10000: L(Train): 0.32437774538993835; L(Test): 0.30199986696243286\n",
            "Epoch 3448/10000: L(Train): 0.3303370177745819; L(Test): 0.30248644948005676\n",
            "Epoch 3449/10000: L(Train): 0.325304239988327; L(Test): 0.30495038628578186\n",
            "Epoch 3450/10000: L(Train): 0.3302318751811981; L(Test): 0.30322930216789246\n",
            "Epoch 3451/10000: L(Train): 0.3184649646282196; L(Test): 0.30457809567451477\n",
            "Epoch 3452/10000: L(Train): 0.31505754590034485; L(Test): 0.3058742880821228\n",
            "Epoch 3453/10000: L(Train): 0.3196526765823364; L(Test): 0.3015393912792206\n",
            "Epoch 3454/10000: L(Train): 0.32252374291419983; L(Test): 0.30128100514411926\n",
            "Epoch 3455/10000: L(Train): 0.3265765607357025; L(Test): 0.3023534119129181\n",
            "Epoch 3456/10000: L(Train): 0.32972145080566406; L(Test): 0.30115947127342224\n",
            "Epoch 3457/10000: L(Train): 0.32907113432884216; L(Test): 0.30301326513290405\n",
            "Epoch 3458/10000: L(Train): 0.32087552547454834; L(Test): 0.30285409092903137\n",
            "Epoch 3459/10000: L(Train): 0.3198648989200592; L(Test): 0.30231478810310364\n",
            "Epoch 3460/10000: L(Train): 0.3248588442802429; L(Test): 0.3034088611602783\n",
            "Epoch 3461/10000: L(Train): 0.3274323046207428; L(Test): 0.3027872145175934\n",
            "Epoch 3462/10000: L(Train): 0.3257075250148773; L(Test): 0.3025719225406647\n",
            "Epoch 3463/10000: L(Train): 0.32637080550193787; L(Test): 0.3030937612056732\n",
            "Epoch 3464/10000: L(Train): 0.3326595723628998; L(Test): 0.3015037775039673\n",
            "Epoch 3465/10000: L(Train): 0.3189132809638977; L(Test): 0.30262890458106995\n",
            "Epoch 3466/10000: L(Train): 0.3207460343837738; L(Test): 0.3022995889186859\n",
            "Epoch 3467/10000: L(Train): 0.3308964967727661; L(Test): 0.30068108439445496\n",
            "Epoch 3468/10000: L(Train): 0.32529395818710327; L(Test): 0.30117669701576233\n",
            "Epoch 3469/10000: L(Train): 0.3214517831802368; L(Test): 0.3025684952735901\n",
            "Epoch 3470/10000: L(Train): 0.32133036851882935; L(Test): 0.3027692437171936\n",
            "Epoch 3471/10000: L(Train): 0.3288900554180145; L(Test): 0.3028191924095154\n",
            "Epoch 3472/10000: L(Train): 0.32809707522392273; L(Test): 0.3029412031173706\n",
            "Epoch 3473/10000: L(Train): 0.3225332200527191; L(Test): 0.30191516876220703\n",
            "Epoch 3474/10000: L(Train): 0.32356908917427063; L(Test): 0.301362544298172\n",
            "Epoch 3475/10000: L(Train): 0.33277469873428345; L(Test): 0.30169177055358887\n",
            "Epoch 3476/10000: L(Train): 0.3212030231952667; L(Test): 0.3015926778316498\n",
            "Epoch 3477/10000: L(Train): 0.32748502492904663; L(Test): 0.30229711532592773\n",
            "Epoch 3478/10000: L(Train): 0.3147986829280853; L(Test): 0.302590548992157\n",
            "Epoch 3479/10000: L(Train): 0.3296510875225067; L(Test): 0.3018225133419037\n",
            "Epoch 3480/10000: L(Train): 0.32046374678611755; L(Test): 0.30187469720840454\n",
            "Epoch 3481/10000: L(Train): 0.3255883753299713; L(Test): 0.301504909992218\n",
            "Epoch 3482/10000: L(Train): 0.3272084593772888; L(Test): 0.3006773591041565\n",
            "Epoch 3483/10000: L(Train): 0.32177188992500305; L(Test): 0.3008734881877899\n",
            "Epoch 3484/10000: L(Train): 0.3310282826423645; L(Test): 0.30237913131713867\n",
            "Epoch 3485/10000: L(Train): 0.3136487603187561; L(Test): 0.30187714099884033\n",
            "Epoch 3486/10000: L(Train): 0.3225196301937103; L(Test): 0.3010920286178589\n",
            "Epoch 3487/10000: L(Train): 0.3208780586719513; L(Test): 0.30110159516334534\n",
            "Epoch 3488/10000: L(Train): 0.32771411538124084; L(Test): 0.3011045455932617\n",
            "Epoch 3489/10000: L(Train): 0.3182760179042816; L(Test): 0.3018227219581604\n",
            "Epoch 3490/10000: L(Train): 0.3343277871608734; L(Test): 0.3013959228992462\n",
            "Epoch 3491/10000: L(Train): 0.32107535004615784; L(Test): 0.2999914288520813\n",
            "Epoch 3492/10000: L(Train): 0.32997235655784607; L(Test): 0.29998335242271423\n",
            "Epoch 3493/10000: L(Train): 0.3291921317577362; L(Test): 0.3007851243019104\n",
            "Epoch 3494/10000: L(Train): 0.32651057839393616; L(Test): 0.30062463879585266\n",
            "Epoch 3495/10000: L(Train): 0.32781386375427246; L(Test): 0.3024909198284149\n",
            "Epoch 3496/10000: L(Train): 0.3271777927875519; L(Test): 0.3015272319316864\n",
            "Epoch 3497/10000: L(Train): 0.3308100998401642; L(Test): 0.3017650544643402\n",
            "Epoch 3498/10000: L(Train): 0.3197876214981079; L(Test): 0.30255699157714844\n",
            "Epoch 3499/10000: L(Train): 0.31486228108406067; L(Test): 0.30181899666786194\n",
            "Epoch 3500/10000: L(Train): 0.33031171560287476; L(Test): 0.3030340373516083\n",
            "Epoch 3501/10000: L(Train): 0.32482242584228516; L(Test): 0.30233484506607056\n",
            "Epoch 3502/10000: L(Train): 0.3326546549797058; L(Test): 0.30174553394317627\n",
            "Epoch 3503/10000: L(Train): 0.3256314992904663; L(Test): 0.3013678789138794\n",
            "Epoch 3504/10000: L(Train): 0.32250121235847473; L(Test): 0.3011401295661926\n",
            "Epoch 3505/10000: L(Train): 0.32655295729637146; L(Test): 0.3027774393558502\n",
            "Epoch 3506/10000: L(Train): 0.33072733879089355; L(Test): 0.3020325303077698\n",
            "Epoch 3507/10000: L(Train): 0.33509060740470886; L(Test): 0.3007020950317383\n",
            "Epoch 3508/10000: L(Train): 0.3204251825809479; L(Test): 0.3014943599700928\n",
            "Epoch 3509/10000: L(Train): 0.3268256187438965; L(Test): 0.3011725842952728\n",
            "Epoch 3510/10000: L(Train): 0.3265780508518219; L(Test): 0.301523357629776\n",
            "Epoch 3511/10000: L(Train): 0.32386282086372375; L(Test): 0.3017873466014862\n",
            "Epoch 3512/10000: L(Train): 0.315607488155365; L(Test): 0.3014221787452698\n",
            "Epoch 3513/10000: L(Train): 0.32575979828834534; L(Test): 0.3013622462749481\n",
            "Epoch 3514/10000: L(Train): 0.33303701877593994; L(Test): 0.3010842502117157\n",
            "Epoch 3515/10000: L(Train): 0.32399308681488037; L(Test): 0.3013702630996704\n",
            "Epoch 3516/10000: L(Train): 0.32750606536865234; L(Test): 0.3024439513683319\n",
            "Epoch 3517/10000: L(Train): 0.3216037452220917; L(Test): 0.30225038528442383\n",
            "Epoch 3518/10000: L(Train): 0.33360061049461365; L(Test): 0.301501989364624\n",
            "Epoch 3519/10000: L(Train): 0.32396259903907776; L(Test): 0.3006434142589569\n",
            "Epoch 3520/10000: L(Train): 0.32084032893180847; L(Test): 0.30167296528816223\n",
            "Epoch 3521/10000: L(Train): 0.3187234103679657; L(Test): 0.30235210061073303\n",
            "Epoch 3522/10000: L(Train): 0.3215819001197815; L(Test): 0.3008744418621063\n",
            "Epoch 3523/10000: L(Train): 0.32428157329559326; L(Test): 0.30094853043556213\n",
            "Epoch 3524/10000: L(Train): 0.324716717004776; L(Test): 0.3020568788051605\n",
            "Epoch 3525/10000: L(Train): 0.3279103934764862; L(Test): 0.30244356393814087\n",
            "Epoch 3526/10000: L(Train): 0.3264816403388977; L(Test): 0.30342015624046326\n",
            "Epoch 3527/10000: L(Train): 0.32421502470970154; L(Test): 0.30192646384239197\n",
            "Epoch 3528/10000: L(Train): 0.33540067076683044; L(Test): 0.3007062077522278\n",
            "Epoch 3529/10000: L(Train): 0.3209053575992584; L(Test): 0.30267125368118286\n",
            "Epoch 3530/10000: L(Train): 0.321999728679657; L(Test): 0.30131083726882935\n",
            "Epoch 3531/10000: L(Train): 0.3164340555667877; L(Test): 0.3006862998008728\n",
            "Epoch 3532/10000: L(Train): 0.33184897899627686; L(Test): 0.30204445123672485\n",
            "Epoch 3533/10000: L(Train): 0.3147522211074829; L(Test): 0.30211320519447327\n",
            "Epoch 3534/10000: L(Train): 0.335244745016098; L(Test): 0.30089566111564636\n",
            "Epoch 3535/10000: L(Train): 0.32944151759147644; L(Test): 0.30070263147354126\n",
            "Epoch 3536/10000: L(Train): 0.33316874504089355; L(Test): 0.300997257232666\n",
            "Epoch 3537/10000: L(Train): 0.3278733491897583; L(Test): 0.3016050457954407\n",
            "Epoch 3538/10000: L(Train): 0.3213578462600708; L(Test): 0.3017309010028839\n",
            "Epoch 3539/10000: L(Train): 0.33381161093711853; L(Test): 0.3005501627922058\n",
            "Epoch 3540/10000: L(Train): 0.33211550116539; L(Test): 0.3001212775707245\n",
            "Epoch 3541/10000: L(Train): 0.3299667239189148; L(Test): 0.3015742003917694\n",
            "Epoch 3542/10000: L(Train): 0.3244534730911255; L(Test): 0.3014357388019562\n",
            "Epoch 3543/10000: L(Train): 0.3290526866912842; L(Test): 0.30172204971313477\n",
            "Epoch 3544/10000: L(Train): 0.33541804552078247; L(Test): 0.30087143182754517\n",
            "Epoch 3545/10000: L(Train): 0.32527634501457214; L(Test): 0.300473690032959\n",
            "Epoch 3546/10000: L(Train): 0.3128022253513336; L(Test): 0.30100518465042114\n",
            "Epoch 3547/10000: L(Train): 0.325670063495636; L(Test): 0.30034181475639343\n",
            "Epoch 3548/10000: L(Train): 0.32344484329223633; L(Test): 0.30024251341819763\n",
            "Epoch 3549/10000: L(Train): 0.3262821137905121; L(Test): 0.3018437325954437\n",
            "Epoch 3550/10000: L(Train): 0.31568849086761475; L(Test): 0.3015812337398529\n",
            "Epoch 3551/10000: L(Train): 0.32618504762649536; L(Test): 0.3011540472507477\n",
            "Epoch 3552/10000: L(Train): 0.32374051213264465; L(Test): 0.3002023994922638\n",
            "Epoch 3553/10000: L(Train): 0.31942999362945557; L(Test): 0.2996344566345215\n",
            "Epoch 3554/10000: L(Train): 0.32678115367889404; L(Test): 0.3005864918231964\n",
            "Epoch 3555/10000: L(Train): 0.31953343749046326; L(Test): 0.300115704536438\n",
            "Epoch 3556/10000: L(Train): 0.3289262354373932; L(Test): 0.2993846833705902\n",
            "Epoch 3557/10000: L(Train): 0.30867645144462585; L(Test): 0.2988477647304535\n",
            "Epoch 3558/10000: L(Train): 0.3243466317653656; L(Test): 0.29981347918510437\n",
            "Epoch 3559/10000: L(Train): 0.3326665759086609; L(Test): 0.299813836812973\n",
            "Epoch 3560/10000: L(Train): 0.3271884620189667; L(Test): 0.29901695251464844\n",
            "Epoch 3561/10000: L(Train): 0.3166014552116394; L(Test): 0.2988353669643402\n",
            "Epoch 3562/10000: L(Train): 0.3196221590042114; L(Test): 0.29891565442085266\n",
            "Epoch 3563/10000: L(Train): 0.31136417388916016; L(Test): 0.29933664202690125\n",
            "Epoch 3564/10000: L(Train): 0.3171136677265167; L(Test): 0.30000802874565125\n",
            "Epoch 3565/10000: L(Train): 0.3169758915901184; L(Test): 0.30044424533843994\n",
            "Epoch 3566/10000: L(Train): 0.32419347763061523; L(Test): 0.3012183904647827\n",
            "Epoch 3567/10000: L(Train): 0.3230752646923065; L(Test): 0.300678551197052\n",
            "Epoch 3568/10000: L(Train): 0.3290661573410034; L(Test): 0.299969345331192\n",
            "Epoch 3569/10000: L(Train): 0.31982725858688354; L(Test): 0.30121713876724243\n",
            "Epoch 3570/10000: L(Train): 0.315868616104126; L(Test): 0.3002605438232422\n",
            "Epoch 3571/10000: L(Train): 0.32467761635780334; L(Test): 0.30007389187812805\n",
            "Epoch 3572/10000: L(Train): 0.32395806908607483; L(Test): 0.3014492690563202\n",
            "Epoch 3573/10000: L(Train): 0.31655609607696533; L(Test): 0.30120959877967834\n",
            "Epoch 3574/10000: L(Train): 0.32828572392463684; L(Test): 0.30075886845588684\n",
            "Epoch 3575/10000: L(Train): 0.3312876224517822; L(Test): 0.29987311363220215\n",
            "Epoch 3576/10000: L(Train): 0.3321990370750427; L(Test): 0.30065295100212097\n",
            "Epoch 3577/10000: L(Train): 0.3260462284088135; L(Test): 0.30083736777305603\n",
            "Epoch 3578/10000: L(Train): 0.32062771916389465; L(Test): 0.3000231087207794\n",
            "Epoch 3579/10000: L(Train): 0.320864737033844; L(Test): 0.30183058977127075\n",
            "Epoch 3580/10000: L(Train): 0.33591195940971375; L(Test): 0.30077284574508667\n",
            "Epoch 3581/10000: L(Train): 0.3260471522808075; L(Test): 0.30078256130218506\n",
            "Epoch 3582/10000: L(Train): 0.32869550585746765; L(Test): 0.3027525544166565\n",
            "Epoch 3583/10000: L(Train): 0.32879990339279175; L(Test): 0.3002188205718994\n",
            "Epoch 3584/10000: L(Train): 0.314815491437912; L(Test): 0.30012762546539307\n",
            "Epoch 3585/10000: L(Train): 0.32399648427963257; L(Test): 0.3016728460788727\n",
            "Epoch 3586/10000: L(Train): 0.33703920245170593; L(Test): 0.30101701617240906\n",
            "Epoch 3587/10000: L(Train): 0.3243999481201172; L(Test): 0.3014056086540222\n",
            "Epoch 3588/10000: L(Train): 0.320178747177124; L(Test): 0.30159690976142883\n",
            "Epoch 3589/10000: L(Train): 0.33037886023521423; L(Test): 0.3023471534252167\n",
            "Epoch 3590/10000: L(Train): 0.3182103931903839; L(Test): 0.3018970191478729\n",
            "Epoch 3591/10000: L(Train): 0.31956717371940613; L(Test): 0.301524817943573\n",
            "Epoch 3592/10000: L(Train): 0.32322192192077637; L(Test): 0.30176272988319397\n",
            "Epoch 3593/10000: L(Train): 0.3337494134902954; L(Test): 0.3013998568058014\n",
            "Epoch 3594/10000: L(Train): 0.318183958530426; L(Test): 0.30097582936286926\n",
            "Epoch 3595/10000: L(Train): 0.3281475901603699; L(Test): 0.3002893328666687\n",
            "Epoch 3596/10000: L(Train): 0.32587459683418274; L(Test): 0.30039024353027344\n",
            "Epoch 3597/10000: L(Train): 0.32801908254623413; L(Test): 0.300899863243103\n",
            "Epoch 3598/10000: L(Train): 0.31760838627815247; L(Test): 0.30108627676963806\n",
            "Epoch 3599/10000: L(Train): 0.31841373443603516; L(Test): 0.3003273606300354\n",
            "Epoch 3600/10000: L(Train): 0.32106271386146545; L(Test): 0.3002413213253021\n",
            "Epoch 3601/10000: L(Train): 0.31844761967658997; L(Test): 0.3000013530254364\n",
            "Epoch 3602/10000: L(Train): 0.31460055708885193; L(Test): 0.2993767559528351\n",
            "Epoch 3603/10000: L(Train): 0.3221077024936676; L(Test): 0.2996453642845154\n",
            "Epoch 3604/10000: L(Train): 0.3209535777568817; L(Test): 0.3000214993953705\n",
            "Epoch 3605/10000: L(Train): 0.32273298501968384; L(Test): 0.30019181966781616\n",
            "Epoch 3606/10000: L(Train): 0.3203076422214508; L(Test): 0.3001072108745575\n",
            "Epoch 3607/10000: L(Train): 0.32813698053359985; L(Test): 0.3004685342311859\n",
            "Epoch 3608/10000: L(Train): 0.32545778155326843; L(Test): 0.30030322074890137\n",
            "Epoch 3609/10000: L(Train): 0.31690916419029236; L(Test): 0.29995691776275635\n",
            "Epoch 3610/10000: L(Train): 0.3300657272338867; L(Test): 0.29916834831237793\n",
            "Epoch 3611/10000: L(Train): 0.32289186120033264; L(Test): 0.299038290977478\n",
            "Epoch 3612/10000: L(Train): 0.3325444161891937; L(Test): 0.29935503005981445\n",
            "Epoch 3613/10000: L(Train): 0.3184870183467865; L(Test): 0.2996971905231476\n",
            "Epoch 3614/10000: L(Train): 0.3313792645931244; L(Test): 0.29909634590148926\n",
            "Epoch 3615/10000: L(Train): 0.32816392183303833; L(Test): 0.29983335733413696\n",
            "Epoch 3616/10000: L(Train): 0.32557255029678345; L(Test): 0.30005785822868347\n",
            "Epoch 3617/10000: L(Train): 0.3283054530620575; L(Test): 0.3002289831638336\n",
            "Epoch 3618/10000: L(Train): 0.32747170329093933; L(Test): 0.3005867600440979\n",
            "Epoch 3619/10000: L(Train): 0.3272255063056946; L(Test): 0.29998841881752014\n",
            "Epoch 3620/10000: L(Train): 0.31894028186798096; L(Test): 0.2995740473270416\n",
            "Epoch 3621/10000: L(Train): 0.322338342666626; L(Test): 0.29973870515823364\n",
            "Epoch 3622/10000: L(Train): 0.31901493668556213; L(Test): 0.30016839504241943\n",
            "Epoch 3623/10000: L(Train): 0.31988075375556946; L(Test): 0.30024832487106323\n",
            "Epoch 3624/10000: L(Train): 0.3273261785507202; L(Test): 0.299993097782135\n",
            "Epoch 3625/10000: L(Train): 0.32180264592170715; L(Test): 0.29893583059310913\n",
            "Epoch 3626/10000: L(Train): 0.32706090807914734; L(Test): 0.2982809841632843\n",
            "Epoch 3627/10000: L(Train): 0.3224444091320038; L(Test): 0.2992790937423706\n",
            "Epoch 3628/10000: L(Train): 0.3198254406452179; L(Test): 0.2994837462902069\n",
            "Epoch 3629/10000: L(Train): 0.32942453026771545; L(Test): 0.29972100257873535\n",
            "Epoch 3630/10000: L(Train): 0.31446948647499084; L(Test): 0.3011130094528198\n",
            "Epoch 3631/10000: L(Train): 0.3287690281867981; L(Test): 0.30190443992614746\n",
            "Epoch 3632/10000: L(Train): 0.3208180367946625; L(Test): 0.3005705773830414\n",
            "Epoch 3633/10000: L(Train): 0.3321569263935089; L(Test): 0.30085086822509766\n",
            "Epoch 3634/10000: L(Train): 0.31713584065437317; L(Test): 0.30102065205574036\n",
            "Epoch 3635/10000: L(Train): 0.327730268239975; L(Test): 0.29952681064605713\n",
            "Epoch 3636/10000: L(Train): 0.3142397105693817; L(Test): 0.3012503683567047\n",
            "Epoch 3637/10000: L(Train): 0.3141755163669586; L(Test): 0.30246517062187195\n",
            "Epoch 3638/10000: L(Train): 0.3250906765460968; L(Test): 0.30214762687683105\n",
            "Epoch 3639/10000: L(Train): 0.31343019008636475; L(Test): 0.30278921127319336\n",
            "Epoch 3640/10000: L(Train): 0.3277870714664459; L(Test): 0.3007027804851532\n",
            "Epoch 3641/10000: L(Train): 0.3250979483127594; L(Test): 0.3016413748264313\n",
            "Epoch 3642/10000: L(Train): 0.3229726254940033; L(Test): 0.3014383614063263\n",
            "Epoch 3643/10000: L(Train): 0.33034443855285645; L(Test): 0.3011850416660309\n",
            "Epoch 3644/10000: L(Train): 0.3225092589855194; L(Test): 0.30258801579475403\n",
            "Epoch 3645/10000: L(Train): 0.3159143626689911; L(Test): 0.3023020029067993\n",
            "Epoch 3646/10000: L(Train): 0.326376348733902; L(Test): 0.300748348236084\n",
            "Epoch 3647/10000: L(Train): 0.3292950689792633; L(Test): 0.30081233382225037\n",
            "Epoch 3648/10000: L(Train): 0.3318951725959778; L(Test): 0.3009263277053833\n",
            "Epoch 3649/10000: L(Train): 0.3248023986816406; L(Test): 0.3008405864238739\n",
            "Epoch 3650/10000: L(Train): 0.3210156261920929; L(Test): 0.3018704354763031\n",
            "Epoch 3651/10000: L(Train): 0.3237982392311096; L(Test): 0.30181485414505005\n",
            "Epoch 3652/10000: L(Train): 0.33367547392845154; L(Test): 0.3005374073982239\n",
            "Epoch 3653/10000: L(Train): 0.3156953752040863; L(Test): 0.30146417021751404\n",
            "Epoch 3654/10000: L(Train): 0.3294188380241394; L(Test): 0.3013068735599518\n",
            "Epoch 3655/10000: L(Train): 0.3260831832885742; L(Test): 0.30094194412231445\n",
            "Epoch 3656/10000: L(Train): 0.32086512446403503; L(Test): 0.3016566038131714\n",
            "Epoch 3657/10000: L(Train): 0.32955795526504517; L(Test): 0.30138349533081055\n",
            "Epoch 3658/10000: L(Train): 0.3224875032901764; L(Test): 0.30049416422843933\n",
            "Epoch 3659/10000: L(Train): 0.32934877276420593; L(Test): 0.3001084327697754\n",
            "Epoch 3660/10000: L(Train): 0.3264262080192566; L(Test): 0.30038881301879883\n",
            "Epoch 3661/10000: L(Train): 0.3216875493526459; L(Test): 0.3010910749435425\n",
            "Epoch 3662/10000: L(Train): 0.3288026750087738; L(Test): 0.30040207505226135\n",
            "Epoch 3663/10000: L(Train): 0.3276614248752594; L(Test): 0.2995195686817169\n",
            "Epoch 3664/10000: L(Train): 0.3288082778453827; L(Test): 0.30072665214538574\n",
            "Epoch 3665/10000: L(Train): 0.32327035069465637; L(Test): 0.3015662729740143\n",
            "Epoch 3666/10000: L(Train): 0.33055517077445984; L(Test): 0.3005233407020569\n",
            "Epoch 3667/10000: L(Train): 0.32342344522476196; L(Test): 0.3016960024833679\n",
            "Epoch 3668/10000: L(Train): 0.3299276530742645; L(Test): 0.3014160394668579\n",
            "Epoch 3669/10000: L(Train): 0.32796114683151245; L(Test): 0.2998753488063812\n",
            "Epoch 3670/10000: L(Train): 0.3235630691051483; L(Test): 0.30093419551849365\n",
            "Epoch 3671/10000: L(Train): 0.32104817032814026; L(Test): 0.2997775375843048\n",
            "Epoch 3672/10000: L(Train): 0.3211047947406769; L(Test): 0.3004627823829651\n",
            "Epoch 3673/10000: L(Train): 0.32730740308761597; L(Test): 0.30018946528434753\n",
            "Epoch 3674/10000: L(Train): 0.323560506105423; L(Test): 0.29973840713500977\n",
            "Epoch 3675/10000: L(Train): 0.3266131579875946; L(Test): 0.29944512248039246\n",
            "Epoch 3676/10000: L(Train): 0.3355499804019928; L(Test): 0.29893752932548523\n",
            "Epoch 3677/10000: L(Train): 0.3201485574245453; L(Test): 0.29946452379226685\n",
            "Epoch 3678/10000: L(Train): 0.32797664403915405; L(Test): 0.29963648319244385\n",
            "Epoch 3679/10000: L(Train): 0.3234717845916748; L(Test): 0.2997840642929077\n",
            "Epoch 3680/10000: L(Train): 0.3210579752922058; L(Test): 0.2996193766593933\n",
            "Epoch 3681/10000: L(Train): 0.32951346039772034; L(Test): 0.30013900995254517\n",
            "Epoch 3682/10000: L(Train): 0.3194027245044708; L(Test): 0.30054402351379395\n",
            "Epoch 3683/10000: L(Train): 0.32917532324790955; L(Test): 0.2998218536376953\n",
            "Epoch 3684/10000: L(Train): 0.31886720657348633; L(Test): 0.2996002733707428\n",
            "Epoch 3685/10000: L(Train): 0.3264561593532562; L(Test): 0.3003552556037903\n",
            "Epoch 3686/10000: L(Train): 0.31505051255226135; L(Test): 0.299310564994812\n",
            "Epoch 3687/10000: L(Train): 0.32530495524406433; L(Test): 0.30128371715545654\n",
            "Epoch 3688/10000: L(Train): 0.32568731904029846; L(Test): 0.30204153060913086\n",
            "Epoch 3689/10000: L(Train): 0.31919410824775696; L(Test): 0.3001764416694641\n",
            "Epoch 3690/10000: L(Train): 0.32863184809684753; L(Test): 0.3004837930202484\n",
            "Epoch 3691/10000: L(Train): 0.3253081142902374; L(Test): 0.3007277846336365\n",
            "Epoch 3692/10000: L(Train): 0.3261460065841675; L(Test): 0.30083325505256653\n",
            "Epoch 3693/10000: L(Train): 0.3249150514602661; L(Test): 0.3002280294895172\n",
            "Epoch 3694/10000: L(Train): 0.32828375697135925; L(Test): 0.3000432252883911\n",
            "Epoch 3695/10000: L(Train): 0.32877665758132935; L(Test): 0.29989001154899597\n",
            "Epoch 3696/10000: L(Train): 0.32054585218429565; L(Test): 0.30019932985305786\n",
            "Epoch 3697/10000: L(Train): 0.32748109102249146; L(Test): 0.30042707920074463\n",
            "Epoch 3698/10000: L(Train): 0.3329697251319885; L(Test): 0.3000260591506958\n",
            "Epoch 3699/10000: L(Train): 0.32450950145721436; L(Test): 0.2999536991119385\n",
            "Epoch 3700/10000: L(Train): 0.32008251547813416; L(Test): 0.3006231486797333\n",
            "Epoch 3701/10000: L(Train): 0.31600895524024963; L(Test): 0.3009204566478729\n",
            "Epoch 3702/10000: L(Train): 0.32538318634033203; L(Test): 0.3000575006008148\n",
            "Epoch 3703/10000: L(Train): 0.3241839110851288; L(Test): 0.3001468777656555\n",
            "Epoch 3704/10000: L(Train): 0.3111477196216583; L(Test): 0.3014404773712158\n",
            "Epoch 3705/10000: L(Train): 0.32666146755218506; L(Test): 0.30054771900177\n",
            "Epoch 3706/10000: L(Train): 0.3208903670310974; L(Test): 0.29943475127220154\n",
            "Epoch 3707/10000: L(Train): 0.3213222026824951; L(Test): 0.3005982041358948\n",
            "Epoch 3708/10000: L(Train): 0.3239375054836273; L(Test): 0.29983168840408325\n",
            "Epoch 3709/10000: L(Train): 0.3183381259441376; L(Test): 0.29978108406066895\n",
            "Epoch 3710/10000: L(Train): 0.32377326488494873; L(Test): 0.3008677363395691\n",
            "Epoch 3711/10000: L(Train): 0.32600849866867065; L(Test): 0.30047938227653503\n",
            "Epoch 3712/10000: L(Train): 0.32101351022720337; L(Test): 0.29979759454727173\n",
            "Epoch 3713/10000: L(Train): 0.3206552267074585; L(Test): 0.30011650919914246\n",
            "Epoch 3714/10000: L(Train): 0.3271375000476837; L(Test): 0.29950323700904846\n",
            "Epoch 3715/10000: L(Train): 0.32243436574935913; L(Test): 0.2990340292453766\n",
            "Epoch 3716/10000: L(Train): 0.32263627648353577; L(Test): 0.2997058033943176\n",
            "Epoch 3717/10000: L(Train): 0.31416451930999756; L(Test): 0.29955169558525085\n",
            "Epoch 3718/10000: L(Train): 0.3192041218280792; L(Test): 0.29894179105758667\n",
            "Epoch 3719/10000: L(Train): 0.3208671808242798; L(Test): 0.2992144823074341\n",
            "Epoch 3720/10000: L(Train): 0.31987324357032776; L(Test): 0.2994702160358429\n",
            "Epoch 3721/10000: L(Train): 0.3243706226348877; L(Test): 0.29906973242759705\n",
            "Epoch 3722/10000: L(Train): 0.32462602853775024; L(Test): 0.29963597655296326\n",
            "Epoch 3723/10000: L(Train): 0.32415470480918884; L(Test): 0.29940518736839294\n",
            "Epoch 3724/10000: L(Train): 0.3204539120197296; L(Test): 0.29941439628601074\n",
            "Epoch 3725/10000: L(Train): 0.3141317665576935; L(Test): 0.298994243144989\n",
            "Epoch 3726/10000: L(Train): 0.31917253136634827; L(Test): 0.2984980344772339\n",
            "Epoch 3727/10000: L(Train): 0.31511664390563965; L(Test): 0.2985689342021942\n",
            "Epoch 3728/10000: L(Train): 0.3192174434661865; L(Test): 0.3001461327075958\n",
            "Epoch 3729/10000: L(Train): 0.31518319249153137; L(Test): 0.29999226331710815\n",
            "Epoch 3730/10000: L(Train): 0.3183715343475342; L(Test): 0.2989424765110016\n",
            "Epoch 3731/10000: L(Train): 0.3235522508621216; L(Test): 0.2995280921459198\n",
            "Epoch 3732/10000: L(Train): 0.32644540071487427; L(Test): 0.29933279752731323\n",
            "Epoch 3733/10000: L(Train): 0.31350699067115784; L(Test): 0.2993090748786926\n",
            "Epoch 3734/10000: L(Train): 0.3253431022167206; L(Test): 0.3009589612483978\n",
            "Epoch 3735/10000: L(Train): 0.3240238130092621; L(Test): 0.3000110387802124\n",
            "Epoch 3736/10000: L(Train): 0.3239506185054779; L(Test): 0.29971379041671753\n",
            "Epoch 3737/10000: L(Train): 0.31861159205436707; L(Test): 0.2995104491710663\n",
            "Epoch 3738/10000: L(Train): 0.3223380744457245; L(Test): 0.29926592111587524\n",
            "Epoch 3739/10000: L(Train): 0.3294396996498108; L(Test): 0.3004433214664459\n",
            "Epoch 3740/10000: L(Train): 0.32813751697540283; L(Test): 0.30182376503944397\n",
            "Epoch 3741/10000: L(Train): 0.328182190656662; L(Test): 0.3001672625541687\n",
            "Epoch 3742/10000: L(Train): 0.315317302942276; L(Test): 0.29996299743652344\n",
            "Epoch 3743/10000: L(Train): 0.32526731491088867; L(Test): 0.2996620535850525\n",
            "Epoch 3744/10000: L(Train): 0.3207877576351166; L(Test): 0.29910361766815186\n",
            "Epoch 3745/10000: L(Train): 0.33040502667427063; L(Test): 0.2990482747554779\n",
            "Epoch 3746/10000: L(Train): 0.3136931359767914; L(Test): 0.2998417317867279\n",
            "Epoch 3747/10000: L(Train): 0.3194862902164459; L(Test): 0.3003097176551819\n",
            "Epoch 3748/10000: L(Train): 0.3142220675945282; L(Test): 0.2995716631412506\n",
            "Epoch 3749/10000: L(Train): 0.330950528383255; L(Test): 0.30045321583747864\n",
            "Epoch 3750/10000: L(Train): 0.32809776067733765; L(Test): 0.30052250623703003\n",
            "Epoch 3751/10000: L(Train): 0.32096096873283386; L(Test): 0.3014458119869232\n",
            "Epoch 3752/10000: L(Train): 0.3233957588672638; L(Test): 0.3004305064678192\n",
            "Epoch 3753/10000: L(Train): 0.32510337233543396; L(Test): 0.300081729888916\n",
            "Epoch 3754/10000: L(Train): 0.3275715410709381; L(Test): 0.30061957240104675\n",
            "Epoch 3755/10000: L(Train): 0.33548256754875183; L(Test): 0.3009456396102905\n",
            "Epoch 3756/10000: L(Train): 0.3173068165779114; L(Test): 0.30039912462234497\n",
            "Epoch 3757/10000: L(Train): 0.3297308385372162; L(Test): 0.30017685890197754\n",
            "Epoch 3758/10000: L(Train): 0.3277814984321594; L(Test): 0.30000928044319153\n",
            "Epoch 3759/10000: L(Train): 0.31649667024612427; L(Test): 0.3000999987125397\n",
            "Epoch 3760/10000: L(Train): 0.3273998498916626; L(Test): 0.2995685636997223\n",
            "Epoch 3761/10000: L(Train): 0.32032284140586853; L(Test): 0.29952678084373474\n",
            "Epoch 3762/10000: L(Train): 0.3209688365459442; L(Test): 0.29910531640052795\n",
            "Epoch 3763/10000: L(Train): 0.3241734504699707; L(Test): 0.2983703315258026\n",
            "Epoch 3764/10000: L(Train): 0.31587105989456177; L(Test): 0.2982710599899292\n",
            "Epoch 3765/10000: L(Train): 0.3220924437046051; L(Test): 0.29830724000930786\n",
            "Epoch 3766/10000: L(Train): 0.31352758407592773; L(Test): 0.2984864413738251\n",
            "Epoch 3767/10000: L(Train): 0.315931111574173; L(Test): 0.2982487380504608\n",
            "Epoch 3768/10000: L(Train): 0.3321702778339386; L(Test): 0.2977459728717804\n",
            "Epoch 3769/10000: L(Train): 0.3175860643386841; L(Test): 0.29826799035072327\n",
            "Epoch 3770/10000: L(Train): 0.3267045021057129; L(Test): 0.2984952926635742\n",
            "Epoch 3771/10000: L(Train): 0.3175439238548279; L(Test): 0.29858049750328064\n",
            "Epoch 3772/10000: L(Train): 0.3320458233356476; L(Test): 0.2988581359386444\n",
            "Epoch 3773/10000: L(Train): 0.32362568378448486; L(Test): 0.2980676293373108\n",
            "Epoch 3774/10000: L(Train): 0.3220556378364563; L(Test): 0.2985091209411621\n",
            "Epoch 3775/10000: L(Train): 0.3184707760810852; L(Test): 0.2986057698726654\n",
            "Epoch 3776/10000: L(Train): 0.32184717059135437; L(Test): 0.2976706027984619\n",
            "Epoch 3777/10000: L(Train): 0.32937759160995483; L(Test): 0.29751092195510864\n",
            "Epoch 3778/10000: L(Train): 0.32543042302131653; L(Test): 0.2979961633682251\n",
            "Epoch 3779/10000: L(Train): 0.31999117136001587; L(Test): 0.29840755462646484\n",
            "Epoch 3780/10000: L(Train): 0.3280104100704193; L(Test): 0.2987827658653259\n",
            "Epoch 3781/10000: L(Train): 0.32269486784935; L(Test): 0.29912710189819336\n",
            "Epoch 3782/10000: L(Train): 0.3273743689060211; L(Test): 0.29895198345184326\n",
            "Epoch 3783/10000: L(Train): 0.3273472189903259; L(Test): 0.29819250106811523\n",
            "Epoch 3784/10000: L(Train): 0.31640562415122986; L(Test): 0.2979494333267212\n",
            "Epoch 3785/10000: L(Train): 0.32397931814193726; L(Test): 0.29842904210090637\n",
            "Epoch 3786/10000: L(Train): 0.32114940881729126; L(Test): 0.2987443804740906\n",
            "Epoch 3787/10000: L(Train): 0.317947655916214; L(Test): 0.29851141571998596\n",
            "Epoch 3788/10000: L(Train): 0.3251604437828064; L(Test): 0.2980239987373352\n",
            "Epoch 3789/10000: L(Train): 0.32555800676345825; L(Test): 0.29860562086105347\n",
            "Epoch 3790/10000: L(Train): 0.31686753034591675; L(Test): 0.2987270951271057\n",
            "Epoch 3791/10000: L(Train): 0.32989826798439026; L(Test): 0.2990531623363495\n",
            "Epoch 3792/10000: L(Train): 0.3235911726951599; L(Test): 0.2979949414730072\n",
            "Epoch 3793/10000: L(Train): 0.3254946172237396; L(Test): 0.29813259840011597\n",
            "Epoch 3794/10000: L(Train): 0.3177178204059601; L(Test): 0.29959508776664734\n",
            "Epoch 3795/10000: L(Train): 0.32151535153388977; L(Test): 0.2987653911113739\n",
            "Epoch 3796/10000: L(Train): 0.3271760940551758; L(Test): 0.29891666769981384\n",
            "Epoch 3797/10000: L(Train): 0.320185124874115; L(Test): 0.2997920513153076\n",
            "Epoch 3798/10000: L(Train): 0.32729634642601013; L(Test): 0.3002024292945862\n",
            "Epoch 3799/10000: L(Train): 0.3146826922893524; L(Test): 0.3019692897796631\n",
            "Epoch 3800/10000: L(Train): 0.32567355036735535; L(Test): 0.3011781871318817\n",
            "Epoch 3801/10000: L(Train): 0.32442528009414673; L(Test): 0.29998213052749634\n",
            "Epoch 3802/10000: L(Train): 0.33387094736099243; L(Test): 0.29940521717071533\n",
            "Epoch 3803/10000: L(Train): 0.33149656653404236; L(Test): 0.2993279695510864\n",
            "Epoch 3804/10000: L(Train): 0.31505316495895386; L(Test): 0.2998425364494324\n",
            "Epoch 3805/10000: L(Train): 0.31937137246131897; L(Test): 0.30208247900009155\n",
            "Epoch 3806/10000: L(Train): 0.32389533519744873; L(Test): 0.30017364025115967\n",
            "Epoch 3807/10000: L(Train): 0.32173582911491394; L(Test): 0.3000545799732208\n",
            "Epoch 3808/10000: L(Train): 0.3225592076778412; L(Test): 0.3003830909729004\n",
            "Epoch 3809/10000: L(Train): 0.32356610894203186; L(Test): 0.29993730783462524\n",
            "Epoch 3810/10000: L(Train): 0.3167119026184082; L(Test): 0.3009801208972931\n",
            "Epoch 3811/10000: L(Train): 0.3244061768054962; L(Test): 0.3005680739879608\n",
            "Epoch 3812/10000: L(Train): 0.3214882016181946; L(Test): 0.3009609580039978\n",
            "Epoch 3813/10000: L(Train): 0.33078664541244507; L(Test): 0.3025244176387787\n",
            "Epoch 3814/10000: L(Train): 0.31912294030189514; L(Test): 0.30212152004241943\n",
            "Epoch 3815/10000: L(Train): 0.32337841391563416; L(Test): 0.30004167556762695\n",
            "Epoch 3816/10000: L(Train): 0.3139245808124542; L(Test): 0.30151045322418213\n",
            "Epoch 3817/10000: L(Train): 0.3300153911113739; L(Test): 0.30185186862945557\n",
            "Epoch 3818/10000: L(Train): 0.33321818709373474; L(Test): 0.3001844584941864\n",
            "Epoch 3819/10000: L(Train): 0.3208104074001312; L(Test): 0.30158278346061707\n",
            "Epoch 3820/10000: L(Train): 0.32332220673561096; L(Test): 0.3011045455932617\n",
            "Epoch 3821/10000: L(Train): 0.3322209417819977; L(Test): 0.30040085315704346\n",
            "Epoch 3822/10000: L(Train): 0.3306083083152771; L(Test): 0.29997292160987854\n",
            "Epoch 3823/10000: L(Train): 0.3266655206680298; L(Test): 0.30057671666145325\n",
            "Epoch 3824/10000: L(Train): 0.32674211263656616; L(Test): 0.30049067735671997\n",
            "Epoch 3825/10000: L(Train): 0.32866206765174866; L(Test): 0.3000718057155609\n",
            "Epoch 3826/10000: L(Train): 0.32575520873069763; L(Test): 0.30089864134788513\n",
            "Epoch 3827/10000: L(Train): 0.3240795135498047; L(Test): 0.3009015619754791\n",
            "Epoch 3828/10000: L(Train): 0.31423419713974; L(Test): 0.30083298683166504\n",
            "Epoch 3829/10000: L(Train): 0.33041834831237793; L(Test): 0.30182063579559326\n",
            "Epoch 3830/10000: L(Train): 0.32005199790000916; L(Test): 0.30237331986427307\n",
            "Epoch 3831/10000: L(Train): 0.32744476199150085; L(Test): 0.3017238974571228\n",
            "Epoch 3832/10000: L(Train): 0.31939783692359924; L(Test): 0.30225083231925964\n",
            "Epoch 3833/10000: L(Train): 0.3232593536376953; L(Test): 0.3016625642776489\n",
            "Epoch 3834/10000: L(Train): 0.3202453553676605; L(Test): 0.30033230781555176\n",
            "Epoch 3835/10000: L(Train): 0.31383249163627625; L(Test): 0.3009493350982666\n",
            "Epoch 3836/10000: L(Train): 0.3253970444202423; L(Test): 0.30122464895248413\n",
            "Epoch 3837/10000: L(Train): 0.3268297016620636; L(Test): 0.3003765940666199\n",
            "Epoch 3838/10000: L(Train): 0.32986587285995483; L(Test): 0.30071738362312317\n",
            "Epoch 3839/10000: L(Train): 0.31803208589553833; L(Test): 0.3015916645526886\n",
            "Epoch 3840/10000: L(Train): 0.3211369514465332; L(Test): 0.30118852853775024\n",
            "Epoch 3841/10000: L(Train): 0.3295869827270508; L(Test): 0.30035385489463806\n",
            "Epoch 3842/10000: L(Train): 0.3217894434928894; L(Test): 0.2999790608882904\n",
            "Epoch 3843/10000: L(Train): 0.3206637501716614; L(Test): 0.3022814691066742\n",
            "Epoch 3844/10000: L(Train): 0.3232956826686859; L(Test): 0.3017665445804596\n",
            "Epoch 3845/10000: L(Train): 0.33191075921058655; L(Test): 0.30006644129753113\n",
            "Epoch 3846/10000: L(Train): 0.3294166028499603; L(Test): 0.30128106474876404\n",
            "Epoch 3847/10000: L(Train): 0.32781651616096497; L(Test): 0.3009607493877411\n",
            "Epoch 3848/10000: L(Train): 0.33593887090682983; L(Test): 0.3010895252227783\n",
            "Epoch 3849/10000: L(Train): 0.3240528106689453; L(Test): 0.30138540267944336\n",
            "Epoch 3850/10000: L(Train): 0.32917213439941406; L(Test): 0.3006035089492798\n",
            "Epoch 3851/10000: L(Train): 0.33035367727279663; L(Test): 0.30109521746635437\n",
            "Epoch 3852/10000: L(Train): 0.31687018275260925; L(Test): 0.3013990819454193\n",
            "Epoch 3853/10000: L(Train): 0.32220980525016785; L(Test): 0.29994848370552063\n",
            "Epoch 3854/10000: L(Train): 0.31828412413597107; L(Test): 0.3006986379623413\n",
            "Epoch 3855/10000: L(Train): 0.324130654335022; L(Test): 0.3006533086299896\n",
            "Epoch 3856/10000: L(Train): 0.3156222105026245; L(Test): 0.29861533641815186\n",
            "Epoch 3857/10000: L(Train): 0.31723031401634216; L(Test): 0.2990463376045227\n",
            "Epoch 3858/10000: L(Train): 0.3206024765968323; L(Test): 0.2988876700401306\n",
            "Epoch 3859/10000: L(Train): 0.320085734128952; L(Test): 0.29840001463890076\n",
            "Epoch 3860/10000: L(Train): 0.3258959650993347; L(Test): 0.298598974943161\n",
            "Epoch 3861/10000: L(Train): 0.32410213351249695; L(Test): 0.2993725836277008\n",
            "Epoch 3862/10000: L(Train): 0.3173361122608185; L(Test): 0.29929688572883606\n",
            "Epoch 3863/10000: L(Train): 0.32078418135643005; L(Test): 0.2994272708892822\n",
            "Epoch 3864/10000: L(Train): 0.3226662278175354; L(Test): 0.3001038730144501\n",
            "Epoch 3865/10000: L(Train): 0.324088454246521; L(Test): 0.29950442910194397\n",
            "Epoch 3866/10000: L(Train): 0.32273927330970764; L(Test): 0.29878148436546326\n",
            "Epoch 3867/10000: L(Train): 0.3222145140171051; L(Test): 0.29913753271102905\n",
            "Epoch 3868/10000: L(Train): 0.3271597623825073; L(Test): 0.2987501919269562\n",
            "Epoch 3869/10000: L(Train): 0.32269829511642456; L(Test): 0.29921725392341614\n",
            "Epoch 3870/10000: L(Train): 0.33203473687171936; L(Test): 0.29995498061180115\n",
            "Epoch 3871/10000: L(Train): 0.3231832981109619; L(Test): 0.3002932071685791\n",
            "Epoch 3872/10000: L(Train): 0.3162776827812195; L(Test): 0.301370233297348\n",
            "Epoch 3873/10000: L(Train): 0.31987228989601135; L(Test): 0.3018417954444885\n",
            "Epoch 3874/10000: L(Train): 0.329606831073761; L(Test): 0.30016028881073\n",
            "Epoch 3875/10000: L(Train): 0.3165171444416046; L(Test): 0.3003814220428467\n",
            "Epoch 3876/10000: L(Train): 0.33974549174308777; L(Test): 0.3008604049682617\n",
            "Epoch 3877/10000: L(Train): 0.32562369108200073; L(Test): 0.299960732460022\n",
            "Epoch 3878/10000: L(Train): 0.3216474652290344; L(Test): 0.2999913990497589\n",
            "Epoch 3879/10000: L(Train): 0.3142191469669342; L(Test): 0.300144761800766\n",
            "Epoch 3880/10000: L(Train): 0.32449719309806824; L(Test): 0.2995225787162781\n",
            "Epoch 3881/10000: L(Train): 0.3271334767341614; L(Test): 0.2995631694793701\n",
            "Epoch 3882/10000: L(Train): 0.31853610277175903; L(Test): 0.30028894543647766\n",
            "Epoch 3883/10000: L(Train): 0.3276331126689911; L(Test): 0.3007659614086151\n",
            "Epoch 3884/10000: L(Train): 0.32088151574134827; L(Test): 0.3000342547893524\n",
            "Epoch 3885/10000: L(Train): 0.3202628791332245; L(Test): 0.2997744679450989\n",
            "Epoch 3886/10000: L(Train): 0.32631421089172363; L(Test): 0.29982101917266846\n",
            "Epoch 3887/10000: L(Train): 0.32061469554901123; L(Test): 0.30024245381355286\n",
            "Epoch 3888/10000: L(Train): 0.3220980167388916; L(Test): 0.3003509044647217\n",
            "Epoch 3889/10000: L(Train): 0.3164372146129608; L(Test): 0.2998606562614441\n",
            "Epoch 3890/10000: L(Train): 0.3198714554309845; L(Test): 0.2997690439224243\n",
            "Epoch 3891/10000: L(Train): 0.32285076379776; L(Test): 0.2999096214771271\n",
            "Epoch 3892/10000: L(Train): 0.3194340169429779; L(Test): 0.2998708486557007\n",
            "Epoch 3893/10000: L(Train): 0.32748204469680786; L(Test): 0.2999541759490967\n",
            "Epoch 3894/10000: L(Train): 0.3247811794281006; L(Test): 0.29977351427078247\n",
            "Epoch 3895/10000: L(Train): 0.3213922381401062; L(Test): 0.30020660161972046\n",
            "Epoch 3896/10000: L(Train): 0.3287041485309601; L(Test): 0.3010765314102173\n",
            "Epoch 3897/10000: L(Train): 0.3194945454597473; L(Test): 0.30052101612091064\n",
            "Epoch 3898/10000: L(Train): 0.3150595724582672; L(Test): 0.3001566529273987\n",
            "Epoch 3899/10000: L(Train): 0.3223651051521301; L(Test): 0.2988162338733673\n",
            "Epoch 3900/10000: L(Train): 0.3243003785610199; L(Test): 0.29919368028640747\n",
            "Epoch 3901/10000: L(Train): 0.3205682337284088; L(Test): 0.2990463972091675\n",
            "Epoch 3902/10000: L(Train): 0.3308789134025574; L(Test): 0.29909801483154297\n",
            "Epoch 3903/10000: L(Train): 0.32441446185112; L(Test): 0.2998501658439636\n",
            "Epoch 3904/10000: L(Train): 0.31567034125328064; L(Test): 0.2996864914894104\n",
            "Epoch 3905/10000: L(Train): 0.32373735308647156; L(Test): 0.2989959120750427\n",
            "Epoch 3906/10000: L(Train): 0.32129615545272827; L(Test): 0.2989993691444397\n",
            "Epoch 3907/10000: L(Train): 0.32601794600486755; L(Test): 0.29872483015060425\n",
            "Epoch 3908/10000: L(Train): 0.3222406804561615; L(Test): 0.2988443970680237\n",
            "Epoch 3909/10000: L(Train): 0.32606613636016846; L(Test): 0.29823505878448486\n",
            "Epoch 3910/10000: L(Train): 0.32010337710380554; L(Test): 0.2976023554801941\n",
            "Epoch 3911/10000: L(Train): 0.3249232769012451; L(Test): 0.2973765432834625\n",
            "Epoch 3912/10000: L(Train): 0.3188358247280121; L(Test): 0.2978191077709198\n",
            "Epoch 3913/10000: L(Train): 0.32681578397750854; L(Test): 0.29752689599990845\n",
            "Epoch 3914/10000: L(Train): 0.3199857771396637; L(Test): 0.29834648966789246\n",
            "Epoch 3915/10000: L(Train): 0.3161601424217224; L(Test): 0.298544317483902\n",
            "Epoch 3916/10000: L(Train): 0.3281867206096649; L(Test): 0.2987692952156067\n",
            "Epoch 3917/10000: L(Train): 0.32713401317596436; L(Test): 0.29825663566589355\n",
            "Epoch 3918/10000: L(Train): 0.31901025772094727; L(Test): 0.2989642322063446\n",
            "Epoch 3919/10000: L(Train): 0.3251764476299286; L(Test): 0.2990068793296814\n",
            "Epoch 3920/10000: L(Train): 0.3310413360595703; L(Test): 0.2984764575958252\n",
            "Epoch 3921/10000: L(Train): 0.3141009509563446; L(Test): 0.2989642322063446\n",
            "Epoch 3922/10000: L(Train): 0.3224770724773407; L(Test): 0.29824212193489075\n",
            "Epoch 3923/10000: L(Train): 0.3165188431739807; L(Test): 0.2991809546947479\n",
            "Epoch 3924/10000: L(Train): 0.3225267231464386; L(Test): 0.3002658486366272\n",
            "Epoch 3925/10000: L(Train): 0.3296176493167877; L(Test): 0.29916319251060486\n",
            "Epoch 3926/10000: L(Train): 0.31845033168792725; L(Test): 0.29998430609703064\n",
            "Epoch 3927/10000: L(Train): 0.3311126232147217; L(Test): 0.300035685300827\n",
            "Epoch 3928/10000: L(Train): 0.3220416307449341; L(Test): 0.30013132095336914\n",
            "Epoch 3929/10000: L(Train): 0.31475508213043213; L(Test): 0.30076828598976135\n",
            "Epoch 3930/10000: L(Train): 0.32566386461257935; L(Test): 0.3001646101474762\n",
            "Epoch 3931/10000: L(Train): 0.3281700611114502; L(Test): 0.2993820011615753\n",
            "Epoch 3932/10000: L(Train): 0.32812240719795227; L(Test): 0.30029168725013733\n",
            "Epoch 3933/10000: L(Train): 0.3212277591228485; L(Test): 0.29961150884628296\n",
            "Epoch 3934/10000: L(Train): 0.32842904329299927; L(Test): 0.29924944043159485\n",
            "Epoch 3935/10000: L(Train): 0.3266541063785553; L(Test): 0.3005485236644745\n",
            "Epoch 3936/10000: L(Train): 0.322593629360199; L(Test): 0.3010629117488861\n",
            "Epoch 3937/10000: L(Train): 0.3236362040042877; L(Test): 0.3000425398349762\n",
            "Epoch 3938/10000: L(Train): 0.3246771991252899; L(Test): 0.2996762990951538\n",
            "Epoch 3939/10000: L(Train): 0.32684361934661865; L(Test): 0.29954978823661804\n",
            "Epoch 3940/10000: L(Train): 0.31458359956741333; L(Test): 0.3004993200302124\n",
            "Epoch 3941/10000: L(Train): 0.32907038927078247; L(Test): 0.30038711428642273\n",
            "Epoch 3942/10000: L(Train): 0.3245147466659546; L(Test): 0.3000323474407196\n",
            "Epoch 3943/10000: L(Train): 0.32448065280914307; L(Test): 0.29969796538352966\n",
            "Epoch 3944/10000: L(Train): 0.32321661710739136; L(Test): 0.2993644177913666\n",
            "Epoch 3945/10000: L(Train): 0.3421221971511841; L(Test): 0.2999154031276703\n",
            "Epoch 3946/10000: L(Train): 0.32114893198013306; L(Test): 0.30007147789001465\n",
            "Epoch 3947/10000: L(Train): 0.32036107778549194; L(Test): 0.3007650673389435\n",
            "Epoch 3948/10000: L(Train): 0.32393306493759155; L(Test): 0.3004648685455322\n",
            "Epoch 3949/10000: L(Train): 0.32148420810699463; L(Test): 0.2980770468711853\n",
            "Epoch 3950/10000: L(Train): 0.31213921308517456; L(Test): 0.2990034818649292\n",
            "Epoch 3951/10000: L(Train): 0.3292638659477234; L(Test): 0.2995360493659973\n",
            "Epoch 3952/10000: L(Train): 0.3219870328903198; L(Test): 0.29855912923812866\n",
            "Epoch 3953/10000: L(Train): 0.31761205196380615; L(Test): 0.29904669523239136\n",
            "Epoch 3954/10000: L(Train): 0.3329535722732544; L(Test): 0.29910188913345337\n",
            "Epoch 3955/10000: L(Train): 0.33006277680397034; L(Test): 0.2975321412086487\n",
            "Epoch 3956/10000: L(Train): 0.3099319338798523; L(Test): 0.298728346824646\n",
            "Epoch 3957/10000: L(Train): 0.31669744849205017; L(Test): 0.2997661530971527\n",
            "Epoch 3958/10000: L(Train): 0.3250540792942047; L(Test): 0.29887664318084717\n",
            "Epoch 3959/10000: L(Train): 0.3251059055328369; L(Test): 0.30129870772361755\n",
            "Epoch 3960/10000: L(Train): 0.31828904151916504; L(Test): 0.299714595079422\n",
            "Epoch 3961/10000: L(Train): 0.3256501257419586; L(Test): 0.2995162904262543\n",
            "Epoch 3962/10000: L(Train): 0.3246862292289734; L(Test): 0.3003382384777069\n",
            "Epoch 3963/10000: L(Train): 0.32289597392082214; L(Test): 0.29888200759887695\n",
            "Epoch 3964/10000: L(Train): 0.3228110671043396; L(Test): 0.2999216914176941\n",
            "Epoch 3965/10000: L(Train): 0.31938162446022034; L(Test): 0.3009445369243622\n",
            "Epoch 3966/10000: L(Train): 0.3316199779510498; L(Test): 0.29998379945755005\n",
            "Epoch 3967/10000: L(Train): 0.3173665404319763; L(Test): 0.29970434308052063\n",
            "Epoch 3968/10000: L(Train): 0.3242914378643036; L(Test): 0.29932400584220886\n",
            "Epoch 3969/10000: L(Train): 0.3228846490383148; L(Test): 0.2995441257953644\n",
            "Epoch 3970/10000: L(Train): 0.3239615857601166; L(Test): 0.3001890778541565\n",
            "Epoch 3971/10000: L(Train): 0.3294239938259125; L(Test): 0.29911619424819946\n",
            "Epoch 3972/10000: L(Train): 0.32434219121932983; L(Test): 0.29960501194000244\n",
            "Epoch 3973/10000: L(Train): 0.3289649188518524; L(Test): 0.29950571060180664\n",
            "Epoch 3974/10000: L(Train): 0.32826879620552063; L(Test): 0.2992256283760071\n",
            "Epoch 3975/10000: L(Train): 0.3260321617126465; L(Test): 0.2990354895591736\n",
            "Epoch 3976/10000: L(Train): 0.3126102387905121; L(Test): 0.29917559027671814\n",
            "Epoch 3977/10000: L(Train): 0.3155902028083801; L(Test): 0.2994912564754486\n",
            "Epoch 3978/10000: L(Train): 0.31543609499931335; L(Test): 0.2994610667228699\n",
            "Epoch 3979/10000: L(Train): 0.3210148513317108; L(Test): 0.29914700984954834\n",
            "Epoch 3980/10000: L(Train): 0.32531359791755676; L(Test): 0.3002626597881317\n",
            "Epoch 3981/10000: L(Train): 0.32640182971954346; L(Test): 0.3008289039134979\n",
            "Epoch 3982/10000: L(Train): 0.32387876510620117; L(Test): 0.29990094900131226\n",
            "Epoch 3983/10000: L(Train): 0.3268308639526367; L(Test): 0.29912349581718445\n",
            "Epoch 3984/10000: L(Train): 0.3237305283546448; L(Test): 0.3006149232387543\n",
            "Epoch 3985/10000: L(Train): 0.3247341811656952; L(Test): 0.3013666868209839\n",
            "Epoch 3986/10000: L(Train): 0.3240765631198883; L(Test): 0.3007167875766754\n",
            "Epoch 3987/10000: L(Train): 0.32611843943595886; L(Test): 0.30070337653160095\n",
            "Epoch 3988/10000: L(Train): 0.33159691095352173; L(Test): 0.299548864364624\n",
            "Epoch 3989/10000: L(Train): 0.3273961544036865; L(Test): 0.29922884702682495\n",
            "Epoch 3990/10000: L(Train): 0.32554152607917786; L(Test): 0.30049750208854675\n",
            "Epoch 3991/10000: L(Train): 0.32074010372161865; L(Test): 0.30051472783088684\n",
            "Epoch 3992/10000: L(Train): 0.3260740339756012; L(Test): 0.30032140016555786\n",
            "Epoch 3993/10000: L(Train): 0.32444828748703003; L(Test): 0.29949212074279785\n",
            "Epoch 3994/10000: L(Train): 0.3288287818431854; L(Test): 0.2997649610042572\n",
            "Epoch 3995/10000: L(Train): 0.3201528787612915; L(Test): 0.30057963728904724\n",
            "Epoch 3996/10000: L(Train): 0.31658312678337097; L(Test): 0.3005087375640869\n",
            "Epoch 3997/10000: L(Train): 0.34061846137046814; L(Test): 0.3006553053855896\n",
            "Epoch 3998/10000: L(Train): 0.3257702887058258; L(Test): 0.3009268045425415\n",
            "Epoch 3999/10000: L(Train): 0.3269829750061035; L(Test): 0.3009914457798004\n",
            "Epoch 4000/10000: L(Train): 0.3230465352535248; L(Test): 0.30186891555786133\n",
            "Epoch 4001/10000: L(Train): 0.3236604332923889; L(Test): 0.3030563294887543\n",
            "Epoch 4002/10000: L(Train): 0.3212435245513916; L(Test): 0.3026912212371826\n",
            "Epoch 4003/10000: L(Train): 0.3345828354358673; L(Test): 0.3008071184158325\n",
            "Epoch 4004/10000: L(Train): 0.3288370370864868; L(Test): 0.3014053404331207\n",
            "Epoch 4005/10000: L(Train): 0.3253614902496338; L(Test): 0.3032943606376648\n",
            "Epoch 4006/10000: L(Train): 0.33067113161087036; L(Test): 0.3026832044124603\n",
            "Epoch 4007/10000: L(Train): 0.32319167256355286; L(Test): 0.3004988729953766\n",
            "Epoch 4008/10000: L(Train): 0.32205909490585327; L(Test): 0.3013436496257782\n",
            "Epoch 4009/10000: L(Train): 0.32214874029159546; L(Test): 0.3015473484992981\n",
            "Epoch 4010/10000: L(Train): 0.3244840204715729; L(Test): 0.3011070787906647\n",
            "Epoch 4011/10000: L(Train): 0.3287345767021179; L(Test): 0.30151835083961487\n",
            "Epoch 4012/10000: L(Train): 0.3225181996822357; L(Test): 0.3027895390987396\n",
            "Epoch 4013/10000: L(Train): 0.3243940770626068; L(Test): 0.30273565649986267\n",
            "Epoch 4014/10000: L(Train): 0.3329605758190155; L(Test): 0.3012714982032776\n",
            "Epoch 4015/10000: L(Train): 0.32056924700737; L(Test): 0.3009284436702728\n",
            "Epoch 4016/10000: L(Train): 0.31633278727531433; L(Test): 0.30102717876434326\n",
            "Epoch 4017/10000: L(Train): 0.32084032893180847; L(Test): 0.30087265372276306\n",
            "Epoch 4018/10000: L(Train): 0.33035916090011597; L(Test): 0.3010258078575134\n",
            "Epoch 4019/10000: L(Train): 0.3247359097003937; L(Test): 0.3022071421146393\n",
            "Epoch 4020/10000: L(Train): 0.3230670690536499; L(Test): 0.3037281334400177\n",
            "Epoch 4021/10000: L(Train): 0.3181573450565338; L(Test): 0.303270161151886\n",
            "Epoch 4022/10000: L(Train): 0.3210738003253937; L(Test): 0.30117693543434143\n",
            "Epoch 4023/10000: L(Train): 0.31314846873283386; L(Test): 0.3011073172092438\n",
            "Epoch 4024/10000: L(Train): 0.3311008810997009; L(Test): 0.3013775646686554\n",
            "Epoch 4025/10000: L(Train): 0.32520389556884766; L(Test): 0.30172473192214966\n",
            "Epoch 4026/10000: L(Train): 0.33075815439224243; L(Test): 0.30063197016716003\n",
            "Epoch 4027/10000: L(Train): 0.3160589933395386; L(Test): 0.30187326669692993\n",
            "Epoch 4028/10000: L(Train): 0.3242358863353729; L(Test): 0.3023930490016937\n",
            "Epoch 4029/10000: L(Train): 0.3283851146697998; L(Test): 0.3005658984184265\n",
            "Epoch 4030/10000: L(Train): 0.3302070200443268; L(Test): 0.30022740364074707\n",
            "Epoch 4031/10000: L(Train): 0.32242631912231445; L(Test): 0.3019651472568512\n",
            "Epoch 4032/10000: L(Train): 0.32504549622535706; L(Test): 0.3029043674468994\n",
            "Epoch 4033/10000: L(Train): 0.33245334029197693; L(Test): 0.30094656348228455\n",
            "Epoch 4034/10000: L(Train): 0.3191515803337097; L(Test): 0.30139756202697754\n",
            "Epoch 4035/10000: L(Train): 0.3191240429878235; L(Test): 0.3019024133682251\n",
            "Epoch 4036/10000: L(Train): 0.3252328336238861; L(Test): 0.30031025409698486\n",
            "Epoch 4037/10000: L(Train): 0.3155783414840698; L(Test): 0.29950225353240967\n",
            "Epoch 4038/10000: L(Train): 0.3265993893146515; L(Test): 0.299477219581604\n",
            "Epoch 4039/10000: L(Train): 0.3211691677570343; L(Test): 0.2998293936252594\n",
            "Epoch 4040/10000: L(Train): 0.32200196385383606; L(Test): 0.3006215989589691\n",
            "Epoch 4041/10000: L(Train): 0.32409578561782837; L(Test): 0.3032136857509613\n",
            "Epoch 4042/10000: L(Train): 0.32652732729911804; L(Test): 0.3062582314014435\n",
            "Epoch 4043/10000: L(Train): 0.3360345959663391; L(Test): 0.30946147441864014\n",
            "Epoch 4044/10000: L(Train): 0.33471590280532837; L(Test): 0.30788230895996094\n",
            "Epoch 4045/10000: L(Train): 0.3316757082939148; L(Test): 0.3049205541610718\n",
            "Epoch 4046/10000: L(Train): 0.3308650553226471; L(Test): 0.3088659942150116\n",
            "Epoch 4047/10000: L(Train): 0.3318331241607666; L(Test): 0.3071795701980591\n",
            "Epoch 4048/10000: L(Train): 0.32235318422317505; L(Test): 0.3052224814891815\n",
            "Epoch 4049/10000: L(Train): 0.33468830585479736; L(Test): 0.3104405701160431\n",
            "Epoch 4050/10000: L(Train): 0.3225843608379364; L(Test): 0.31082820892333984\n",
            "Epoch 4051/10000: L(Train): 0.32262736558914185; L(Test): 0.3069688379764557\n",
            "Epoch 4052/10000: L(Train): 0.3302333354949951; L(Test): 0.30676454305648804\n",
            "Epoch 4053/10000: L(Train): 0.32632988691329956; L(Test): 0.30679237842559814\n",
            "Epoch 4054/10000: L(Train): 0.327859103679657; L(Test): 0.30540916323661804\n",
            "Epoch 4055/10000: L(Train): 0.3273452818393707; L(Test): 0.30567118525505066\n",
            "Epoch 4056/10000: L(Train): 0.32738417387008667; L(Test): 0.30515941977500916\n",
            "Epoch 4057/10000: L(Train): 0.32532504200935364; L(Test): 0.30356842279434204\n",
            "Epoch 4058/10000: L(Train): 0.3223121166229248; L(Test): 0.30354687571525574\n",
            "Epoch 4059/10000: L(Train): 0.32562175393104553; L(Test): 0.3042544424533844\n",
            "Epoch 4060/10000: L(Train): 0.32550257444381714; L(Test): 0.3038468360900879\n",
            "Epoch 4061/10000: L(Train): 0.3303380310535431; L(Test): 0.30330196022987366\n",
            "Epoch 4062/10000: L(Train): 0.32853174209594727; L(Test): 0.3047291338443756\n",
            "Epoch 4063/10000: L(Train): 0.32884708046913147; L(Test): 0.30314522981643677\n",
            "Epoch 4064/10000: L(Train): 0.3223629593849182; L(Test): 0.3010658621788025\n",
            "Epoch 4065/10000: L(Train): 0.3222329914569855; L(Test): 0.3033233880996704\n",
            "Epoch 4066/10000: L(Train): 0.32923659682273865; L(Test): 0.30328497290611267\n",
            "Epoch 4067/10000: L(Train): 0.3286854028701782; L(Test): 0.3004031777381897\n",
            "Epoch 4068/10000: L(Train): 0.3206138014793396; L(Test): 0.3025851547718048\n",
            "Epoch 4069/10000: L(Train): 0.33403217792510986; L(Test): 0.3032274544239044\n",
            "Epoch 4070/10000: L(Train): 0.32529184222221375; L(Test): 0.3015454113483429\n",
            "Epoch 4071/10000: L(Train): 0.3122390806674957; L(Test): 0.30287617444992065\n",
            "Epoch 4072/10000: L(Train): 0.32498112320899963; L(Test): 0.30261337757110596\n",
            "Epoch 4073/10000: L(Train): 0.3292313814163208; L(Test): 0.3017742335796356\n",
            "Epoch 4074/10000: L(Train): 0.3273802399635315; L(Test): 0.3015104830265045\n",
            "Epoch 4075/10000: L(Train): 0.3264288306236267; L(Test): 0.3000869154930115\n",
            "Epoch 4076/10000: L(Train): 0.32650667428970337; L(Test): 0.30179786682128906\n",
            "Epoch 4077/10000: L(Train): 0.3253171443939209; L(Test): 0.3007867932319641\n",
            "Epoch 4078/10000: L(Train): 0.3267998695373535; L(Test): 0.29995059967041016\n",
            "Epoch 4079/10000: L(Train): 0.3228525221347809; L(Test): 0.3006855845451355\n",
            "Epoch 4080/10000: L(Train): 0.32850921154022217; L(Test): 0.30082032084465027\n",
            "Epoch 4081/10000: L(Train): 0.3363947868347168; L(Test): 0.2998637557029724\n",
            "Epoch 4082/10000: L(Train): 0.32369107007980347; L(Test): 0.30006155371665955\n",
            "Epoch 4083/10000: L(Train): 0.3256215751171112; L(Test): 0.30008935928344727\n",
            "Epoch 4084/10000: L(Train): 0.32597777247428894; L(Test): 0.2997640371322632\n",
            "Epoch 4085/10000: L(Train): 0.3220974802970886; L(Test): 0.30070436000823975\n",
            "Epoch 4086/10000: L(Train): 0.33408844470977783; L(Test): 0.30057111382484436\n",
            "Epoch 4087/10000: L(Train): 0.33139529824256897; L(Test): 0.29998400807380676\n",
            "Epoch 4088/10000: L(Train): 0.3221406936645508; L(Test): 0.2992495000362396\n",
            "Epoch 4089/10000: L(Train): 0.3246622383594513; L(Test): 0.2992917597293854\n",
            "Epoch 4090/10000: L(Train): 0.32116562128067017; L(Test): 0.29988622665405273\n",
            "Epoch 4091/10000: L(Train): 0.3169649541378021; L(Test): 0.30014485120773315\n",
            "Epoch 4092/10000: L(Train): 0.3206969201564789; L(Test): 0.2993601858615875\n",
            "Epoch 4093/10000: L(Train): 0.32280707359313965; L(Test): 0.2986801266670227\n",
            "Epoch 4094/10000: L(Train): 0.3259766697883606; L(Test): 0.3000590205192566\n",
            "Epoch 4095/10000: L(Train): 0.3233911097049713; L(Test): 0.29945993423461914\n",
            "Epoch 4096/10000: L(Train): 0.3335306644439697; L(Test): 0.298672616481781\n",
            "Epoch 4097/10000: L(Train): 0.31738394498825073; L(Test): 0.30026447772979736\n",
            "Epoch 4098/10000: L(Train): 0.32592085003852844; L(Test): 0.2996358573436737\n",
            "Epoch 4099/10000: L(Train): 0.3205861747264862; L(Test): 0.29861944913864136\n",
            "Epoch 4100/10000: L(Train): 0.32383108139038086; L(Test): 0.29858964681625366\n",
            "Epoch 4101/10000: L(Train): 0.3360897898674011; L(Test): 0.2989862859249115\n",
            "Epoch 4102/10000: L(Train): 0.32605087757110596; L(Test): 0.29995837807655334\n",
            "Epoch 4103/10000: L(Train): 0.32381027936935425; L(Test): 0.298869252204895\n",
            "Epoch 4104/10000: L(Train): 0.32065510749816895; L(Test): 0.29907113313674927\n",
            "Epoch 4105/10000: L(Train): 0.32420027256011963; L(Test): 0.29953533411026\n",
            "Epoch 4106/10000: L(Train): 0.3317941427230835; L(Test): 0.2990729510784149\n",
            "Epoch 4107/10000: L(Train): 0.33163949847221375; L(Test): 0.29878926277160645\n",
            "Epoch 4108/10000: L(Train): 0.32762211561203003; L(Test): 0.3000616431236267\n",
            "Epoch 4109/10000: L(Train): 0.32561779022216797; L(Test): 0.3002738058567047\n",
            "Epoch 4110/10000: L(Train): 0.3165881931781769; L(Test): 0.2995522916316986\n",
            "Epoch 4111/10000: L(Train): 0.31693029403686523; L(Test): 0.2995474338531494\n",
            "Epoch 4112/10000: L(Train): 0.3225077986717224; L(Test): 0.2995360195636749\n",
            "Epoch 4113/10000: L(Train): 0.31561926007270813; L(Test): 0.29984933137893677\n",
            "Epoch 4114/10000: L(Train): 0.3249537944793701; L(Test): 0.3000132441520691\n",
            "Epoch 4115/10000: L(Train): 0.32187068462371826; L(Test): 0.29994893074035645\n",
            "Epoch 4116/10000: L(Train): 0.3254929482936859; L(Test): 0.2999444603919983\n",
            "Epoch 4117/10000: L(Train): 0.31380563974380493; L(Test): 0.3000020384788513\n",
            "Epoch 4118/10000: L(Train): 0.3315722644329071; L(Test): 0.3007831573486328\n",
            "Epoch 4119/10000: L(Train): 0.31485608220100403; L(Test): 0.2990555763244629\n",
            "Epoch 4120/10000: L(Train): 0.31401365995407104; L(Test): 0.30085432529449463\n",
            "Epoch 4121/10000: L(Train): 0.3180656135082245; L(Test): 0.300458163022995\n",
            "Epoch 4122/10000: L(Train): 0.3186967670917511; L(Test): 0.30022314190864563\n",
            "Epoch 4123/10000: L(Train): 0.32289132475852966; L(Test): 0.2997186779975891\n",
            "Epoch 4124/10000: L(Train): 0.3186189532279968; L(Test): 0.29982611536979675\n",
            "Epoch 4125/10000: L(Train): 0.31279295682907104; L(Test): 0.3013439476490021\n",
            "Epoch 4126/10000: L(Train): 0.3195410370826721; L(Test): 0.3012084662914276\n",
            "Epoch 4127/10000: L(Train): 0.32818037271499634; L(Test): 0.2999495267868042\n",
            "Epoch 4128/10000: L(Train): 0.32079148292541504; L(Test): 0.2992747724056244\n",
            "Epoch 4129/10000: L(Train): 0.32905682921409607; L(Test): 0.2997761070728302\n",
            "Epoch 4130/10000: L(Train): 0.32381656765937805; L(Test): 0.30095604062080383\n",
            "Epoch 4131/10000: L(Train): 0.32541701197624207; L(Test): 0.30028143525123596\n",
            "Epoch 4132/10000: L(Train): 0.32099196314811707; L(Test): 0.29935869574546814\n",
            "Epoch 4133/10000: L(Train): 0.33102867007255554; L(Test): 0.29938241839408875\n",
            "Epoch 4134/10000: L(Train): 0.32191744446754456; L(Test): 0.29988744854927063\n",
            "Epoch 4135/10000: L(Train): 0.3191837966442108; L(Test): 0.29959553480148315\n",
            "Epoch 4136/10000: L(Train): 0.3243373930454254; L(Test): 0.30066215991973877\n",
            "Epoch 4137/10000: L(Train): 0.32395264506340027; L(Test): 0.3007281720638275\n",
            "Epoch 4138/10000: L(Train): 0.32576984167099; L(Test): 0.2997840642929077\n",
            "Epoch 4139/10000: L(Train): 0.3171791136264801; L(Test): 0.299521803855896\n",
            "Epoch 4140/10000: L(Train): 0.32672053575515747; L(Test): 0.29917049407958984\n",
            "Epoch 4141/10000: L(Train): 0.3227863311767578; L(Test): 0.2995671331882477\n",
            "Epoch 4142/10000: L(Train): 0.32375049591064453; L(Test): 0.2999272346496582\n",
            "Epoch 4143/10000: L(Train): 0.3173876106739044; L(Test): 0.30070048570632935\n",
            "Epoch 4144/10000: L(Train): 0.3202757239341736; L(Test): 0.3005616068840027\n",
            "Epoch 4145/10000: L(Train): 0.32550039887428284; L(Test): 0.2986707389354706\n",
            "Epoch 4146/10000: L(Train): 0.31685227155685425; L(Test): 0.2984315752983093\n",
            "Epoch 4147/10000: L(Train): 0.3248326778411865; L(Test): 0.29878678917884827\n",
            "Epoch 4148/10000: L(Train): 0.3247184753417969; L(Test): 0.29844924807548523\n",
            "Epoch 4149/10000: L(Train): 0.3185420036315918; L(Test): 0.29858317971229553\n",
            "Epoch 4150/10000: L(Train): 0.32164761424064636; L(Test): 0.2992391288280487\n",
            "Epoch 4151/10000: L(Train): 0.32441622018814087; L(Test): 0.29930201172828674\n",
            "Epoch 4152/10000: L(Train): 0.32922688126564026; L(Test): 0.299456924200058\n",
            "Epoch 4153/10000: L(Train): 0.326501727104187; L(Test): 0.29927411675453186\n",
            "Epoch 4154/10000: L(Train): 0.3264806866645813; L(Test): 0.2985507547855377\n",
            "Epoch 4155/10000: L(Train): 0.3259926438331604; L(Test): 0.2986632287502289\n",
            "Epoch 4156/10000: L(Train): 0.32403555512428284; L(Test): 0.29888930916786194\n",
            "Epoch 4157/10000: L(Train): 0.3216615617275238; L(Test): 0.29810842871665955\n",
            "Epoch 4158/10000: L(Train): 0.3261234760284424; L(Test): 0.29924798011779785\n",
            "Epoch 4159/10000: L(Train): 0.32713982462882996; L(Test): 0.3001067638397217\n",
            "Epoch 4160/10000: L(Train): 0.3153740465641022; L(Test): 0.3009740114212036\n",
            "Epoch 4161/10000: L(Train): 0.32808977365493774; L(Test): 0.3016403913497925\n",
            "Epoch 4162/10000: L(Train): 0.3180234730243683; L(Test): 0.3014942705631256\n",
            "Epoch 4163/10000: L(Train): 0.32301065325737; L(Test): 0.3009454905986786\n",
            "Epoch 4164/10000: L(Train): 0.31967616081237793; L(Test): 0.3014543652534485\n",
            "Epoch 4165/10000: L(Train): 0.32579678297042847; L(Test): 0.30021026730537415\n",
            "Epoch 4166/10000: L(Train): 0.325029581785202; L(Test): 0.30181393027305603\n",
            "Epoch 4167/10000: L(Train): 0.3257473409175873; L(Test): 0.3028600513935089\n",
            "Epoch 4168/10000: L(Train): 0.318737268447876; L(Test): 0.3009243607521057\n",
            "Epoch 4169/10000: L(Train): 0.31956422328948975; L(Test): 0.2999565005302429\n",
            "Epoch 4170/10000: L(Train): 0.32223445177078247; L(Test): 0.3004242479801178\n",
            "Epoch 4171/10000: L(Train): 0.32726389169692993; L(Test): 0.3010588586330414\n",
            "Epoch 4172/10000: L(Train): 0.3225501775741577; L(Test): 0.30071836709976196\n",
            "Epoch 4173/10000: L(Train): 0.32245680689811707; L(Test): 0.30084091424942017\n",
            "Epoch 4174/10000: L(Train): 0.32945170998573303; L(Test): 0.3005751073360443\n",
            "Epoch 4175/10000: L(Train): 0.3225460350513458; L(Test): 0.2995845377445221\n",
            "Epoch 4176/10000: L(Train): 0.3319309651851654; L(Test): 0.3001723289489746\n",
            "Epoch 4177/10000: L(Train): 0.32560208439826965; L(Test): 0.30004605650901794\n",
            "Epoch 4178/10000: L(Train): 0.3207738995552063; L(Test): 0.2991430461406708\n",
            "Epoch 4179/10000: L(Train): 0.31993919610977173; L(Test): 0.2989721894264221\n",
            "Epoch 4180/10000: L(Train): 0.323590487241745; L(Test): 0.2986268401145935\n",
            "Epoch 4181/10000: L(Train): 0.3196731209754944; L(Test): 0.2988473176956177\n",
            "Epoch 4182/10000: L(Train): 0.3186820149421692; L(Test): 0.29938262701034546\n",
            "Epoch 4183/10000: L(Train): 0.319384902715683; L(Test): 0.29855382442474365\n",
            "Epoch 4184/10000: L(Train): 0.3246675729751587; L(Test): 0.2987939715385437\n",
            "Epoch 4185/10000: L(Train): 0.3263151943683624; L(Test): 0.2995416820049286\n",
            "Epoch 4186/10000: L(Train): 0.3201129734516144; L(Test): 0.29843851923942566\n",
            "Epoch 4187/10000: L(Train): 0.3319648504257202; L(Test): 0.2991921305656433\n",
            "Epoch 4188/10000: L(Train): 0.31790682673454285; L(Test): 0.2995053827762604\n",
            "Epoch 4189/10000: L(Train): 0.3254851698875427; L(Test): 0.29863446950912476\n",
            "Epoch 4190/10000: L(Train): 0.32139256596565247; L(Test): 0.30032965540885925\n",
            "Epoch 4191/10000: L(Train): 0.3287465572357178; L(Test): 0.30125197768211365\n",
            "Epoch 4192/10000: L(Train): 0.3207104504108429; L(Test): 0.3020530343055725\n",
            "Epoch 4193/10000: L(Train): 0.33333078026771545; L(Test): 0.3000291883945465\n",
            "Epoch 4194/10000: L(Train): 0.3180144429206848; L(Test): 0.3005938231945038\n",
            "Epoch 4195/10000: L(Train): 0.32643643021583557; L(Test): 0.3013068437576294\n",
            "Epoch 4196/10000: L(Train): 0.3189088702201843; L(Test): 0.3006802797317505\n",
            "Epoch 4197/10000: L(Train): 0.3245318830013275; L(Test): 0.30258676409721375\n",
            "Epoch 4198/10000: L(Train): 0.32460007071495056; L(Test): 0.3017463982105255\n",
            "Epoch 4199/10000: L(Train): 0.32487496733665466; L(Test): 0.3007557988166809\n",
            "Epoch 4200/10000: L(Train): 0.33780109882354736; L(Test): 0.3009932339191437\n",
            "Epoch 4201/10000: L(Train): 0.3227289021015167; L(Test): 0.30013492703437805\n",
            "Epoch 4202/10000: L(Train): 0.318504273891449; L(Test): 0.30012911558151245\n",
            "Epoch 4203/10000: L(Train): 0.3228761553764343; L(Test): 0.3008976876735687\n",
            "Epoch 4204/10000: L(Train): 0.3243526518344879; L(Test): 0.3001938462257385\n",
            "Epoch 4205/10000: L(Train): 0.32029813528060913; L(Test): 0.29973018169403076\n",
            "Epoch 4206/10000: L(Train): 0.3210460841655731; L(Test): 0.30075064301490784\n",
            "Epoch 4207/10000: L(Train): 0.32946205139160156; L(Test): 0.3014507591724396\n",
            "Epoch 4208/10000: L(Train): 0.3228607475757599; L(Test): 0.30083751678466797\n",
            "Epoch 4209/10000: L(Train): 0.32210054993629456; L(Test): 0.3011992871761322\n",
            "Epoch 4210/10000: L(Train): 0.32129600644111633; L(Test): 0.30152103304862976\n",
            "Epoch 4211/10000: L(Train): 0.32971134781837463; L(Test): 0.3014317750930786\n",
            "Epoch 4212/10000: L(Train): 0.3219544589519501; L(Test): 0.30096012353897095\n",
            "Epoch 4213/10000: L(Train): 0.32231396436691284; L(Test): 0.301157146692276\n",
            "Epoch 4214/10000: L(Train): 0.3277256190776825; L(Test): 0.30159831047058105\n",
            "Epoch 4215/10000: L(Train): 0.3340505063533783; L(Test): 0.30098482966423035\n",
            "Epoch 4216/10000: L(Train): 0.3383593261241913; L(Test): 0.3003799319267273\n",
            "Epoch 4217/10000: L(Train): 0.3230004608631134; L(Test): 0.3004149794578552\n",
            "Epoch 4218/10000: L(Train): 0.3166045844554901; L(Test): 0.30253157019615173\n",
            "Epoch 4219/10000: L(Train): 0.3320551812648773; L(Test): 0.30212172865867615\n",
            "Epoch 4220/10000: L(Train): 0.3254183828830719; L(Test): 0.3007020056247711\n",
            "Epoch 4221/10000: L(Train): 0.32059186697006226; L(Test): 0.3014039993286133\n",
            "Epoch 4222/10000: L(Train): 0.3328993618488312; L(Test): 0.30239489674568176\n",
            "Epoch 4223/10000: L(Train): 0.32862359285354614; L(Test): 0.30235326290130615\n",
            "Epoch 4224/10000: L(Train): 0.3177214562892914; L(Test): 0.3027316629886627\n",
            "Epoch 4225/10000: L(Train): 0.3313361406326294; L(Test): 0.30247533321380615\n",
            "Epoch 4226/10000: L(Train): 0.31861424446105957; L(Test): 0.30129727721214294\n",
            "Epoch 4227/10000: L(Train): 0.32715052366256714; L(Test): 0.3003460764884949\n",
            "Epoch 4228/10000: L(Train): 0.32293859124183655; L(Test): 0.30056333541870117\n",
            "Epoch 4229/10000: L(Train): 0.32112792134284973; L(Test): 0.300187885761261\n",
            "Epoch 4230/10000: L(Train): 0.3291538655757904; L(Test): 0.30051153898239136\n",
            "Epoch 4231/10000: L(Train): 0.33166229724884033; L(Test): 0.29953834414482117\n",
            "Epoch 4232/10000: L(Train): 0.32009878754615784; L(Test): 0.2997818887233734\n",
            "Epoch 4233/10000: L(Train): 0.3213842213153839; L(Test): 0.3014114499092102\n",
            "Epoch 4234/10000: L(Train): 0.3306599259376526; L(Test): 0.30223706364631653\n",
            "Epoch 4235/10000: L(Train): 0.33287471532821655; L(Test): 0.3016514778137207\n",
            "Epoch 4236/10000: L(Train): 0.3260771632194519; L(Test): 0.3009396195411682\n",
            "Epoch 4237/10000: L(Train): 0.32520976662635803; L(Test): 0.3003307580947876\n",
            "Epoch 4238/10000: L(Train): 0.3189655840396881; L(Test): 0.30125027894973755\n",
            "Epoch 4239/10000: L(Train): 0.32569602131843567; L(Test): 0.3004412055015564\n",
            "Epoch 4240/10000: L(Train): 0.32553237676620483; L(Test): 0.30310940742492676\n",
            "Epoch 4241/10000: L(Train): 0.32878759503364563; L(Test): 0.3016356825828552\n",
            "Epoch 4242/10000: L(Train): 0.3223153352737427; L(Test): 0.299731582403183\n",
            "Epoch 4243/10000: L(Train): 0.32158172130584717; L(Test): 0.300502210855484\n",
            "Epoch 4244/10000: L(Train): 0.3208833932876587; L(Test): 0.30155837535858154\n",
            "Epoch 4245/10000: L(Train): 0.3254762291908264; L(Test): 0.3035140633583069\n",
            "Epoch 4246/10000: L(Train): 0.32538139820098877; L(Test): 0.303806871175766\n",
            "Epoch 4247/10000: L(Train): 0.32951653003692627; L(Test): 0.3018990755081177\n",
            "Epoch 4248/10000: L(Train): 0.33087652921676636; L(Test): 0.3002873361110687\n",
            "Epoch 4249/10000: L(Train): 0.328897088766098; L(Test): 0.3002873361110687\n",
            "Epoch 4250/10000: L(Train): 0.32303664088249207; L(Test): 0.2998586595058441\n",
            "Epoch 4251/10000: L(Train): 0.3296078145503998; L(Test): 0.29983919858932495\n",
            "Epoch 4252/10000: L(Train): 0.3220688998699188; L(Test): 0.3012637794017792\n",
            "Epoch 4253/10000: L(Train): 0.3259281516075134; L(Test): 0.30214130878448486\n",
            "Epoch 4254/10000: L(Train): 0.32905179262161255; L(Test): 0.30152982473373413\n",
            "Epoch 4255/10000: L(Train): 0.3338910639286041; L(Test): 0.3010655343532562\n",
            "Epoch 4256/10000: L(Train): 0.31341221928596497; L(Test): 0.3018065392971039\n",
            "Epoch 4257/10000: L(Train): 0.32187193632125854; L(Test): 0.30190086364746094\n",
            "Epoch 4258/10000: L(Train): 0.3227355480194092; L(Test): 0.3003980219364166\n",
            "Epoch 4259/10000: L(Train): 0.3227580487728119; L(Test): 0.2999725341796875\n",
            "Epoch 4260/10000: L(Train): 0.32250359654426575; L(Test): 0.30183857679367065\n",
            "Epoch 4261/10000: L(Train): 0.33225175738334656; L(Test): 0.3021412491798401\n",
            "Epoch 4262/10000: L(Train): 0.3243784010410309; L(Test): 0.2993626892566681\n",
            "Epoch 4263/10000: L(Train): 0.3164546489715576; L(Test): 0.2991219460964203\n",
            "Epoch 4264/10000: L(Train): 0.310831218957901; L(Test): 0.30042800307273865\n",
            "Epoch 4265/10000: L(Train): 0.3268948495388031; L(Test): 0.30109795928001404\n",
            "Epoch 4266/10000: L(Train): 0.3248266577720642; L(Test): 0.3012216091156006\n",
            "Epoch 4267/10000: L(Train): 0.3289761245250702; L(Test): 0.30115756392478943\n",
            "Epoch 4268/10000: L(Train): 0.32686468958854675; L(Test): 0.2993525266647339\n",
            "Epoch 4269/10000: L(Train): 0.33074671030044556; L(Test): 0.2984946072101593\n",
            "Epoch 4270/10000: L(Train): 0.32595646381378174; L(Test): 0.30016106367111206\n",
            "Epoch 4271/10000: L(Train): 0.3268384039402008; L(Test): 0.30035465955734253\n",
            "Epoch 4272/10000: L(Train): 0.3168083727359772; L(Test): 0.3006151616573334\n",
            "Epoch 4273/10000: L(Train): 0.3209630250930786; L(Test): 0.30317217111587524\n",
            "Epoch 4274/10000: L(Train): 0.3183952867984772; L(Test): 0.3039253354072571\n",
            "Epoch 4275/10000: L(Train): 0.32785043120384216; L(Test): 0.3013445734977722\n",
            "Epoch 4276/10000: L(Train): 0.31623145937919617; L(Test): 0.3006631135940552\n",
            "Epoch 4277/10000: L(Train): 0.3244918882846832; L(Test): 0.30230772495269775\n",
            "Epoch 4278/10000: L(Train): 0.3266660273075104; L(Test): 0.30038514733314514\n",
            "Epoch 4279/10000: L(Train): 0.3181329071521759; L(Test): 0.3010615408420563\n",
            "Epoch 4280/10000: L(Train): 0.32981619238853455; L(Test): 0.30221691727638245\n",
            "Epoch 4281/10000: L(Train): 0.3304673433303833; L(Test): 0.3013384938240051\n",
            "Epoch 4282/10000: L(Train): 0.32825568318367004; L(Test): 0.300597220659256\n",
            "Epoch 4283/10000: L(Train): 0.3247774839401245; L(Test): 0.3007259666919708\n",
            "Epoch 4284/10000: L(Train): 0.32417574524879456; L(Test): 0.30188918113708496\n",
            "Epoch 4285/10000: L(Train): 0.3222159445285797; L(Test): 0.30176660418510437\n",
            "Epoch 4286/10000: L(Train): 0.32628586888313293; L(Test): 0.3010457456111908\n",
            "Epoch 4287/10000: L(Train): 0.32929515838623047; L(Test): 0.3011299669742584\n",
            "Epoch 4288/10000: L(Train): 0.3222748637199402; L(Test): 0.3023327887058258\n",
            "Epoch 4289/10000: L(Train): 0.32661280035972595; L(Test): 0.3027116060256958\n",
            "Epoch 4290/10000: L(Train): 0.33530521392822266; L(Test): 0.30074039101600647\n",
            "Epoch 4291/10000: L(Train): 0.3264365792274475; L(Test): 0.3016180694103241\n",
            "Epoch 4292/10000: L(Train): 0.3273773193359375; L(Test): 0.3019784390926361\n",
            "Epoch 4293/10000: L(Train): 0.3272542357444763; L(Test): 0.30146148800849915\n",
            "Epoch 4294/10000: L(Train): 0.3264341354370117; L(Test): 0.3006671071052551\n",
            "Epoch 4295/10000: L(Train): 0.32622048258781433; L(Test): 0.3003750145435333\n",
            "Epoch 4296/10000: L(Train): 0.32628339529037476; L(Test): 0.3004356622695923\n",
            "Epoch 4297/10000: L(Train): 0.31774985790252686; L(Test): 0.30023983120918274\n",
            "Epoch 4298/10000: L(Train): 0.31995099782943726; L(Test): 0.30021175742149353\n",
            "Epoch 4299/10000: L(Train): 0.3167318105697632; L(Test): 0.3008252680301666\n",
            "Epoch 4300/10000: L(Train): 0.31955820322036743; L(Test): 0.3000599443912506\n",
            "Epoch 4301/10000: L(Train): 0.3147834539413452; L(Test): 0.2989242374897003\n",
            "Epoch 4302/10000: L(Train): 0.3299638032913208; L(Test): 0.2995571196079254\n",
            "Epoch 4303/10000: L(Train): 0.321296751499176; L(Test): 0.29909422993659973\n",
            "Epoch 4304/10000: L(Train): 0.3187389373779297; L(Test): 0.29868650436401367\n",
            "Epoch 4305/10000: L(Train): 0.322544127702713; L(Test): 0.2995525896549225\n",
            "Epoch 4306/10000: L(Train): 0.3199542462825775; L(Test): 0.29973217844963074\n",
            "Epoch 4307/10000: L(Train): 0.3097018599510193; L(Test): 0.29890376329421997\n",
            "Epoch 4308/10000: L(Train): 0.3196393847465515; L(Test): 0.29852810502052307\n",
            "Epoch 4309/10000: L(Train): 0.3262157738208771; L(Test): 0.2981702387332916\n",
            "Epoch 4310/10000: L(Train): 0.31682562828063965; L(Test): 0.2977913022041321\n",
            "Epoch 4311/10000: L(Train): 0.32376596331596375; L(Test): 0.29779553413391113\n",
            "Epoch 4312/10000: L(Train): 0.31937071681022644; L(Test): 0.2983587682247162\n",
            "Epoch 4313/10000: L(Train): 0.32221880555152893; L(Test): 0.29984360933303833\n",
            "Epoch 4314/10000: L(Train): 0.3178493082523346; L(Test): 0.2986678183078766\n",
            "Epoch 4315/10000: L(Train): 0.31596264243125916; L(Test): 0.2991005778312683\n",
            "Epoch 4316/10000: L(Train): 0.32072898745536804; L(Test): 0.3002627491950989\n",
            "Epoch 4317/10000: L(Train): 0.3187485933303833; L(Test): 0.29973840713500977\n",
            "Epoch 4318/10000: L(Train): 0.3289819061756134; L(Test): 0.300418883562088\n",
            "Epoch 4319/10000: L(Train): 0.33092403411865234; L(Test): 0.3005634546279907\n",
            "Epoch 4320/10000: L(Train): 0.3206339478492737; L(Test): 0.30040478706359863\n",
            "Epoch 4321/10000: L(Train): 0.31536364555358887; L(Test): 0.2999500334262848\n",
            "Epoch 4322/10000: L(Train): 0.32027825713157654; L(Test): 0.30037251114845276\n",
            "Epoch 4323/10000: L(Train): 0.32061997056007385; L(Test): 0.3005094528198242\n",
            "Epoch 4324/10000: L(Train): 0.321485698223114; L(Test): 0.2992364466190338\n",
            "Epoch 4325/10000: L(Train): 0.3233114778995514; L(Test): 0.3004295825958252\n",
            "Epoch 4326/10000: L(Train): 0.3318689465522766; L(Test): 0.30069640278816223\n",
            "Epoch 4327/10000: L(Train): 0.32584089040756226; L(Test): 0.30023467540740967\n",
            "Epoch 4328/10000: L(Train): 0.32261893153190613; L(Test): 0.30298885703086853\n",
            "Epoch 4329/10000: L(Train): 0.3219085931777954; L(Test): 0.30368736386299133\n",
            "Epoch 4330/10000: L(Train): 0.325016051530838; L(Test): 0.3012523651123047\n",
            "Epoch 4331/10000: L(Train): 0.3271887004375458; L(Test): 0.3027341663837433\n",
            "Epoch 4332/10000: L(Train): 0.32518142461776733; L(Test): 0.3041206896305084\n",
            "Epoch 4333/10000: L(Train): 0.3477990925312042; L(Test): 0.3002525568008423\n",
            "Epoch 4334/10000: L(Train): 0.3233233690261841; L(Test): 0.3015972971916199\n",
            "Epoch 4335/10000: L(Train): 0.32480183243751526; L(Test): 0.30274537205696106\n",
            "Epoch 4336/10000: L(Train): 0.3345755636692047; L(Test): 0.3020402491092682\n",
            "Epoch 4337/10000: L(Train): 0.3305283188819885; L(Test): 0.3024255633354187\n",
            "Epoch 4338/10000: L(Train): 0.32927581667900085; L(Test): 0.30349281430244446\n",
            "Epoch 4339/10000: L(Train): 0.3274766802787781; L(Test): 0.30376550555229187\n",
            "Epoch 4340/10000: L(Train): 0.3256334364414215; L(Test): 0.3024030327796936\n",
            "Epoch 4341/10000: L(Train): 0.32190945744514465; L(Test): 0.3018876612186432\n",
            "Epoch 4342/10000: L(Train): 0.32594528794288635; L(Test): 0.30213621258735657\n",
            "Epoch 4343/10000: L(Train): 0.3259333074092865; L(Test): 0.3001413643360138\n",
            "Epoch 4344/10000: L(Train): 0.315067857503891; L(Test): 0.2992734909057617\n",
            "Epoch 4345/10000: L(Train): 0.31348058581352234; L(Test): 0.30033352971076965\n",
            "Epoch 4346/10000: L(Train): 0.3145054280757904; L(Test): 0.30114883184432983\n",
            "Epoch 4347/10000: L(Train): 0.3199516534805298; L(Test): 0.3008384108543396\n",
            "Epoch 4348/10000: L(Train): 0.32963183522224426; L(Test): 0.3005097508430481\n",
            "Epoch 4349/10000: L(Train): 0.30863067507743835; L(Test): 0.3004325032234192\n",
            "Epoch 4350/10000: L(Train): 0.3237657845020294; L(Test): 0.2997129261493683\n",
            "Epoch 4351/10000: L(Train): 0.3236963152885437; L(Test): 0.30002284049987793\n",
            "Epoch 4352/10000: L(Train): 0.31816917657852173; L(Test): 0.2994688153266907\n",
            "Epoch 4353/10000: L(Train): 0.321377694606781; L(Test): 0.29864567518234253\n",
            "Epoch 4354/10000: L(Train): 0.32915198802948; L(Test): 0.2993185818195343\n",
            "Epoch 4355/10000: L(Train): 0.32288965582847595; L(Test): 0.2989368736743927\n",
            "Epoch 4356/10000: L(Train): 0.3317110538482666; L(Test): 0.30020076036453247\n",
            "Epoch 4357/10000: L(Train): 0.3272932767868042; L(Test): 0.30112528800964355\n",
            "Epoch 4358/10000: L(Train): 0.3210059702396393; L(Test): 0.2989850342273712\n",
            "Epoch 4359/10000: L(Train): 0.3167823553085327; L(Test): 0.3016239404678345\n",
            "Epoch 4360/10000: L(Train): 0.3280034065246582; L(Test): 0.299054890871048\n",
            "Epoch 4361/10000: L(Train): 0.3295249938964844; L(Test): 0.3001386225223541\n",
            "Epoch 4362/10000: L(Train): 0.3261845111846924; L(Test): 0.3005160093307495\n",
            "Epoch 4363/10000: L(Train): 0.31728142499923706; L(Test): 0.2991461753845215\n",
            "Epoch 4364/10000: L(Train): 0.31744828820228577; L(Test): 0.30085307359695435\n",
            "Epoch 4365/10000: L(Train): 0.3286244571208954; L(Test): 0.2984659671783447\n",
            "Epoch 4366/10000: L(Train): 0.31862324476242065; L(Test): 0.29983049631118774\n",
            "Epoch 4367/10000: L(Train): 0.31803008913993835; L(Test): 0.3022744953632355\n",
            "Epoch 4368/10000: L(Train): 0.32062748074531555; L(Test): 0.30031225085258484\n",
            "Epoch 4369/10000: L(Train): 0.32720625400543213; L(Test): 0.30039182305336\n",
            "Epoch 4370/10000: L(Train): 0.3215099275112152; L(Test): 0.30088305473327637\n",
            "Epoch 4371/10000: L(Train): 0.33556994795799255; L(Test): 0.2989250123500824\n",
            "Epoch 4372/10000: L(Train): 0.32425686717033386; L(Test): 0.3008051812648773\n",
            "Epoch 4373/10000: L(Train): 0.3232080042362213; L(Test): 0.30025985836982727\n",
            "Epoch 4374/10000: L(Train): 0.3188779354095459; L(Test): 0.2983601689338684\n",
            "Epoch 4375/10000: L(Train): 0.32349440455436707; L(Test): 0.3008960783481598\n",
            "Epoch 4376/10000: L(Train): 0.3306296765804291; L(Test): 0.300400048494339\n",
            "Epoch 4377/10000: L(Train): 0.32348692417144775; L(Test): 0.29946571588516235\n",
            "Epoch 4378/10000: L(Train): 0.3197764754295349; L(Test): 0.3007320463657379\n",
            "Epoch 4379/10000: L(Train): 0.3181496858596802; L(Test): 0.30099955201148987\n",
            "Epoch 4380/10000: L(Train): 0.3277953565120697; L(Test): 0.2996218204498291\n",
            "Epoch 4381/10000: L(Train): 0.32418256998062134; L(Test): 0.2992687523365021\n",
            "Epoch 4382/10000: L(Train): 0.3301818370819092; L(Test): 0.29952946305274963\n",
            "Epoch 4383/10000: L(Train): 0.32422780990600586; L(Test): 0.29895323514938354\n",
            "Epoch 4384/10000: L(Train): 0.3248012363910675; L(Test): 0.298801451921463\n",
            "Epoch 4385/10000: L(Train): 0.31547051668167114; L(Test): 0.30066004395484924\n",
            "Epoch 4386/10000: L(Train): 0.33168551325798035; L(Test): 0.3004682958126068\n",
            "Epoch 4387/10000: L(Train): 0.32230889797210693; L(Test): 0.30017003417015076\n",
            "Epoch 4388/10000: L(Train): 0.32354092597961426; L(Test): 0.3012632727622986\n",
            "Epoch 4389/10000: L(Train): 0.32237476110458374; L(Test): 0.3019547164440155\n",
            "Epoch 4390/10000: L(Train): 0.32400622963905334; L(Test): 0.30106809735298157\n",
            "Epoch 4391/10000: L(Train): 0.32537055015563965; L(Test): 0.30133506655693054\n",
            "Epoch 4392/10000: L(Train): 0.3288755416870117; L(Test): 0.2996556758880615\n",
            "Epoch 4393/10000: L(Train): 0.3228742480278015; L(Test): 0.29912981390953064\n",
            "Epoch 4394/10000: L(Train): 0.31497815251350403; L(Test): 0.30028772354125977\n",
            "Epoch 4395/10000: L(Train): 0.3210689127445221; L(Test): 0.30130723118782043\n",
            "Epoch 4396/10000: L(Train): 0.33794155716896057; L(Test): 0.3004460036754608\n",
            "Epoch 4397/10000: L(Train): 0.3280186057090759; L(Test): 0.3004608452320099\n",
            "Epoch 4398/10000: L(Train): 0.3269743323326111; L(Test): 0.3004235625267029\n",
            "Epoch 4399/10000: L(Train): 0.31450217962265015; L(Test): 0.2988998591899872\n",
            "Epoch 4400/10000: L(Train): 0.32561060786247253; L(Test): 0.29867446422576904\n",
            "Epoch 4401/10000: L(Train): 0.3308267295360565; L(Test): 0.2991009056568146\n",
            "Epoch 4402/10000: L(Train): 0.3213946223258972; L(Test): 0.29897207021713257\n",
            "Epoch 4403/10000: L(Train): 0.31730127334594727; L(Test): 0.2989027798175812\n",
            "Epoch 4404/10000: L(Train): 0.3161369264125824; L(Test): 0.2998601794242859\n",
            "Epoch 4405/10000: L(Train): 0.3277001678943634; L(Test): 0.3001105487346649\n",
            "Epoch 4406/10000: L(Train): 0.315885990858078; L(Test): 0.3005811870098114\n",
            "Epoch 4407/10000: L(Train): 0.32625287771224976; L(Test): 0.29926756024360657\n",
            "Epoch 4408/10000: L(Train): 0.32089248299598694; L(Test): 0.30306121706962585\n",
            "Epoch 4409/10000: L(Train): 0.3289768397808075; L(Test): 0.29935571551322937\n",
            "Epoch 4410/10000: L(Train): 0.3207663595676422; L(Test): 0.29840758442878723\n",
            "Epoch 4411/10000: L(Train): 0.32496076822280884; L(Test): 0.29959070682525635\n",
            "Epoch 4412/10000: L(Train): 0.3169856667518616; L(Test): 0.2990369498729706\n",
            "Epoch 4413/10000: L(Train): 0.31632497906684875; L(Test): 0.29962387681007385\n",
            "Epoch 4414/10000: L(Train): 0.33640244603157043; L(Test): 0.30047377943992615\n",
            "Epoch 4415/10000: L(Train): 0.3081527650356293; L(Test): 0.30020394921302795\n",
            "Epoch 4416/10000: L(Train): 0.31758180260658264; L(Test): 0.29889240860939026\n",
            "Epoch 4417/10000: L(Train): 0.3362262547016144; L(Test): 0.2990785837173462\n",
            "Epoch 4418/10000: L(Train): 0.3220403790473938; L(Test): 0.2995232045650482\n",
            "Epoch 4419/10000: L(Train): 0.3322601914405823; L(Test): 0.2994687855243683\n",
            "Epoch 4420/10000: L(Train): 0.33226892352104187; L(Test): 0.29944995045661926\n",
            "Epoch 4421/10000: L(Train): 0.3289487063884735; L(Test): 0.29999807476997375\n",
            "Epoch 4422/10000: L(Train): 0.32205870747566223; L(Test): 0.2994438111782074\n",
            "Epoch 4423/10000: L(Train): 0.3289056122303009; L(Test): 0.2992469072341919\n",
            "Epoch 4424/10000: L(Train): 0.31971263885498047; L(Test): 0.29863440990448\n",
            "Epoch 4425/10000: L(Train): 0.3263421654701233; L(Test): 0.2980058193206787\n",
            "Epoch 4426/10000: L(Train): 0.3170470893383026; L(Test): 0.2982761859893799\n",
            "Epoch 4427/10000: L(Train): 0.3209039866924286; L(Test): 0.2979385256767273\n",
            "Epoch 4428/10000: L(Train): 0.3211309313774109; L(Test): 0.2988014817237854\n",
            "Epoch 4429/10000: L(Train): 0.32527124881744385; L(Test): 0.30033430457115173\n",
            "Epoch 4430/10000: L(Train): 0.32368627190589905; L(Test): 0.29940471053123474\n",
            "Epoch 4431/10000: L(Train): 0.3244094252586365; L(Test): 0.2992760241031647\n",
            "Epoch 4432/10000: L(Train): 0.329210489988327; L(Test): 0.2990244925022125\n",
            "Epoch 4433/10000: L(Train): 0.3192427158355713; L(Test): 0.29881370067596436\n",
            "Epoch 4434/10000: L(Train): 0.3266938328742981; L(Test): 0.29884418845176697\n",
            "Epoch 4435/10000: L(Train): 0.3195330500602722; L(Test): 0.3002450466156006\n",
            "Epoch 4436/10000: L(Train): 0.32787007093429565; L(Test): 0.29928165674209595\n",
            "Epoch 4437/10000: L(Train): 0.32286399602890015; L(Test): 0.2992394268512726\n",
            "Epoch 4438/10000: L(Train): 0.327324241399765; L(Test): 0.2989451289176941\n",
            "Epoch 4439/10000: L(Train): 0.32949286699295044; L(Test): 0.29833275079727173\n",
            "Epoch 4440/10000: L(Train): 0.3315543532371521; L(Test): 0.29840996861457825\n",
            "Epoch 4441/10000: L(Train): 0.32618457078933716; L(Test): 0.29863905906677246\n",
            "Epoch 4442/10000: L(Train): 0.3217649459838867; L(Test): 0.2984062135219574\n",
            "Epoch 4443/10000: L(Train): 0.32732492685317993; L(Test): 0.29904353618621826\n",
            "Epoch 4444/10000: L(Train): 0.32712310552597046; L(Test): 0.29854267835617065\n",
            "Epoch 4445/10000: L(Train): 0.32284530997276306; L(Test): 0.29841867089271545\n",
            "Epoch 4446/10000: L(Train): 0.32582902908325195; L(Test): 0.2987138330936432\n",
            "Epoch 4447/10000: L(Train): 0.31969699263572693; L(Test): 0.2988380789756775\n",
            "Epoch 4448/10000: L(Train): 0.3224029839038849; L(Test): 0.29833927750587463\n",
            "Epoch 4449/10000: L(Train): 0.3271803855895996; L(Test): 0.2984924614429474\n",
            "Epoch 4450/10000: L(Train): 0.31856846809387207; L(Test): 0.29867029190063477\n",
            "Epoch 4451/10000: L(Train): 0.3307024836540222; L(Test): 0.29852333664894104\n",
            "Epoch 4452/10000: L(Train): 0.32628896832466125; L(Test): 0.29947933554649353\n",
            "Epoch 4453/10000: L(Train): 0.32417577505111694; L(Test): 0.300583153963089\n",
            "Epoch 4454/10000: L(Train): 0.3162830173969269; L(Test): 0.30121150612831116\n",
            "Epoch 4455/10000: L(Train): 0.32149866223335266; L(Test): 0.29985111951828003\n",
            "Epoch 4456/10000: L(Train): 0.31298887729644775; L(Test): 0.3006512224674225\n",
            "Epoch 4457/10000: L(Train): 0.3151518702507019; L(Test): 0.3008745312690735\n",
            "Epoch 4458/10000: L(Train): 0.32591548562049866; L(Test): 0.301769495010376\n",
            "Epoch 4459/10000: L(Train): 0.33230629563331604; L(Test): 0.30053189396858215\n",
            "Epoch 4460/10000: L(Train): 0.3154126703739166; L(Test): 0.3009710907936096\n",
            "Epoch 4461/10000: L(Train): 0.3306712806224823; L(Test): 0.30059581995010376\n",
            "Epoch 4462/10000: L(Train): 0.32103124260902405; L(Test): 0.2995300590991974\n",
            "Epoch 4463/10000: L(Train): 0.3149031102657318; L(Test): 0.2997967004776001\n",
            "Epoch 4464/10000: L(Train): 0.31360575556755066; L(Test): 0.29977890849113464\n",
            "Epoch 4465/10000: L(Train): 0.3355467915534973; L(Test): 0.2987993359565735\n",
            "Epoch 4466/10000: L(Train): 0.32715940475463867; L(Test): 0.29858162999153137\n",
            "Epoch 4467/10000: L(Train): 0.32344868779182434; L(Test): 0.298918753862381\n",
            "Epoch 4468/10000: L(Train): 0.3200197219848633; L(Test): 0.29824113845825195\n",
            "Epoch 4469/10000: L(Train): 0.3189910352230072; L(Test): 0.2991376519203186\n",
            "Epoch 4470/10000: L(Train): 0.33136263489723206; L(Test): 0.29948940873146057\n",
            "Epoch 4471/10000: L(Train): 0.3318325877189636; L(Test): 0.29904481768608093\n",
            "Epoch 4472/10000: L(Train): 0.32943424582481384; L(Test): 0.29955601692199707\n",
            "Epoch 4473/10000: L(Train): 0.32618698477745056; L(Test): 0.30038154125213623\n",
            "Epoch 4474/10000: L(Train): 0.3229655623435974; L(Test): 0.29970264434814453\n",
            "Epoch 4475/10000: L(Train): 0.32590797543525696; L(Test): 0.29856571555137634\n",
            "Epoch 4476/10000: L(Train): 0.31777894496917725; L(Test): 0.29896026849746704\n",
            "Epoch 4477/10000: L(Train): 0.3270067572593689; L(Test): 0.29920417070388794\n",
            "Epoch 4478/10000: L(Train): 0.3234783709049225; L(Test): 0.29814985394477844\n",
            "Epoch 4479/10000: L(Train): 0.3195773661136627; L(Test): 0.2981288433074951\n",
            "Epoch 4480/10000: L(Train): 0.32374414801597595; L(Test): 0.2986558973789215\n",
            "Epoch 4481/10000: L(Train): 0.3201439678668976; L(Test): 0.2994827628135681\n",
            "Epoch 4482/10000: L(Train): 0.32870933413505554; L(Test): 0.29914143681526184\n",
            "Epoch 4483/10000: L(Train): 0.32059115171432495; L(Test): 0.29897642135620117\n",
            "Epoch 4484/10000: L(Train): 0.3177444636821747; L(Test): 0.29919731616973877\n",
            "Epoch 4485/10000: L(Train): 0.33399340510368347; L(Test): 0.2981669008731842\n",
            "Epoch 4486/10000: L(Train): 0.30826708674430847; L(Test): 0.29777243733406067\n",
            "Epoch 4487/10000: L(Train): 0.3353174030780792; L(Test): 0.29911354184150696\n",
            "Epoch 4488/10000: L(Train): 0.3269622325897217; L(Test): 0.29958006739616394\n",
            "Epoch 4489/10000: L(Train): 0.32848796248435974; L(Test): 0.29816362261772156\n",
            "Epoch 4490/10000: L(Train): 0.3225078284740448; L(Test): 0.2983695864677429\n",
            "Epoch 4491/10000: L(Train): 0.3232804834842682; L(Test): 0.2985897958278656\n",
            "Epoch 4492/10000: L(Train): 0.32025420665740967; L(Test): 0.2978159785270691\n",
            "Epoch 4493/10000: L(Train): 0.3252926170825958; L(Test): 0.2990988492965698\n",
            "Epoch 4494/10000: L(Train): 0.32480260729789734; L(Test): 0.2983103096485138\n",
            "Epoch 4495/10000: L(Train): 0.3218040466308594; L(Test): 0.2981511354446411\n",
            "Epoch 4496/10000: L(Train): 0.32467904686927795; L(Test): 0.2986390292644501\n",
            "Epoch 4497/10000: L(Train): 0.3272870182991028; L(Test): 0.2977178990840912\n",
            "Epoch 4498/10000: L(Train): 0.3227921426296234; L(Test): 0.2978477478027344\n",
            "Epoch 4499/10000: L(Train): 0.3219372630119324; L(Test): 0.3013008236885071\n",
            "Epoch 4500/10000: L(Train): 0.3190933167934418; L(Test): 0.30259788036346436\n",
            "Epoch 4501/10000: L(Train): 0.3241890072822571; L(Test): 0.30458179116249084\n",
            "Epoch 4502/10000: L(Train): 0.32354262471199036; L(Test): 0.30529889464378357\n",
            "Epoch 4503/10000: L(Train): 0.32698988914489746; L(Test): 0.30484455823898315\n",
            "Epoch 4504/10000: L(Train): 0.32749396562576294; L(Test): 0.3063025176525116\n",
            "Epoch 4505/10000: L(Train): 0.33253929018974304; L(Test): 0.30380964279174805\n",
            "Epoch 4506/10000: L(Train): 0.3287162780761719; L(Test): 0.30406683683395386\n",
            "Epoch 4507/10000: L(Train): 0.32156506180763245; L(Test): 0.30475905537605286\n",
            "Epoch 4508/10000: L(Train): 0.3313225209712982; L(Test): 0.3043724000453949\n",
            "Epoch 4509/10000: L(Train): 0.31911396980285645; L(Test): 0.30453354120254517\n",
            "Epoch 4510/10000: L(Train): 0.33433830738067627; L(Test): 0.305794358253479\n",
            "Epoch 4511/10000: L(Train): 0.32980576157569885; L(Test): 0.3067559599876404\n",
            "Epoch 4512/10000: L(Train): 0.3278246223926544; L(Test): 0.30622556805610657\n",
            "Epoch 4513/10000: L(Train): 0.3321313261985779; L(Test): 0.3070131242275238\n",
            "Epoch 4514/10000: L(Train): 0.33528581261634827; L(Test): 0.3079189956188202\n",
            "Epoch 4515/10000: L(Train): 0.33007851243019104; L(Test): 0.30597999691963196\n",
            "Epoch 4516/10000: L(Train): 0.3302253782749176; L(Test): 0.3065846562385559\n",
            "Epoch 4517/10000: L(Train): 0.321734756231308; L(Test): 0.30921420454978943\n",
            "Epoch 4518/10000: L(Train): 0.32296624779701233; L(Test): 0.3064080774784088\n",
            "Epoch 4519/10000: L(Train): 0.3245261311531067; L(Test): 0.3051970601081848\n",
            "Epoch 4520/10000: L(Train): 0.3340219259262085; L(Test): 0.3067519962787628\n",
            "Epoch 4521/10000: L(Train): 0.3364299237728119; L(Test): 0.3048100173473358\n",
            "Epoch 4522/10000: L(Train): 0.3291053771972656; L(Test): 0.3043275773525238\n",
            "Epoch 4523/10000: L(Train): 0.32506558299064636; L(Test): 0.30574727058410645\n",
            "Epoch 4524/10000: L(Train): 0.3206583559513092; L(Test): 0.3050806522369385\n",
            "Epoch 4525/10000: L(Train): 0.3327549397945404; L(Test): 0.30346986651420593\n",
            "Epoch 4526/10000: L(Train): 0.31987079977989197; L(Test): 0.3049503564834595\n",
            "Epoch 4527/10000: L(Train): 0.3230779469013214; L(Test): 0.3054530620574951\n",
            "Epoch 4528/10000: L(Train): 0.3301701843738556; L(Test): 0.30448755621910095\n",
            "Epoch 4529/10000: L(Train): 0.33235055208206177; L(Test): 0.30353474617004395\n",
            "Epoch 4530/10000: L(Train): 0.3291722238063812; L(Test): 0.30209481716156006\n",
            "Epoch 4531/10000: L(Train): 0.32184550166130066; L(Test): 0.30104944109916687\n",
            "Epoch 4532/10000: L(Train): 0.31337347626686096; L(Test): 0.3008798360824585\n",
            "Epoch 4533/10000: L(Train): 0.32035312056541443; L(Test): 0.30080682039260864\n",
            "Epoch 4534/10000: L(Train): 0.32826998829841614; L(Test): 0.3015780746936798\n",
            "Epoch 4535/10000: L(Train): 0.32363080978393555; L(Test): 0.30135640501976013\n",
            "Epoch 4536/10000: L(Train): 0.3200283944606781; L(Test): 0.30088502168655396\n",
            "Epoch 4537/10000: L(Train): 0.32448437809944153; L(Test): 0.30051156878471375\n",
            "Epoch 4538/10000: L(Train): 0.32084134221076965; L(Test): 0.30042198300361633\n",
            "Epoch 4539/10000: L(Train): 0.3250068128108978; L(Test): 0.3002932369709015\n",
            "Epoch 4540/10000: L(Train): 0.3208540081977844; L(Test): 0.30062392354011536\n",
            "Epoch 4541/10000: L(Train): 0.3264855742454529; L(Test): 0.3005686402320862\n",
            "Epoch 4542/10000: L(Train): 0.32689422369003296; L(Test): 0.3002063035964966\n",
            "Epoch 4543/10000: L(Train): 0.3226069509983063; L(Test): 0.29907289147377014\n",
            "Epoch 4544/10000: L(Train): 0.31566479802131653; L(Test): 0.2993149757385254\n",
            "Epoch 4545/10000: L(Train): 0.31929925084114075; L(Test): 0.2998013198375702\n",
            "Epoch 4546/10000: L(Train): 0.31804755330085754; L(Test): 0.29942062497138977\n",
            "Epoch 4547/10000: L(Train): 0.3304230570793152; L(Test): 0.2988702356815338\n",
            "Epoch 4548/10000: L(Train): 0.329501748085022; L(Test): 0.29840752482414246\n",
            "Epoch 4549/10000: L(Train): 0.3204648196697235; L(Test): 0.2984392046928406\n",
            "Epoch 4550/10000: L(Train): 0.3124014437198639; L(Test): 0.29841935634613037\n",
            "Epoch 4551/10000: L(Train): 0.32590949535369873; L(Test): 0.2981604337692261\n",
            "Epoch 4552/10000: L(Train): 0.3178170621395111; L(Test): 0.29819515347480774\n",
            "Epoch 4553/10000: L(Train): 0.32766810059547424; L(Test): 0.2980065643787384\n",
            "Epoch 4554/10000: L(Train): 0.31780514121055603; L(Test): 0.2977999150753021\n",
            "Epoch 4555/10000: L(Train): 0.3133489191532135; L(Test): 0.2980284094810486\n",
            "Epoch 4556/10000: L(Train): 0.31488966941833496; L(Test): 0.2974383234977722\n",
            "Epoch 4557/10000: L(Train): 0.324903279542923; L(Test): 0.2980509400367737\n",
            "Epoch 4558/10000: L(Train): 0.3243426978588104; L(Test): 0.2985578179359436\n",
            "Epoch 4559/10000: L(Train): 0.3300374150276184; L(Test): 0.2977793216705322\n",
            "Epoch 4560/10000: L(Train): 0.31284162402153015; L(Test): 0.2972165644168854\n",
            "Epoch 4561/10000: L(Train): 0.32156258821487427; L(Test): 0.29749372601509094\n",
            "Epoch 4562/10000: L(Train): 0.3206130266189575; L(Test): 0.29734182357788086\n",
            "Epoch 4563/10000: L(Train): 0.316134512424469; L(Test): 0.29703107476234436\n",
            "Epoch 4564/10000: L(Train): 0.3197953402996063; L(Test): 0.29772114753723145\n",
            "Epoch 4565/10000: L(Train): 0.32729649543762207; L(Test): 0.29835593700408936\n",
            "Epoch 4566/10000: L(Train): 0.32033658027648926; L(Test): 0.2981962263584137\n",
            "Epoch 4567/10000: L(Train): 0.32178404927253723; L(Test): 0.297873318195343\n",
            "Epoch 4568/10000: L(Train): 0.32582056522369385; L(Test): 0.2972138822078705\n",
            "Epoch 4569/10000: L(Train): 0.3157871663570404; L(Test): 0.29709234833717346\n",
            "Epoch 4570/10000: L(Train): 0.316217839717865; L(Test): 0.29855984449386597\n",
            "Epoch 4571/10000: L(Train): 0.32089024782180786; L(Test): 0.298606812953949\n",
            "Epoch 4572/10000: L(Train): 0.31859707832336426; L(Test): 0.29837021231651306\n",
            "Epoch 4573/10000: L(Train): 0.32700708508491516; L(Test): 0.2979767918586731\n",
            "Epoch 4574/10000: L(Train): 0.33337917923927307; L(Test): 0.2988838255405426\n",
            "Epoch 4575/10000: L(Train): 0.32503223419189453; L(Test): 0.30047082901000977\n",
            "Epoch 4576/10000: L(Train): 0.32932180166244507; L(Test): 0.2989560663700104\n",
            "Epoch 4577/10000: L(Train): 0.3250477910041809; L(Test): 0.29876959323883057\n",
            "Epoch 4578/10000: L(Train): 0.3257582485675812; L(Test): 0.29913467168807983\n",
            "Epoch 4579/10000: L(Train): 0.32130005955696106; L(Test): 0.30075713992118835\n",
            "Epoch 4580/10000: L(Train): 0.3162521421909332; L(Test): 0.3014530539512634\n",
            "Epoch 4581/10000: L(Train): 0.3306894898414612; L(Test): 0.298828125\n",
            "Epoch 4582/10000: L(Train): 0.32910528779029846; L(Test): 0.29904863238334656\n",
            "Epoch 4583/10000: L(Train): 0.33201101422309875; L(Test): 0.2993980944156647\n",
            "Epoch 4584/10000: L(Train): 0.32673442363739014; L(Test): 0.2983115017414093\n",
            "Epoch 4585/10000: L(Train): 0.3212813436985016; L(Test): 0.29818442463874817\n",
            "Epoch 4586/10000: L(Train): 0.32349342107772827; L(Test): 0.2981932759284973\n",
            "Epoch 4587/10000: L(Train): 0.3181101083755493; L(Test): 0.2987423539161682\n",
            "Epoch 4588/10000: L(Train): 0.33313435316085815; L(Test): 0.29997363686561584\n",
            "Epoch 4589/10000: L(Train): 0.31899014115333557; L(Test): 0.2992556095123291\n",
            "Epoch 4590/10000: L(Train): 0.3335970640182495; L(Test): 0.29896172881126404\n",
            "Epoch 4591/10000: L(Train): 0.320019006729126; L(Test): 0.3040562570095062\n",
            "Epoch 4592/10000: L(Train): 0.3364713490009308; L(Test): 0.3032682240009308\n",
            "Epoch 4593/10000: L(Train): 0.3307889997959137; L(Test): 0.3017663359642029\n",
            "Epoch 4594/10000: L(Train): 0.32265225052833557; L(Test): 0.30567678809165955\n",
            "Epoch 4595/10000: L(Train): 0.32879140973091125; L(Test): 0.3045836389064789\n",
            "Epoch 4596/10000: L(Train): 0.31821203231811523; L(Test): 0.30424991250038147\n",
            "Epoch 4597/10000: L(Train): 0.32722532749176025; L(Test): 0.30588826537132263\n",
            "Epoch 4598/10000: L(Train): 0.3266693651676178; L(Test): 0.305146723985672\n",
            "Epoch 4599/10000: L(Train): 0.32353588938713074; L(Test): 0.3039945662021637\n",
            "Epoch 4600/10000: L(Train): 0.3253841996192932; L(Test): 0.3042459189891815\n",
            "Epoch 4601/10000: L(Train): 0.3272649943828583; L(Test): 0.30212199687957764\n",
            "Epoch 4602/10000: L(Train): 0.32834768295288086; L(Test): 0.3033570349216461\n",
            "Epoch 4603/10000: L(Train): 0.32303133606910706; L(Test): 0.3053436577320099\n",
            "Epoch 4604/10000: L(Train): 0.32613468170166016; L(Test): 0.30408698320388794\n",
            "Epoch 4605/10000: L(Train): 0.32607486844062805; L(Test): 0.3029053211212158\n",
            "Epoch 4606/10000: L(Train): 0.33556875586509705; L(Test): 0.3032265901565552\n",
            "Epoch 4607/10000: L(Train): 0.3308503329753876; L(Test): 0.30254486203193665\n",
            "Epoch 4608/10000: L(Train): 0.3280218839645386; L(Test): 0.3058750629425049\n",
            "Epoch 4609/10000: L(Train): 0.32590407133102417; L(Test): 0.3053447902202606\n",
            "Epoch 4610/10000: L(Train): 0.3325954079627991; L(Test): 0.30247992277145386\n",
            "Epoch 4611/10000: L(Train): 0.32457810640335083; L(Test): 0.30472418665885925\n",
            "Epoch 4612/10000: L(Train): 0.3303585350513458; L(Test): 0.30351001024246216\n",
            "Epoch 4613/10000: L(Train): 0.3192761540412903; L(Test): 0.3024883270263672\n",
            "Epoch 4614/10000: L(Train): 0.3208978474140167; L(Test): 0.3060285151004791\n",
            "Epoch 4615/10000: L(Train): 0.32620084285736084; L(Test): 0.3047317564487457\n",
            "Epoch 4616/10000: L(Train): 0.3283766210079193; L(Test): 0.3024459779262543\n",
            "Epoch 4617/10000: L(Train): 0.32162532210350037; L(Test): 0.3046451508998871\n",
            "Epoch 4618/10000: L(Train): 0.3351593017578125; L(Test): 0.30320724844932556\n",
            "Epoch 4619/10000: L(Train): 0.3259512484073639; L(Test): 0.3017544448375702\n",
            "Epoch 4620/10000: L(Train): 0.32190945744514465; L(Test): 0.3046984374523163\n",
            "Epoch 4621/10000: L(Train): 0.32154151797294617; L(Test): 0.30385681986808777\n",
            "Epoch 4622/10000: L(Train): 0.33228906989097595; L(Test): 0.3003973662853241\n",
            "Epoch 4623/10000: L(Train): 0.327445387840271; L(Test): 0.3034796714782715\n",
            "Epoch 4624/10000: L(Train): 0.32069823145866394; L(Test): 0.3041125535964966\n",
            "Epoch 4625/10000: L(Train): 0.32378438115119934; L(Test): 0.30312731862068176\n",
            "Epoch 4626/10000: L(Train): 0.3200191855430603; L(Test): 0.3037668466567993\n",
            "Epoch 4627/10000: L(Train): 0.33020737767219543; L(Test): 0.30464693903923035\n",
            "Epoch 4628/10000: L(Train): 0.32034292817115784; L(Test): 0.3028661906719208\n",
            "Epoch 4629/10000: L(Train): 0.3302903175354004; L(Test): 0.3012881577014923\n",
            "Epoch 4630/10000: L(Train): 0.3238048851490021; L(Test): 0.30139821767807007\n",
            "Epoch 4631/10000: L(Train): 0.32234853506088257; L(Test): 0.3004443049430847\n",
            "Epoch 4632/10000: L(Train): 0.32266777753829956; L(Test): 0.29968392848968506\n",
            "Epoch 4633/10000: L(Train): 0.3174446225166321; L(Test): 0.30041587352752686\n",
            "Epoch 4634/10000: L(Train): 0.31110599637031555; L(Test): 0.3017881214618683\n",
            "Epoch 4635/10000: L(Train): 0.32682493329048157; L(Test): 0.2995296120643616\n",
            "Epoch 4636/10000: L(Train): 0.3129931688308716; L(Test): 0.30018118023872375\n",
            "Epoch 4637/10000: L(Train): 0.32689327001571655; L(Test): 0.3000754117965698\n",
            "Epoch 4638/10000: L(Train): 0.3208611309528351; L(Test): 0.29860231280326843\n",
            "Epoch 4639/10000: L(Train): 0.31151875853538513; L(Test): 0.2987981140613556\n",
            "Epoch 4640/10000: L(Train): 0.3313736915588379; L(Test): 0.30121198296546936\n",
            "Epoch 4641/10000: L(Train): 0.32701635360717773; L(Test): 0.303183376789093\n",
            "Epoch 4642/10000: L(Train): 0.32888931035995483; L(Test): 0.3003302812576294\n",
            "Epoch 4643/10000: L(Train): 0.3269960880279541; L(Test): 0.2990681529045105\n",
            "Epoch 4644/10000: L(Train): 0.31973639130592346; L(Test): 0.3005198538303375\n",
            "Epoch 4645/10000: L(Train): 0.3150823414325714; L(Test): 0.30106231570243835\n",
            "Epoch 4646/10000: L(Train): 0.3298960030078888; L(Test): 0.30101460218429565\n",
            "Epoch 4647/10000: L(Train): 0.32426127791404724; L(Test): 0.30023393034935\n",
            "Epoch 4648/10000: L(Train): 0.31775373220443726; L(Test): 0.3011588454246521\n",
            "Epoch 4649/10000: L(Train): 0.3143077790737152; L(Test): 0.30134260654449463\n",
            "Epoch 4650/10000: L(Train): 0.3279113173484802; L(Test): 0.2993934750556946\n",
            "Epoch 4651/10000: L(Train): 0.3296787440776825; L(Test): 0.29806962609291077\n",
            "Epoch 4652/10000: L(Train): 0.32113492488861084; L(Test): 0.29893550276756287\n",
            "Epoch 4653/10000: L(Train): 0.31612157821655273; L(Test): 0.29862725734710693\n",
            "Epoch 4654/10000: L(Train): 0.32509317994117737; L(Test): 0.2985518276691437\n",
            "Epoch 4655/10000: L(Train): 0.321688711643219; L(Test): 0.29939764738082886\n",
            "Epoch 4656/10000: L(Train): 0.32758408784866333; L(Test): 0.29845690727233887\n",
            "Epoch 4657/10000: L(Train): 0.3207632303237915; L(Test): 0.2980062961578369\n",
            "Epoch 4658/10000: L(Train): 0.3272578716278076; L(Test): 0.29755914211273193\n",
            "Epoch 4659/10000: L(Train): 0.31242915987968445; L(Test): 0.29754966497421265\n",
            "Epoch 4660/10000: L(Train): 0.32175973057746887; L(Test): 0.2979297935962677\n",
            "Epoch 4661/10000: L(Train): 0.31515640020370483; L(Test): 0.29848769307136536\n",
            "Epoch 4662/10000: L(Train): 0.335354208946228; L(Test): 0.2986396849155426\n",
            "Epoch 4663/10000: L(Train): 0.3345583379268646; L(Test): 0.29814326763153076\n",
            "Epoch 4664/10000: L(Train): 0.3269771933555603; L(Test): 0.29820945858955383\n",
            "Epoch 4665/10000: L(Train): 0.3250609040260315; L(Test): 0.2988932728767395\n",
            "Epoch 4666/10000: L(Train): 0.3219869136810303; L(Test): 0.2980814278125763\n",
            "Epoch 4667/10000: L(Train): 0.323479026556015; L(Test): 0.29740187525749207\n",
            "Epoch 4668/10000: L(Train): 0.3199300467967987; L(Test): 0.2983280420303345\n",
            "Epoch 4669/10000: L(Train): 0.3188236355781555; L(Test): 0.2994728088378906\n",
            "Epoch 4670/10000: L(Train): 0.32718566060066223; L(Test): 0.29818394780158997\n",
            "Epoch 4671/10000: L(Train): 0.3172166347503662; L(Test): 0.2994380593299866\n",
            "Epoch 4672/10000: L(Train): 0.326053649187088; L(Test): 0.2990996837615967\n",
            "Epoch 4673/10000: L(Train): 0.3263585865497589; L(Test): 0.29763978719711304\n",
            "Epoch 4674/10000: L(Train): 0.3226251006126404; L(Test): 0.29706719517707825\n",
            "Epoch 4675/10000: L(Train): 0.318051815032959; L(Test): 0.29774191975593567\n",
            "Epoch 4676/10000: L(Train): 0.3209417462348938; L(Test): 0.2983357310295105\n",
            "Epoch 4677/10000: L(Train): 0.3177665174007416; L(Test): 0.2984946668148041\n",
            "Epoch 4678/10000: L(Train): 0.32194995880126953; L(Test): 0.298833429813385\n",
            "Epoch 4679/10000: L(Train): 0.3204447031021118; L(Test): 0.2990744411945343\n",
            "Epoch 4680/10000: L(Train): 0.32118871808052063; L(Test): 0.29856300354003906\n",
            "Epoch 4681/10000: L(Train): 0.32035067677497864; L(Test): 0.29890936613082886\n",
            "Epoch 4682/10000: L(Train): 0.3291729688644409; L(Test): 0.30173203349113464\n",
            "Epoch 4683/10000: L(Train): 0.3228307068347931; L(Test): 0.3005916476249695\n",
            "Epoch 4684/10000: L(Train): 0.323995441198349; L(Test): 0.297808438539505\n",
            "Epoch 4685/10000: L(Train): 0.3172991871833801; L(Test): 0.2981315851211548\n",
            "Epoch 4686/10000: L(Train): 0.3275924026966095; L(Test): 0.2986561059951782\n",
            "Epoch 4687/10000: L(Train): 0.3235088586807251; L(Test): 0.29843875765800476\n",
            "Epoch 4688/10000: L(Train): 0.33272644877433777; L(Test): 0.2988136410713196\n",
            "Epoch 4689/10000: L(Train): 0.32093364000320435; L(Test): 0.2990146577358246\n",
            "Epoch 4690/10000: L(Train): 0.32304954528808594; L(Test): 0.2986450493335724\n",
            "Epoch 4691/10000: L(Train): 0.31947237253189087; L(Test): 0.29884329438209534\n",
            "Epoch 4692/10000: L(Train): 0.31619107723236084; L(Test): 0.2982706129550934\n",
            "Epoch 4693/10000: L(Train): 0.3275070786476135; L(Test): 0.29955989122390747\n",
            "Epoch 4694/10000: L(Train): 0.3273758292198181; L(Test): 0.2980878949165344\n",
            "Epoch 4695/10000: L(Train): 0.3351317048072815; L(Test): 0.29756438732147217\n",
            "Epoch 4696/10000: L(Train): 0.32434067130088806; L(Test): 0.29730233550071716\n",
            "Epoch 4697/10000: L(Train): 0.32828080654144287; L(Test): 0.2974017560482025\n",
            "Epoch 4698/10000: L(Train): 0.3202812671661377; L(Test): 0.29841846227645874\n",
            "Epoch 4699/10000: L(Train): 0.3261679410934448; L(Test): 0.2985008955001831\n",
            "Epoch 4700/10000: L(Train): 0.3252866566181183; L(Test): 0.2975964844226837\n",
            "Epoch 4701/10000: L(Train): 0.32832273840904236; L(Test): 0.29699063301086426\n",
            "Epoch 4702/10000: L(Train): 0.3257482349872589; L(Test): 0.2974437177181244\n",
            "Epoch 4703/10000: L(Train): 0.31941044330596924; L(Test): 0.29875442385673523\n",
            "Epoch 4704/10000: L(Train): 0.31509220600128174; L(Test): 0.29872846603393555\n",
            "Epoch 4705/10000: L(Train): 0.3229750096797943; L(Test): 0.2981647253036499\n",
            "Epoch 4706/10000: L(Train): 0.3231281340122223; L(Test): 0.29806211590766907\n",
            "Epoch 4707/10000: L(Train): 0.3228219449520111; L(Test): 0.2975046634674072\n",
            "Epoch 4708/10000: L(Train): 0.3136770725250244; L(Test): 0.2980792820453644\n",
            "Epoch 4709/10000: L(Train): 0.3226431608200073; L(Test): 0.29787495732307434\n",
            "Epoch 4710/10000: L(Train): 0.3237508535385132; L(Test): 0.29833507537841797\n",
            "Epoch 4711/10000: L(Train): 0.32811903953552246; L(Test): 0.2988731861114502\n",
            "Epoch 4712/10000: L(Train): 0.3167758584022522; L(Test): 0.29777711629867554\n",
            "Epoch 4713/10000: L(Train): 0.32252588868141174; L(Test): 0.2966725826263428\n",
            "Epoch 4714/10000: L(Train): 0.32880979776382446; L(Test): 0.2976817190647125\n",
            "Epoch 4715/10000: L(Train): 0.31641727685928345; L(Test): 0.2990368604660034\n",
            "Epoch 4716/10000: L(Train): 0.31861382722854614; L(Test): 0.29810047149658203\n",
            "Epoch 4717/10000: L(Train): 0.3219338059425354; L(Test): 0.2982107698917389\n",
            "Epoch 4718/10000: L(Train): 0.31336989998817444; L(Test): 0.29906341433525085\n",
            "Epoch 4719/10000: L(Train): 0.3340391516685486; L(Test): 0.29824960231781006\n",
            "Epoch 4720/10000: L(Train): 0.3345755934715271; L(Test): 0.2982298731803894\n",
            "Epoch 4721/10000: L(Train): 0.32462257146835327; L(Test): 0.29996785521507263\n",
            "Epoch 4722/10000: L(Train): 0.3268432319164276; L(Test): 0.29989078640937805\n",
            "Epoch 4723/10000: L(Train): 0.3203195631504059; L(Test): 0.2987152934074402\n",
            "Epoch 4724/10000: L(Train): 0.32118746638298035; L(Test): 0.29810649156570435\n",
            "Epoch 4725/10000: L(Train): 0.3293757736682892; L(Test): 0.29962584376335144\n",
            "Epoch 4726/10000: L(Train): 0.32139357924461365; L(Test): 0.29848361015319824\n",
            "Epoch 4727/10000: L(Train): 0.3181070387363434; L(Test): 0.299299031496048\n",
            "Epoch 4728/10000: L(Train): 0.3221134543418884; L(Test): 0.29983729124069214\n",
            "Epoch 4729/10000: L(Train): 0.3256952464580536; L(Test): 0.2991441786289215\n",
            "Epoch 4730/10000: L(Train): 0.32511964440345764; L(Test): 0.29852232336997986\n",
            "Epoch 4731/10000: L(Train): 0.3221544325351715; L(Test): 0.29968470335006714\n",
            "Epoch 4732/10000: L(Train): 0.3246122896671295; L(Test): 0.29949671030044556\n",
            "Epoch 4733/10000: L(Train): 0.32006895542144775; L(Test): 0.3004698157310486\n",
            "Epoch 4734/10000: L(Train): 0.3146432936191559; L(Test): 0.30295050144195557\n",
            "Epoch 4735/10000: L(Train): 0.32497164607048035; L(Test): 0.3004617989063263\n",
            "Epoch 4736/10000: L(Train): 0.31537556648254395; L(Test): 0.30136603116989136\n",
            "Epoch 4737/10000: L(Train): 0.3237685561180115; L(Test): 0.3006470203399658\n",
            "Epoch 4738/10000: L(Train): 0.31705909967422485; L(Test): 0.29954618215560913\n",
            "Epoch 4739/10000: L(Train): 0.3259083330631256; L(Test): 0.30031752586364746\n",
            "Epoch 4740/10000: L(Train): 0.31927916407585144; L(Test): 0.29908493161201477\n",
            "Epoch 4741/10000: L(Train): 0.3235785961151123; L(Test): 0.29873964190483093\n",
            "Epoch 4742/10000: L(Train): 0.31755203008651733; L(Test): 0.2986885607242584\n",
            "Epoch 4743/10000: L(Train): 0.32125088572502136; L(Test): 0.2988138496875763\n",
            "Epoch 4744/10000: L(Train): 0.31601884961128235; L(Test): 0.30054032802581787\n",
            "Epoch 4745/10000: L(Train): 0.3244326412677765; L(Test): 0.3019424378871918\n",
            "Epoch 4746/10000: L(Train): 0.3244266211986542; L(Test): 0.3002129793167114\n",
            "Epoch 4747/10000: L(Train): 0.31685006618499756; L(Test): 0.298503577709198\n",
            "Epoch 4748/10000: L(Train): 0.32404881715774536; L(Test): 0.2993682622909546\n",
            "Epoch 4749/10000: L(Train): 0.31572970747947693; L(Test): 0.29956915974617004\n",
            "Epoch 4750/10000: L(Train): 0.312142014503479; L(Test): 0.298495352268219\n",
            "Epoch 4751/10000: L(Train): 0.32723888754844666; L(Test): 0.29838526248931885\n",
            "Epoch 4752/10000: L(Train): 0.32143011689186096; L(Test): 0.29869723320007324\n",
            "Epoch 4753/10000: L(Train): 0.3177473545074463; L(Test): 0.29978927969932556\n",
            "Epoch 4754/10000: L(Train): 0.3180529773235321; L(Test): 0.29984697699546814\n",
            "Epoch 4755/10000: L(Train): 0.3252626657485962; L(Test): 0.29900434613227844\n",
            "Epoch 4756/10000: L(Train): 0.3340304493904114; L(Test): 0.29854387044906616\n",
            "Epoch 4757/10000: L(Train): 0.33174026012420654; L(Test): 0.29893141984939575\n",
            "Epoch 4758/10000: L(Train): 0.3275686502456665; L(Test): 0.29939737915992737\n",
            "Epoch 4759/10000: L(Train): 0.3165854513645172; L(Test): 0.2991316318511963\n",
            "Epoch 4760/10000: L(Train): 0.3319057524204254; L(Test): 0.2993423342704773\n",
            "Epoch 4761/10000: L(Train): 0.31841012835502625; L(Test): 0.2994386851787567\n",
            "Epoch 4762/10000: L(Train): 0.33115941286087036; L(Test): 0.29900479316711426\n",
            "Epoch 4763/10000: L(Train): 0.3214854598045349; L(Test): 0.2988426089286804\n",
            "Epoch 4764/10000: L(Train): 0.33102208375930786; L(Test): 0.29900673031806946\n",
            "Epoch 4765/10000: L(Train): 0.31640344858169556; L(Test): 0.2993725538253784\n",
            "Epoch 4766/10000: L(Train): 0.3254450261592865; L(Test): 0.2991735339164734\n",
            "Epoch 4767/10000: L(Train): 0.3257017135620117; L(Test): 0.2981663644313812\n",
            "Epoch 4768/10000: L(Train): 0.32468748092651367; L(Test): 0.29821085929870605\n",
            "Epoch 4769/10000: L(Train): 0.3317382335662842; L(Test): 0.298988938331604\n",
            "Epoch 4770/10000: L(Train): 0.31853917241096497; L(Test): 0.29950615763664246\n",
            "Epoch 4771/10000: L(Train): 0.3253932297229767; L(Test): 0.2996123135089874\n",
            "Epoch 4772/10000: L(Train): 0.3221897780895233; L(Test): 0.29969269037246704\n",
            "Epoch 4773/10000: L(Train): 0.3286413848400116; L(Test): 0.30007296800613403\n",
            "Epoch 4774/10000: L(Train): 0.3203382194042206; L(Test): 0.3012905418872833\n",
            "Epoch 4775/10000: L(Train): 0.326630562543869; L(Test): 0.3007490634918213\n",
            "Epoch 4776/10000: L(Train): 0.3276634216308594; L(Test): 0.29899248480796814\n",
            "Epoch 4777/10000: L(Train): 0.322959303855896; L(Test): 0.29939892888069153\n",
            "Epoch 4778/10000: L(Train): 0.32708555459976196; L(Test): 0.3014792799949646\n",
            "Epoch 4779/10000: L(Train): 0.3278307616710663; L(Test): 0.3008136749267578\n",
            "Epoch 4780/10000: L(Train): 0.328431099653244; L(Test): 0.3000606894493103\n",
            "Epoch 4781/10000: L(Train): 0.3148651719093323; L(Test): 0.304611474275589\n",
            "Epoch 4782/10000: L(Train): 0.33994463086128235; L(Test): 0.3039766550064087\n",
            "Epoch 4783/10000: L(Train): 0.3346889913082123; L(Test): 0.3056943714618683\n",
            "Epoch 4784/10000: L(Train): 0.3229132294654846; L(Test): 0.30607521533966064\n",
            "Epoch 4785/10000: L(Train): 0.32285451889038086; L(Test): 0.3043442964553833\n",
            "Epoch 4786/10000: L(Train): 0.31671079993247986; L(Test): 0.3039385676383972\n",
            "Epoch 4787/10000: L(Train): 0.3272787034511566; L(Test): 0.3042793571949005\n",
            "Epoch 4788/10000: L(Train): 0.3363048732280731; L(Test): 0.30514925718307495\n",
            "Epoch 4789/10000: L(Train): 0.3305002450942993; L(Test): 0.3046244978904724\n",
            "Epoch 4790/10000: L(Train): 0.32035142183303833; L(Test): 0.3020583391189575\n",
            "Epoch 4791/10000: L(Train): 0.3196505010128021; L(Test): 0.30196791887283325\n",
            "Epoch 4792/10000: L(Train): 0.3230361342430115; L(Test): 0.30211400985717773\n",
            "Epoch 4793/10000: L(Train): 0.3257055878639221; L(Test): 0.30147382616996765\n",
            "Epoch 4794/10000: L(Train): 0.3201654255390167; L(Test): 0.3011220693588257\n",
            "Epoch 4795/10000: L(Train): 0.3304903209209442; L(Test): 0.300802081823349\n",
            "Epoch 4796/10000: L(Train): 0.3292933404445648; L(Test): 0.30038079619407654\n",
            "Epoch 4797/10000: L(Train): 0.32367143034935; L(Test): 0.3001093864440918\n",
            "Epoch 4798/10000: L(Train): 0.3286599814891815; L(Test): 0.29955169558525085\n",
            "Epoch 4799/10000: L(Train): 0.31632256507873535; L(Test): 0.29975229501724243\n",
            "Epoch 4800/10000: L(Train): 0.3307889997959137; L(Test): 0.2994251549243927\n",
            "Epoch 4801/10000: L(Train): 0.3263431191444397; L(Test): 0.2988818287849426\n",
            "Epoch 4802/10000: L(Train): 0.322064071893692; L(Test): 0.2995935082435608\n",
            "Epoch 4803/10000: L(Train): 0.3245771527290344; L(Test): 0.29930874705314636\n",
            "Epoch 4804/10000: L(Train): 0.32138076424598694; L(Test): 0.2982051372528076\n",
            "Epoch 4805/10000: L(Train): 0.3193286955356598; L(Test): 0.2999846339225769\n",
            "Epoch 4806/10000: L(Train): 0.3285224437713623; L(Test): 0.29968753457069397\n",
            "Epoch 4807/10000: L(Train): 0.32493677735328674; L(Test): 0.2978084087371826\n",
            "Epoch 4808/10000: L(Train): 0.3133489787578583; L(Test): 0.29894641041755676\n",
            "Epoch 4809/10000: L(Train): 0.3251241147518158; L(Test): 0.2987552285194397\n",
            "Epoch 4810/10000: L(Train): 0.32153451442718506; L(Test): 0.2981511354446411\n",
            "Epoch 4811/10000: L(Train): 0.31806662678718567; L(Test): 0.3013579249382019\n",
            "Epoch 4812/10000: L(Train): 0.31759771704673767; L(Test): 0.3010469079017639\n",
            "Epoch 4813/10000: L(Train): 0.3120107054710388; L(Test): 0.2987772524356842\n",
            "Epoch 4814/10000: L(Train): 0.3196101188659668; L(Test): 0.2991313636302948\n",
            "Epoch 4815/10000: L(Train): 0.3185157775878906; L(Test): 0.299363911151886\n",
            "Epoch 4816/10000: L(Train): 0.3205752670764923; L(Test): 0.2987872064113617\n",
            "Epoch 4817/10000: L(Train): 0.3311947286128998; L(Test): 0.2992663085460663\n",
            "Epoch 4818/10000: L(Train): 0.32542991638183594; L(Test): 0.2990490198135376\n",
            "Epoch 4819/10000: L(Train): 0.3297012150287628; L(Test): 0.2989419996738434\n",
            "Epoch 4820/10000: L(Train): 0.31751784682273865; L(Test): 0.2988606095314026\n",
            "Epoch 4821/10000: L(Train): 0.33535659313201904; L(Test): 0.2982484996318817\n",
            "Epoch 4822/10000: L(Train): 0.32449692487716675; L(Test): 0.2993939518928528\n",
            "Epoch 4823/10000: L(Train): 0.3206881284713745; L(Test): 0.2986755073070526\n",
            "Epoch 4824/10000: L(Train): 0.322601318359375; L(Test): 0.2973426878452301\n",
            "Epoch 4825/10000: L(Train): 0.3255806267261505; L(Test): 0.29858633875846863\n",
            "Epoch 4826/10000: L(Train): 0.31987297534942627; L(Test): 0.29855620861053467\n",
            "Epoch 4827/10000: L(Train): 0.3199416399002075; L(Test): 0.29976320266723633\n",
            "Epoch 4828/10000: L(Train): 0.32372161746025085; L(Test): 0.29883772134780884\n",
            "Epoch 4829/10000: L(Train): 0.32579928636550903; L(Test): 0.2985515296459198\n",
            "Epoch 4830/10000: L(Train): 0.32071295380592346; L(Test): 0.29915308952331543\n",
            "Epoch 4831/10000: L(Train): 0.3314513862133026; L(Test): 0.29885995388031006\n",
            "Epoch 4832/10000: L(Train): 0.32535526156425476; L(Test): 0.29911112785339355\n",
            "Epoch 4833/10000: L(Train): 0.31605878472328186; L(Test): 0.2993353009223938\n",
            "Epoch 4834/10000: L(Train): 0.3167024552822113; L(Test): 0.2994216978549957\n",
            "Epoch 4835/10000: L(Train): 0.3235183358192444; L(Test): 0.3060718774795532\n",
            "Epoch 4836/10000: L(Train): 0.3327527940273285; L(Test): 0.3050653636455536\n",
            "Epoch 4837/10000: L(Train): 0.31446585059165955; L(Test): 0.3030361831188202\n",
            "Epoch 4838/10000: L(Train): 0.33255308866500854; L(Test): 0.30690473318099976\n",
            "Epoch 4839/10000: L(Train): 0.3321022093296051; L(Test): 0.3042290508747101\n",
            "Epoch 4840/10000: L(Train): 0.32239583134651184; L(Test): 0.3047172427177429\n",
            "Epoch 4841/10000: L(Train): 0.32669129967689514; L(Test): 0.3069455623626709\n",
            "Epoch 4842/10000: L(Train): 0.33520591259002686; L(Test): 0.3035241961479187\n",
            "Epoch 4843/10000: L(Train): 0.3332039415836334; L(Test): 0.3025900423526764\n",
            "Epoch 4844/10000: L(Train): 0.3232139050960541; L(Test): 0.3046873211860657\n",
            "Epoch 4845/10000: L(Train): 0.32855427265167236; L(Test): 0.30289870500564575\n",
            "Epoch 4846/10000: L(Train): 0.33002009987831116; L(Test): 0.30247727036476135\n",
            "Epoch 4847/10000: L(Train): 0.3224029541015625; L(Test): 0.30384400486946106\n",
            "Epoch 4848/10000: L(Train): 0.33932822942733765; L(Test): 0.30216994881629944\n",
            "Epoch 4849/10000: L(Train): 0.3295402228832245; L(Test): 0.3006782531738281\n",
            "Epoch 4850/10000: L(Train): 0.31815576553344727; L(Test): 0.30527830123901367\n",
            "Epoch 4851/10000: L(Train): 0.34017282724380493; L(Test): 0.30572208762168884\n",
            "Epoch 4852/10000: L(Train): 0.33428260684013367; L(Test): 0.3038794696331024\n",
            "Epoch 4853/10000: L(Train): 0.3219371736049652; L(Test): 0.3075760006904602\n",
            "Epoch 4854/10000: L(Train): 0.34298646450042725; L(Test): 0.30586913228034973\n",
            "Epoch 4855/10000: L(Train): 0.32991328835487366; L(Test): 0.3036329448223114\n",
            "Epoch 4856/10000: L(Train): 0.3329812288284302; L(Test): 0.3074634373188019\n",
            "Epoch 4857/10000: L(Train): 0.32078656554222107; L(Test): 0.30786392092704773\n",
            "Epoch 4858/10000: L(Train): 0.32697927951812744; L(Test): 0.3033111095428467\n",
            "Epoch 4859/10000: L(Train): 0.3384891450405121; L(Test): 0.3057255446910858\n",
            "Epoch 4860/10000: L(Train): 0.3198704719543457; L(Test): 0.3083155155181885\n",
            "Epoch 4861/10000: L(Train): 0.33016079664230347; L(Test): 0.3034273087978363\n",
            "Epoch 4862/10000: L(Train): 0.3241862952709198; L(Test): 0.301973432302475\n",
            "Epoch 4863/10000: L(Train): 0.3236403465270996; L(Test): 0.30314043164253235\n",
            "Epoch 4864/10000: L(Train): 0.328919917345047; L(Test): 0.3021773099899292\n",
            "Epoch 4865/10000: L(Train): 0.32253143191337585; L(Test): 0.3014596700668335\n",
            "Epoch 4866/10000: L(Train): 0.3228209912776947; L(Test): 0.30226701498031616\n",
            "Epoch 4867/10000: L(Train): 0.3278663158416748; L(Test): 0.3024895191192627\n",
            "Epoch 4868/10000: L(Train): 0.3156963288784027; L(Test): 0.3021901547908783\n",
            "Epoch 4869/10000: L(Train): 0.32447314262390137; L(Test): 0.3012430965900421\n",
            "Epoch 4870/10000: L(Train): 0.3149774670600891; L(Test): 0.3011147379875183\n",
            "Epoch 4871/10000: L(Train): 0.31872814893722534; L(Test): 0.3021339476108551\n",
            "Epoch 4872/10000: L(Train): 0.3321498930454254; L(Test): 0.30265161395072937\n",
            "Epoch 4873/10000: L(Train): 0.32846885919570923; L(Test): 0.300130695104599\n",
            "Epoch 4874/10000: L(Train): 0.3237273693084717; L(Test): 0.29933685064315796\n",
            "Epoch 4875/10000: L(Train): 0.32498693466186523; L(Test): 0.3008915185928345\n",
            "Epoch 4876/10000: L(Train): 0.3191825747489929; L(Test): 0.3008430004119873\n",
            "Epoch 4877/10000: L(Train): 0.3207542896270752; L(Test): 0.30000096559524536\n",
            "Epoch 4878/10000: L(Train): 0.325939416885376; L(Test): 0.29894503951072693\n",
            "Epoch 4879/10000: L(Train): 0.3208613693714142; L(Test): 0.29961174726486206\n",
            "Epoch 4880/10000: L(Train): 0.32627344131469727; L(Test): 0.29953092336654663\n",
            "Epoch 4881/10000: L(Train): 0.32461950182914734; L(Test): 0.29927951097488403\n",
            "Epoch 4882/10000: L(Train): 0.31682294607162476; L(Test): 0.29934313893318176\n",
            "Epoch 4883/10000: L(Train): 0.3302070200443268; L(Test): 0.29862529039382935\n",
            "Epoch 4884/10000: L(Train): 0.32486671209335327; L(Test): 0.2988233268260956\n",
            "Epoch 4885/10000: L(Train): 0.32381269335746765; L(Test): 0.2985268235206604\n",
            "Epoch 4886/10000: L(Train): 0.3194192349910736; L(Test): 0.29832592606544495\n",
            "Epoch 4887/10000: L(Train): 0.3113183379173279; L(Test): 0.2987384796142578\n",
            "Epoch 4888/10000: L(Train): 0.3248826861381531; L(Test): 0.29902464151382446\n",
            "Epoch 4889/10000: L(Train): 0.3206338584423065; L(Test): 0.2989388108253479\n",
            "Epoch 4890/10000: L(Train): 0.3257941007614136; L(Test): 0.2980729341506958\n",
            "Epoch 4891/10000: L(Train): 0.31737080216407776; L(Test): 0.2987266480922699\n",
            "Epoch 4892/10000: L(Train): 0.3191429674625397; L(Test): 0.2987956404685974\n",
            "Epoch 4893/10000: L(Train): 0.3125082552433014; L(Test): 0.2997947931289673\n",
            "Epoch 4894/10000: L(Train): 0.32853779196739197; L(Test): 0.30017054080963135\n",
            "Epoch 4895/10000: L(Train): 0.3174391984939575; L(Test): 0.29844027757644653\n",
            "Epoch 4896/10000: L(Train): 0.3201499581336975; L(Test): 0.29867514967918396\n",
            "Epoch 4897/10000: L(Train): 0.3217056393623352; L(Test): 0.2987658381462097\n",
            "Epoch 4898/10000: L(Train): 0.3242718279361725; L(Test): 0.29897207021713257\n",
            "Epoch 4899/10000: L(Train): 0.3269208073616028; L(Test): 0.2988850772380829\n",
            "Epoch 4900/10000: L(Train): 0.32408106327056885; L(Test): 0.29835745692253113\n",
            "Epoch 4901/10000: L(Train): 0.32374611496925354; L(Test): 0.29822301864624023\n",
            "Epoch 4902/10000: L(Train): 0.31858426332473755; L(Test): 0.2983916103839874\n",
            "Epoch 4903/10000: L(Train): 0.3174813985824585; L(Test): 0.2987217903137207\n",
            "Epoch 4904/10000: L(Train): 0.31977713108062744; L(Test): 0.29851388931274414\n",
            "Epoch 4905/10000: L(Train): 0.3261246681213379; L(Test): 0.29858124256134033\n",
            "Epoch 4906/10000: L(Train): 0.3240138590335846; L(Test): 0.2988818883895874\n",
            "Epoch 4907/10000: L(Train): 0.3176119327545166; L(Test): 0.2986041009426117\n",
            "Epoch 4908/10000: L(Train): 0.3247474133968353; L(Test): 0.2986270487308502\n",
            "Epoch 4909/10000: L(Train): 0.32479777932167053; L(Test): 0.298289954662323\n",
            "Epoch 4910/10000: L(Train): 0.312838613986969; L(Test): 0.29767248034477234\n",
            "Epoch 4911/10000: L(Train): 0.32979366183280945; L(Test): 0.2983322739601135\n",
            "Epoch 4912/10000: L(Train): 0.3170463442802429; L(Test): 0.300051212310791\n",
            "Epoch 4913/10000: L(Train): 0.3178565204143524; L(Test): 0.30054664611816406\n",
            "Epoch 4914/10000: L(Train): 0.3254644572734833; L(Test): 0.29872214794158936\n",
            "Epoch 4915/10000: L(Train): 0.31942102313041687; L(Test): 0.29844579100608826\n",
            "Epoch 4916/10000: L(Train): 0.3273877203464508; L(Test): 0.29888203740119934\n",
            "Epoch 4917/10000: L(Train): 0.3150801658630371; L(Test): 0.29922616481781006\n",
            "Epoch 4918/10000: L(Train): 0.31740280985832214; L(Test): 0.29942986369132996\n",
            "Epoch 4919/10000: L(Train): 0.31765061616897583; L(Test): 0.29970693588256836\n",
            "Epoch 4920/10000: L(Train): 0.3151073455810547; L(Test): 0.30013328790664673\n",
            "Epoch 4921/10000: L(Train): 0.32547542452812195; L(Test): 0.3000524044036865\n",
            "Epoch 4922/10000: L(Train): 0.31815558671951294; L(Test): 0.2996581792831421\n",
            "Epoch 4923/10000: L(Train): 0.32373031973838806; L(Test): 0.29953840374946594\n",
            "Epoch 4924/10000: L(Train): 0.3159238398075104; L(Test): 0.2992972731590271\n",
            "Epoch 4925/10000: L(Train): 0.32871100306510925; L(Test): 0.2990555763244629\n",
            "Epoch 4926/10000: L(Train): 0.32013651728630066; L(Test): 0.2994868755340576\n",
            "Epoch 4927/10000: L(Train): 0.33625364303588867; L(Test): 0.2992746829986572\n",
            "Epoch 4928/10000: L(Train): 0.3261282742023468; L(Test): 0.29893916845321655\n",
            "Epoch 4929/10000: L(Train): 0.31527888774871826; L(Test): 0.29898515343666077\n",
            "Epoch 4930/10000: L(Train): 0.3199097514152527; L(Test): 0.29879269003868103\n",
            "Epoch 4931/10000: L(Train): 0.32758021354675293; L(Test): 0.2979101836681366\n",
            "Epoch 4932/10000: L(Train): 0.32395896315574646; L(Test): 0.29902583360671997\n",
            "Epoch 4933/10000: L(Train): 0.32990598678588867; L(Test): 0.29991257190704346\n",
            "Epoch 4934/10000: L(Train): 0.32686924934387207; L(Test): 0.299016535282135\n",
            "Epoch 4935/10000: L(Train): 0.3240835666656494; L(Test): 0.2980678975582123\n",
            "Epoch 4936/10000: L(Train): 0.3217115104198456; L(Test): 0.29754316806793213\n",
            "Epoch 4937/10000: L(Train): 0.32978877425193787; L(Test): 0.29794615507125854\n",
            "Epoch 4938/10000: L(Train): 0.31283313035964966; L(Test): 0.29837754368782043\n",
            "Epoch 4939/10000: L(Train): 0.31529343128204346; L(Test): 0.2987489104270935\n",
            "Epoch 4940/10000: L(Train): 0.3256469666957855; L(Test): 0.2980930507183075\n",
            "Epoch 4941/10000: L(Train): 0.3187578022480011; L(Test): 0.2975090444087982\n",
            "Epoch 4942/10000: L(Train): 0.32513928413391113; L(Test): 0.2983696460723877\n",
            "Epoch 4943/10000: L(Train): 0.3279550075531006; L(Test): 0.29854893684387207\n",
            "Epoch 4944/10000: L(Train): 0.3184033930301666; L(Test): 0.2976974844932556\n",
            "Epoch 4945/10000: L(Train): 0.3223361670970917; L(Test): 0.29732927680015564\n",
            "Epoch 4946/10000: L(Train): 0.31439921259880066; L(Test): 0.2970588207244873\n",
            "Epoch 4947/10000: L(Train): 0.3288673460483551; L(Test): 0.2971552908420563\n",
            "Epoch 4948/10000: L(Train): 0.31412604451179504; L(Test): 0.2977873682975769\n",
            "Epoch 4949/10000: L(Train): 0.3195379674434662; L(Test): 0.296792209148407\n",
            "Epoch 4950/10000: L(Train): 0.32768064737319946; L(Test): 0.2962808907032013\n",
            "Epoch 4951/10000: L(Train): 0.31873831152915955; L(Test): 0.29710328578948975\n",
            "Epoch 4952/10000: L(Train): 0.3239315450191498; L(Test): 0.2968190908432007\n",
            "Epoch 4953/10000: L(Train): 0.3141723573207855; L(Test): 0.29583626985549927\n",
            "Epoch 4954/10000: L(Train): 0.32367759943008423; L(Test): 0.29693543910980225\n",
            "Epoch 4955/10000: L(Train): 0.32359471917152405; L(Test): 0.29829931259155273\n",
            "Epoch 4956/10000: L(Train): 0.3163264989852905; L(Test): 0.29863226413726807\n",
            "Epoch 4957/10000: L(Train): 0.32813602685928345; L(Test): 0.297381192445755\n",
            "Epoch 4958/10000: L(Train): 0.32141831517219543; L(Test): 0.2967452108860016\n",
            "Epoch 4959/10000: L(Train): 0.31245356798171997; L(Test): 0.29837727546691895\n",
            "Epoch 4960/10000: L(Train): 0.3276980221271515; L(Test): 0.2991015315055847\n",
            "Epoch 4961/10000: L(Train): 0.3184541165828705; L(Test): 0.2980682849884033\n",
            "Epoch 4962/10000: L(Train): 0.31716257333755493; L(Test): 0.30094584822654724\n",
            "Epoch 4963/10000: L(Train): 0.3225916624069214; L(Test): 0.30246463418006897\n",
            "Epoch 4964/10000: L(Train): 0.328482985496521; L(Test): 0.3014519512653351\n",
            "Epoch 4965/10000: L(Train): 0.32558512687683105; L(Test): 0.3007557988166809\n",
            "Epoch 4966/10000: L(Train): 0.3290253281593323; L(Test): 0.3001425266265869\n",
            "Epoch 4967/10000: L(Train): 0.33226507902145386; L(Test): 0.29924991726875305\n",
            "Epoch 4968/10000: L(Train): 0.32458990812301636; L(Test): 0.29951387643814087\n",
            "Epoch 4969/10000: L(Train): 0.31835636496543884; L(Test): 0.30042943358421326\n",
            "Epoch 4970/10000: L(Train): 0.31720802187919617; L(Test): 0.300572007894516\n",
            "Epoch 4971/10000: L(Train): 0.3207951784133911; L(Test): 0.29999446868896484\n",
            "Epoch 4972/10000: L(Train): 0.3135606050491333; L(Test): 0.300244003534317\n",
            "Epoch 4973/10000: L(Train): 0.32050037384033203; L(Test): 0.29950085282325745\n",
            "Epoch 4974/10000: L(Train): 0.3250940442085266; L(Test): 0.29850250482559204\n",
            "Epoch 4975/10000: L(Train): 0.3297830820083618; L(Test): 0.2989940345287323\n",
            "Epoch 4976/10000: L(Train): 0.3259199261665344; L(Test): 0.3001421391963959\n",
            "Epoch 4977/10000: L(Train): 0.3192521035671234; L(Test): 0.2996498644351959\n",
            "Epoch 4978/10000: L(Train): 0.32332322001457214; L(Test): 0.29855674505233765\n",
            "Epoch 4979/10000: L(Train): 0.3211861550807953; L(Test): 0.2981019914150238\n",
            "Epoch 4980/10000: L(Train): 0.3254956007003784; L(Test): 0.29712384939193726\n",
            "Epoch 4981/10000: L(Train): 0.33031415939331055; L(Test): 0.2969754934310913\n",
            "Epoch 4982/10000: L(Train): 0.3296000361442566; L(Test): 0.2977619767189026\n",
            "Epoch 4983/10000: L(Train): 0.31777676939964294; L(Test): 0.29804083704948425\n",
            "Epoch 4984/10000: L(Train): 0.32229721546173096; L(Test): 0.29825252294540405\n",
            "Epoch 4985/10000: L(Train): 0.3151046931743622; L(Test): 0.2978943884372711\n",
            "Epoch 4986/10000: L(Train): 0.3236164152622223; L(Test): 0.2976325452327728\n",
            "Epoch 4987/10000: L(Train): 0.32859572768211365; L(Test): 0.29674220085144043\n",
            "Epoch 4988/10000: L(Train): 0.3237287998199463; L(Test): 0.2975042462348938\n",
            "Epoch 4989/10000: L(Train): 0.325899213552475; L(Test): 0.2977636158466339\n",
            "Epoch 4990/10000: L(Train): 0.3209080100059509; L(Test): 0.29734376072883606\n",
            "Epoch 4991/10000: L(Train): 0.3226015567779541; L(Test): 0.297958105802536\n",
            "Epoch 4992/10000: L(Train): 0.32219067215919495; L(Test): 0.297187864780426\n",
            "Epoch 4993/10000: L(Train): 0.32897695899009705; L(Test): 0.2967938184738159\n",
            "Epoch 4994/10000: L(Train): 0.32485246658325195; L(Test): 0.29730114340782166\n",
            "Epoch 4995/10000: L(Train): 0.3180910348892212; L(Test): 0.29839783906936646\n",
            "Epoch 4996/10000: L(Train): 0.32001793384552; L(Test): 0.2995266616344452\n",
            "Epoch 4997/10000: L(Train): 0.3288237452507019; L(Test): 0.2989487648010254\n",
            "Epoch 4998/10000: L(Train): 0.32002967596054077; L(Test): 0.29798197746276855\n",
            "Epoch 4999/10000: L(Train): 0.3211609721183777; L(Test): 0.29751142859458923\n",
            "Epoch 5000/10000: L(Train): 0.3104311227798462; L(Test): 0.29717501997947693\n",
            "Epoch 5001/10000: L(Train): 0.32483726739883423; L(Test): 0.29759112000465393\n",
            "Epoch 5002/10000: L(Train): 0.322592169046402; L(Test): 0.29660844802856445\n",
            "Epoch 5003/10000: L(Train): 0.3163304626941681; L(Test): 0.2968258559703827\n",
            "Epoch 5004/10000: L(Train): 0.32666051387786865; L(Test): 0.29808858036994934\n",
            "Epoch 5005/10000: L(Train): 0.33094483613967896; L(Test): 0.29780256748199463\n",
            "Epoch 5006/10000: L(Train): 0.3269110321998596; L(Test): 0.2976459264755249\n",
            "Epoch 5007/10000: L(Train): 0.3262477219104767; L(Test): 0.29895544052124023\n",
            "Epoch 5008/10000: L(Train): 0.33413177728652954; L(Test): 0.29895585775375366\n",
            "Epoch 5009/10000: L(Train): 0.3134012520313263; L(Test): 0.29853689670562744\n",
            "Epoch 5010/10000: L(Train): 0.3333114981651306; L(Test): 0.29884716868400574\n",
            "Epoch 5011/10000: L(Train): 0.3202482759952545; L(Test): 0.2981645464897156\n",
            "Epoch 5012/10000: L(Train): 0.33061483502388; L(Test): 0.2968505620956421\n",
            "Epoch 5013/10000: L(Train): 0.30732569098472595; L(Test): 0.2981541156768799\n",
            "Epoch 5014/10000: L(Train): 0.32888248562812805; L(Test): 0.2992490828037262\n",
            "Epoch 5015/10000: L(Train): 0.3188418745994568; L(Test): 0.2991170287132263\n",
            "Epoch 5016/10000: L(Train): 0.3326064348220825; L(Test): 0.29896828532218933\n",
            "Epoch 5017/10000: L(Train): 0.32219836115837097; L(Test): 0.2990607023239136\n",
            "Epoch 5018/10000: L(Train): 0.320382297039032; L(Test): 0.2985937297344208\n",
            "Epoch 5019/10000: L(Train): 0.32405224442481995; L(Test): 0.2978566288948059\n",
            "Epoch 5020/10000: L(Train): 0.3204399645328522; L(Test): 0.29775092005729675\n",
            "Epoch 5021/10000: L(Train): 0.3259456157684326; L(Test): 0.29798686504364014\n",
            "Epoch 5022/10000: L(Train): 0.32468417286872864; L(Test): 0.2984979748725891\n",
            "Epoch 5023/10000: L(Train): 0.33217135071754456; L(Test): 0.29826411604881287\n",
            "Epoch 5024/10000: L(Train): 0.31742456555366516; L(Test): 0.2975401282310486\n",
            "Epoch 5025/10000: L(Train): 0.3236726224422455; L(Test): 0.2977566123008728\n",
            "Epoch 5026/10000: L(Train): 0.32771211862564087; L(Test): 0.2974550127983093\n",
            "Epoch 5027/10000: L(Train): 0.31771978735923767; L(Test): 0.29806268215179443\n",
            "Epoch 5028/10000: L(Train): 0.3147644102573395; L(Test): 0.29816389083862305\n",
            "Epoch 5029/10000: L(Train): 0.3182239234447479; L(Test): 0.2973194718360901\n",
            "Epoch 5030/10000: L(Train): 0.31492871046066284; L(Test): 0.296709805727005\n",
            "Epoch 5031/10000: L(Train): 0.3143864572048187; L(Test): 0.29639503359794617\n",
            "Epoch 5032/10000: L(Train): 0.3182130753993988; L(Test): 0.29650211334228516\n",
            "Epoch 5033/10000: L(Train): 0.3178656995296478; L(Test): 0.2969845235347748\n",
            "Epoch 5034/10000: L(Train): 0.31854337453842163; L(Test): 0.2967057228088379\n",
            "Epoch 5035/10000: L(Train): 0.33230656385421753; L(Test): 0.2963491380214691\n",
            "Epoch 5036/10000: L(Train): 0.31781622767448425; L(Test): 0.2975848913192749\n",
            "Epoch 5037/10000: L(Train): 0.3228479027748108; L(Test): 0.29739055037498474\n",
            "Epoch 5038/10000: L(Train): 0.32710760831832886; L(Test): 0.29625481367111206\n",
            "Epoch 5039/10000: L(Train): 0.3207760453224182; L(Test): 0.2977466881275177\n",
            "Epoch 5040/10000: L(Train): 0.3112083077430725; L(Test): 0.2983204126358032\n",
            "Epoch 5041/10000: L(Train): 0.32939642667770386; L(Test): 0.29761168360710144\n",
            "Epoch 5042/10000: L(Train): 0.33183324337005615; L(Test): 0.2980700731277466\n",
            "Epoch 5043/10000: L(Train): 0.32523414492607117; L(Test): 0.29821541905403137\n",
            "Epoch 5044/10000: L(Train): 0.31821298599243164; L(Test): 0.29743894934654236\n",
            "Epoch 5045/10000: L(Train): 0.3229900300502777; L(Test): 0.297513872385025\n",
            "Epoch 5046/10000: L(Train): 0.31958410143852234; L(Test): 0.298321008682251\n",
            "Epoch 5047/10000: L(Train): 0.32175734639167786; L(Test): 0.2983069121837616\n",
            "Epoch 5048/10000: L(Train): 0.3266945481300354; L(Test): 0.2979370951652527\n",
            "Epoch 5049/10000: L(Train): 0.3139716386795044; L(Test): 0.29826027154922485\n",
            "Epoch 5050/10000: L(Train): 0.322836697101593; L(Test): 0.2979525327682495\n",
            "Epoch 5051/10000: L(Train): 0.3139498829841614; L(Test): 0.2980062961578369\n",
            "Epoch 5052/10000: L(Train): 0.32430270314216614; L(Test): 0.29930418729782104\n",
            "Epoch 5053/10000: L(Train): 0.3218148648738861; L(Test): 0.2995508015155792\n",
            "Epoch 5054/10000: L(Train): 0.32312533259391785; L(Test): 0.2974608838558197\n",
            "Epoch 5055/10000: L(Train): 0.3205524981021881; L(Test): 0.2979522943496704\n",
            "Epoch 5056/10000: L(Train): 0.33166438341140747; L(Test): 0.3014683723449707\n",
            "Epoch 5057/10000: L(Train): 0.3219005763530731; L(Test): 0.30019843578338623\n",
            "Epoch 5058/10000: L(Train): 0.3215068280696869; L(Test): 0.29878512024879456\n",
            "Epoch 5059/10000: L(Train): 0.3145023286342621; L(Test): 0.30180492997169495\n",
            "Epoch 5060/10000: L(Train): 0.33908021450042725; L(Test): 0.3024490475654602\n",
            "Epoch 5061/10000: L(Train): 0.3306885063648224; L(Test): 0.30002957582473755\n",
            "Epoch 5062/10000: L(Train): 0.31773459911346436; L(Test): 0.30104920268058777\n",
            "Epoch 5063/10000: L(Train): 0.3234981596469879; L(Test): 0.3016645908355713\n",
            "Epoch 5064/10000: L(Train): 0.3244452178478241; L(Test): 0.3026525378227234\n",
            "Epoch 5065/10000: L(Train): 0.32226523756980896; L(Test): 0.3033931255340576\n",
            "Epoch 5066/10000: L(Train): 0.3316478431224823; L(Test): 0.3021707236766815\n",
            "Epoch 5067/10000: L(Train): 0.31644296646118164; L(Test): 0.3020084798336029\n",
            "Epoch 5068/10000: L(Train): 0.32933878898620605; L(Test): 0.3021363615989685\n",
            "Epoch 5069/10000: L(Train): 0.32291722297668457; L(Test): 0.3024888038635254\n",
            "Epoch 5070/10000: L(Train): 0.31819096207618713; L(Test): 0.30384689569473267\n",
            "Epoch 5071/10000: L(Train): 0.32700082659721375; L(Test): 0.3025830388069153\n",
            "Epoch 5072/10000: L(Train): 0.3317295014858246; L(Test): 0.3034341633319855\n",
            "Epoch 5073/10000: L(Train): 0.31849029660224915; L(Test): 0.3039911389350891\n",
            "Epoch 5074/10000: L(Train): 0.32653844356536865; L(Test): 0.302664190530777\n",
            "Epoch 5075/10000: L(Train): 0.33026984333992004; L(Test): 0.3035549521446228\n",
            "Epoch 5076/10000: L(Train): 0.32190075516700745; L(Test): 0.3031459450721741\n",
            "Epoch 5077/10000: L(Train): 0.31721949577331543; L(Test): 0.301298588514328\n",
            "Epoch 5078/10000: L(Train): 0.3256986737251282; L(Test): 0.30153393745422363\n",
            "Epoch 5079/10000: L(Train): 0.3187248706817627; L(Test): 0.3021171987056732\n",
            "Epoch 5080/10000: L(Train): 0.32887834310531616; L(Test): 0.30151820182800293\n",
            "Epoch 5081/10000: L(Train): 0.3188159763813019; L(Test): 0.3025217354297638\n",
            "Epoch 5082/10000: L(Train): 0.3277001678943634; L(Test): 0.30093103647232056\n",
            "Epoch 5083/10000: L(Train): 0.317105770111084; L(Test): 0.3001021444797516\n",
            "Epoch 5084/10000: L(Train): 0.3273870050907135; L(Test): 0.3002185523509979\n",
            "Epoch 5085/10000: L(Train): 0.32285788655281067; L(Test): 0.3010668158531189\n",
            "Epoch 5086/10000: L(Train): 0.3320958614349365; L(Test): 0.3010605573654175\n",
            "Epoch 5087/10000: L(Train): 0.32137635350227356; L(Test): 0.30047354102134705\n",
            "Epoch 5088/10000: L(Train): 0.31341853737831116; L(Test): 0.30083921551704407\n",
            "Epoch 5089/10000: L(Train): 0.32208624482154846; L(Test): 0.30121326446533203\n",
            "Epoch 5090/10000: L(Train): 0.32181158661842346; L(Test): 0.29980719089508057\n",
            "Epoch 5091/10000: L(Train): 0.32050782442092896; L(Test): 0.29909005761146545\n",
            "Epoch 5092/10000: L(Train): 0.3234705924987793; L(Test): 0.2988379895687103\n",
            "Epoch 5093/10000: L(Train): 0.32434388995170593; L(Test): 0.2996533215045929\n",
            "Epoch 5094/10000: L(Train): 0.31479841470718384; L(Test): 0.2999041676521301\n",
            "Epoch 5095/10000: L(Train): 0.31582918763160706; L(Test): 0.29970458149909973\n",
            "Epoch 5096/10000: L(Train): 0.33235225081443787; L(Test): 0.29952943325042725\n",
            "Epoch 5097/10000: L(Train): 0.3236202299594879; L(Test): 0.2990061640739441\n",
            "Epoch 5098/10000: L(Train): 0.3255205452442169; L(Test): 0.29903140664100647\n",
            "Epoch 5099/10000: L(Train): 0.3319357931613922; L(Test): 0.2995392382144928\n",
            "Epoch 5100/10000: L(Train): 0.3233376145362854; L(Test): 0.29938411712646484\n",
            "Epoch 5101/10000: L(Train): 0.32245752215385437; L(Test): 0.29873892664909363\n",
            "Epoch 5102/10000: L(Train): 0.3296668529510498; L(Test): 0.29810631275177\n",
            "Epoch 5103/10000: L(Train): 0.3223609924316406; L(Test): 0.29768842458724976\n",
            "Epoch 5104/10000: L(Train): 0.31919658184051514; L(Test): 0.2978840470314026\n",
            "Epoch 5105/10000: L(Train): 0.32576748728752136; L(Test): 0.2981295883655548\n",
            "Epoch 5106/10000: L(Train): 0.31083038449287415; L(Test): 0.29788967967033386\n",
            "Epoch 5107/10000: L(Train): 0.31731709837913513; L(Test): 0.2973344624042511\n",
            "Epoch 5108/10000: L(Train): 0.32739752531051636; L(Test): 0.2968634366989136\n",
            "Epoch 5109/10000: L(Train): 0.3275550305843353; L(Test): 0.29675984382629395\n",
            "Epoch 5110/10000: L(Train): 0.31437885761260986; L(Test): 0.2963772416114807\n",
            "Epoch 5111/10000: L(Train): 0.3150823712348938; L(Test): 0.2983492910861969\n",
            "Epoch 5112/10000: L(Train): 0.3231734037399292; L(Test): 0.29871881008148193\n",
            "Epoch 5113/10000: L(Train): 0.3200319707393646; L(Test): 0.29784631729125977\n",
            "Epoch 5114/10000: L(Train): 0.3203681707382202; L(Test): 0.29814404249191284\n",
            "Epoch 5115/10000: L(Train): 0.32881543040275574; L(Test): 0.2976940870285034\n",
            "Epoch 5116/10000: L(Train): 0.32870185375213623; L(Test): 0.297118216753006\n",
            "Epoch 5117/10000: L(Train): 0.3133458197116852; L(Test): 0.29965105652809143\n",
            "Epoch 5118/10000: L(Train): 0.3280530869960785; L(Test): 0.29832497239112854\n",
            "Epoch 5119/10000: L(Train): 0.31886976957321167; L(Test): 0.2975822687149048\n",
            "Epoch 5120/10000: L(Train): 0.32063478231430054; L(Test): 0.29862430691719055\n",
            "Epoch 5121/10000: L(Train): 0.31790223717689514; L(Test): 0.29933544993400574\n",
            "Epoch 5122/10000: L(Train): 0.32002657651901245; L(Test): 0.3002934753894806\n",
            "Epoch 5123/10000: L(Train): 0.3237457275390625; L(Test): 0.3013918399810791\n",
            "Epoch 5124/10000: L(Train): 0.32803234457969666; L(Test): 0.29996880888938904\n",
            "Epoch 5125/10000: L(Train): 0.3123771548271179; L(Test): 0.2994382977485657\n",
            "Epoch 5126/10000: L(Train): 0.32686036825180054; L(Test): 0.2987936735153198\n",
            "Epoch 5127/10000: L(Train): 0.31408780813217163; L(Test): 0.2987578511238098\n",
            "Epoch 5128/10000: L(Train): 0.32345372438430786; L(Test): 0.2993527352809906\n",
            "Epoch 5129/10000: L(Train): 0.32069942355155945; L(Test): 0.2991468012332916\n",
            "Epoch 5130/10000: L(Train): 0.3188094198703766; L(Test): 0.29859459400177\n",
            "Epoch 5131/10000: L(Train): 0.32204705476760864; L(Test): 0.2999850809574127\n",
            "Epoch 5132/10000: L(Train): 0.3266640603542328; L(Test): 0.2997705340385437\n",
            "Epoch 5133/10000: L(Train): 0.3281920552253723; L(Test): 0.30041080713272095\n",
            "Epoch 5134/10000: L(Train): 0.3151278793811798; L(Test): 0.30060821771621704\n",
            "Epoch 5135/10000: L(Train): 0.3243723213672638; L(Test): 0.2993568480014801\n",
            "Epoch 5136/10000: L(Train): 0.3247081935405731; L(Test): 0.30010542273521423\n",
            "Epoch 5137/10000: L(Train): 0.3236665725708008; L(Test): 0.3010517656803131\n",
            "Epoch 5138/10000: L(Train): 0.3203258514404297; L(Test): 0.3015162944793701\n",
            "Epoch 5139/10000: L(Train): 0.3290275037288666; L(Test): 0.301776260137558\n",
            "Epoch 5140/10000: L(Train): 0.3244521915912628; L(Test): 0.30283552408218384\n",
            "Epoch 5141/10000: L(Train): 0.3272892236709595; L(Test): 0.30265727639198303\n",
            "Epoch 5142/10000: L(Train): 0.322866290807724; L(Test): 0.3017476499080658\n",
            "Epoch 5143/10000: L(Train): 0.32156774401664734; L(Test): 0.3020859658718109\n",
            "Epoch 5144/10000: L(Train): 0.32827523350715637; L(Test): 0.3007616400718689\n",
            "Epoch 5145/10000: L(Train): 0.3227679431438446; L(Test): 0.30112510919570923\n",
            "Epoch 5146/10000: L(Train): 0.3291145861148834; L(Test): 0.3026936650276184\n",
            "Epoch 5147/10000: L(Train): 0.3276916444301605; L(Test): 0.30172428488731384\n",
            "Epoch 5148/10000: L(Train): 0.3158571124076843; L(Test): 0.30098679661750793\n",
            "Epoch 5149/10000: L(Train): 0.314864844083786; L(Test): 0.30070361495018005\n",
            "Epoch 5150/10000: L(Train): 0.32048821449279785; L(Test): 0.2999650835990906\n",
            "Epoch 5151/10000: L(Train): 0.3286590278148651; L(Test): 0.2997128963470459\n",
            "Epoch 5152/10000: L(Train): 0.325294554233551; L(Test): 0.29906412959098816\n",
            "Epoch 5153/10000: L(Train): 0.32538509368896484; L(Test): 0.2989543378353119\n",
            "Epoch 5154/10000: L(Train): 0.32155176997184753; L(Test): 0.2986162602901459\n",
            "Epoch 5155/10000: L(Train): 0.32593420147895813; L(Test): 0.2984180748462677\n",
            "Epoch 5156/10000: L(Train): 0.32485154271125793; L(Test): 0.2987319231033325\n",
            "Epoch 5157/10000: L(Train): 0.32234427332878113; L(Test): 0.2986098527908325\n",
            "Epoch 5158/10000: L(Train): 0.3215561509132385; L(Test): 0.29857853055000305\n",
            "Epoch 5159/10000: L(Train): 0.32497328519821167; L(Test): 0.2987625002861023\n",
            "Epoch 5160/10000: L(Train): 0.31249380111694336; L(Test): 0.29840123653411865\n",
            "Epoch 5161/10000: L(Train): 0.3142266869544983; L(Test): 0.29872366786003113\n",
            "Epoch 5162/10000: L(Train): 0.3236296772956848; L(Test): 0.29891854524612427\n",
            "Epoch 5163/10000: L(Train): 0.3225427269935608; L(Test): 0.29834118485450745\n",
            "Epoch 5164/10000: L(Train): 0.3267171382904053; L(Test): 0.29797062277793884\n",
            "Epoch 5165/10000: L(Train): 0.32607588171958923; L(Test): 0.2979453206062317\n",
            "Epoch 5166/10000: L(Train): 0.32136306166648865; L(Test): 0.2981526553630829\n",
            "Epoch 5167/10000: L(Train): 0.32317206263542175; L(Test): 0.29877999424934387\n",
            "Epoch 5168/10000: L(Train): 0.3325837552547455; L(Test): 0.297807514667511\n",
            "Epoch 5169/10000: L(Train): 0.3305155634880066; L(Test): 0.2982160449028015\n",
            "Epoch 5170/10000: L(Train): 0.31721362471580505; L(Test): 0.2982472777366638\n",
            "Epoch 5171/10000: L(Train): 0.3168485462665558; L(Test): 0.297503262758255\n",
            "Epoch 5172/10000: L(Train): 0.32612940669059753; L(Test): 0.29861679673194885\n",
            "Epoch 5173/10000: L(Train): 0.32052770256996155; L(Test): 0.2981192171573639\n",
            "Epoch 5174/10000: L(Train): 0.3190699517726898; L(Test): 0.2971980571746826\n",
            "Epoch 5175/10000: L(Train): 0.310403972864151; L(Test): 0.2976050078868866\n",
            "Epoch 5176/10000: L(Train): 0.32037118077278137; L(Test): 0.29802238941192627\n",
            "Epoch 5177/10000: L(Train): 0.32489150762557983; L(Test): 0.2978074848651886\n",
            "Epoch 5178/10000: L(Train): 0.32812193036079407; L(Test): 0.2980581820011139\n",
            "Epoch 5179/10000: L(Train): 0.32341885566711426; L(Test): 0.29872795939445496\n",
            "Epoch 5180/10000: L(Train): 0.3176150918006897; L(Test): 0.29958871006965637\n",
            "Epoch 5181/10000: L(Train): 0.3184269070625305; L(Test): 0.29897773265838623\n",
            "Epoch 5182/10000: L(Train): 0.3303326666355133; L(Test): 0.2998557984828949\n",
            "Epoch 5183/10000: L(Train): 0.3314453661441803; L(Test): 0.29972973465919495\n",
            "Epoch 5184/10000: L(Train): 0.32808151841163635; L(Test): 0.29824939370155334\n",
            "Epoch 5185/10000: L(Train): 0.32120487093925476; L(Test): 0.2978171706199646\n",
            "Epoch 5186/10000: L(Train): 0.31696194410324097; L(Test): 0.2972007691860199\n",
            "Epoch 5187/10000: L(Train): 0.31957778334617615; L(Test): 0.2975084185600281\n",
            "Epoch 5188/10000: L(Train): 0.319175124168396; L(Test): 0.2978627681732178\n",
            "Epoch 5189/10000: L(Train): 0.3156048059463501; L(Test): 0.2972985506057739\n",
            "Epoch 5190/10000: L(Train): 0.3189336061477661; L(Test): 0.29668840765953064\n",
            "Epoch 5191/10000: L(Train): 0.32210177183151245; L(Test): 0.2968178689479828\n",
            "Epoch 5192/10000: L(Train): 0.3284718692302704; L(Test): 0.29651913046836853\n",
            "Epoch 5193/10000: L(Train): 0.3182961940765381; L(Test): 0.2963821291923523\n",
            "Epoch 5194/10000: L(Train): 0.31703829765319824; L(Test): 0.29690954089164734\n",
            "Epoch 5195/10000: L(Train): 0.3156231939792633; L(Test): 0.2970600128173828\n",
            "Epoch 5196/10000: L(Train): 0.31710678339004517; L(Test): 0.29628700017929077\n",
            "Epoch 5197/10000: L(Train): 0.3199400305747986; L(Test): 0.29600927233695984\n",
            "Epoch 5198/10000: L(Train): 0.3179420530796051; L(Test): 0.2963963449001312\n",
            "Epoch 5199/10000: L(Train): 0.324632465839386; L(Test): 0.2954238951206207\n",
            "Epoch 5200/10000: L(Train): 0.31745481491088867; L(Test): 0.2946793735027313\n",
            "Epoch 5201/10000: L(Train): 0.31336548924446106; L(Test): 0.2951854467391968\n",
            "Epoch 5202/10000: L(Train): 0.3114965558052063; L(Test): 0.29521241784095764\n",
            "Epoch 5203/10000: L(Train): 0.31234210729599; L(Test): 0.2956721782684326\n",
            "Epoch 5204/10000: L(Train): 0.3149637281894684; L(Test): 0.2963971793651581\n",
            "Epoch 5205/10000: L(Train): 0.3268245756626129; L(Test): 0.2964054346084595\n",
            "Epoch 5206/10000: L(Train): 0.3135966956615448; L(Test): 0.2966712415218353\n",
            "Epoch 5207/10000: L(Train): 0.32586225867271423; L(Test): 0.29565730690956116\n",
            "Epoch 5208/10000: L(Train): 0.3220885396003723; L(Test): 0.2955968379974365\n",
            "Epoch 5209/10000: L(Train): 0.31846609711647034; L(Test): 0.29534032940864563\n",
            "Epoch 5210/10000: L(Train): 0.32644495368003845; L(Test): 0.29546427726745605\n",
            "Epoch 5211/10000: L(Train): 0.3187153935432434; L(Test): 0.29559239745140076\n",
            "Epoch 5212/10000: L(Train): 0.3232274055480957; L(Test): 0.2960948646068573\n",
            "Epoch 5213/10000: L(Train): 0.3167588412761688; L(Test): 0.2958361506462097\n",
            "Epoch 5214/10000: L(Train): 0.3231748342514038; L(Test): 0.29693102836608887\n",
            "Epoch 5215/10000: L(Train): 0.3235282897949219; L(Test): 0.296664297580719\n",
            "Epoch 5216/10000: L(Train): 0.32130515575408936; L(Test): 0.296833336353302\n",
            "Epoch 5217/10000: L(Train): 0.3216140866279602; L(Test): 0.29766610264778137\n",
            "Epoch 5218/10000: L(Train): 0.3230932652950287; L(Test): 0.2975687086582184\n",
            "Epoch 5219/10000: L(Train): 0.32353678345680237; L(Test): 0.29797184467315674\n",
            "Epoch 5220/10000: L(Train): 0.32047468423843384; L(Test): 0.297618567943573\n",
            "Epoch 5221/10000: L(Train): 0.31764668226242065; L(Test): 0.2975466251373291\n",
            "Epoch 5222/10000: L(Train): 0.3320784568786621; L(Test): 0.29694825410842896\n",
            "Epoch 5223/10000: L(Train): 0.3192389905452728; L(Test): 0.29650548100471497\n",
            "Epoch 5224/10000: L(Train): 0.3189620077610016; L(Test): 0.29769366979599\n",
            "Epoch 5225/10000: L(Train): 0.32858508825302124; L(Test): 0.29850590229034424\n",
            "Epoch 5226/10000: L(Train): 0.31706586480140686; L(Test): 0.29713818430900574\n",
            "Epoch 5227/10000: L(Train): 0.31662431359291077; L(Test): 0.29733312129974365\n",
            "Epoch 5228/10000: L(Train): 0.32101011276245117; L(Test): 0.29696786403656006\n",
            "Epoch 5229/10000: L(Train): 0.31560251116752625; L(Test): 0.2961113750934601\n",
            "Epoch 5230/10000: L(Train): 0.3157508075237274; L(Test): 0.29694506525993347\n",
            "Epoch 5231/10000: L(Train): 0.3179101347923279; L(Test): 0.2976282835006714\n",
            "Epoch 5232/10000: L(Train): 0.3217542767524719; L(Test): 0.29600533843040466\n",
            "Epoch 5233/10000: L(Train): 0.3116678297519684; L(Test): 0.2958211302757263\n",
            "Epoch 5234/10000: L(Train): 0.31844639778137207; L(Test): 0.29647642374038696\n",
            "Epoch 5235/10000: L(Train): 0.32437440752983093; L(Test): 0.2969309687614441\n",
            "Epoch 5236/10000: L(Train): 0.3219217360019684; L(Test): 0.29803338646888733\n",
            "Epoch 5237/10000: L(Train): 0.3274366855621338; L(Test): 0.2965923845767975\n",
            "Epoch 5238/10000: L(Train): 0.3218235373497009; L(Test): 0.298831045627594\n",
            "Epoch 5239/10000: L(Train): 0.32981687784194946; L(Test): 0.29920944571495056\n",
            "Epoch 5240/10000: L(Train): 0.3183285593986511; L(Test): 0.2973598539829254\n",
            "Epoch 5241/10000: L(Train): 0.329621285200119; L(Test): 0.29835206270217896\n",
            "Epoch 5242/10000: L(Train): 0.3181217610836029; L(Test): 0.30047762393951416\n",
            "Epoch 5243/10000: L(Train): 0.321500301361084; L(Test): 0.3015871047973633\n",
            "Epoch 5244/10000: L(Train): 0.3163354694843292; L(Test): 0.30343976616859436\n",
            "Epoch 5245/10000: L(Train): 0.3154061734676361; L(Test): 0.30341827869415283\n",
            "Epoch 5246/10000: L(Train): 0.33141395449638367; L(Test): 0.30412811040878296\n",
            "Epoch 5247/10000: L(Train): 0.33237454295158386; L(Test): 0.30453625321388245\n",
            "Epoch 5248/10000: L(Train): 0.3313118815422058; L(Test): 0.302115797996521\n",
            "Epoch 5249/10000: L(Train): 0.32615721225738525; L(Test): 0.30153802037239075\n",
            "Epoch 5250/10000: L(Train): 0.3215814232826233; L(Test): 0.3026213049888611\n",
            "Epoch 5251/10000: L(Train): 0.3279365599155426; L(Test): 0.3034154176712036\n",
            "Epoch 5252/10000: L(Train): 0.316091924905777; L(Test): 0.30418673157691956\n",
            "Epoch 5253/10000: L(Train): 0.3321070373058319; L(Test): 0.3021390438079834\n",
            "Epoch 5254/10000: L(Train): 0.33659785985946655; L(Test): 0.3038828372955322\n",
            "Epoch 5255/10000: L(Train): 0.33304858207702637; L(Test): 0.3051053583621979\n",
            "Epoch 5256/10000: L(Train): 0.3229093551635742; L(Test): 0.30363115668296814\n",
            "Epoch 5257/10000: L(Train): 0.32397031784057617; L(Test): 0.3029405176639557\n",
            "Epoch 5258/10000: L(Train): 0.328042209148407; L(Test): 0.30347007513046265\n",
            "Epoch 5259/10000: L(Train): 0.3229140341281891; L(Test): 0.30368778109550476\n",
            "Epoch 5260/10000: L(Train): 0.31781908869743347; L(Test): 0.3034309148788452\n",
            "Epoch 5261/10000: L(Train): 0.3354518413543701; L(Test): 0.3028262257575989\n",
            "Epoch 5262/10000: L(Train): 0.3292340040206909; L(Test): 0.3027457296848297\n",
            "Epoch 5263/10000: L(Train): 0.3232339918613434; L(Test): 0.3023677170276642\n",
            "Epoch 5264/10000: L(Train): 0.32626664638519287; L(Test): 0.30284082889556885\n",
            "Epoch 5265/10000: L(Train): 0.32388412952423096; L(Test): 0.3028488755226135\n",
            "Epoch 5266/10000: L(Train): 0.3227880597114563; L(Test): 0.3015391230583191\n",
            "Epoch 5267/10000: L(Train): 0.32216179370880127; L(Test): 0.3004983961582184\n",
            "Epoch 5268/10000: L(Train): 0.3200523853302002; L(Test): 0.30036842823028564\n",
            "Epoch 5269/10000: L(Train): 0.31788530945777893; L(Test): 0.30025771260261536\n",
            "Epoch 5270/10000: L(Train): 0.32393336296081543; L(Test): 0.3003344237804413\n",
            "Epoch 5271/10000: L(Train): 0.3256781995296478; L(Test): 0.30002135038375854\n",
            "Epoch 5272/10000: L(Train): 0.3288840055465698; L(Test): 0.29886138439178467\n",
            "Epoch 5273/10000: L(Train): 0.32717788219451904; L(Test): 0.29900816082954407\n",
            "Epoch 5274/10000: L(Train): 0.32244813442230225; L(Test): 0.29887643456459045\n",
            "Epoch 5275/10000: L(Train): 0.31077149510383606; L(Test): 0.2991267144680023\n",
            "Epoch 5276/10000: L(Train): 0.3281909227371216; L(Test): 0.2990998327732086\n",
            "Epoch 5277/10000: L(Train): 0.3238983452320099; L(Test): 0.29899534583091736\n",
            "Epoch 5278/10000: L(Train): 0.3160778880119324; L(Test): 0.29919302463531494\n",
            "Epoch 5279/10000: L(Train): 0.3247852921485901; L(Test): 0.29846733808517456\n",
            "Epoch 5280/10000: L(Train): 0.3171926438808441; L(Test): 0.29970604181289673\n",
            "Epoch 5281/10000: L(Train): 0.3236635625362396; L(Test): 0.3007011115550995\n",
            "Epoch 5282/10000: L(Train): 0.3207451403141022; L(Test): 0.2989692986011505\n",
            "Epoch 5283/10000: L(Train): 0.3220231533050537; L(Test): 0.2979543209075928\n",
            "Epoch 5284/10000: L(Train): 0.31747350096702576; L(Test): 0.2980649769306183\n",
            "Epoch 5285/10000: L(Train): 0.3223732113838196; L(Test): 0.29853135347366333\n",
            "Epoch 5286/10000: L(Train): 0.315570592880249; L(Test): 0.2979430556297302\n",
            "Epoch 5287/10000: L(Train): 0.3164619207382202; L(Test): 0.2978995442390442\n",
            "Epoch 5288/10000: L(Train): 0.3215981721878052; L(Test): 0.2972756028175354\n",
            "Epoch 5289/10000: L(Train): 0.3194655478000641; L(Test): 0.2969813644886017\n",
            "Epoch 5290/10000: L(Train): 0.32065510749816895; L(Test): 0.29682210087776184\n",
            "Epoch 5291/10000: L(Train): 0.32333293557167053; L(Test): 0.2969764173030853\n",
            "Epoch 5292/10000: L(Train): 0.324968159198761; L(Test): 0.29618579149246216\n",
            "Epoch 5293/10000: L(Train): 0.3175138533115387; L(Test): 0.2962017059326172\n",
            "Epoch 5294/10000: L(Train): 0.3121066391468048; L(Test): 0.29673200845718384\n",
            "Epoch 5295/10000: L(Train): 0.3146931231021881; L(Test): 0.29638025164604187\n",
            "Epoch 5296/10000: L(Train): 0.32375597953796387; L(Test): 0.29608628153800964\n",
            "Epoch 5297/10000: L(Train): 0.32614853978157043; L(Test): 0.29662659764289856\n",
            "Epoch 5298/10000: L(Train): 0.324027419090271; L(Test): 0.2961307466030121\n",
            "Epoch 5299/10000: L(Train): 0.3148229122161865; L(Test): 0.2952898144721985\n",
            "Epoch 5300/10000: L(Train): 0.30976393818855286; L(Test): 0.295890748500824\n",
            "Epoch 5301/10000: L(Train): 0.308735191822052; L(Test): 0.29584458470344543\n",
            "Epoch 5302/10000: L(Train): 0.3258359432220459; L(Test): 0.29623568058013916\n",
            "Epoch 5303/10000: L(Train): 0.31730908155441284; L(Test): 0.29808953404426575\n",
            "Epoch 5304/10000: L(Train): 0.3232644498348236; L(Test): 0.29700735211372375\n",
            "Epoch 5305/10000: L(Train): 0.3194408118724823; L(Test): 0.2959234118461609\n",
            "Epoch 5306/10000: L(Train): 0.32125502824783325; L(Test): 0.29651403427124023\n",
            "Epoch 5307/10000: L(Train): 0.32230836153030396; L(Test): 0.29660850763320923\n",
            "Epoch 5308/10000: L(Train): 0.3136063814163208; L(Test): 0.29718467593193054\n",
            "Epoch 5309/10000: L(Train): 0.3264645040035248; L(Test): 0.2962797284126282\n",
            "Epoch 5310/10000: L(Train): 0.321290522813797; L(Test): 0.2969439625740051\n",
            "Epoch 5311/10000: L(Train): 0.3263697922229767; L(Test): 0.297016978263855\n",
            "Epoch 5312/10000: L(Train): 0.31462007761001587; L(Test): 0.296905517578125\n",
            "Epoch 5313/10000: L(Train): 0.32038614153862; L(Test): 0.29725900292396545\n",
            "Epoch 5314/10000: L(Train): 0.3246493637561798; L(Test): 0.2975681722164154\n",
            "Epoch 5315/10000: L(Train): 0.3190157413482666; L(Test): 0.3002234697341919\n",
            "Epoch 5316/10000: L(Train): 0.33346155285835266; L(Test): 0.30412936210632324\n",
            "Epoch 5317/10000: L(Train): 0.32914918661117554; L(Test): 0.3042328953742981\n",
            "Epoch 5318/10000: L(Train): 0.33599168062210083; L(Test): 0.3044504225254059\n",
            "Epoch 5319/10000: L(Train): 0.32562077045440674; L(Test): 0.30428197979927063\n",
            "Epoch 5320/10000: L(Train): 0.3300904333591461; L(Test): 0.3017312288284302\n",
            "Epoch 5321/10000: L(Train): 0.32285234332084656; L(Test): 0.3048441708087921\n",
            "Epoch 5322/10000: L(Train): 0.3341944217681885; L(Test): 0.3069915473461151\n",
            "Epoch 5323/10000: L(Train): 0.33108770847320557; L(Test): 0.3045455813407898\n",
            "Epoch 5324/10000: L(Train): 0.32963231205940247; L(Test): 0.3055959939956665\n",
            "Epoch 5325/10000: L(Train): 0.33074629306793213; L(Test): 0.3066316246986389\n",
            "Epoch 5326/10000: L(Train): 0.3338504135608673; L(Test): 0.3036096394062042\n",
            "Epoch 5327/10000: L(Train): 0.32659757137298584; L(Test): 0.3033309578895569\n",
            "Epoch 5328/10000: L(Train): 0.3201603293418884; L(Test): 0.30526408553123474\n",
            "Epoch 5329/10000: L(Train): 0.3282274603843689; L(Test): 0.30483371019363403\n",
            "Epoch 5330/10000: L(Train): 0.33407318592071533; L(Test): 0.3021293878555298\n",
            "Epoch 5331/10000: L(Train): 0.32790619134902954; L(Test): 0.30221155285835266\n",
            "Epoch 5332/10000: L(Train): 0.3278965353965759; L(Test): 0.30343762040138245\n",
            "Epoch 5333/10000: L(Train): 0.32916805148124695; L(Test): 0.30204764008522034\n",
            "Epoch 5334/10000: L(Train): 0.3252946734428406; L(Test): 0.3007087707519531\n",
            "Epoch 5335/10000: L(Train): 0.3264225423336029; L(Test): 0.30125319957733154\n",
            "Epoch 5336/10000: L(Train): 0.32360661029815674; L(Test): 0.302778422832489\n",
            "Epoch 5337/10000: L(Train): 0.3216990828514099; L(Test): 0.3022972345352173\n",
            "Epoch 5338/10000: L(Train): 0.32932722568511963; L(Test): 0.2998860776424408\n",
            "Epoch 5339/10000: L(Train): 0.3242219090461731; L(Test): 0.3006581664085388\n",
            "Epoch 5340/10000: L(Train): 0.3298155665397644; L(Test): 0.30230188369750977\n",
            "Epoch 5341/10000: L(Train): 0.31907111406326294; L(Test): 0.30119606852531433\n",
            "Epoch 5342/10000: L(Train): 0.32571548223495483; L(Test): 0.301021933555603\n",
            "Epoch 5343/10000: L(Train): 0.3296441435813904; L(Test): 0.3001135587692261\n",
            "Epoch 5344/10000: L(Train): 0.32557615637779236; L(Test): 0.29910603165626526\n",
            "Epoch 5345/10000: L(Train): 0.3277645707130432; L(Test): 0.29957959055900574\n",
            "Epoch 5346/10000: L(Train): 0.32461613416671753; L(Test): 0.2995927631855011\n",
            "Epoch 5347/10000: L(Train): 0.32721593976020813; L(Test): 0.2991783916950226\n",
            "Epoch 5348/10000: L(Train): 0.3235950469970703; L(Test): 0.2982006072998047\n",
            "Epoch 5349/10000: L(Train): 0.31599318981170654; L(Test): 0.2986028790473938\n",
            "Epoch 5350/10000: L(Train): 0.32908451557159424; L(Test): 0.29873159527778625\n",
            "Epoch 5351/10000: L(Train): 0.32847830653190613; L(Test): 0.29809123277664185\n",
            "Epoch 5352/10000: L(Train): 0.33156099915504456; L(Test): 0.29859524965286255\n",
            "Epoch 5353/10000: L(Train): 0.3227088451385498; L(Test): 0.29918086528778076\n",
            "Epoch 5354/10000: L(Train): 0.3266807496547699; L(Test): 0.29882827401161194\n",
            "Epoch 5355/10000: L(Train): 0.3225431442260742; L(Test): 0.2985905706882477\n",
            "Epoch 5356/10000: L(Train): 0.31899401545524597; L(Test): 0.2982076108455658\n",
            "Epoch 5357/10000: L(Train): 0.32226237654685974; L(Test): 0.297738254070282\n",
            "Epoch 5358/10000: L(Train): 0.3237141966819763; L(Test): 0.2983151376247406\n",
            "Epoch 5359/10000: L(Train): 0.32482191920280457; L(Test): 0.29747819900512695\n",
            "Epoch 5360/10000: L(Train): 0.3305683434009552; L(Test): 0.29806017875671387\n",
            "Epoch 5361/10000: L(Train): 0.3180334270000458; L(Test): 0.2974204421043396\n",
            "Epoch 5362/10000: L(Train): 0.3255401849746704; L(Test): 0.29740503430366516\n",
            "Epoch 5363/10000: L(Train): 0.31960538029670715; L(Test): 0.29776880145072937\n",
            "Epoch 5364/10000: L(Train): 0.32717013359069824; L(Test): 0.2977825403213501\n",
            "Epoch 5365/10000: L(Train): 0.3188108801841736; L(Test): 0.2971387505531311\n",
            "Epoch 5366/10000: L(Train): 0.3129257261753082; L(Test): 0.29688888788223267\n",
            "Epoch 5367/10000: L(Train): 0.3274693489074707; L(Test): 0.2971145510673523\n",
            "Epoch 5368/10000: L(Train): 0.3216449022293091; L(Test): 0.2974774241447449\n",
            "Epoch 5369/10000: L(Train): 0.32544079422950745; L(Test): 0.2979220449924469\n",
            "Epoch 5370/10000: L(Train): 0.31195738911628723; L(Test): 0.29799044132232666\n",
            "Epoch 5371/10000: L(Train): 0.32122746109962463; L(Test): 0.29772356152534485\n",
            "Epoch 5372/10000: L(Train): 0.31833866238594055; L(Test): 0.296810120344162\n",
            "Epoch 5373/10000: L(Train): 0.3256336450576782; L(Test): 0.2971583902835846\n",
            "Epoch 5374/10000: L(Train): 0.3292748034000397; L(Test): 0.29857170581817627\n",
            "Epoch 5375/10000: L(Train): 0.32917651534080505; L(Test): 0.29760289192199707\n",
            "Epoch 5376/10000: L(Train): 0.3216654360294342; L(Test): 0.2978118658065796\n",
            "Epoch 5377/10000: L(Train): 0.3244248032569885; L(Test): 0.29741162061691284\n",
            "Epoch 5378/10000: L(Train): 0.32678714394569397; L(Test): 0.29723212122917175\n",
            "Epoch 5379/10000: L(Train): 0.31466802954673767; L(Test): 0.29646217823028564\n",
            "Epoch 5380/10000: L(Train): 0.3258894681930542; L(Test): 0.2964856028556824\n",
            "Epoch 5381/10000: L(Train): 0.3198431432247162; L(Test): 0.29671767354011536\n",
            "Epoch 5382/10000: L(Train): 0.3270089626312256; L(Test): 0.29702845215797424\n",
            "Epoch 5383/10000: L(Train): 0.3107050955295563; L(Test): 0.29721537232398987\n",
            "Epoch 5384/10000: L(Train): 0.33156517148017883; L(Test): 0.2973753809928894\n",
            "Epoch 5385/10000: L(Train): 0.31964948773384094; L(Test): 0.299137145280838\n",
            "Epoch 5386/10000: L(Train): 0.3302060067653656; L(Test): 0.2970634400844574\n",
            "Epoch 5387/10000: L(Train): 0.3172104060649872; L(Test): 0.2970433533191681\n",
            "Epoch 5388/10000: L(Train): 0.3217313885688782; L(Test): 0.2971211373806\n",
            "Epoch 5389/10000: L(Train): 0.3218618333339691; L(Test): 0.2969568371772766\n",
            "Epoch 5390/10000: L(Train): 0.3267136514186859; L(Test): 0.2975080609321594\n",
            "Epoch 5391/10000: L(Train): 0.32849952578544617; L(Test): 0.29679131507873535\n",
            "Epoch 5392/10000: L(Train): 0.3239170014858246; L(Test): 0.2989747226238251\n",
            "Epoch 5393/10000: L(Train): 0.3230188190937042; L(Test): 0.29816293716430664\n",
            "Epoch 5394/10000: L(Train): 0.31522586941719055; L(Test): 0.2992248237133026\n",
            "Epoch 5395/10000: L(Train): 0.3189246654510498; L(Test): 0.29957395792007446\n",
            "Epoch 5396/10000: L(Train): 0.3238922655582428; L(Test): 0.2978556454181671\n",
            "Epoch 5397/10000: L(Train): 0.3212485909461975; L(Test): 0.298318088054657\n",
            "Epoch 5398/10000: L(Train): 0.3208341598510742; L(Test): 0.29946765303611755\n",
            "Epoch 5399/10000: L(Train): 0.3240315914154053; L(Test): 0.29983019828796387\n",
            "Epoch 5400/10000: L(Train): 0.3126908242702484; L(Test): 0.2992202937602997\n",
            "Epoch 5401/10000: L(Train): 0.33126184344291687; L(Test): 0.2992255687713623\n",
            "Epoch 5402/10000: L(Train): 0.31779348850250244; L(Test): 0.2964339256286621\n",
            "Epoch 5403/10000: L(Train): 0.3222983479499817; L(Test): 0.29623615741729736\n",
            "Epoch 5404/10000: L(Train): 0.3192881941795349; L(Test): 0.2977844476699829\n",
            "Epoch 5405/10000: L(Train): 0.3189711570739746; L(Test): 0.29736781120300293\n",
            "Epoch 5406/10000: L(Train): 0.31684795022010803; L(Test): 0.2970343828201294\n",
            "Epoch 5407/10000: L(Train): 0.32129746675491333; L(Test): 0.2982703447341919\n",
            "Epoch 5408/10000: L(Train): 0.3251710832118988; L(Test): 0.29813438653945923\n",
            "Epoch 5409/10000: L(Train): 0.3269472122192383; L(Test): 0.2975902259349823\n",
            "Epoch 5410/10000: L(Train): 0.31911391019821167; L(Test): 0.2971912622451782\n",
            "Epoch 5411/10000: L(Train): 0.3228272795677185; L(Test): 0.29870468378067017\n",
            "Epoch 5412/10000: L(Train): 0.3204553425312042; L(Test): 0.29870182275772095\n",
            "Epoch 5413/10000: L(Train): 0.32089534401893616; L(Test): 0.2985047399997711\n",
            "Epoch 5414/10000: L(Train): 0.3168523907661438; L(Test): 0.3004153072834015\n",
            "Epoch 5415/10000: L(Train): 0.32233726978302; L(Test): 0.300346702337265\n",
            "Epoch 5416/10000: L(Train): 0.31316500902175903; L(Test): 0.3002021014690399\n",
            "Epoch 5417/10000: L(Train): 0.32715386152267456; L(Test): 0.298165500164032\n",
            "Epoch 5418/10000: L(Train): 0.3265479505062103; L(Test): 0.29984959959983826\n",
            "Epoch 5419/10000: L(Train): 0.3200799524784088; L(Test): 0.3003416061401367\n",
            "Epoch 5420/10000: L(Train): 0.32419487833976746; L(Test): 0.2989749312400818\n",
            "Epoch 5421/10000: L(Train): 0.3320324122905731; L(Test): 0.2992275655269623\n",
            "Epoch 5422/10000: L(Train): 0.3160282075405121; L(Test): 0.2999376654624939\n",
            "Epoch 5423/10000: L(Train): 0.3238944113254547; L(Test): 0.2988470792770386\n",
            "Epoch 5424/10000: L(Train): 0.32402464747428894; L(Test): 0.2976473867893219\n",
            "Epoch 5425/10000: L(Train): 0.31769874691963196; L(Test): 0.2981571555137634\n",
            "Epoch 5426/10000: L(Train): 0.32259005308151245; L(Test): 0.29840216040611267\n",
            "Epoch 5427/10000: L(Train): 0.3158895969390869; L(Test): 0.29958683252334595\n",
            "Epoch 5428/10000: L(Train): 0.3197423219680786; L(Test): 0.3005354404449463\n",
            "Epoch 5429/10000: L(Train): 0.3317582309246063; L(Test): 0.3027445375919342\n",
            "Epoch 5430/10000: L(Train): 0.3326682150363922; L(Test): 0.30213260650634766\n",
            "Epoch 5431/10000: L(Train): 0.3320367932319641; L(Test): 0.2997954189777374\n",
            "Epoch 5432/10000: L(Train): 0.3244604468345642; L(Test): 0.30017349123954773\n",
            "Epoch 5433/10000: L(Train): 0.32528725266456604; L(Test): 0.29960569739341736\n",
            "Epoch 5434/10000: L(Train): 0.321703165769577; L(Test): 0.29850149154663086\n",
            "Epoch 5435/10000: L(Train): 0.3248368203639984; L(Test): 0.29933351278305054\n",
            "Epoch 5436/10000: L(Train): 0.32862651348114014; L(Test): 0.3014044165611267\n",
            "Epoch 5437/10000: L(Train): 0.32390159368515015; L(Test): 0.3008248805999756\n",
            "Epoch 5438/10000: L(Train): 0.32325291633605957; L(Test): 0.2989935874938965\n",
            "Epoch 5439/10000: L(Train): 0.31343215703964233; L(Test): 0.2982020974159241\n",
            "Epoch 5440/10000: L(Train): 0.3281640410423279; L(Test): 0.29894915223121643\n",
            "Epoch 5441/10000: L(Train): 0.3190903663635254; L(Test): 0.2992650866508484\n",
            "Epoch 5442/10000: L(Train): 0.3254827857017517; L(Test): 0.2979987859725952\n",
            "Epoch 5443/10000: L(Train): 0.32494452595710754; L(Test): 0.2975285053253174\n",
            "Epoch 5444/10000: L(Train): 0.3199542760848999; L(Test): 0.29826533794403076\n",
            "Epoch 5445/10000: L(Train): 0.33380937576293945; L(Test): 0.298484206199646\n",
            "Epoch 5446/10000: L(Train): 0.324637234210968; L(Test): 0.2978314161300659\n",
            "Epoch 5447/10000: L(Train): 0.32387807965278625; L(Test): 0.2974899113178253\n",
            "Epoch 5448/10000: L(Train): 0.32332661747932434; L(Test): 0.29736074805259705\n",
            "Epoch 5449/10000: L(Train): 0.32431691884994507; L(Test): 0.2973839044570923\n",
            "Epoch 5450/10000: L(Train): 0.3230089545249939; L(Test): 0.29708877205848694\n",
            "Epoch 5451/10000: L(Train): 0.316242516040802; L(Test): 0.29667818546295166\n",
            "Epoch 5452/10000: L(Train): 0.31507864594459534; L(Test): 0.2970592677593231\n",
            "Epoch 5453/10000: L(Train): 0.3187997043132782; L(Test): 0.29832327365875244\n",
            "Epoch 5454/10000: L(Train): 0.32468125224113464; L(Test): 0.2966974675655365\n",
            "Epoch 5455/10000: L(Train): 0.3199978470802307; L(Test): 0.29622724652290344\n",
            "Epoch 5456/10000: L(Train): 0.3241124153137207; L(Test): 0.29663342237472534\n",
            "Epoch 5457/10000: L(Train): 0.32045432925224304; L(Test): 0.29729607701301575\n",
            "Epoch 5458/10000: L(Train): 0.3117281198501587; L(Test): 0.29751989245414734\n",
            "Epoch 5459/10000: L(Train): 0.3199521005153656; L(Test): 0.297920823097229\n",
            "Epoch 5460/10000: L(Train): 0.33260297775268555; L(Test): 0.2970331907272339\n",
            "Epoch 5461/10000: L(Train): 0.31438228487968445; L(Test): 0.29581305384635925\n",
            "Epoch 5462/10000: L(Train): 0.3160286247730255; L(Test): 0.29589566588401794\n",
            "Epoch 5463/10000: L(Train): 0.3083260953426361; L(Test): 0.2965014576911926\n",
            "Epoch 5464/10000: L(Train): 0.32461363077163696; L(Test): 0.29754146933555603\n",
            "Epoch 5465/10000: L(Train): 0.3194827437400818; L(Test): 0.2974388003349304\n",
            "Epoch 5466/10000: L(Train): 0.3130967915058136; L(Test): 0.2979653775691986\n",
            "Epoch 5467/10000: L(Train): 0.317421555519104; L(Test): 0.2988133728504181\n",
            "Epoch 5468/10000: L(Train): 0.3106956481933594; L(Test): 0.2987762689590454\n",
            "Epoch 5469/10000: L(Train): 0.32420191168785095; L(Test): 0.29894399642944336\n",
            "Epoch 5470/10000: L(Train): 0.3324710428714752; L(Test): 0.29908493161201477\n",
            "Epoch 5471/10000: L(Train): 0.32199835777282715; L(Test): 0.29796308279037476\n",
            "Epoch 5472/10000: L(Train): 0.3273513913154602; L(Test): 0.2976354956626892\n",
            "Epoch 5473/10000: L(Train): 0.3221515119075775; L(Test): 0.2971518635749817\n",
            "Epoch 5474/10000: L(Train): 0.32594770193099976; L(Test): 0.2974107563495636\n",
            "Epoch 5475/10000: L(Train): 0.31437948346138; L(Test): 0.2984103262424469\n",
            "Epoch 5476/10000: L(Train): 0.31396350264549255; L(Test): 0.2979241609573364\n",
            "Epoch 5477/10000: L(Train): 0.3186013400554657; L(Test): 0.29690465331077576\n",
            "Epoch 5478/10000: L(Train): 0.3236601650714874; L(Test): 0.29779866337776184\n",
            "Epoch 5479/10000: L(Train): 0.31784698367118835; L(Test): 0.29809778928756714\n",
            "Epoch 5480/10000: L(Train): 0.32165777683258057; L(Test): 0.29682618379592896\n",
            "Epoch 5481/10000: L(Train): 0.32629650831222534; L(Test): 0.29646340012550354\n",
            "Epoch 5482/10000: L(Train): 0.32211410999298096; L(Test): 0.29681065678596497\n",
            "Epoch 5483/10000: L(Train): 0.31990888714790344; L(Test): 0.29643481969833374\n",
            "Epoch 5484/10000: L(Train): 0.3260953724384308; L(Test): 0.29700225591659546\n",
            "Epoch 5485/10000: L(Train): 0.3266735076904297; L(Test): 0.2966267466545105\n",
            "Epoch 5486/10000: L(Train): 0.31890952587127686; L(Test): 0.29697751998901367\n",
            "Epoch 5487/10000: L(Train): 0.32557371258735657; L(Test): 0.2969685196876526\n",
            "Epoch 5488/10000: L(Train): 0.3186434209346771; L(Test): 0.29783767461776733\n",
            "Epoch 5489/10000: L(Train): 0.3253624737262726; L(Test): 0.2979007065296173\n",
            "Epoch 5490/10000: L(Train): 0.3230474591255188; L(Test): 0.29699990153312683\n",
            "Epoch 5491/10000: L(Train): 0.3181116282939911; L(Test): 0.29708200693130493\n",
            "Epoch 5492/10000: L(Train): 0.32546544075012207; L(Test): 0.29662883281707764\n",
            "Epoch 5493/10000: L(Train): 0.3188784718513489; L(Test): 0.29756826162338257\n",
            "Epoch 5494/10000: L(Train): 0.31692832708358765; L(Test): 0.29844772815704346\n",
            "Epoch 5495/10000: L(Train): 0.3190423250198364; L(Test): 0.2969859540462494\n",
            "Epoch 5496/10000: L(Train): 0.32583627104759216; L(Test): 0.2966935634613037\n",
            "Epoch 5497/10000: L(Train): 0.31934887170791626; L(Test): 0.2983996570110321\n",
            "Epoch 5498/10000: L(Train): 0.3244000971317291; L(Test): 0.29759135842323303\n",
            "Epoch 5499/10000: L(Train): 0.3280893564224243; L(Test): 0.2972731590270996\n",
            "Epoch 5500/10000: L(Train): 0.32442706823349; L(Test): 0.29719310998916626\n",
            "Epoch 5501/10000: L(Train): 0.3194693326950073; L(Test): 0.297042578458786\n",
            "Epoch 5502/10000: L(Train): 0.3191397190093994; L(Test): 0.29813024401664734\n",
            "Epoch 5503/10000: L(Train): 0.3196912705898285; L(Test): 0.29660913348197937\n",
            "Epoch 5504/10000: L(Train): 0.3123343884944916; L(Test): 0.2981019616127014\n",
            "Epoch 5505/10000: L(Train): 0.3231426775455475; L(Test): 0.29880231618881226\n",
            "Epoch 5506/10000: L(Train): 0.32693588733673096; L(Test): 0.29679611325263977\n",
            "Epoch 5507/10000: L(Train): 0.32823121547698975; L(Test): 0.2963375747203827\n",
            "Epoch 5508/10000: L(Train): 0.325596421957016; L(Test): 0.2969970703125\n",
            "Epoch 5509/10000: L(Train): 0.33326202630996704; L(Test): 0.29750514030456543\n",
            "Epoch 5510/10000: L(Train): 0.32109764218330383; L(Test): 0.29662153124809265\n",
            "Epoch 5511/10000: L(Train): 0.32352522015571594; L(Test): 0.2954690158367157\n",
            "Epoch 5512/10000: L(Train): 0.31821608543395996; L(Test): 0.2953311502933502\n",
            "Epoch 5513/10000: L(Train): 0.32185640931129456; L(Test): 0.29512497782707214\n",
            "Epoch 5514/10000: L(Train): 0.32392460107803345; L(Test): 0.29605811834335327\n",
            "Epoch 5515/10000: L(Train): 0.32702401280403137; L(Test): 0.2958384156227112\n",
            "Epoch 5516/10000: L(Train): 0.3215238153934479; L(Test): 0.2952462434768677\n",
            "Epoch 5517/10000: L(Train): 0.31621089577674866; L(Test): 0.2964000701904297\n",
            "Epoch 5518/10000: L(Train): 0.3207988440990448; L(Test): 0.2979101836681366\n",
            "Epoch 5519/10000: L(Train): 0.3263612985610962; L(Test): 0.29565370082855225\n",
            "Epoch 5520/10000: L(Train): 0.3187996447086334; L(Test): 0.2959483861923218\n",
            "Epoch 5521/10000: L(Train): 0.3192998766899109; L(Test): 0.2969895899295807\n",
            "Epoch 5522/10000: L(Train): 0.3223181962966919; L(Test): 0.2956392765045166\n",
            "Epoch 5523/10000: L(Train): 0.31919386982917786; L(Test): 0.2974422872066498\n",
            "Epoch 5524/10000: L(Train): 0.32903173565864563; L(Test): 0.29788753390312195\n",
            "Epoch 5525/10000: L(Train): 0.3282749354839325; L(Test): 0.2964787483215332\n",
            "Epoch 5526/10000: L(Train): 0.3283906579017639; L(Test): 0.29721957445144653\n",
            "Epoch 5527/10000: L(Train): 0.3201262056827545; L(Test): 0.29831311106681824\n",
            "Epoch 5528/10000: L(Train): 0.32185521721839905; L(Test): 0.2958328425884247\n",
            "Epoch 5529/10000: L(Train): 0.32531410455703735; L(Test): 0.29697421193122864\n",
            "Epoch 5530/10000: L(Train): 0.31594184041023254; L(Test): 0.2990134060382843\n",
            "Epoch 5531/10000: L(Train): 0.32440048456192017; L(Test): 0.2974637746810913\n",
            "Epoch 5532/10000: L(Train): 0.3178580403327942; L(Test): 0.29819056391716003\n",
            "Epoch 5533/10000: L(Train): 0.31344345211982727; L(Test): 0.2988191545009613\n",
            "Epoch 5534/10000: L(Train): 0.3131874203681946; L(Test): 0.29671764373779297\n",
            "Epoch 5535/10000: L(Train): 0.3176610469818115; L(Test): 0.29763728380203247\n",
            "Epoch 5536/10000: L(Train): 0.318695068359375; L(Test): 0.29655879735946655\n",
            "Epoch 5537/10000: L(Train): 0.32888859510421753; L(Test): 0.2972235679626465\n",
            "Epoch 5538/10000: L(Train): 0.3175223469734192; L(Test): 0.2979393005371094\n",
            "Epoch 5539/10000: L(Train): 0.32049813866615295; L(Test): 0.29646143317222595\n",
            "Epoch 5540/10000: L(Train): 0.31752780079841614; L(Test): 0.29602134227752686\n",
            "Epoch 5541/10000: L(Train): 0.317170113325119; L(Test): 0.2978498637676239\n",
            "Epoch 5542/10000: L(Train): 0.319729745388031; L(Test): 0.29610908031463623\n",
            "Epoch 5543/10000: L(Train): 0.32137438654899597; L(Test): 0.29731279611587524\n",
            "Epoch 5544/10000: L(Train): 0.3230821192264557; L(Test): 0.2974596917629242\n",
            "Epoch 5545/10000: L(Train): 0.32428836822509766; L(Test): 0.2960774600505829\n",
            "Epoch 5546/10000: L(Train): 0.32022249698638916; L(Test): 0.29741472005844116\n",
            "Epoch 5547/10000: L(Train): 0.319595068693161; L(Test): 0.29681137204170227\n",
            "Epoch 5548/10000: L(Train): 0.3237050771713257; L(Test): 0.2965189814567566\n",
            "Epoch 5549/10000: L(Train): 0.3156260848045349; L(Test): 0.2976917326450348\n",
            "Epoch 5550/10000: L(Train): 0.3201642334461212; L(Test): 0.29744410514831543\n",
            "Epoch 5551/10000: L(Train): 0.3221953511238098; L(Test): 0.29627734422683716\n",
            "Epoch 5552/10000: L(Train): 0.3241928815841675; L(Test): 0.29622918367385864\n",
            "Epoch 5553/10000: L(Train): 0.32675686478614807; L(Test): 0.296482652425766\n",
            "Epoch 5554/10000: L(Train): 0.3200262486934662; L(Test): 0.29829704761505127\n",
            "Epoch 5555/10000: L(Train): 0.3219943344593048; L(Test): 0.2978975176811218\n",
            "Epoch 5556/10000: L(Train): 0.31892767548561096; L(Test): 0.2965143918991089\n",
            "Epoch 5557/10000: L(Train): 0.3258190453052521; L(Test): 0.2973262667655945\n",
            "Epoch 5558/10000: L(Train): 0.31668969988822937; L(Test): 0.2984934151172638\n",
            "Epoch 5559/10000: L(Train): 0.3253687620162964; L(Test): 0.2981259822845459\n",
            "Epoch 5560/10000: L(Train): 0.3184807300567627; L(Test): 0.2988755404949188\n",
            "Epoch 5561/10000: L(Train): 0.31672021746635437; L(Test): 0.29939383268356323\n",
            "Epoch 5562/10000: L(Train): 0.33041128516197205; L(Test): 0.2977999150753021\n",
            "Epoch 5563/10000: L(Train): 0.3260633647441864; L(Test): 0.29740768671035767\n",
            "Epoch 5564/10000: L(Train): 0.3166508972644806; L(Test): 0.29787033796310425\n",
            "Epoch 5565/10000: L(Train): 0.32718712091445923; L(Test): 0.29758286476135254\n",
            "Epoch 5566/10000: L(Train): 0.31903591752052307; L(Test): 0.2967509329319\n",
            "Epoch 5567/10000: L(Train): 0.3208913505077362; L(Test): 0.2968640923500061\n",
            "Epoch 5568/10000: L(Train): 0.32834330201148987; L(Test): 0.2975686490535736\n",
            "Epoch 5569/10000: L(Train): 0.32735708355903625; L(Test): 0.29813116788864136\n",
            "Epoch 5570/10000: L(Train): 0.3187922537326813; L(Test): 0.29844754934310913\n",
            "Epoch 5571/10000: L(Train): 0.32258620858192444; L(Test): 0.2979426383972168\n",
            "Epoch 5572/10000: L(Train): 0.3218609690666199; L(Test): 0.2974950075149536\n",
            "Epoch 5573/10000: L(Train): 0.3312114477157593; L(Test): 0.2979319393634796\n",
            "Epoch 5574/10000: L(Train): 0.31749603152275085; L(Test): 0.296429306268692\n",
            "Epoch 5575/10000: L(Train): 0.3240436613559723; L(Test): 0.29929637908935547\n",
            "Epoch 5576/10000: L(Train): 0.32547274231910706; L(Test): 0.29886242747306824\n",
            "Epoch 5577/10000: L(Train): 0.32694748044013977; L(Test): 0.29998838901519775\n",
            "Epoch 5578/10000: L(Train): 0.3317252993583679; L(Test): 0.3011321425437927\n",
            "Epoch 5579/10000: L(Train): 0.32338860630989075; L(Test): 0.2988448739051819\n",
            "Epoch 5580/10000: L(Train): 0.32157647609710693; L(Test): 0.3002357482910156\n",
            "Epoch 5581/10000: L(Train): 0.32249578833580017; L(Test): 0.30257707834243774\n",
            "Epoch 5582/10000: L(Train): 0.33224979043006897; L(Test): 0.29807332158088684\n",
            "Epoch 5583/10000: L(Train): 0.32250505685806274; L(Test): 0.2986173629760742\n",
            "Epoch 5584/10000: L(Train): 0.31257203221321106; L(Test): 0.2980707585811615\n",
            "Epoch 5585/10000: L(Train): 0.3189276158809662; L(Test): 0.2989136278629303\n",
            "Epoch 5586/10000: L(Train): 0.3265743553638458; L(Test): 0.3008705675601959\n",
            "Epoch 5587/10000: L(Train): 0.3295586109161377; L(Test): 0.3008502125740051\n",
            "Epoch 5588/10000: L(Train): 0.3126014769077301; L(Test): 0.298572301864624\n",
            "Epoch 5589/10000: L(Train): 0.32081323862075806; L(Test): 0.2985360324382782\n",
            "Epoch 5590/10000: L(Train): 0.3279500901699066; L(Test): 0.2981674075126648\n",
            "Epoch 5591/10000: L(Train): 0.3310166299343109; L(Test): 0.29813218116760254\n",
            "Epoch 5592/10000: L(Train): 0.33052054047584534; L(Test): 0.2975185811519623\n",
            "Epoch 5593/10000: L(Train): 0.3138526678085327; L(Test): 0.2979738414287567\n",
            "Epoch 5594/10000: L(Train): 0.32241377234458923; L(Test): 0.29888203740119934\n",
            "Epoch 5595/10000: L(Train): 0.31994521617889404; L(Test): 0.29940134286880493\n",
            "Epoch 5596/10000: L(Train): 0.3283849060535431; L(Test): 0.2982730567455292\n",
            "Epoch 5597/10000: L(Train): 0.30896639823913574; L(Test): 0.29859593510627747\n",
            "Epoch 5598/10000: L(Train): 0.32451948523521423; L(Test): 0.29882195591926575\n",
            "Epoch 5599/10000: L(Train): 0.3272235095500946; L(Test): 0.2981500029563904\n",
            "Epoch 5600/10000: L(Train): 0.31733036041259766; L(Test): 0.2976471781730652\n",
            "Epoch 5601/10000: L(Train): 0.3241407871246338; L(Test): 0.29731449484825134\n",
            "Epoch 5602/10000: L(Train): 0.3235604763031006; L(Test): 0.2966063320636749\n",
            "Epoch 5603/10000: L(Train): 0.31783628463745117; L(Test): 0.29691416025161743\n",
            "Epoch 5604/10000: L(Train): 0.3202233910560608; L(Test): 0.2968226969242096\n",
            "Epoch 5605/10000: L(Train): 0.3163648247718811; L(Test): 0.2956737279891968\n",
            "Epoch 5606/10000: L(Train): 0.3150237798690796; L(Test): 0.29602301120758057\n",
            "Epoch 5607/10000: L(Train): 0.3272360861301422; L(Test): 0.2961519658565521\n",
            "Epoch 5608/10000: L(Train): 0.31689560413360596; L(Test): 0.2964008152484894\n",
            "Epoch 5609/10000: L(Train): 0.32160356640815735; L(Test): 0.2974115014076233\n",
            "Epoch 5610/10000: L(Train): 0.33258742094039917; L(Test): 0.29694414138793945\n",
            "Epoch 5611/10000: L(Train): 0.3068787753582001; L(Test): 0.2961055338382721\n",
            "Epoch 5612/10000: L(Train): 0.3197263777256012; L(Test): 0.29626595973968506\n",
            "Epoch 5613/10000: L(Train): 0.3149416744709015; L(Test): 0.2966729700565338\n",
            "Epoch 5614/10000: L(Train): 0.32407665252685547; L(Test): 0.2962688207626343\n",
            "Epoch 5615/10000: L(Train): 0.32594189047813416; L(Test): 0.29582494497299194\n",
            "Epoch 5616/10000: L(Train): 0.3225899636745453; L(Test): 0.2962401509284973\n",
            "Epoch 5617/10000: L(Train): 0.3179369866847992; L(Test): 0.29684197902679443\n",
            "Epoch 5618/10000: L(Train): 0.3191984295845032; L(Test): 0.2965902090072632\n",
            "Epoch 5619/10000: L(Train): 0.3249267637729645; L(Test): 0.296140193939209\n",
            "Epoch 5620/10000: L(Train): 0.32208094000816345; L(Test): 0.2957534193992615\n",
            "Epoch 5621/10000: L(Train): 0.3251992464065552; L(Test): 0.29580986499786377\n",
            "Epoch 5622/10000: L(Train): 0.3208138942718506; L(Test): 0.29688504338264465\n",
            "Epoch 5623/10000: L(Train): 0.32381492853164673; L(Test): 0.2968060076236725\n",
            "Epoch 5624/10000: L(Train): 0.325740784406662; L(Test): 0.29661038517951965\n",
            "Epoch 5625/10000: L(Train): 0.3335101902484894; L(Test): 0.2968987822532654\n",
            "Epoch 5626/10000: L(Train): 0.32533589005470276; L(Test): 0.2971038520336151\n",
            "Epoch 5627/10000: L(Train): 0.32226502895355225; L(Test): 0.2963162064552307\n",
            "Epoch 5628/10000: L(Train): 0.31269264221191406; L(Test): 0.29690876603126526\n",
            "Epoch 5629/10000: L(Train): 0.31673863530158997; L(Test): 0.29643553495407104\n",
            "Epoch 5630/10000: L(Train): 0.3212736248970032; L(Test): 0.29563581943511963\n",
            "Epoch 5631/10000: L(Train): 0.31684380769729614; L(Test): 0.2965884804725647\n",
            "Epoch 5632/10000: L(Train): 0.3271198272705078; L(Test): 0.29740193486213684\n",
            "Epoch 5633/10000: L(Train): 0.3172779381275177; L(Test): 0.29746416211128235\n",
            "Epoch 5634/10000: L(Train): 0.3246370255947113; L(Test): 0.2973366379737854\n",
            "Epoch 5635/10000: L(Train): 0.3189292848110199; L(Test): 0.2971571683883667\n",
            "Epoch 5636/10000: L(Train): 0.32122376561164856; L(Test): 0.29711562395095825\n",
            "Epoch 5637/10000: L(Train): 0.32140985131263733; L(Test): 0.2971930205821991\n",
            "Epoch 5638/10000: L(Train): 0.32464364171028137; L(Test): 0.29659655690193176\n",
            "Epoch 5639/10000: L(Train): 0.31995895504951477; L(Test): 0.2957426607608795\n",
            "Epoch 5640/10000: L(Train): 0.315818727016449; L(Test): 0.29595640301704407\n",
            "Epoch 5641/10000: L(Train): 0.31158456206321716; L(Test): 0.2960891127586365\n",
            "Epoch 5642/10000: L(Train): 0.3201238512992859; L(Test): 0.2961180508136749\n",
            "Epoch 5643/10000: L(Train): 0.3199009299278259; L(Test): 0.2953815758228302\n",
            "Epoch 5644/10000: L(Train): 0.32187288999557495; L(Test): 0.29653990268707275\n",
            "Epoch 5645/10000: L(Train): 0.32372817397117615; L(Test): 0.29767534136772156\n",
            "Epoch 5646/10000: L(Train): 0.3197144567966461; L(Test): 0.2971270680427551\n",
            "Epoch 5647/10000: L(Train): 0.3157708942890167; L(Test): 0.2970798909664154\n",
            "Epoch 5648/10000: L(Train): 0.3212634027004242; L(Test): 0.2971636950969696\n",
            "Epoch 5649/10000: L(Train): 0.327548623085022; L(Test): 0.29762136936187744\n",
            "Epoch 5650/10000: L(Train): 0.32310470938682556; L(Test): 0.29926061630249023\n",
            "Epoch 5651/10000: L(Train): 0.3250635862350464; L(Test): 0.29912590980529785\n",
            "Epoch 5652/10000: L(Train): 0.3195551633834839; L(Test): 0.29830923676490784\n",
            "Epoch 5653/10000: L(Train): 0.32111042737960815; L(Test): 0.2998344898223877\n",
            "Epoch 5654/10000: L(Train): 0.33112695813179016; L(Test): 0.2988468110561371\n",
            "Epoch 5655/10000: L(Train): 0.3183692395687103; L(Test): 0.29982897639274597\n",
            "Epoch 5656/10000: L(Train): 0.3215252459049225; L(Test): 0.3020887076854706\n",
            "Epoch 5657/10000: L(Train): 0.32358652353286743; L(Test): 0.29996049404144287\n",
            "Epoch 5658/10000: L(Train): 0.32435646653175354; L(Test): 0.29826483130455017\n",
            "Epoch 5659/10000: L(Train): 0.3212559223175049; L(Test): 0.3001706004142761\n",
            "Epoch 5660/10000: L(Train): 0.32104575634002686; L(Test): 0.300441712141037\n",
            "Epoch 5661/10000: L(Train): 0.3227664530277252; L(Test): 0.3021649122238159\n",
            "Epoch 5662/10000: L(Train): 0.3241993188858032; L(Test): 0.3003961443901062\n",
            "Epoch 5663/10000: L(Train): 0.32575467228889465; L(Test): 0.29919031262397766\n",
            "Epoch 5664/10000: L(Train): 0.3239952027797699; L(Test): 0.30127471685409546\n",
            "Epoch 5665/10000: L(Train): 0.3206946551799774; L(Test): 0.29969480633735657\n",
            "Epoch 5666/10000: L(Train): 0.31965208053588867; L(Test): 0.30073878169059753\n",
            "Epoch 5667/10000: L(Train): 0.3224930167198181; L(Test): 0.3027954399585724\n",
            "Epoch 5668/10000: L(Train): 0.3198547661304474; L(Test): 0.3014373779296875\n",
            "Epoch 5669/10000: L(Train): 0.3269113302230835; L(Test): 0.2995509207248688\n",
            "Epoch 5670/10000: L(Train): 0.3326050639152527; L(Test): 0.3018878698348999\n",
            "Epoch 5671/10000: L(Train): 0.32806941866874695; L(Test): 0.3022073805332184\n",
            "Epoch 5672/10000: L(Train): 0.31781479716300964; L(Test): 0.3014562427997589\n",
            "Epoch 5673/10000: L(Train): 0.328264057636261; L(Test): 0.3037932813167572\n",
            "Epoch 5674/10000: L(Train): 0.3234063386917114; L(Test): 0.3025398850440979\n",
            "Epoch 5675/10000: L(Train): 0.32420045137405396; L(Test): 0.3008958399295807\n",
            "Epoch 5676/10000: L(Train): 0.33083823323249817; L(Test): 0.29952526092529297\n",
            "Epoch 5677/10000: L(Train): 0.32451552152633667; L(Test): 0.29783228039741516\n",
            "Epoch 5678/10000: L(Train): 0.3145211637020111; L(Test): 0.2994988262653351\n",
            "Epoch 5679/10000: L(Train): 0.31190261244773865; L(Test): 0.3001801371574402\n",
            "Epoch 5680/10000: L(Train): 0.3347473740577698; L(Test): 0.29905134439468384\n",
            "Epoch 5681/10000: L(Train): 0.3231225609779358; L(Test): 0.29874488711357117\n",
            "Epoch 5682/10000: L(Train): 0.3205801248550415; L(Test): 0.2988198697566986\n",
            "Epoch 5683/10000: L(Train): 0.32419347763061523; L(Test): 0.29863160848617554\n",
            "Epoch 5684/10000: L(Train): 0.32070979475975037; L(Test): 0.2989838719367981\n",
            "Epoch 5685/10000: L(Train): 0.32245755195617676; L(Test): 0.29970961809158325\n",
            "Epoch 5686/10000: L(Train): 0.3150273859500885; L(Test): 0.2986741065979004\n",
            "Epoch 5687/10000: L(Train): 0.31726139783859253; L(Test): 0.29922887682914734\n",
            "Epoch 5688/10000: L(Train): 0.32680243253707886; L(Test): 0.30002346634864807\n",
            "Epoch 5689/10000: L(Train): 0.32249465584754944; L(Test): 0.29871052503585815\n",
            "Epoch 5690/10000: L(Train): 0.3273772597312927; L(Test): 0.2980157136917114\n",
            "Epoch 5691/10000: L(Train): 0.32669317722320557; L(Test): 0.2982063889503479\n",
            "Epoch 5692/10000: L(Train): 0.3172289729118347; L(Test): 0.2976714074611664\n",
            "Epoch 5693/10000: L(Train): 0.32693272829055786; L(Test): 0.2977007329463959\n",
            "Epoch 5694/10000: L(Train): 0.3188275098800659; L(Test): 0.29858705401420593\n",
            "Epoch 5695/10000: L(Train): 0.3232877850532532; L(Test): 0.29911914467811584\n",
            "Epoch 5696/10000: L(Train): 0.33048367500305176; L(Test): 0.2985314130783081\n",
            "Epoch 5697/10000: L(Train): 0.32427698373794556; L(Test): 0.29874539375305176\n",
            "Epoch 5698/10000: L(Train): 0.32882726192474365; L(Test): 0.29981520771980286\n",
            "Epoch 5699/10000: L(Train): 0.31363117694854736; L(Test): 0.30067479610443115\n",
            "Epoch 5700/10000: L(Train): 0.33789515495300293; L(Test): 0.29955220222473145\n",
            "Epoch 5701/10000: L(Train): 0.3307810425758362; L(Test): 0.29946985840797424\n",
            "Epoch 5702/10000: L(Train): 0.32082489132881165; L(Test): 0.29985037446022034\n",
            "Epoch 5703/10000: L(Train): 0.3255409598350525; L(Test): 0.2979992628097534\n",
            "Epoch 5704/10000: L(Train): 0.31876516342163086; L(Test): 0.2978375256061554\n",
            "Epoch 5705/10000: L(Train): 0.32409706711769104; L(Test): 0.2993845045566559\n",
            "Epoch 5706/10000: L(Train): 0.3296736776828766; L(Test): 0.29931920766830444\n",
            "Epoch 5707/10000: L(Train): 0.3348565697669983; L(Test): 0.297525018453598\n",
            "Epoch 5708/10000: L(Train): 0.31959378719329834; L(Test): 0.29903507232666016\n",
            "Epoch 5709/10000: L(Train): 0.31574684381484985; L(Test): 0.298303484916687\n",
            "Epoch 5710/10000: L(Train): 0.31533083319664; L(Test): 0.2980251908302307\n",
            "Epoch 5711/10000: L(Train): 0.3183112144470215; L(Test): 0.2977723479270935\n",
            "Epoch 5712/10000: L(Train): 0.3204483985900879; L(Test): 0.2981736660003662\n",
            "Epoch 5713/10000: L(Train): 0.32444292306900024; L(Test): 0.2981649339199066\n",
            "Epoch 5714/10000: L(Train): 0.3245793282985687; L(Test): 0.29766780138015747\n",
            "Epoch 5715/10000: L(Train): 0.3213326930999756; L(Test): 0.2971460819244385\n",
            "Epoch 5716/10000: L(Train): 0.3264671862125397; L(Test): 0.2972869277000427\n",
            "Epoch 5717/10000: L(Train): 0.3135906159877777; L(Test): 0.29750362038612366\n",
            "Epoch 5718/10000: L(Train): 0.3162716329097748; L(Test): 0.29771849513053894\n",
            "Epoch 5719/10000: L(Train): 0.3131934702396393; L(Test): 0.2980324625968933\n",
            "Epoch 5720/10000: L(Train): 0.3155100643634796; L(Test): 0.29785972833633423\n",
            "Epoch 5721/10000: L(Train): 0.3334208130836487; L(Test): 0.29843083024024963\n",
            "Epoch 5722/10000: L(Train): 0.3188942074775696; L(Test): 0.29827404022216797\n",
            "Epoch 5723/10000: L(Train): 0.3235413134098053; L(Test): 0.29730209708213806\n",
            "Epoch 5724/10000: L(Train): 0.32188349962234497; L(Test): 0.2978763282299042\n",
            "Epoch 5725/10000: L(Train): 0.32641422748565674; L(Test): 0.2989174425601959\n",
            "Epoch 5726/10000: L(Train): 0.3208105266094208; L(Test): 0.29959508776664734\n",
            "Epoch 5727/10000: L(Train): 0.32248231768608093; L(Test): 0.29836606979370117\n",
            "Epoch 5728/10000: L(Train): 0.32192033529281616; L(Test): 0.29725655913352966\n",
            "Epoch 5729/10000: L(Train): 0.3260147273540497; L(Test): 0.29780516028404236\n",
            "Epoch 5730/10000: L(Train): 0.32535287737846375; L(Test): 0.2973436713218689\n",
            "Epoch 5731/10000: L(Train): 0.3153798580169678; L(Test): 0.29694128036499023\n",
            "Epoch 5732/10000: L(Train): 0.31714746356010437; L(Test): 0.2977997958660126\n",
            "Epoch 5733/10000: L(Train): 0.3243366479873657; L(Test): 0.2980598211288452\n",
            "Epoch 5734/10000: L(Train): 0.3287770748138428; L(Test): 0.29709070920944214\n",
            "Epoch 5735/10000: L(Train): 0.3229452073574066; L(Test): 0.2965840697288513\n",
            "Epoch 5736/10000: L(Train): 0.31370651721954346; L(Test): 0.2965649366378784\n",
            "Epoch 5737/10000: L(Train): 0.3188181221485138; L(Test): 0.29741615056991577\n",
            "Epoch 5738/10000: L(Train): 0.3247281610965729; L(Test): 0.298340767621994\n",
            "Epoch 5739/10000: L(Train): 0.33603742718696594; L(Test): 0.2980709671974182\n",
            "Epoch 5740/10000: L(Train): 0.323833703994751; L(Test): 0.2973952889442444\n",
            "Epoch 5741/10000: L(Train): 0.3201543390750885; L(Test): 0.2968236804008484\n",
            "Epoch 5742/10000: L(Train): 0.31808802485466003; L(Test): 0.2964010536670685\n",
            "Epoch 5743/10000: L(Train): 0.3201216161251068; L(Test): 0.29642048478126526\n",
            "Epoch 5744/10000: L(Train): 0.3240399658679962; L(Test): 0.2964438199996948\n",
            "Epoch 5745/10000: L(Train): 0.32922425866127014; L(Test): 0.2964037358760834\n",
            "Epoch 5746/10000: L(Train): 0.316598117351532; L(Test): 0.2961527705192566\n",
            "Epoch 5747/10000: L(Train): 0.31652238965034485; L(Test): 0.29681313037872314\n",
            "Epoch 5748/10000: L(Train): 0.3217029273509979; L(Test): 0.29733705520629883\n",
            "Epoch 5749/10000: L(Train): 0.3252398669719696; L(Test): 0.2974145710468292\n",
            "Epoch 5750/10000: L(Train): 0.3308659493923187; L(Test): 0.29602012038230896\n",
            "Epoch 5751/10000: L(Train): 0.32798418402671814; L(Test): 0.29560795426368713\n",
            "Epoch 5752/10000: L(Train): 0.3213377892971039; L(Test): 0.2959553599357605\n",
            "Epoch 5753/10000: L(Train): 0.3145686686038971; L(Test): 0.29578790068626404\n",
            "Epoch 5754/10000: L(Train): 0.3182358741760254; L(Test): 0.2957777976989746\n",
            "Epoch 5755/10000: L(Train): 0.328244149684906; L(Test): 0.2959754467010498\n",
            "Epoch 5756/10000: L(Train): 0.3204571008682251; L(Test): 0.2966313660144806\n",
            "Epoch 5757/10000: L(Train): 0.31759846210479736; L(Test): 0.29714158177375793\n",
            "Epoch 5758/10000: L(Train): 0.31576111912727356; L(Test): 0.2979013919830322\n",
            "Epoch 5759/10000: L(Train): 0.3212336003780365; L(Test): 0.29751989245414734\n",
            "Epoch 5760/10000: L(Train): 0.32460951805114746; L(Test): 0.29631733894348145\n",
            "Epoch 5761/10000: L(Train): 0.3195861876010895; L(Test): 0.29733365774154663\n",
            "Epoch 5762/10000: L(Train): 0.3245788812637329; L(Test): 0.298024982213974\n",
            "Epoch 5763/10000: L(Train): 0.32344308495521545; L(Test): 0.2971481680870056\n",
            "Epoch 5764/10000: L(Train): 0.32338449358940125; L(Test): 0.2976926863193512\n",
            "Epoch 5765/10000: L(Train): 0.31857165694236755; L(Test): 0.2962448298931122\n",
            "Epoch 5766/10000: L(Train): 0.3115234673023224; L(Test): 0.29624781012535095\n",
            "Epoch 5767/10000: L(Train): 0.31782078742980957; L(Test): 0.29764240980148315\n",
            "Epoch 5768/10000: L(Train): 0.3254009187221527; L(Test): 0.2972162663936615\n",
            "Epoch 5769/10000: L(Train): 0.33106449246406555; L(Test): 0.29726484417915344\n",
            "Epoch 5770/10000: L(Train): 0.31476545333862305; L(Test): 0.2961946129798889\n",
            "Epoch 5771/10000: L(Train): 0.3144253194332123; L(Test): 0.29562461376190186\n",
            "Epoch 5772/10000: L(Train): 0.3250068128108978; L(Test): 0.2950955033302307\n",
            "Epoch 5773/10000: L(Train): 0.32042819261550903; L(Test): 0.29530656337738037\n",
            "Epoch 5774/10000: L(Train): 0.3234077990055084; L(Test): 0.2956058084964752\n",
            "Epoch 5775/10000: L(Train): 0.3183106780052185; L(Test): 0.2963840663433075\n",
            "Epoch 5776/10000: L(Train): 0.3181174397468567; L(Test): 0.2960658073425293\n",
            "Epoch 5777/10000: L(Train): 0.31679433584213257; L(Test): 0.2967352271080017\n",
            "Epoch 5778/10000: L(Train): 0.3234387934207916; L(Test): 0.2966066002845764\n",
            "Epoch 5779/10000: L(Train): 0.31771931052207947; L(Test): 0.29643934965133667\n",
            "Epoch 5780/10000: L(Train): 0.3119981586933136; L(Test): 0.2971932888031006\n",
            "Epoch 5781/10000: L(Train): 0.3214584290981293; L(Test): 0.29716360569000244\n",
            "Epoch 5782/10000: L(Train): 0.32146772742271423; L(Test): 0.2962270677089691\n",
            "Epoch 5783/10000: L(Train): 0.3148418962955475; L(Test): 0.2976933419704437\n",
            "Epoch 5784/10000: L(Train): 0.3213631510734558; L(Test): 0.2972899377346039\n",
            "Epoch 5785/10000: L(Train): 0.3196379542350769; L(Test): 0.29590946435928345\n",
            "Epoch 5786/10000: L(Train): 0.31834086775779724; L(Test): 0.2964055836200714\n",
            "Epoch 5787/10000: L(Train): 0.3139999508857727; L(Test): 0.29649990797042847\n",
            "Epoch 5788/10000: L(Train): 0.3186122477054596; L(Test): 0.29662075638771057\n",
            "Epoch 5789/10000: L(Train): 0.31803426146507263; L(Test): 0.2966192960739136\n",
            "Epoch 5790/10000: L(Train): 0.31719690561294556; L(Test): 0.2963969111442566\n",
            "Epoch 5791/10000: L(Train): 0.31319063901901245; L(Test): 0.29682663083076477\n",
            "Epoch 5792/10000: L(Train): 0.32469475269317627; L(Test): 0.2963990569114685\n",
            "Epoch 5793/10000: L(Train): 0.32859477400779724; L(Test): 0.2957010865211487\n",
            "Epoch 5794/10000: L(Train): 0.31877601146698; L(Test): 0.2958880364894867\n",
            "Epoch 5795/10000: L(Train): 0.31965741515159607; L(Test): 0.29608869552612305\n",
            "Epoch 5796/10000: L(Train): 0.3269413113594055; L(Test): 0.2960948646068573\n",
            "Epoch 5797/10000: L(Train): 0.3172021210193634; L(Test): 0.29678094387054443\n",
            "Epoch 5798/10000: L(Train): 0.316635400056839; L(Test): 0.29783013463020325\n",
            "Epoch 5799/10000: L(Train): 0.32347211241722107; L(Test): 0.297005295753479\n",
            "Epoch 5800/10000: L(Train): 0.32161945104599; L(Test): 0.29630547761917114\n",
            "Epoch 5801/10000: L(Train): 0.31886664032936096; L(Test): 0.296664297580719\n",
            "Epoch 5802/10000: L(Train): 0.30887967348098755; L(Test): 0.29580625891685486\n",
            "Epoch 5803/10000: L(Train): 0.31789734959602356; L(Test): 0.294590026140213\n",
            "Epoch 5804/10000: L(Train): 0.3178648054599762; L(Test): 0.295473575592041\n",
            "Epoch 5805/10000: L(Train): 0.3271619379520416; L(Test): 0.2955630123615265\n",
            "Epoch 5806/10000: L(Train): 0.3178142309188843; L(Test): 0.29554787278175354\n",
            "Epoch 5807/10000: L(Train): 0.3190498650074005; L(Test): 0.2967125177383423\n",
            "Epoch 5808/10000: L(Train): 0.323616623878479; L(Test): 0.29593104124069214\n",
            "Epoch 5809/10000: L(Train): 0.31781843304634094; L(Test): 0.29706475138664246\n",
            "Epoch 5810/10000: L(Train): 0.3231319189071655; L(Test): 0.29718679189682007\n",
            "Epoch 5811/10000: L(Train): 0.3303889334201813; L(Test): 0.2963024973869324\n",
            "Epoch 5812/10000: L(Train): 0.3243676722049713; L(Test): 0.2987368404865265\n",
            "Epoch 5813/10000: L(Train): 0.31724244356155396; L(Test): 0.29673221707344055\n",
            "Epoch 5814/10000: L(Train): 0.32456541061401367; L(Test): 0.2974114418029785\n",
            "Epoch 5815/10000: L(Train): 0.3305305242538452; L(Test): 0.29665109515190125\n",
            "Epoch 5816/10000: L(Train): 0.32370322942733765; L(Test): 0.2966713011264801\n",
            "Epoch 5817/10000: L(Train): 0.3272571265697479; L(Test): 0.29730555415153503\n",
            "Epoch 5818/10000: L(Train): 0.3216988742351532; L(Test): 0.2977510094642639\n",
            "Epoch 5819/10000: L(Train): 0.3212284445762634; L(Test): 0.2967461049556732\n",
            "Epoch 5820/10000: L(Train): 0.32958197593688965; L(Test): 0.29727327823638916\n",
            "Epoch 5821/10000: L(Train): 0.32615014910697937; L(Test): 0.29841095209121704\n",
            "Epoch 5822/10000: L(Train): 0.3214266896247864; L(Test): 0.29831641912460327\n",
            "Epoch 5823/10000: L(Train): 0.31662777066230774; L(Test): 0.2967524528503418\n",
            "Epoch 5824/10000: L(Train): 0.31396254897117615; L(Test): 0.29713332653045654\n",
            "Epoch 5825/10000: L(Train): 0.3164258301258087; L(Test): 0.2981695532798767\n",
            "Epoch 5826/10000: L(Train): 0.3227077126502991; L(Test): 0.2982088029384613\n",
            "Epoch 5827/10000: L(Train): 0.31417176127433777; L(Test): 0.2979089617729187\n",
            "Epoch 5828/10000: L(Train): 0.3233482837677002; L(Test): 0.2983734607696533\n",
            "Epoch 5829/10000: L(Train): 0.3182818293571472; L(Test): 0.2995930314064026\n",
            "Epoch 5830/10000: L(Train): 0.3221319019794464; L(Test): 0.29705438017845154\n",
            "Epoch 5831/10000: L(Train): 0.3098890483379364; L(Test): 0.2998095452785492\n",
            "Epoch 5832/10000: L(Train): 0.3108869194984436; L(Test): 0.2998011112213135\n",
            "Epoch 5833/10000: L(Train): 0.3221782445907593; L(Test): 0.29961034655570984\n",
            "Epoch 5834/10000: L(Train): 0.32700115442276; L(Test): 0.30085790157318115\n",
            "Epoch 5835/10000: L(Train): 0.33353152871131897; L(Test): 0.29865768551826477\n",
            "Epoch 5836/10000: L(Train): 0.3231815695762634; L(Test): 0.29781317710876465\n",
            "Epoch 5837/10000: L(Train): 0.32403525710105896; L(Test): 0.3003722131252289\n",
            "Epoch 5838/10000: L(Train): 0.31762343645095825; L(Test): 0.29996466636657715\n",
            "Epoch 5839/10000: L(Train): 0.3265775740146637; L(Test): 0.29733943939208984\n",
            "Epoch 5840/10000: L(Train): 0.32726985216140747; L(Test): 0.2987845242023468\n",
            "Epoch 5841/10000: L(Train): 0.31801190972328186; L(Test): 0.3002046048641205\n",
            "Epoch 5842/10000: L(Train): 0.3185359835624695; L(Test): 0.2989431917667389\n",
            "Epoch 5843/10000: L(Train): 0.32217952609062195; L(Test): 0.2984180450439453\n",
            "Epoch 5844/10000: L(Train): 0.3180917501449585; L(Test): 0.2988947033882141\n",
            "Epoch 5845/10000: L(Train): 0.31974324584007263; L(Test): 0.2999178171157837\n",
            "Epoch 5846/10000: L(Train): 0.32504579424858093; L(Test): 0.298959881067276\n",
            "Epoch 5847/10000: L(Train): 0.32105639576911926; L(Test): 0.29915428161621094\n",
            "Epoch 5848/10000: L(Train): 0.31028056144714355; L(Test): 0.3001105487346649\n",
            "Epoch 5849/10000: L(Train): 0.3270808160305023; L(Test): 0.30084601044654846\n",
            "Epoch 5850/10000: L(Train): 0.32600271701812744; L(Test): 0.3011034429073334\n",
            "Epoch 5851/10000: L(Train): 0.33055537939071655; L(Test): 0.2985994517803192\n",
            "Epoch 5852/10000: L(Train): 0.33030328154563904; L(Test): 0.2979884743690491\n",
            "Epoch 5853/10000: L(Train): 0.3222162127494812; L(Test): 0.2986007332801819\n",
            "Epoch 5854/10000: L(Train): 0.3256296217441559; L(Test): 0.2987903654575348\n",
            "Epoch 5855/10000: L(Train): 0.3233334422111511; L(Test): 0.2982310354709625\n",
            "Epoch 5856/10000: L(Train): 0.3183766007423401; L(Test): 0.298248291015625\n",
            "Epoch 5857/10000: L(Train): 0.3152427673339844; L(Test): 0.2978225350379944\n",
            "Epoch 5858/10000: L(Train): 0.3234403729438782; L(Test): 0.2997426390647888\n",
            "Epoch 5859/10000: L(Train): 0.3287408649921417; L(Test): 0.2996768653392792\n",
            "Epoch 5860/10000: L(Train): 0.32209885120391846; L(Test): 0.29662561416625977\n",
            "Epoch 5861/10000: L(Train): 0.3242204487323761; L(Test): 0.29696255922317505\n",
            "Epoch 5862/10000: L(Train): 0.32710790634155273; L(Test): 0.2976922392845154\n",
            "Epoch 5863/10000: L(Train): 0.3294984698295593; L(Test): 0.2978953421115875\n",
            "Epoch 5864/10000: L(Train): 0.3239169418811798; L(Test): 0.29741233587265015\n",
            "Epoch 5865/10000: L(Train): 0.32740381360054016; L(Test): 0.29760217666625977\n",
            "Epoch 5866/10000: L(Train): 0.31569594144821167; L(Test): 0.2974869906902313\n",
            "Epoch 5867/10000: L(Train): 0.31756192445755005; L(Test): 0.2959648370742798\n",
            "Epoch 5868/10000: L(Train): 0.3274703919887543; L(Test): 0.2965565621852875\n",
            "Epoch 5869/10000: L(Train): 0.31741511821746826; L(Test): 0.29616424441337585\n",
            "Epoch 5870/10000: L(Train): 0.32796216011047363; L(Test): 0.29607394337654114\n",
            "Epoch 5871/10000: L(Train): 0.31674209237098694; L(Test): 0.2965300977230072\n",
            "Epoch 5872/10000: L(Train): 0.32190418243408203; L(Test): 0.2965119481086731\n",
            "Epoch 5873/10000: L(Train): 0.33028629422187805; L(Test): 0.2965797185897827\n",
            "Epoch 5874/10000: L(Train): 0.3241231143474579; L(Test): 0.2971537113189697\n",
            "Epoch 5875/10000: L(Train): 0.32042938470840454; L(Test): 0.29809120297431946\n",
            "Epoch 5876/10000: L(Train): 0.3242471218109131; L(Test): 0.29692143201828003\n",
            "Epoch 5877/10000: L(Train): 0.3288891017436981; L(Test): 0.2955684959888458\n",
            "Epoch 5878/10000: L(Train): 0.3157653212547302; L(Test): 0.2967208921909332\n",
            "Epoch 5879/10000: L(Train): 0.320332407951355; L(Test): 0.29482966661453247\n",
            "Epoch 5880/10000: L(Train): 0.3293379843235016; L(Test): 0.29610562324523926\n",
            "Epoch 5881/10000: L(Train): 0.3160175383090973; L(Test): 0.2984151542186737\n",
            "Epoch 5882/10000: L(Train): 0.3222213387489319; L(Test): 0.2965572774410248\n",
            "Epoch 5883/10000: L(Train): 0.3257519602775574; L(Test): 0.2970156967639923\n",
            "Epoch 5884/10000: L(Train): 0.32684484124183655; L(Test): 0.298734575510025\n",
            "Epoch 5885/10000: L(Train): 0.3260645866394043; L(Test): 0.298082560300827\n",
            "Epoch 5886/10000: L(Train): 0.3248845934867859; L(Test): 0.2969050705432892\n",
            "Epoch 5887/10000: L(Train): 0.3240414261817932; L(Test): 0.29719698429107666\n",
            "Epoch 5888/10000: L(Train): 0.3249092102050781; L(Test): 0.29702264070510864\n",
            "Epoch 5889/10000: L(Train): 0.3159392178058624; L(Test): 0.2960366904735565\n",
            "Epoch 5890/10000: L(Train): 0.3189210295677185; L(Test): 0.29636645317077637\n",
            "Epoch 5891/10000: L(Train): 0.3223936855792999; L(Test): 0.29838988184928894\n",
            "Epoch 5892/10000: L(Train): 0.32072320580482483; L(Test): 0.2981623411178589\n",
            "Epoch 5893/10000: L(Train): 0.31576138734817505; L(Test): 0.2971651256084442\n",
            "Epoch 5894/10000: L(Train): 0.325607568025589; L(Test): 0.2968651056289673\n",
            "Epoch 5895/10000: L(Train): 0.32252180576324463; L(Test): 0.29651200771331787\n",
            "Epoch 5896/10000: L(Train): 0.3161848783493042; L(Test): 0.2965233325958252\n",
            "Epoch 5897/10000: L(Train): 0.32935813069343567; L(Test): 0.29618367552757263\n",
            "Epoch 5898/10000: L(Train): 0.3133198618888855; L(Test): 0.2974015474319458\n",
            "Epoch 5899/10000: L(Train): 0.32056668400764465; L(Test): 0.2972700297832489\n",
            "Epoch 5900/10000: L(Train): 0.3189238905906677; L(Test): 0.2962724566459656\n",
            "Epoch 5901/10000: L(Train): 0.3216078281402588; L(Test): 0.2965380847454071\n",
            "Epoch 5902/10000: L(Train): 0.3219560980796814; L(Test): 0.29665330052375793\n",
            "Epoch 5903/10000: L(Train): 0.3253020644187927; L(Test): 0.29651519656181335\n",
            "Epoch 5904/10000: L(Train): 0.32570549845695496; L(Test): 0.2958088219165802\n",
            "Epoch 5905/10000: L(Train): 0.3235754668712616; L(Test): 0.29546919465065\n",
            "Epoch 5906/10000: L(Train): 0.32384026050567627; L(Test): 0.29533636569976807\n",
            "Epoch 5907/10000: L(Train): 0.31911173462867737; L(Test): 0.2957746088504791\n",
            "Epoch 5908/10000: L(Train): 0.3243253529071808; L(Test): 0.2940979301929474\n",
            "Epoch 5909/10000: L(Train): 0.31836605072021484; L(Test): 0.2943706214427948\n",
            "Epoch 5910/10000: L(Train): 0.318981796503067; L(Test): 0.29509150981903076\n",
            "Epoch 5911/10000: L(Train): 0.3157894015312195; L(Test): 0.29603663086891174\n",
            "Epoch 5912/10000: L(Train): 0.33148518204689026; L(Test): 0.2968020737171173\n",
            "Epoch 5913/10000: L(Train): 0.3205578029155731; L(Test): 0.29602718353271484\n",
            "Epoch 5914/10000: L(Train): 0.3225063383579254; L(Test): 0.296035498380661\n",
            "Epoch 5915/10000: L(Train): 0.31649577617645264; L(Test): 0.2965710759162903\n",
            "Epoch 5916/10000: L(Train): 0.3197058141231537; L(Test): 0.29691630601882935\n",
            "Epoch 5917/10000: L(Train): 0.32236167788505554; L(Test): 0.29721254110336304\n",
            "Epoch 5918/10000: L(Train): 0.31762009859085083; L(Test): 0.2964818477630615\n",
            "Epoch 5919/10000: L(Train): 0.3105555474758148; L(Test): 0.296173095703125\n",
            "Epoch 5920/10000: L(Train): 0.3178170919418335; L(Test): 0.29712730646133423\n",
            "Epoch 5921/10000: L(Train): 0.3317013680934906; L(Test): 0.29608070850372314\n",
            "Epoch 5922/10000: L(Train): 0.31337231397628784; L(Test): 0.29758092761039734\n",
            "Epoch 5923/10000: L(Train): 0.32287833094596863; L(Test): 0.2970550060272217\n",
            "Epoch 5924/10000: L(Train): 0.3216839134693146; L(Test): 0.2957935631275177\n",
            "Epoch 5925/10000: L(Train): 0.3211981952190399; L(Test): 0.29643046855926514\n",
            "Epoch 5926/10000: L(Train): 0.32454776763916016; L(Test): 0.29534122347831726\n",
            "Epoch 5927/10000: L(Train): 0.32324087619781494; L(Test): 0.2969202995300293\n",
            "Epoch 5928/10000: L(Train): 0.320303738117218; L(Test): 0.29935622215270996\n",
            "Epoch 5929/10000: L(Train): 0.31887149810791016; L(Test): 0.29833507537841797\n",
            "Epoch 5930/10000: L(Train): 0.3151717483997345; L(Test): 0.29772621393203735\n",
            "Epoch 5931/10000: L(Train): 0.32220199704170227; L(Test): 0.29784438014030457\n",
            "Epoch 5932/10000: L(Train): 0.32327231764793396; L(Test): 0.29700204730033875\n",
            "Epoch 5933/10000: L(Train): 0.31587862968444824; L(Test): 0.29722633957862854\n",
            "Epoch 5934/10000: L(Train): 0.32751020789146423; L(Test): 0.29776617884635925\n",
            "Epoch 5935/10000: L(Train): 0.31783246994018555; L(Test): 0.2968807816505432\n",
            "Epoch 5936/10000: L(Train): 0.31886783242225647; L(Test): 0.29695478081703186\n",
            "Epoch 5937/10000: L(Train): 0.31461167335510254; L(Test): 0.29698479175567627\n",
            "Epoch 5938/10000: L(Train): 0.3267665207386017; L(Test): 0.29722869396209717\n",
            "Epoch 5939/10000: L(Train): 0.3173687160015106; L(Test): 0.296343594789505\n",
            "Epoch 5940/10000: L(Train): 0.3177242577075958; L(Test): 0.29532569646835327\n",
            "Epoch 5941/10000: L(Train): 0.3155190646648407; L(Test): 0.2955939471721649\n",
            "Epoch 5942/10000: L(Train): 0.327043354511261; L(Test): 0.2975025475025177\n",
            "Epoch 5943/10000: L(Train): 0.31341078877449036; L(Test): 0.30011072754859924\n",
            "Epoch 5944/10000: L(Train): 0.3268088698387146; L(Test): 0.300863116979599\n",
            "Epoch 5945/10000: L(Train): 0.31560656428337097; L(Test): 0.30246591567993164\n",
            "Epoch 5946/10000: L(Train): 0.3230687379837036; L(Test): 0.3014613687992096\n",
            "Epoch 5947/10000: L(Train): 0.32784396409988403; L(Test): 0.3020045757293701\n",
            "Epoch 5948/10000: L(Train): 0.32956990599632263; L(Test): 0.3038824796676636\n",
            "Epoch 5949/10000: L(Train): 0.33170753717422485; L(Test): 0.30161595344543457\n",
            "Epoch 5950/10000: L(Train): 0.32048773765563965; L(Test): 0.30281662940979004\n",
            "Epoch 5951/10000: L(Train): 0.32433414459228516; L(Test): 0.30486464500427246\n",
            "Epoch 5952/10000: L(Train): 0.3350526988506317; L(Test): 0.30298370122909546\n",
            "Epoch 5953/10000: L(Train): 0.32094937562942505; L(Test): 0.3027384281158447\n",
            "Epoch 5954/10000: L(Train): 0.32808414101600647; L(Test): 0.3034708499908447\n",
            "Epoch 5955/10000: L(Train): 0.3247795104980469; L(Test): 0.302238792181015\n",
            "Epoch 5956/10000: L(Train): 0.3223077356815338; L(Test): 0.3018169403076172\n",
            "Epoch 5957/10000: L(Train): 0.3250861167907715; L(Test): 0.3010868728160858\n",
            "Epoch 5958/10000: L(Train): 0.3264336884021759; L(Test): 0.30120038986206055\n",
            "Epoch 5959/10000: L(Train): 0.3283238410949707; L(Test): 0.3011429011821747\n",
            "Epoch 5960/10000: L(Train): 0.33603566884994507; L(Test): 0.3031313121318817\n",
            "Epoch 5961/10000: L(Train): 0.3238866627216339; L(Test): 0.30158278346061707\n",
            "Epoch 5962/10000: L(Train): 0.326224148273468; L(Test): 0.3017401099205017\n",
            "Epoch 5963/10000: L(Train): 0.32315587997436523; L(Test): 0.30253371596336365\n",
            "Epoch 5964/10000: L(Train): 0.31879907846450806; L(Test): 0.30080896615982056\n",
            "Epoch 5965/10000: L(Train): 0.3288213312625885; L(Test): 0.2994600236415863\n",
            "Epoch 5966/10000: L(Train): 0.3260335326194763; L(Test): 0.2997303009033203\n",
            "Epoch 5967/10000: L(Train): 0.3217453062534332; L(Test): 0.29914912581443787\n",
            "Epoch 5968/10000: L(Train): 0.32272621989250183; L(Test): 0.30095475912094116\n",
            "Epoch 5969/10000: L(Train): 0.31395792961120605; L(Test): 0.29963183403015137\n",
            "Epoch 5970/10000: L(Train): 0.3303717076778412; L(Test): 0.2990688681602478\n",
            "Epoch 5971/10000: L(Train): 0.324618935585022; L(Test): 0.29932546615600586\n",
            "Epoch 5972/10000: L(Train): 0.32338836789131165; L(Test): 0.2995738983154297\n",
            "Epoch 5973/10000: L(Train): 0.32383400201797485; L(Test): 0.30113881826400757\n",
            "Epoch 5974/10000: L(Train): 0.3284008204936981; L(Test): 0.3002297580242157\n",
            "Epoch 5975/10000: L(Train): 0.3205453157424927; L(Test): 0.2998889088630676\n",
            "Epoch 5976/10000: L(Train): 0.33165451884269714; L(Test): 0.3009989559650421\n",
            "Epoch 5977/10000: L(Train): 0.3327805995941162; L(Test): 0.2986363470554352\n",
            "Epoch 5978/10000: L(Train): 0.31248998641967773; L(Test): 0.2985748052597046\n",
            "Epoch 5979/10000: L(Train): 0.3215295970439911; L(Test): 0.3001255989074707\n",
            "Epoch 5980/10000: L(Train): 0.3204570710659027; L(Test): 0.2982284724712372\n",
            "Epoch 5981/10000: L(Train): 0.3160395622253418; L(Test): 0.29924890398979187\n",
            "Epoch 5982/10000: L(Train): 0.3247714340686798; L(Test): 0.2995128631591797\n",
            "Epoch 5983/10000: L(Train): 0.31962576508522034; L(Test): 0.2978450357913971\n",
            "Epoch 5984/10000: L(Train): 0.3190276026725769; L(Test): 0.2977459132671356\n",
            "Epoch 5985/10000: L(Train): 0.3089616000652313; L(Test): 0.29929834604263306\n",
            "Epoch 5986/10000: L(Train): 0.3287085294723511; L(Test): 0.29881545901298523\n",
            "Epoch 5987/10000: L(Train): 0.3327851891517639; L(Test): 0.29908692836761475\n",
            "Epoch 5988/10000: L(Train): 0.3265191316604614; L(Test): 0.29969748854637146\n",
            "Epoch 5989/10000: L(Train): 0.3212810456752777; L(Test): 0.2977156639099121\n",
            "Epoch 5990/10000: L(Train): 0.3163783550262451; L(Test): 0.297788143157959\n",
            "Epoch 5991/10000: L(Train): 0.3220457434654236; L(Test): 0.30112773180007935\n",
            "Epoch 5992/10000: L(Train): 0.3212577700614929; L(Test): 0.3008265495300293\n",
            "Epoch 5993/10000: L(Train): 0.31794747710227966; L(Test): 0.30023038387298584\n",
            "Epoch 5994/10000: L(Train): 0.3325502872467041; L(Test): 0.3010542392730713\n",
            "Epoch 5995/10000: L(Train): 0.32693612575531006; L(Test): 0.30203479528427124\n",
            "Epoch 5996/10000: L(Train): 0.33012473583221436; L(Test): 0.30227407813072205\n",
            "Epoch 5997/10000: L(Train): 0.31971094012260437; L(Test): 0.3005317449569702\n",
            "Epoch 5998/10000: L(Train): 0.31859132647514343; L(Test): 0.301196813583374\n",
            "Epoch 5999/10000: L(Train): 0.3193967938423157; L(Test): 0.3009795546531677\n",
            "Epoch 6000/10000: L(Train): 0.3141498863697052; L(Test): 0.29914551973342896\n",
            "Epoch 6001/10000: L(Train): 0.3250443935394287; L(Test): 0.29848864674568176\n",
            "Epoch 6002/10000: L(Train): 0.31662482023239136; L(Test): 0.29879242181777954\n",
            "Epoch 6003/10000: L(Train): 0.32015836238861084; L(Test): 0.30036839842796326\n",
            "Epoch 6004/10000: L(Train): 0.324817031621933; L(Test): 0.2999553084373474\n",
            "Epoch 6005/10000: L(Train): 0.32340681552886963; L(Test): 0.2986961603164673\n",
            "Epoch 6006/10000: L(Train): 0.3171282708644867; L(Test): 0.3005688488483429\n",
            "Epoch 6007/10000: L(Train): 0.3182893693447113; L(Test): 0.3003123104572296\n",
            "Epoch 6008/10000: L(Train): 0.3232184648513794; L(Test): 0.29919910430908203\n",
            "Epoch 6009/10000: L(Train): 0.33253633975982666; L(Test): 0.3036879003047943\n",
            "Epoch 6010/10000: L(Train): 0.32155323028564453; L(Test): 0.3046077787876129\n",
            "Epoch 6011/10000: L(Train): 0.3275747001171112; L(Test): 0.30145463347435\n",
            "Epoch 6012/10000: L(Train): 0.32690146565437317; L(Test): 0.3018237054347992\n",
            "Epoch 6013/10000: L(Train): 0.3212905824184418; L(Test): 0.3013794422149658\n",
            "Epoch 6014/10000: L(Train): 0.3230245113372803; L(Test): 0.3000994026660919\n",
            "Epoch 6015/10000: L(Train): 0.3258625864982605; L(Test): 0.30085882544517517\n",
            "Epoch 6016/10000: L(Train): 0.3292890191078186; L(Test): 0.3015035092830658\n",
            "Epoch 6017/10000: L(Train): 0.3263297975063324; L(Test): 0.3001873791217804\n",
            "Epoch 6018/10000: L(Train): 0.3269161880016327; L(Test): 0.3000450134277344\n",
            "Epoch 6019/10000: L(Train): 0.3306921124458313; L(Test): 0.30148351192474365\n",
            "Epoch 6020/10000: L(Train): 0.3180982768535614; L(Test): 0.29988357424736023\n",
            "Epoch 6021/10000: L(Train): 0.3168604373931885; L(Test): 0.2995762825012207\n",
            "Epoch 6022/10000: L(Train): 0.3263436555862427; L(Test): 0.29989558458328247\n",
            "Epoch 6023/10000: L(Train): 0.3329509496688843; L(Test): 0.30025342106819153\n",
            "Epoch 6024/10000: L(Train): 0.32868412137031555; L(Test): 0.30022135376930237\n",
            "Epoch 6025/10000: L(Train): 0.3240926265716553; L(Test): 0.2993619441986084\n",
            "Epoch 6026/10000: L(Train): 0.321069598197937; L(Test): 0.2991754710674286\n",
            "Epoch 6027/10000: L(Train): 0.31805434823036194; L(Test): 0.2993541955947876\n",
            "Epoch 6028/10000: L(Train): 0.3255721926689148; L(Test): 0.29803481698036194\n",
            "Epoch 6029/10000: L(Train): 0.3276386559009552; L(Test): 0.2977062463760376\n",
            "Epoch 6030/10000: L(Train): 0.31568005681037903; L(Test): 0.29789531230926514\n",
            "Epoch 6031/10000: L(Train): 0.3189387023448944; L(Test): 0.2978819012641907\n",
            "Epoch 6032/10000: L(Train): 0.3158074617385864; L(Test): 0.29803043603897095\n",
            "Epoch 6033/10000: L(Train): 0.3143843710422516; L(Test): 0.29731157422065735\n",
            "Epoch 6034/10000: L(Train): 0.32704928517341614; L(Test): 0.2969224154949188\n",
            "Epoch 6035/10000: L(Train): 0.32206064462661743; L(Test): 0.2977633774280548\n",
            "Epoch 6036/10000: L(Train): 0.317737340927124; L(Test): 0.2975054681301117\n",
            "Epoch 6037/10000: L(Train): 0.31778979301452637; L(Test): 0.2973209619522095\n",
            "Epoch 6038/10000: L(Train): 0.3254510760307312; L(Test): 0.298101544380188\n",
            "Epoch 6039/10000: L(Train): 0.3323568105697632; L(Test): 0.29792699217796326\n",
            "Epoch 6040/10000: L(Train): 0.3202059864997864; L(Test): 0.2974039912223816\n",
            "Epoch 6041/10000: L(Train): 0.3186376094818115; L(Test): 0.2969294488430023\n",
            "Epoch 6042/10000: L(Train): 0.327423095703125; L(Test): 0.29657477140426636\n",
            "Epoch 6043/10000: L(Train): 0.3208988606929779; L(Test): 0.2968006134033203\n",
            "Epoch 6044/10000: L(Train): 0.31748107075691223; L(Test): 0.29704782366752625\n",
            "Epoch 6045/10000: L(Train): 0.32021084427833557; L(Test): 0.2972947061061859\n",
            "Epoch 6046/10000: L(Train): 0.3269915282726288; L(Test): 0.29852941632270813\n",
            "Epoch 6047/10000: L(Train): 0.32282349467277527; L(Test): 0.29870137572288513\n",
            "Epoch 6048/10000: L(Train): 0.31691014766693115; L(Test): 0.2996864914894104\n",
            "Epoch 6049/10000: L(Train): 0.32350802421569824; L(Test): 0.30191555619239807\n",
            "Epoch 6050/10000: L(Train): 0.3190670609474182; L(Test): 0.3024013042449951\n",
            "Epoch 6051/10000: L(Train): 0.31887996196746826; L(Test): 0.3153897523880005\n",
            "Epoch 6052/10000: L(Train): 0.32728245854377747; L(Test): 0.3225368559360504\n",
            "Epoch 6053/10000: L(Train): 0.34973981976509094; L(Test): 0.322273313999176\n",
            "Epoch 6054/10000: L(Train): 0.33475759625434875; L(Test): 0.3176337778568268\n",
            "Epoch 6055/10000: L(Train): 0.3380192816257477; L(Test): 0.3114827871322632\n",
            "Epoch 6056/10000: L(Train): 0.3351060450077057; L(Test): 0.3122962415218353\n",
            "Epoch 6057/10000: L(Train): 0.33328011631965637; L(Test): 0.3167021870613098\n",
            "Epoch 6058/10000: L(Train): 0.33896324038505554; L(Test): 0.31857892870903015\n",
            "Epoch 6059/10000: L(Train): 0.34601831436157227; L(Test): 0.31597599387168884\n",
            "Epoch 6060/10000: L(Train): 0.34245777130126953; L(Test): 0.31289345026016235\n",
            "Epoch 6061/10000: L(Train): 0.33510512113571167; L(Test): 0.31176885962486267\n",
            "Epoch 6062/10000: L(Train): 0.3279593288898468; L(Test): 0.3130161464214325\n",
            "Epoch 6063/10000: L(Train): 0.3169979751110077; L(Test): 0.3139913082122803\n",
            "Epoch 6064/10000: L(Train): 0.33256998658180237; L(Test): 0.31286343932151794\n",
            "Epoch 6065/10000: L(Train): 0.3419288396835327; L(Test): 0.3104163408279419\n",
            "Epoch 6066/10000: L(Train): 0.32482847571372986; L(Test): 0.3096230924129486\n",
            "Epoch 6067/10000: L(Train): 0.3314911723136902; L(Test): 0.31089383363723755\n",
            "Epoch 6068/10000: L(Train): 0.33277782797813416; L(Test): 0.3118918538093567\n",
            "Epoch 6069/10000: L(Train): 0.3276645839214325; L(Test): 0.31084346771240234\n",
            "Epoch 6070/10000: L(Train): 0.32773226499557495; L(Test): 0.3095380663871765\n",
            "Epoch 6071/10000: L(Train): 0.33094504475593567; L(Test): 0.30798494815826416\n",
            "Epoch 6072/10000: L(Train): 0.33186718821525574; L(Test): 0.3070335388183594\n",
            "Epoch 6073/10000: L(Train): 0.3296295404434204; L(Test): 0.3078228831291199\n",
            "Epoch 6074/10000: L(Train): 0.3303598463535309; L(Test): 0.3066596984863281\n",
            "Epoch 6075/10000: L(Train): 0.32677844166755676; L(Test): 0.30463531613349915\n",
            "Epoch 6076/10000: L(Train): 0.33096766471862793; L(Test): 0.30349063873291016\n",
            "Epoch 6077/10000: L(Train): 0.3272521495819092; L(Test): 0.30301180481910706\n",
            "Epoch 6078/10000: L(Train): 0.3170605003833771; L(Test): 0.30278295278549194\n",
            "Epoch 6079/10000: L(Train): 0.3197563588619232; L(Test): 0.302635133266449\n",
            "Epoch 6080/10000: L(Train): 0.3260582685470581; L(Test): 0.3022392690181732\n",
            "Epoch 6081/10000: L(Train): 0.31711545586586; L(Test): 0.30171582102775574\n",
            "Epoch 6082/10000: L(Train): 0.3241581618785858; L(Test): 0.3005813658237457\n",
            "Epoch 6083/10000: L(Train): 0.32216745615005493; L(Test): 0.3000299036502838\n",
            "Epoch 6084/10000: L(Train): 0.32030484080314636; L(Test): 0.2995613217353821\n",
            "Epoch 6085/10000: L(Train): 0.32255128026008606; L(Test): 0.2992919981479645\n",
            "Epoch 6086/10000: L(Train): 0.32090163230895996; L(Test): 0.29851555824279785\n",
            "Epoch 6087/10000: L(Train): 0.3228219747543335; L(Test): 0.29796674847602844\n",
            "Epoch 6088/10000: L(Train): 0.3279062807559967; L(Test): 0.2982211410999298\n",
            "Epoch 6089/10000: L(Train): 0.3269530236721039; L(Test): 0.29925084114074707\n",
            "Epoch 6090/10000: L(Train): 0.3095409572124481; L(Test): 0.2992262840270996\n",
            "Epoch 6091/10000: L(Train): 0.3199382722377777; L(Test): 0.2984254062175751\n",
            "Epoch 6092/10000: L(Train): 0.31811583042144775; L(Test): 0.2976020574569702\n",
            "Epoch 6093/10000: L(Train): 0.32501673698425293; L(Test): 0.297224223613739\n",
            "Epoch 6094/10000: L(Train): 0.32043227553367615; L(Test): 0.2980538308620453\n",
            "Epoch 6095/10000: L(Train): 0.3209936022758484; L(Test): 0.298877477645874\n",
            "Epoch 6096/10000: L(Train): 0.3068976104259491; L(Test): 0.29868149757385254\n",
            "Epoch 6097/10000: L(Train): 0.3295488655567169; L(Test): 0.29763007164001465\n",
            "Epoch 6098/10000: L(Train): 0.32536596059799194; L(Test): 0.29779499769210815\n",
            "Epoch 6099/10000: L(Train): 0.32194411754608154; L(Test): 0.2987685799598694\n",
            "Epoch 6100/10000: L(Train): 0.32405099272727966; L(Test): 0.2966715693473816\n",
            "Epoch 6101/10000: L(Train): 0.3233676254749298; L(Test): 0.29703277349472046\n",
            "Epoch 6102/10000: L(Train): 0.3223760724067688; L(Test): 0.2989557981491089\n",
            "Epoch 6103/10000: L(Train): 0.32484495639801025; L(Test): 0.29666805267333984\n",
            "Epoch 6104/10000: L(Train): 0.3158332407474518; L(Test): 0.2961345911026001\n",
            "Epoch 6105/10000: L(Train): 0.30648842453956604; L(Test): 0.298640638589859\n",
            "Epoch 6106/10000: L(Train): 0.3169131875038147; L(Test): 0.29821065068244934\n",
            "Epoch 6107/10000: L(Train): 0.31868961453437805; L(Test): 0.297366738319397\n",
            "Epoch 6108/10000: L(Train): 0.31252896785736084; L(Test): 0.29735153913497925\n",
            "Epoch 6109/10000: L(Train): 0.3268216848373413; L(Test): 0.2962597906589508\n",
            "Epoch 6110/10000: L(Train): 0.3122253715991974; L(Test): 0.2968117296695709\n",
            "Epoch 6111/10000: L(Train): 0.3180901110172272; L(Test): 0.2963120937347412\n",
            "Epoch 6112/10000: L(Train): 0.32319533824920654; L(Test): 0.29670071601867676\n",
            "Epoch 6113/10000: L(Train): 0.3219830095767975; L(Test): 0.2976960241794586\n",
            "Epoch 6114/10000: L(Train): 0.33398815989494324; L(Test): 0.2967222332954407\n",
            "Epoch 6115/10000: L(Train): 0.31869807839393616; L(Test): 0.2952345311641693\n",
            "Epoch 6116/10000: L(Train): 0.3320838212966919; L(Test): 0.29554644227027893\n",
            "Epoch 6117/10000: L(Train): 0.3250250220298767; L(Test): 0.29558783769607544\n",
            "Epoch 6118/10000: L(Train): 0.31682872772216797; L(Test): 0.2958345413208008\n",
            "Epoch 6119/10000: L(Train): 0.314740389585495; L(Test): 0.29628056287765503\n",
            "Epoch 6120/10000: L(Train): 0.32611754536628723; L(Test): 0.2971435785293579\n",
            "Epoch 6121/10000: L(Train): 0.3142135441303253; L(Test): 0.2979949712753296\n",
            "Epoch 6122/10000: L(Train): 0.3202844262123108; L(Test): 0.2956122159957886\n",
            "Epoch 6123/10000: L(Train): 0.320655882358551; L(Test): 0.2960943579673767\n",
            "Epoch 6124/10000: L(Train): 0.32128891348838806; L(Test): 0.29625794291496277\n",
            "Epoch 6125/10000: L(Train): 0.3188798427581787; L(Test): 0.29596245288848877\n",
            "Epoch 6126/10000: L(Train): 0.32129591703414917; L(Test): 0.2958179712295532\n",
            "Epoch 6127/10000: L(Train): 0.3116435408592224; L(Test): 0.29515519738197327\n",
            "Epoch 6128/10000: L(Train): 0.32598379254341125; L(Test): 0.29522740840911865\n",
            "Epoch 6129/10000: L(Train): 0.31302016973495483; L(Test): 0.29493027925491333\n",
            "Epoch 6130/10000: L(Train): 0.3168611526489258; L(Test): 0.29503777623176575\n",
            "Epoch 6131/10000: L(Train): 0.32810837030410767; L(Test): 0.2961859405040741\n",
            "Epoch 6132/10000: L(Train): 0.3322598338127136; L(Test): 0.29684457182884216\n",
            "Epoch 6133/10000: L(Train): 0.3155651390552521; L(Test): 0.2952301502227783\n",
            "Epoch 6134/10000: L(Train): 0.32650884985923767; L(Test): 0.29540419578552246\n",
            "Epoch 6135/10000: L(Train): 0.324352502822876; L(Test): 0.29555973410606384\n",
            "Epoch 6136/10000: L(Train): 0.3165326714515686; L(Test): 0.29674309492111206\n",
            "Epoch 6137/10000: L(Train): 0.3252256512641907; L(Test): 0.2962650954723358\n",
            "Epoch 6138/10000: L(Train): 0.3192789852619171; L(Test): 0.2956245541572571\n",
            "Epoch 6139/10000: L(Train): 0.3205215632915497; L(Test): 0.2980421483516693\n",
            "Epoch 6140/10000: L(Train): 0.33119457960128784; L(Test): 0.2983637750148773\n",
            "Epoch 6141/10000: L(Train): 0.3177582919597626; L(Test): 0.2981763482093811\n",
            "Epoch 6142/10000: L(Train): 0.3308711349964142; L(Test): 0.3016456663608551\n",
            "Epoch 6143/10000: L(Train): 0.3279707133769989; L(Test): 0.3008435070514679\n",
            "Epoch 6144/10000: L(Train): 0.3352324962615967; L(Test): 0.29927077889442444\n",
            "Epoch 6145/10000: L(Train): 0.31969088315963745; L(Test): 0.3007059693336487\n",
            "Epoch 6146/10000: L(Train): 0.3278789520263672; L(Test): 0.30023202300071716\n",
            "Epoch 6147/10000: L(Train): 0.33111223578453064; L(Test): 0.29955926537513733\n",
            "Epoch 6148/10000: L(Train): 0.32793691754341125; L(Test): 0.3008599877357483\n",
            "Epoch 6149/10000: L(Train): 0.32625052332878113; L(Test): 0.29941877722740173\n",
            "Epoch 6150/10000: L(Train): 0.32656335830688477; L(Test): 0.2996968924999237\n",
            "Epoch 6151/10000: L(Train): 0.3214292526245117; L(Test): 0.3001401722431183\n",
            "Epoch 6152/10000: L(Train): 0.32723724842071533; L(Test): 0.30038946866989136\n",
            "Epoch 6153/10000: L(Train): 0.3214847147464752; L(Test): 0.2987247705459595\n",
            "Epoch 6154/10000: L(Train): 0.32342007756233215; L(Test): 0.29738107323646545\n",
            "Epoch 6155/10000: L(Train): 0.3193014860153198; L(Test): 0.2987048923969269\n",
            "Epoch 6156/10000: L(Train): 0.32957735657691956; L(Test): 0.2990153133869171\n",
            "Epoch 6157/10000: L(Train): 0.3270498812198639; L(Test): 0.29840943217277527\n",
            "Epoch 6158/10000: L(Train): 0.3258686661720276; L(Test): 0.3015209138393402\n",
            "Epoch 6159/10000: L(Train): 0.3214309513568878; L(Test): 0.3050156533718109\n",
            "Epoch 6160/10000: L(Train): 0.31553611159324646; L(Test): 0.3038720488548279\n",
            "Epoch 6161/10000: L(Train): 0.3300444185733795; L(Test): 0.3007221817970276\n",
            "Epoch 6162/10000: L(Train): 0.3267388641834259; L(Test): 0.30210235714912415\n",
            "Epoch 6163/10000: L(Train): 0.32332929968833923; L(Test): 0.30309823155403137\n",
            "Epoch 6164/10000: L(Train): 0.3297891914844513; L(Test): 0.30126044154167175\n",
            "Epoch 6165/10000: L(Train): 0.32814499735832214; L(Test): 0.30337101221084595\n",
            "Epoch 6166/10000: L(Train): 0.3328549563884735; L(Test): 0.30562782287597656\n",
            "Epoch 6167/10000: L(Train): 0.32347533106803894; L(Test): 0.3061816096305847\n",
            "Epoch 6168/10000: L(Train): 0.3267069458961487; L(Test): 0.3047387897968292\n",
            "Epoch 6169/10000: L(Train): 0.3176822066307068; L(Test): 0.3015018105506897\n",
            "Epoch 6170/10000: L(Train): 0.3211268484592438; L(Test): 0.3016873300075531\n",
            "Epoch 6171/10000: L(Train): 0.3215296268463135; L(Test): 0.3029817044734955\n",
            "Epoch 6172/10000: L(Train): 0.3316454589366913; L(Test): 0.3005504012107849\n",
            "Epoch 6173/10000: L(Train): 0.3165533244609833; L(Test): 0.3014468550682068\n",
            "Epoch 6174/10000: L(Train): 0.32316911220550537; L(Test): 0.3035854399204254\n",
            "Epoch 6175/10000: L(Train): 0.3258250057697296; L(Test): 0.30191874504089355\n",
            "Epoch 6176/10000: L(Train): 0.32014137506484985; L(Test): 0.30035683512687683\n",
            "Epoch 6177/10000: L(Train): 0.32300594449043274; L(Test): 0.30020007491111755\n",
            "Epoch 6178/10000: L(Train): 0.3261526823043823; L(Test): 0.3002474904060364\n",
            "Epoch 6179/10000: L(Train): 0.3280978500843048; L(Test): 0.30006030201911926\n",
            "Epoch 6180/10000: L(Train): 0.32348495721817017; L(Test): 0.30028021335601807\n",
            "Epoch 6181/10000: L(Train): 0.336047887802124; L(Test): 0.2992858290672302\n",
            "Epoch 6182/10000: L(Train): 0.3321309983730316; L(Test): 0.2992815673351288\n",
            "Epoch 6183/10000: L(Train): 0.3290858268737793; L(Test): 0.29926109313964844\n",
            "Epoch 6184/10000: L(Train): 0.3193715810775757; L(Test): 0.29945382475852966\n",
            "Epoch 6185/10000: L(Train): 0.3231028616428375; L(Test): 0.2992396354675293\n",
            "Epoch 6186/10000: L(Train): 0.3296179175376892; L(Test): 0.2979123890399933\n",
            "Epoch 6187/10000: L(Train): 0.3340977132320404; L(Test): 0.29824936389923096\n",
            "Epoch 6188/10000: L(Train): 0.3190222978591919; L(Test): 0.2985261082649231\n",
            "Epoch 6189/10000: L(Train): 0.32615214586257935; L(Test): 0.29782727360725403\n",
            "Epoch 6190/10000: L(Train): 0.3116168677806854; L(Test): 0.2982162535190582\n",
            "Epoch 6191/10000: L(Train): 0.32028070092201233; L(Test): 0.2978626489639282\n",
            "Epoch 6192/10000: L(Train): 0.3243457078933716; L(Test): 0.2973005473613739\n",
            "Epoch 6193/10000: L(Train): 0.31894153356552124; L(Test): 0.29711854457855225\n",
            "Epoch 6194/10000: L(Train): 0.31669890880584717; L(Test): 0.2971429228782654\n",
            "Epoch 6195/10000: L(Train): 0.32172060012817383; L(Test): 0.2967904508113861\n",
            "Epoch 6196/10000: L(Train): 0.3192920684814453; L(Test): 0.2966381907463074\n",
            "Epoch 6197/10000: L(Train): 0.31878340244293213; L(Test): 0.2967299818992615\n",
            "Epoch 6198/10000: L(Train): 0.30949631333351135; L(Test): 0.2966296374797821\n",
            "Epoch 6199/10000: L(Train): 0.3199249804019928; L(Test): 0.29686832427978516\n",
            "Epoch 6200/10000: L(Train): 0.3111571967601776; L(Test): 0.2967483699321747\n",
            "Epoch 6201/10000: L(Train): 0.32184699177742004; L(Test): 0.2966494858264923\n",
            "Epoch 6202/10000: L(Train): 0.3201290965080261; L(Test): 0.29590120911598206\n",
            "Epoch 6203/10000: L(Train): 0.3277042806148529; L(Test): 0.2952474057674408\n",
            "Epoch 6204/10000: L(Train): 0.32076770067214966; L(Test): 0.29569077491760254\n",
            "Epoch 6205/10000: L(Train): 0.3212619721889496; L(Test): 0.2956444025039673\n",
            "Epoch 6206/10000: L(Train): 0.3221457004547119; L(Test): 0.2950815260410309\n",
            "Epoch 6207/10000: L(Train): 0.31821736693382263; L(Test): 0.2950535714626312\n",
            "Epoch 6208/10000: L(Train): 0.31991368532180786; L(Test): 0.29499414563179016\n",
            "Epoch 6209/10000: L(Train): 0.3210292458534241; L(Test): 0.2954188883304596\n",
            "Epoch 6210/10000: L(Train): 0.32291319966316223; L(Test): 0.29718905687332153\n",
            "Epoch 6211/10000: L(Train): 0.3195069432258606; L(Test): 0.29618701338768005\n",
            "Epoch 6212/10000: L(Train): 0.3141725957393646; L(Test): 0.29482749104499817\n",
            "Epoch 6213/10000: L(Train): 0.3274160921573639; L(Test): 0.2949860692024231\n",
            "Epoch 6214/10000: L(Train): 0.32167550921440125; L(Test): 0.29490232467651367\n",
            "Epoch 6215/10000: L(Train): 0.3233836591243744; L(Test): 0.294943243265152\n",
            "Epoch 6216/10000: L(Train): 0.32344356179237366; L(Test): 0.2956635057926178\n",
            "Epoch 6217/10000: L(Train): 0.3182612955570221; L(Test): 0.29577386379241943\n",
            "Epoch 6218/10000: L(Train): 0.32741501927375793; L(Test): 0.29457712173461914\n",
            "Epoch 6219/10000: L(Train): 0.31947803497314453; L(Test): 0.29569002985954285\n",
            "Epoch 6220/10000: L(Train): 0.3270758390426636; L(Test): 0.2967533469200134\n",
            "Epoch 6221/10000: L(Train): 0.3108902871608734; L(Test): 0.2964189648628235\n",
            "Epoch 6222/10000: L(Train): 0.3218117952346802; L(Test): 0.29668763279914856\n",
            "Epoch 6223/10000: L(Train): 0.31927207112312317; L(Test): 0.29631251096725464\n",
            "Epoch 6224/10000: L(Train): 0.31509077548980713; L(Test): 0.29634392261505127\n",
            "Epoch 6225/10000: L(Train): 0.31440892815589905; L(Test): 0.2962450683116913\n",
            "Epoch 6226/10000: L(Train): 0.31311026215553284; L(Test): 0.2957942485809326\n",
            "Epoch 6227/10000: L(Train): 0.3243330419063568; L(Test): 0.2954646944999695\n",
            "Epoch 6228/10000: L(Train): 0.31490087509155273; L(Test): 0.2955644428730011\n",
            "Epoch 6229/10000: L(Train): 0.3257315158843994; L(Test): 0.2955775558948517\n",
            "Epoch 6230/10000: L(Train): 0.30864569544792175; L(Test): 0.2960983216762543\n",
            "Epoch 6231/10000: L(Train): 0.31556403636932373; L(Test): 0.295757532119751\n",
            "Epoch 6232/10000: L(Train): 0.3196733891963959; L(Test): 0.29493117332458496\n",
            "Epoch 6233/10000: L(Train): 0.31476494669914246; L(Test): 0.2966884672641754\n",
            "Epoch 6234/10000: L(Train): 0.32018348574638367; L(Test): 0.2973022758960724\n",
            "Epoch 6235/10000: L(Train): 0.32317906618118286; L(Test): 0.2962765395641327\n",
            "Epoch 6236/10000: L(Train): 0.32496994733810425; L(Test): 0.29646795988082886\n",
            "Epoch 6237/10000: L(Train): 0.31721433997154236; L(Test): 0.29708555340766907\n",
            "Epoch 6238/10000: L(Train): 0.3252241313457489; L(Test): 0.2957136929035187\n",
            "Epoch 6239/10000: L(Train): 0.32135486602783203; L(Test): 0.2961561381816864\n",
            "Epoch 6240/10000: L(Train): 0.31207796931266785; L(Test): 0.2988987863063812\n",
            "Epoch 6241/10000: L(Train): 0.3131003975868225; L(Test): 0.298348993062973\n",
            "Epoch 6242/10000: L(Train): 0.312649130821228; L(Test): 0.2974322736263275\n",
            "Epoch 6243/10000: L(Train): 0.3144344985485077; L(Test): 0.29699066281318665\n",
            "Epoch 6244/10000: L(Train): 0.31540778279304504; L(Test): 0.297139048576355\n",
            "Epoch 6245/10000: L(Train): 0.3210494816303253; L(Test): 0.2970205843448639\n",
            "Epoch 6246/10000: L(Train): 0.32272788882255554; L(Test): 0.29716387391090393\n",
            "Epoch 6247/10000: L(Train): 0.32632482051849365; L(Test): 0.29806777834892273\n",
            "Epoch 6248/10000: L(Train): 0.3270905613899231; L(Test): 0.2991998493671417\n",
            "Epoch 6249/10000: L(Train): 0.32772403955459595; L(Test): 0.29809901118278503\n",
            "Epoch 6250/10000: L(Train): 0.32446378469467163; L(Test): 0.2986885905265808\n",
            "Epoch 6251/10000: L(Train): 0.313458651304245; L(Test): 0.2998650372028351\n",
            "Epoch 6252/10000: L(Train): 0.3222649097442627; L(Test): 0.29843631386756897\n",
            "Epoch 6253/10000: L(Train): 0.33089679479599; L(Test): 0.299337774515152\n",
            "Epoch 6254/10000: L(Train): 0.3226989507675171; L(Test): 0.3004574477672577\n",
            "Epoch 6255/10000: L(Train): 0.3222167491912842; L(Test): 0.2984529435634613\n",
            "Epoch 6256/10000: L(Train): 0.32600322365760803; L(Test): 0.30138862133026123\n",
            "Epoch 6257/10000: L(Train): 0.3303641080856323; L(Test): 0.30302494764328003\n",
            "Epoch 6258/10000: L(Train): 0.3215368092060089; L(Test): 0.30007123947143555\n",
            "Epoch 6259/10000: L(Train): 0.3229653537273407; L(Test): 0.3030226528644562\n",
            "Epoch 6260/10000: L(Train): 0.3169587254524231; L(Test): 0.3035465478897095\n",
            "Epoch 6261/10000: L(Train): 0.3322150409221649; L(Test): 0.30161207914352417\n",
            "Epoch 6262/10000: L(Train): 0.31713443994522095; L(Test): 0.30635711550712585\n",
            "Epoch 6263/10000: L(Train): 0.324612021446228; L(Test): 0.30525654554367065\n",
            "Epoch 6264/10000: L(Train): 0.3403069078922272; L(Test): 0.30039340257644653\n",
            "Epoch 6265/10000: L(Train): 0.33465468883514404; L(Test): 0.3024880290031433\n",
            "Epoch 6266/10000: L(Train): 0.3264118731021881; L(Test): 0.30545714497566223\n",
            "Epoch 6267/10000: L(Train): 0.33790847659111023; L(Test): 0.30181774497032166\n",
            "Epoch 6268/10000: L(Train): 0.32389676570892334; L(Test): 0.30113837122917175\n",
            "Epoch 6269/10000: L(Train): 0.32203665375709534; L(Test): 0.3045879900455475\n",
            "Epoch 6270/10000: L(Train): 0.3315444886684418; L(Test): 0.30209487676620483\n",
            "Epoch 6271/10000: L(Train): 0.3279576599597931; L(Test): 0.3001815676689148\n",
            "Epoch 6272/10000: L(Train): 0.3232904076576233; L(Test): 0.3007603883743286\n",
            "Epoch 6273/10000: L(Train): 0.32102617621421814; L(Test): 0.30057382583618164\n",
            "Epoch 6274/10000: L(Train): 0.3134518563747406; L(Test): 0.3003019690513611\n",
            "Epoch 6275/10000: L(Train): 0.31617170572280884; L(Test): 0.3017204999923706\n",
            "Epoch 6276/10000: L(Train): 0.3248409628868103; L(Test): 0.2989359498023987\n",
            "Epoch 6277/10000: L(Train): 0.31689026951789856; L(Test): 0.299084335565567\n",
            "Epoch 6278/10000: L(Train): 0.3231412470340729; L(Test): 0.2989766597747803\n",
            "Epoch 6279/10000: L(Train): 0.3174594044685364; L(Test): 0.29741016030311584\n",
            "Epoch 6280/10000: L(Train): 0.3261130452156067; L(Test): 0.2971128523349762\n",
            "Epoch 6281/10000: L(Train): 0.31948134303092957; L(Test): 0.299124538898468\n",
            "Epoch 6282/10000: L(Train): 0.31752127408981323; L(Test): 0.2983514070510864\n",
            "Epoch 6283/10000: L(Train): 0.3262636661529541; L(Test): 0.2970394194126129\n",
            "Epoch 6284/10000: L(Train): 0.31856563687324524; L(Test): 0.2968810200691223\n",
            "Epoch 6285/10000: L(Train): 0.3249203860759735; L(Test): 0.2971824109554291\n",
            "Epoch 6286/10000: L(Train): 0.33140915632247925; L(Test): 0.29601600766181946\n",
            "Epoch 6287/10000: L(Train): 0.3239418566226959; L(Test): 0.29493069648742676\n",
            "Epoch 6288/10000: L(Train): 0.31648603081703186; L(Test): 0.2983657121658325\n",
            "Epoch 6289/10000: L(Train): 0.3100030720233917; L(Test): 0.29711851477622986\n",
            "Epoch 6290/10000: L(Train): 0.32692861557006836; L(Test): 0.29525721073150635\n",
            "Epoch 6291/10000: L(Train): 0.30697914958000183; L(Test): 0.29669225215911865\n",
            "Epoch 6292/10000: L(Train): 0.31716179847717285; L(Test): 0.2969363331794739\n",
            "Epoch 6293/10000: L(Train): 0.3184105455875397; L(Test): 0.29689261317253113\n",
            "Epoch 6294/10000: L(Train): 0.31858330965042114; L(Test): 0.2979450523853302\n",
            "Epoch 6295/10000: L(Train): 0.3256531357765198; L(Test): 0.29758593440055847\n",
            "Epoch 6296/10000: L(Train): 0.3198690116405487; L(Test): 0.29668474197387695\n",
            "Epoch 6297/10000: L(Train): 0.31235939264297485; L(Test): 0.2966461777687073\n",
            "Epoch 6298/10000: L(Train): 0.3207700550556183; L(Test): 0.29547318816185\n",
            "Epoch 6299/10000: L(Train): 0.31664037704467773; L(Test): 0.2968123257160187\n",
            "Epoch 6300/10000: L(Train): 0.32303547859191895; L(Test): 0.29835838079452515\n",
            "Epoch 6301/10000: L(Train): 0.31295183300971985; L(Test): 0.29657769203186035\n",
            "Epoch 6302/10000: L(Train): 0.3234354257583618; L(Test): 0.2956025004386902\n",
            "Epoch 6303/10000: L(Train): 0.31892329454421997; L(Test): 0.29751819372177124\n",
            "Epoch 6304/10000: L(Train): 0.33119410276412964; L(Test): 0.2964669167995453\n",
            "Epoch 6305/10000: L(Train): 0.3241181969642639; L(Test): 0.29677054286003113\n",
            "Epoch 6306/10000: L(Train): 0.3279403746128082; L(Test): 0.2986874282360077\n",
            "Epoch 6307/10000: L(Train): 0.32249945402145386; L(Test): 0.2977461814880371\n",
            "Epoch 6308/10000: L(Train): 0.3217736780643463; L(Test): 0.29675939679145813\n",
            "Epoch 6309/10000: L(Train): 0.3247027099132538; L(Test): 0.29694363474845886\n",
            "Epoch 6310/10000: L(Train): 0.31359052658081055; L(Test): 0.2972714602947235\n",
            "Epoch 6311/10000: L(Train): 0.32580462098121643; L(Test): 0.2965796887874603\n",
            "Epoch 6312/10000: L(Train): 0.3249935209751129; L(Test): 0.2985351085662842\n",
            "Epoch 6313/10000: L(Train): 0.31496599316596985; L(Test): 0.29720932245254517\n",
            "Epoch 6314/10000: L(Train): 0.32244232296943665; L(Test): 0.29628586769104004\n",
            "Epoch 6315/10000: L(Train): 0.3294315040111542; L(Test): 0.29598015546798706\n",
            "Epoch 6316/10000: L(Train): 0.33384278416633606; L(Test): 0.2960861921310425\n",
            "Epoch 6317/10000: L(Train): 0.3170028030872345; L(Test): 0.29670724272727966\n",
            "Epoch 6318/10000: L(Train): 0.32788801193237305; L(Test): 0.2971482276916504\n",
            "Epoch 6319/10000: L(Train): 0.324960321187973; L(Test): 0.2967228889465332\n",
            "Epoch 6320/10000: L(Train): 0.31462374329566956; L(Test): 0.29628074169158936\n",
            "Epoch 6321/10000: L(Train): 0.3154333829879761; L(Test): 0.29734668135643005\n",
            "Epoch 6322/10000: L(Train): 0.32463890314102173; L(Test): 0.2956847548484802\n",
            "Epoch 6323/10000: L(Train): 0.3166184425354004; L(Test): 0.2952960133552551\n",
            "Epoch 6324/10000: L(Train): 0.32061636447906494; L(Test): 0.29719606041908264\n",
            "Epoch 6325/10000: L(Train): 0.3239961564540863; L(Test): 0.2966015934944153\n",
            "Epoch 6326/10000: L(Train): 0.32077085971832275; L(Test): 0.29525092244148254\n",
            "Epoch 6327/10000: L(Train): 0.3294781744480133; L(Test): 0.2947070300579071\n",
            "Epoch 6328/10000: L(Train): 0.31635215878486633; L(Test): 0.2955307960510254\n",
            "Epoch 6329/10000: L(Train): 0.3278709948062897; L(Test): 0.2955838441848755\n",
            "Epoch 6330/10000: L(Train): 0.3220590353012085; L(Test): 0.2957180440425873\n",
            "Epoch 6331/10000: L(Train): 0.31737205386161804; L(Test): 0.2965618073940277\n",
            "Epoch 6332/10000: L(Train): 0.32971426844596863; L(Test): 0.2964092195034027\n",
            "Epoch 6333/10000: L(Train): 0.3201129138469696; L(Test): 0.29635950922966003\n",
            "Epoch 6334/10000: L(Train): 0.31995368003845215; L(Test): 0.2977201044559479\n",
            "Epoch 6335/10000: L(Train): 0.313443660736084; L(Test): 0.2977181077003479\n",
            "Epoch 6336/10000: L(Train): 0.31479477882385254; L(Test): 0.29616454243659973\n",
            "Epoch 6337/10000: L(Train): 0.3182756304740906; L(Test): 0.29661327600479126\n",
            "Epoch 6338/10000: L(Train): 0.3221568465232849; L(Test): 0.29690802097320557\n",
            "Epoch 6339/10000: L(Train): 0.313231498003006; L(Test): 0.2961924076080322\n",
            "Epoch 6340/10000: L(Train): 0.32191139459609985; L(Test): 0.29561060667037964\n",
            "Epoch 6341/10000: L(Train): 0.3132990598678589; L(Test): 0.2956337332725525\n",
            "Epoch 6342/10000: L(Train): 0.31808722019195557; L(Test): 0.2956148087978363\n",
            "Epoch 6343/10000: L(Train): 0.33327582478523254; L(Test): 0.29531896114349365\n",
            "Epoch 6344/10000: L(Train): 0.31800585985183716; L(Test): 0.29600265622138977\n",
            "Epoch 6345/10000: L(Train): 0.325243204832077; L(Test): 0.2955031991004944\n",
            "Epoch 6346/10000: L(Train): 0.30825287103652954; L(Test): 0.29500091075897217\n",
            "Epoch 6347/10000: L(Train): 0.31400543451309204; L(Test): 0.29530370235443115\n",
            "Epoch 6348/10000: L(Train): 0.31492048501968384; L(Test): 0.295682817697525\n",
            "Epoch 6349/10000: L(Train): 0.3271680176258087; L(Test): 0.2957615852355957\n",
            "Epoch 6350/10000: L(Train): 0.31971025466918945; L(Test): 0.29473957419395447\n",
            "Epoch 6351/10000: L(Train): 0.3222413957118988; L(Test): 0.29627710580825806\n",
            "Epoch 6352/10000: L(Train): 0.3221639394760132; L(Test): 0.29619380831718445\n",
            "Epoch 6353/10000: L(Train): 0.3269340395927429; L(Test): 0.29506075382232666\n",
            "Epoch 6354/10000: L(Train): 0.31062236428260803; L(Test): 0.2952103614807129\n",
            "Epoch 6355/10000: L(Train): 0.3215380012989044; L(Test): 0.2958821952342987\n",
            "Epoch 6356/10000: L(Train): 0.3265881836414337; L(Test): 0.29541096091270447\n",
            "Epoch 6357/10000: L(Train): 0.32747921347618103; L(Test): 0.29503798484802246\n",
            "Epoch 6358/10000: L(Train): 0.3217112123966217; L(Test): 0.29599663615226746\n",
            "Epoch 6359/10000: L(Train): 0.31880897283554077; L(Test): 0.2979259192943573\n",
            "Epoch 6360/10000: L(Train): 0.32121285796165466; L(Test): 0.29727840423583984\n",
            "Epoch 6361/10000: L(Train): 0.3263290524482727; L(Test): 0.2965802252292633\n",
            "Epoch 6362/10000: L(Train): 0.3248051106929779; L(Test): 0.29836350679397583\n",
            "Epoch 6363/10000: L(Train): 0.31528738141059875; L(Test): 0.2987821698188782\n",
            "Epoch 6364/10000: L(Train): 0.32730820775032043; L(Test): 0.29683172702789307\n",
            "Epoch 6365/10000: L(Train): 0.3149576485157013; L(Test): 0.2986731231212616\n",
            "Epoch 6366/10000: L(Train): 0.3205777406692505; L(Test): 0.29761090874671936\n",
            "Epoch 6367/10000: L(Train): 0.31731507182121277; L(Test): 0.29582810401916504\n",
            "Epoch 6368/10000: L(Train): 0.3313656151294708; L(Test): 0.2981152832508087\n",
            "Epoch 6369/10000: L(Train): 0.3329813480377197; L(Test): 0.29795876145362854\n",
            "Epoch 6370/10000: L(Train): 0.32680630683898926; L(Test): 0.2972542643547058\n",
            "Epoch 6371/10000: L(Train): 0.31403055787086487; L(Test): 0.29984527826309204\n",
            "Epoch 6372/10000: L(Train): 0.3258225917816162; L(Test): 0.29971811175346375\n",
            "Epoch 6373/10000: L(Train): 0.3254801034927368; L(Test): 0.29693537950515747\n",
            "Epoch 6374/10000: L(Train): 0.3211616575717926; L(Test): 0.29793715476989746\n",
            "Epoch 6375/10000: L(Train): 0.3067859709262848; L(Test): 0.29861751198768616\n",
            "Epoch 6376/10000: L(Train): 0.32365718483924866; L(Test): 0.3008778989315033\n",
            "Epoch 6377/10000: L(Train): 0.31611424684524536; L(Test): 0.29949405789375305\n",
            "Epoch 6378/10000: L(Train): 0.3265264928340912; L(Test): 0.2975470721721649\n",
            "Epoch 6379/10000: L(Train): 0.31796836853027344; L(Test): 0.29902395606040955\n",
            "Epoch 6380/10000: L(Train): 0.31245607137680054; L(Test): 0.29761967062950134\n",
            "Epoch 6381/10000: L(Train): 0.3247312903404236; L(Test): 0.2965541183948517\n",
            "Epoch 6382/10000: L(Train): 0.31425732374191284; L(Test): 0.299789696931839\n",
            "Epoch 6383/10000: L(Train): 0.3244255483150482; L(Test): 0.29867440462112427\n",
            "Epoch 6384/10000: L(Train): 0.314054936170578; L(Test): 0.2963636815547943\n",
            "Epoch 6385/10000: L(Train): 0.3176800310611725; L(Test): 0.29797637462615967\n",
            "Epoch 6386/10000: L(Train): 0.32664966583251953; L(Test): 0.2989135682582855\n",
            "Epoch 6387/10000: L(Train): 0.3234540522098541; L(Test): 0.29701170325279236\n",
            "Epoch 6388/10000: L(Train): 0.31586524844169617; L(Test): 0.29910415410995483\n",
            "Epoch 6389/10000: L(Train): 0.3166458308696747; L(Test): 0.298502117395401\n",
            "Epoch 6390/10000: L(Train): 0.3236161470413208; L(Test): 0.2957419157028198\n",
            "Epoch 6391/10000: L(Train): 0.3232710659503937; L(Test): 0.2972010672092438\n",
            "Epoch 6392/10000: L(Train): 0.3187982141971588; L(Test): 0.2981342077255249\n",
            "Epoch 6393/10000: L(Train): 0.3092885911464691; L(Test): 0.29621797800064087\n",
            "Epoch 6394/10000: L(Train): 0.3147828280925751; L(Test): 0.2959752380847931\n",
            "Epoch 6395/10000: L(Train): 0.32477006316185; L(Test): 0.29676517844200134\n",
            "Epoch 6396/10000: L(Train): 0.32993313670158386; L(Test): 0.297390878200531\n",
            "Epoch 6397/10000: L(Train): 0.31510281562805176; L(Test): 0.2972356975078583\n",
            "Epoch 6398/10000: L(Train): 0.3281339406967163; L(Test): 0.29600095748901367\n",
            "Epoch 6399/10000: L(Train): 0.3215084671974182; L(Test): 0.29647648334503174\n",
            "Epoch 6400/10000: L(Train): 0.31643766164779663; L(Test): 0.2992136478424072\n",
            "Epoch 6401/10000: L(Train): 0.3302938938140869; L(Test): 0.30043545365333557\n",
            "Epoch 6402/10000: L(Train): 0.33222153782844543; L(Test): 0.29782235622406006\n",
            "Epoch 6403/10000: L(Train): 0.31857553124427795; L(Test): 0.29715582728385925\n",
            "Epoch 6404/10000: L(Train): 0.32738593220710754; L(Test): 0.29847195744514465\n",
            "Epoch 6405/10000: L(Train): 0.3410612642765045; L(Test): 0.2989010512828827\n",
            "Epoch 6406/10000: L(Train): 0.3158136010169983; L(Test): 0.2981603741645813\n",
            "Epoch 6407/10000: L(Train): 0.3140062689781189; L(Test): 0.2987052798271179\n",
            "Epoch 6408/10000: L(Train): 0.32605159282684326; L(Test): 0.2979162633419037\n",
            "Epoch 6409/10000: L(Train): 0.32106688618659973; L(Test): 0.2973082363605499\n",
            "Epoch 6410/10000: L(Train): 0.32048919796943665; L(Test): 0.298084020614624\n",
            "Epoch 6411/10000: L(Train): 0.32419663667678833; L(Test): 0.2978690564632416\n",
            "Epoch 6412/10000: L(Train): 0.32685959339141846; L(Test): 0.2968568801879883\n",
            "Epoch 6413/10000: L(Train): 0.31920453906059265; L(Test): 0.29728010296821594\n",
            "Epoch 6414/10000: L(Train): 0.31233835220336914; L(Test): 0.29830870032310486\n",
            "Epoch 6415/10000: L(Train): 0.32510560750961304; L(Test): 0.29857322573661804\n",
            "Epoch 6416/10000: L(Train): 0.318630576133728; L(Test): 0.29736563563346863\n",
            "Epoch 6417/10000: L(Train): 0.3228267431259155; L(Test): 0.29771673679351807\n",
            "Epoch 6418/10000: L(Train): 0.32344916462898254; L(Test): 0.299198716878891\n",
            "Epoch 6419/10000: L(Train): 0.32283174991607666; L(Test): 0.29936376214027405\n",
            "Epoch 6420/10000: L(Train): 0.32756537199020386; L(Test): 0.298835813999176\n",
            "Epoch 6421/10000: L(Train): 0.32433944940567017; L(Test): 0.2975049912929535\n",
            "Epoch 6422/10000: L(Train): 0.3161548376083374; L(Test): 0.2964821457862854\n",
            "Epoch 6423/10000: L(Train): 0.31501448154449463; L(Test): 0.29572272300720215\n",
            "Epoch 6424/10000: L(Train): 0.3195033371448517; L(Test): 0.29553335905075073\n",
            "Epoch 6425/10000: L(Train): 0.32279595732688904; L(Test): 0.29600489139556885\n",
            "Epoch 6426/10000: L(Train): 0.31859123706817627; L(Test): 0.2962282598018646\n",
            "Epoch 6427/10000: L(Train): 0.3299373686313629; L(Test): 0.29709169268608093\n",
            "Epoch 6428/10000: L(Train): 0.32172343134880066; L(Test): 0.2971096634864807\n",
            "Epoch 6429/10000: L(Train): 0.32167181372642517; L(Test): 0.2954998314380646\n",
            "Epoch 6430/10000: L(Train): 0.31678253412246704; L(Test): 0.295667827129364\n",
            "Epoch 6431/10000: L(Train): 0.3126140832901001; L(Test): 0.29583001136779785\n",
            "Epoch 6432/10000: L(Train): 0.32256001234054565; L(Test): 0.2962425947189331\n",
            "Epoch 6433/10000: L(Train): 0.3273654282093048; L(Test): 0.2969619929790497\n",
            "Epoch 6434/10000: L(Train): 0.3149034082889557; L(Test): 0.29635173082351685\n",
            "Epoch 6435/10000: L(Train): 0.32121792435646057; L(Test): 0.29581454396247864\n",
            "Epoch 6436/10000: L(Train): 0.3182470500469208; L(Test): 0.29550135135650635\n",
            "Epoch 6437/10000: L(Train): 0.3197150230407715; L(Test): 0.2954135537147522\n",
            "Epoch 6438/10000: L(Train): 0.3175657391548157; L(Test): 0.2943927049636841\n",
            "Epoch 6439/10000: L(Train): 0.32076072692871094; L(Test): 0.29518428444862366\n",
            "Epoch 6440/10000: L(Train): 0.32597631216049194; L(Test): 0.2949981093406677\n",
            "Epoch 6441/10000: L(Train): 0.3155048191547394; L(Test): 0.29548028111457825\n",
            "Epoch 6442/10000: L(Train): 0.3139801025390625; L(Test): 0.29632461071014404\n",
            "Epoch 6443/10000: L(Train): 0.3264332711696625; L(Test): 0.29659155011177063\n",
            "Epoch 6444/10000: L(Train): 0.3268727660179138; L(Test): 0.29537785053253174\n",
            "Epoch 6445/10000: L(Train): 0.32302045822143555; L(Test): 0.2957938611507416\n",
            "Epoch 6446/10000: L(Train): 0.3246035873889923; L(Test): 0.2946676015853882\n",
            "Epoch 6447/10000: L(Train): 0.3180549144744873; L(Test): 0.29519423842430115\n",
            "Epoch 6448/10000: L(Train): 0.32491907477378845; L(Test): 0.29594770073890686\n",
            "Epoch 6449/10000: L(Train): 0.3209015429019928; L(Test): 0.2965225577354431\n",
            "Epoch 6450/10000: L(Train): 0.32868683338165283; L(Test): 0.296915203332901\n",
            "Epoch 6451/10000: L(Train): 0.32586485147476196; L(Test): 0.29658105969429016\n",
            "Epoch 6452/10000: L(Train): 0.31386175751686096; L(Test): 0.2971341013908386\n",
            "Epoch 6453/10000: L(Train): 0.32757431268692017; L(Test): 0.2967817187309265\n",
            "Epoch 6454/10000: L(Train): 0.32346847653388977; L(Test): 0.2963823676109314\n",
            "Epoch 6455/10000: L(Train): 0.32742398977279663; L(Test): 0.29658350348472595\n",
            "Epoch 6456/10000: L(Train): 0.3254733383655548; L(Test): 0.29800158739089966\n",
            "Epoch 6457/10000: L(Train): 0.31856316328048706; L(Test): 0.2983757555484772\n",
            "Epoch 6458/10000: L(Train): 0.32924017310142517; L(Test): 0.29682689905166626\n",
            "Epoch 6459/10000: L(Train): 0.3322278559207916; L(Test): 0.29718101024627686\n",
            "Epoch 6460/10000: L(Train): 0.3249832093715668; L(Test): 0.29808875918388367\n",
            "Epoch 6461/10000: L(Train): 0.30927637219429016; L(Test): 0.2979156970977783\n",
            "Epoch 6462/10000: L(Train): 0.31463757157325745; L(Test): 0.2986522912979126\n",
            "Epoch 6463/10000: L(Train): 0.3137482702732086; L(Test): 0.3000776171684265\n",
            "Epoch 6464/10000: L(Train): 0.31866613030433655; L(Test): 0.2991606593132019\n",
            "Epoch 6465/10000: L(Train): 0.3191632330417633; L(Test): 0.2976328432559967\n",
            "Epoch 6466/10000: L(Train): 0.3179691433906555; L(Test): 0.29931825399398804\n",
            "Epoch 6467/10000: L(Train): 0.3281843066215515; L(Test): 0.2996942400932312\n",
            "Epoch 6468/10000: L(Train): 0.3219413161277771; L(Test): 0.29776668548583984\n",
            "Epoch 6469/10000: L(Train): 0.3162950575351715; L(Test): 0.30119749903678894\n",
            "Epoch 6470/10000: L(Train): 0.32155606150627136; L(Test): 0.29988113045692444\n",
            "Epoch 6471/10000: L(Train): 0.31977134943008423; L(Test): 0.2995140850543976\n",
            "Epoch 6472/10000: L(Train): 0.3302849233150482; L(Test): 0.29988211393356323\n",
            "Epoch 6473/10000: L(Train): 0.3200247287750244; L(Test): 0.2991145849227905\n",
            "Epoch 6474/10000: L(Train): 0.32516926527023315; L(Test): 0.29960960149765015\n",
            "Epoch 6475/10000: L(Train): 0.3218395411968231; L(Test): 0.2991207540035248\n",
            "Epoch 6476/10000: L(Train): 0.32494452595710754; L(Test): 0.297010600566864\n",
            "Epoch 6477/10000: L(Train): 0.3154902458190918; L(Test): 0.29657259583473206\n",
            "Epoch 6478/10000: L(Train): 0.3242586851119995; L(Test): 0.2972789704799652\n",
            "Epoch 6479/10000: L(Train): 0.3233150243759155; L(Test): 0.29656872153282166\n",
            "Epoch 6480/10000: L(Train): 0.3189905285835266; L(Test): 0.2962716817855835\n",
            "Epoch 6481/10000: L(Train): 0.32611358165740967; L(Test): 0.2963525652885437\n",
            "Epoch 6482/10000: L(Train): 0.317556232213974; L(Test): 0.2960575520992279\n",
            "Epoch 6483/10000: L(Train): 0.31148096919059753; L(Test): 0.2955244481563568\n",
            "Epoch 6484/10000: L(Train): 0.3253183960914612; L(Test): 0.2959807515144348\n",
            "Epoch 6485/10000: L(Train): 0.32465416193008423; L(Test): 0.2958526313304901\n",
            "Epoch 6486/10000: L(Train): 0.321834534406662; L(Test): 0.295345664024353\n",
            "Epoch 6487/10000: L(Train): 0.31939980387687683; L(Test): 0.2964162230491638\n",
            "Epoch 6488/10000: L(Train): 0.31561756134033203; L(Test): 0.2968921363353729\n",
            "Epoch 6489/10000: L(Train): 0.3186676502227783; L(Test): 0.29597824811935425\n",
            "Epoch 6490/10000: L(Train): 0.3239316940307617; L(Test): 0.29795652627944946\n",
            "Epoch 6491/10000: L(Train): 0.317055344581604; L(Test): 0.30033597350120544\n",
            "Epoch 6492/10000: L(Train): 0.3217177391052246; L(Test): 0.29618027806282043\n",
            "Epoch 6493/10000: L(Train): 0.31918665766716003; L(Test): 0.29530420899391174\n",
            "Epoch 6494/10000: L(Train): 0.3266996741294861; L(Test): 0.2979285717010498\n",
            "Epoch 6495/10000: L(Train): 0.3190385401248932; L(Test): 0.2981066107749939\n",
            "Epoch 6496/10000: L(Train): 0.3093368709087372; L(Test): 0.2967677414417267\n",
            "Epoch 6497/10000: L(Train): 0.3278172016143799; L(Test): 0.29647964239120483\n",
            "Epoch 6498/10000: L(Train): 0.3193587362766266; L(Test): 0.297324001789093\n",
            "Epoch 6499/10000: L(Train): 0.32327795028686523; L(Test): 0.29726025462150574\n",
            "Epoch 6500/10000: L(Train): 0.3231357932090759; L(Test): 0.29559075832366943\n",
            "Epoch 6501/10000: L(Train): 0.3255760669708252; L(Test): 0.2965559959411621\n",
            "Epoch 6502/10000: L(Train): 0.3112250566482544; L(Test): 0.29802706837654114\n",
            "Epoch 6503/10000: L(Train): 0.3161238133907318; L(Test): 0.298115074634552\n",
            "Epoch 6504/10000: L(Train): 0.31774717569351196; L(Test): 0.2991933524608612\n",
            "Epoch 6505/10000: L(Train): 0.32266902923583984; L(Test): 0.3004068434238434\n",
            "Epoch 6506/10000: L(Train): 0.318915992975235; L(Test): 0.2977144122123718\n",
            "Epoch 6507/10000: L(Train): 0.3177810311317444; L(Test): 0.29705700278282166\n",
            "Epoch 6508/10000: L(Train): 0.32257765531539917; L(Test): 0.29762598872184753\n",
            "Epoch 6509/10000: L(Train): 0.3250563442707062; L(Test): 0.29793575406074524\n",
            "Epoch 6510/10000: L(Train): 0.32233452796936035; L(Test): 0.2971840500831604\n",
            "Epoch 6511/10000: L(Train): 0.3165683448314667; L(Test): 0.29779180884361267\n",
            "Epoch 6512/10000: L(Train): 0.3188081681728363; L(Test): 0.29731371998786926\n",
            "Epoch 6513/10000: L(Train): 0.3307282328605652; L(Test): 0.2963966727256775\n",
            "Epoch 6514/10000: L(Train): 0.32254838943481445; L(Test): 0.29660093784332275\n",
            "Epoch 6515/10000: L(Train): 0.3257773816585541; L(Test): 0.2961958348751068\n",
            "Epoch 6516/10000: L(Train): 0.3242829740047455; L(Test): 0.2966107428073883\n",
            "Epoch 6517/10000: L(Train): 0.32731691002845764; L(Test): 0.29655447602272034\n",
            "Epoch 6518/10000: L(Train): 0.3155553340911865; L(Test): 0.29705846309661865\n",
            "Epoch 6519/10000: L(Train): 0.3263002634048462; L(Test): 0.29662996530532837\n",
            "Epoch 6520/10000: L(Train): 0.325584352016449; L(Test): 0.2959812581539154\n",
            "Epoch 6521/10000: L(Train): 0.3195231854915619; L(Test): 0.295635849237442\n",
            "Epoch 6522/10000: L(Train): 0.3175508975982666; L(Test): 0.29519122838974\n",
            "Epoch 6523/10000: L(Train): 0.3199075162410736; L(Test): 0.29505956172943115\n",
            "Epoch 6524/10000: L(Train): 0.31095460057258606; L(Test): 0.2958580553531647\n",
            "Epoch 6525/10000: L(Train): 0.31494519114494324; L(Test): 0.2966386675834656\n",
            "Epoch 6526/10000: L(Train): 0.31882917881011963; L(Test): 0.2967642545700073\n",
            "Epoch 6527/10000: L(Train): 0.32067441940307617; L(Test): 0.2962579131126404\n",
            "Epoch 6528/10000: L(Train): 0.3109894394874573; L(Test): 0.29637691378593445\n",
            "Epoch 6529/10000: L(Train): 0.3123997747898102; L(Test): 0.2955445647239685\n",
            "Epoch 6530/10000: L(Train): 0.31689774990081787; L(Test): 0.2949315309524536\n",
            "Epoch 6531/10000: L(Train): 0.3148568868637085; L(Test): 0.2940979301929474\n",
            "Epoch 6532/10000: L(Train): 0.31089967489242554; L(Test): 0.29455462098121643\n",
            "Epoch 6533/10000: L(Train): 0.31309381127357483; L(Test): 0.29436177015304565\n",
            "Epoch 6534/10000: L(Train): 0.31708648800849915; L(Test): 0.2946549355983734\n",
            "Epoch 6535/10000: L(Train): 0.33231744170188904; L(Test): 0.294688880443573\n",
            "Epoch 6536/10000: L(Train): 0.3180103600025177; L(Test): 0.29597604274749756\n",
            "Epoch 6537/10000: L(Train): 0.32560107111930847; L(Test): 0.29544350504875183\n",
            "Epoch 6538/10000: L(Train): 0.3257904350757599; L(Test): 0.2949945032596588\n",
            "Epoch 6539/10000: L(Train): 0.3232285976409912; L(Test): 0.2952095866203308\n",
            "Epoch 6540/10000: L(Train): 0.32500922679901123; L(Test): 0.29544153809547424\n",
            "Epoch 6541/10000: L(Train): 0.3237854540348053; L(Test): 0.2953976094722748\n",
            "Epoch 6542/10000: L(Train): 0.3168545365333557; L(Test): 0.295331209897995\n",
            "Epoch 6543/10000: L(Train): 0.31206831336021423; L(Test): 0.2945912778377533\n",
            "Epoch 6544/10000: L(Train): 0.3197883069515228; L(Test): 0.29482460021972656\n",
            "Epoch 6545/10000: L(Train): 0.32958775758743286; L(Test): 0.2958090603351593\n",
            "Epoch 6546/10000: L(Train): 0.3217577338218689; L(Test): 0.29486188292503357\n",
            "Epoch 6547/10000: L(Train): 0.31484857201576233; L(Test): 0.2961375117301941\n",
            "Epoch 6548/10000: L(Train): 0.3083340525627136; L(Test): 0.29743692278862\n",
            "Epoch 6549/10000: L(Train): 0.3210606575012207; L(Test): 0.2964586615562439\n",
            "Epoch 6550/10000: L(Train): 0.33333078026771545; L(Test): 0.29493722319602966\n",
            "Epoch 6551/10000: L(Train): 0.32606786489486694; L(Test): 0.29488182067871094\n",
            "Epoch 6552/10000: L(Train): 0.3198011815547943; L(Test): 0.29431086778640747\n",
            "Epoch 6553/10000: L(Train): 0.3125600516796112; L(Test): 0.2950957417488098\n",
            "Epoch 6554/10000: L(Train): 0.321907103061676; L(Test): 0.29589107632637024\n",
            "Epoch 6555/10000: L(Train): 0.3146836757659912; L(Test): 0.2952389419078827\n",
            "Epoch 6556/10000: L(Train): 0.3171231150627136; L(Test): 0.2955396771430969\n",
            "Epoch 6557/10000: L(Train): 0.314203679561615; L(Test): 0.29524192214012146\n",
            "Epoch 6558/10000: L(Train): 0.3141055703163147; L(Test): 0.2949408292770386\n",
            "Epoch 6559/10000: L(Train): 0.3264840841293335; L(Test): 0.29502183198928833\n",
            "Epoch 6560/10000: L(Train): 0.3233528137207031; L(Test): 0.2945665717124939\n",
            "Epoch 6561/10000: L(Train): 0.32194238901138306; L(Test): 0.2950584888458252\n",
            "Epoch 6562/10000: L(Train): 0.3196771740913391; L(Test): 0.29604122042655945\n",
            "Epoch 6563/10000: L(Train): 0.31343352794647217; L(Test): 0.2960168719291687\n",
            "Epoch 6564/10000: L(Train): 0.32255056500434875; L(Test): 0.29606470465660095\n",
            "Epoch 6565/10000: L(Train): 0.3166956603527069; L(Test): 0.2964625656604767\n",
            "Epoch 6566/10000: L(Train): 0.3282604217529297; L(Test): 0.2956295907497406\n",
            "Epoch 6567/10000: L(Train): 0.32748696208000183; L(Test): 0.2955980598926544\n",
            "Epoch 6568/10000: L(Train): 0.3201518952846527; L(Test): 0.2951374053955078\n",
            "Epoch 6569/10000: L(Train): 0.32710355520248413; L(Test): 0.2954746186733246\n",
            "Epoch 6570/10000: L(Train): 0.3212059736251831; L(Test): 0.2962492108345032\n",
            "Epoch 6571/10000: L(Train): 0.31732428073883057; L(Test): 0.2967322766780853\n",
            "Epoch 6572/10000: L(Train): 0.3267477750778198; L(Test): 0.29766035079956055\n",
            "Epoch 6573/10000: L(Train): 0.32321441173553467; L(Test): 0.2973545491695404\n",
            "Epoch 6574/10000: L(Train): 0.3177308142185211; L(Test): 0.29686281085014343\n",
            "Epoch 6575/10000: L(Train): 0.31410935521125793; L(Test): 0.29804113507270813\n",
            "Epoch 6576/10000: L(Train): 0.32418927550315857; L(Test): 0.2972728908061981\n",
            "Epoch 6577/10000: L(Train): 0.32310521602630615; L(Test): 0.29652896523475647\n",
            "Epoch 6578/10000: L(Train): 0.3137899339199066; L(Test): 0.29664814472198486\n",
            "Epoch 6579/10000: L(Train): 0.331255704164505; L(Test): 0.29617249965667725\n",
            "Epoch 6580/10000: L(Train): 0.31923097372055054; L(Test): 0.295261412858963\n",
            "Epoch 6581/10000: L(Train): 0.3115984797477722; L(Test): 0.2972787618637085\n",
            "Epoch 6582/10000: L(Train): 0.3194715678691864; L(Test): 0.2973698675632477\n",
            "Epoch 6583/10000: L(Train): 0.32326966524124146; L(Test): 0.2970481812953949\n",
            "Epoch 6584/10000: L(Train): 0.3219950199127197; L(Test): 0.2966502010822296\n",
            "Epoch 6585/10000: L(Train): 0.3152790367603302; L(Test): 0.29595497250556946\n",
            "Epoch 6586/10000: L(Train): 0.3161275386810303; L(Test): 0.29504549503326416\n",
            "Epoch 6587/10000: L(Train): 0.31780168414115906; L(Test): 0.2958070635795593\n",
            "Epoch 6588/10000: L(Train): 0.3236953914165497; L(Test): 0.29532572627067566\n",
            "Epoch 6589/10000: L(Train): 0.3196432292461395; L(Test): 0.29446738958358765\n",
            "Epoch 6590/10000: L(Train): 0.31083768606185913; L(Test): 0.2941616475582123\n",
            "Epoch 6591/10000: L(Train): 0.3134401738643646; L(Test): 0.29595237970352173\n",
            "Epoch 6592/10000: L(Train): 0.3216257691383362; L(Test): 0.29677578806877136\n",
            "Epoch 6593/10000: L(Train): 0.3139497637748718; L(Test): 0.29698893427848816\n",
            "Epoch 6594/10000: L(Train): 0.32157739996910095; L(Test): 0.29620692133903503\n",
            "Epoch 6595/10000: L(Train): 0.3167847692966461; L(Test): 0.296005517244339\n",
            "Epoch 6596/10000: L(Train): 0.31776973605155945; L(Test): 0.2972980737686157\n",
            "Epoch 6597/10000: L(Train): 0.3211416006088257; L(Test): 0.29633215069770813\n",
            "Epoch 6598/10000: L(Train): 0.31803932785987854; L(Test): 0.2955690026283264\n",
            "Epoch 6599/10000: L(Train): 0.3195188045501709; L(Test): 0.29834669828414917\n",
            "Epoch 6600/10000: L(Train): 0.3188384175300598; L(Test): 0.2990741729736328\n",
            "Epoch 6601/10000: L(Train): 0.32861486077308655; L(Test): 0.2967875599861145\n",
            "Epoch 6602/10000: L(Train): 0.3130074143409729; L(Test): 0.2967866063117981\n",
            "Epoch 6603/10000: L(Train): 0.31734150648117065; L(Test): 0.2962871789932251\n",
            "Epoch 6604/10000: L(Train): 0.316754549741745; L(Test): 0.2953166365623474\n",
            "Epoch 6605/10000: L(Train): 0.3233637511730194; L(Test): 0.29620084166526794\n",
            "Epoch 6606/10000: L(Train): 0.32443416118621826; L(Test): 0.2959139049053192\n",
            "Epoch 6607/10000: L(Train): 0.31948328018188477; L(Test): 0.295065701007843\n",
            "Epoch 6608/10000: L(Train): 0.3227059841156006; L(Test): 0.2971212863922119\n",
            "Epoch 6609/10000: L(Train): 0.31930527091026306; L(Test): 0.2949775159358978\n",
            "Epoch 6610/10000: L(Train): 0.3193875849246979; L(Test): 0.29551398754119873\n",
            "Epoch 6611/10000: L(Train): 0.31375694274902344; L(Test): 0.29778537154197693\n",
            "Epoch 6612/10000: L(Train): 0.32947760820388794; L(Test): 0.29500478506088257\n",
            "Epoch 6613/10000: L(Train): 0.3217463493347168; L(Test): 0.2950495183467865\n",
            "Epoch 6614/10000: L(Train): 0.31765711307525635; L(Test): 0.29702141880989075\n",
            "Epoch 6615/10000: L(Train): 0.31920695304870605; L(Test): 0.2950330078601837\n",
            "Epoch 6616/10000: L(Train): 0.3217376470565796; L(Test): 0.2960301339626312\n",
            "Epoch 6617/10000: L(Train): 0.3182879686355591; L(Test): 0.29650622606277466\n",
            "Epoch 6618/10000: L(Train): 0.31603625416755676; L(Test): 0.2956992983818054\n",
            "Epoch 6619/10000: L(Train): 0.3234891891479492; L(Test): 0.29586341977119446\n",
            "Epoch 6620/10000: L(Train): 0.32837313413619995; L(Test): 0.29544782638549805\n",
            "Epoch 6621/10000: L(Train): 0.31207403540611267; L(Test): 0.2950127124786377\n",
            "Epoch 6622/10000: L(Train): 0.31369298696517944; L(Test): 0.29724839329719543\n",
            "Epoch 6623/10000: L(Train): 0.32148829102516174; L(Test): 0.29791682958602905\n",
            "Epoch 6624/10000: L(Train): 0.31976789236068726; L(Test): 0.296137273311615\n",
            "Epoch 6625/10000: L(Train): 0.32626259326934814; L(Test): 0.2961188554763794\n",
            "Epoch 6626/10000: L(Train): 0.31636160612106323; L(Test): 0.2965911626815796\n",
            "Epoch 6627/10000: L(Train): 0.32383406162261963; L(Test): 0.2960989475250244\n",
            "Epoch 6628/10000: L(Train): 0.3299923539161682; L(Test): 0.2968980669975281\n",
            "Epoch 6629/10000: L(Train): 0.322843998670578; L(Test): 0.29502010345458984\n",
            "Epoch 6630/10000: L(Train): 0.31755438446998596; L(Test): 0.29493090510368347\n",
            "Epoch 6631/10000: L(Train): 0.31879153847694397; L(Test): 0.2958142161369324\n",
            "Epoch 6632/10000: L(Train): 0.3168676197528839; L(Test): 0.29572463035583496\n",
            "Epoch 6633/10000: L(Train): 0.32188472151756287; L(Test): 0.2964150309562683\n",
            "Epoch 6634/10000: L(Train): 0.3211704194545746; L(Test): 0.2964821457862854\n",
            "Epoch 6635/10000: L(Train): 0.32619181275367737; L(Test): 0.29568082094192505\n",
            "Epoch 6636/10000: L(Train): 0.3192218542098999; L(Test): 0.29555070400238037\n",
            "Epoch 6637/10000: L(Train): 0.326084166765213; L(Test): 0.2956966459751129\n",
            "Epoch 6638/10000: L(Train): 0.314118891954422; L(Test): 0.29726442694664\n",
            "Epoch 6639/10000: L(Train): 0.32173070311546326; L(Test): 0.2965671718120575\n",
            "Epoch 6640/10000: L(Train): 0.31428295373916626; L(Test): 0.29532521963119507\n",
            "Epoch 6641/10000: L(Train): 0.3212897777557373; L(Test): 0.29666265845298767\n",
            "Epoch 6642/10000: L(Train): 0.31343874335289; L(Test): 0.29625460505485535\n",
            "Epoch 6643/10000: L(Train): 0.327147513628006; L(Test): 0.2958213686943054\n",
            "Epoch 6644/10000: L(Train): 0.3237980008125305; L(Test): 0.2981700599193573\n",
            "Epoch 6645/10000: L(Train): 0.3247296214103699; L(Test): 0.2978762984275818\n",
            "Epoch 6646/10000: L(Train): 0.3244130611419678; L(Test): 0.29701176285743713\n",
            "Epoch 6647/10000: L(Train): 0.3200992941856384; L(Test): 0.29682543873786926\n",
            "Epoch 6648/10000: L(Train): 0.31857380270957947; L(Test): 0.2954971492290497\n",
            "Epoch 6649/10000: L(Train): 0.3152920603752136; L(Test): 0.29456251859664917\n",
            "Epoch 6650/10000: L(Train): 0.31352347135543823; L(Test): 0.2949332594871521\n",
            "Epoch 6651/10000: L(Train): 0.3064914643764496; L(Test): 0.29557162523269653\n",
            "Epoch 6652/10000: L(Train): 0.3181133568286896; L(Test): 0.29482343792915344\n",
            "Epoch 6653/10000: L(Train): 0.3138585388660431; L(Test): 0.2947629988193512\n",
            "Epoch 6654/10000: L(Train): 0.32107269763946533; L(Test): 0.29634809494018555\n",
            "Epoch 6655/10000: L(Train): 0.3184105157852173; L(Test): 0.29635900259017944\n",
            "Epoch 6656/10000: L(Train): 0.32373905181884766; L(Test): 0.295251727104187\n",
            "Epoch 6657/10000: L(Train): 0.31615394353866577; L(Test): 0.2954978048801422\n",
            "Epoch 6658/10000: L(Train): 0.31925731897354126; L(Test): 0.29587551951408386\n",
            "Epoch 6659/10000: L(Train): 0.32208824157714844; L(Test): 0.29418137669563293\n",
            "Epoch 6660/10000: L(Train): 0.3099724054336548; L(Test): 0.29389429092407227\n",
            "Epoch 6661/10000: L(Train): 0.3182721436023712; L(Test): 0.2950475513935089\n",
            "Epoch 6662/10000: L(Train): 0.3235333561897278; L(Test): 0.2957351505756378\n",
            "Epoch 6663/10000: L(Train): 0.324800580739975; L(Test): 0.29580873250961304\n",
            "Epoch 6664/10000: L(Train): 0.31384140253067017; L(Test): 0.2958929240703583\n",
            "Epoch 6665/10000: L(Train): 0.32143986225128174; L(Test): 0.2962661385536194\n",
            "Epoch 6666/10000: L(Train): 0.32363662123680115; L(Test): 0.29643601179122925\n",
            "Epoch 6667/10000: L(Train): 0.3347955346107483; L(Test): 0.2962028384208679\n",
            "Epoch 6668/10000: L(Train): 0.3224920630455017; L(Test): 0.29630422592163086\n",
            "Epoch 6669/10000: L(Train): 0.3186308443546295; L(Test): 0.2965032756328583\n",
            "Epoch 6670/10000: L(Train): 0.3251590430736542; L(Test): 0.29547905921936035\n",
            "Epoch 6671/10000: L(Train): 0.32698652148246765; L(Test): 0.29507750272750854\n",
            "Epoch 6672/10000: L(Train): 0.32351285219192505; L(Test): 0.2945730984210968\n",
            "Epoch 6673/10000: L(Train): 0.31885600090026855; L(Test): 0.2949802577495575\n",
            "Epoch 6674/10000: L(Train): 0.3188168704509735; L(Test): 0.29609230160713196\n",
            "Epoch 6675/10000: L(Train): 0.3291696012020111; L(Test): 0.2965821623802185\n",
            "Epoch 6676/10000: L(Train): 0.32300668954849243; L(Test): 0.2952582836151123\n",
            "Epoch 6677/10000: L(Train): 0.32218730449676514; L(Test): 0.2959216237068176\n",
            "Epoch 6678/10000: L(Train): 0.3144187033176422; L(Test): 0.298088014125824\n",
            "Epoch 6679/10000: L(Train): 0.31888535618782043; L(Test): 0.2970579266548157\n",
            "Epoch 6680/10000: L(Train): 0.31933337450027466; L(Test): 0.29783734679222107\n",
            "Epoch 6681/10000: L(Train): 0.32061752676963806; L(Test): 0.2972802519798279\n",
            "Epoch 6682/10000: L(Train): 0.31262052059173584; L(Test): 0.29609182476997375\n",
            "Epoch 6683/10000: L(Train): 0.32938337326049805; L(Test): 0.2970771789550781\n",
            "Epoch 6684/10000: L(Train): 0.32585474848747253; L(Test): 0.29548007249832153\n",
            "Epoch 6685/10000: L(Train): 0.31932762265205383; L(Test): 0.2952995300292969\n",
            "Epoch 6686/10000: L(Train): 0.3211323916912079; L(Test): 0.2965678870677948\n",
            "Epoch 6687/10000: L(Train): 0.3242143988609314; L(Test): 0.2967190146446228\n",
            "Epoch 6688/10000: L(Train): 0.32234737277030945; L(Test): 0.2962133288383484\n",
            "Epoch 6689/10000: L(Train): 0.3185383081436157; L(Test): 0.29551222920417786\n",
            "Epoch 6690/10000: L(Train): 0.31174328923225403; L(Test): 0.2956358790397644\n",
            "Epoch 6691/10000: L(Train): 0.316099613904953; L(Test): 0.2956356406211853\n",
            "Epoch 6692/10000: L(Train): 0.3204079270362854; L(Test): 0.29642361402511597\n",
            "Epoch 6693/10000: L(Train): 0.32372596859931946; L(Test): 0.29684215784072876\n",
            "Epoch 6694/10000: L(Train): 0.32709452509880066; L(Test): 0.29595229029655457\n",
            "Epoch 6695/10000: L(Train): 0.32401326298713684; L(Test): 0.29503920674324036\n",
            "Epoch 6696/10000: L(Train): 0.3251465857028961; L(Test): 0.2952400743961334\n",
            "Epoch 6697/10000: L(Train): 0.3186042904853821; L(Test): 0.29755887389183044\n",
            "Epoch 6698/10000: L(Train): 0.3220362961292267; L(Test): 0.29889774322509766\n",
            "Epoch 6699/10000: L(Train): 0.32441404461860657; L(Test): 0.29891109466552734\n",
            "Epoch 6700/10000: L(Train): 0.32575279474258423; L(Test): 0.2993009388446808\n",
            "Epoch 6701/10000: L(Train): 0.32958945631980896; L(Test): 0.2977307438850403\n",
            "Epoch 6702/10000: L(Train): 0.32392892241477966; L(Test): 0.29710254073143005\n",
            "Epoch 6703/10000: L(Train): 0.3197900950908661; L(Test): 0.29816174507141113\n",
            "Epoch 6704/10000: L(Train): 0.3224182426929474; L(Test): 0.2980691194534302\n",
            "Epoch 6705/10000: L(Train): 0.3282022774219513; L(Test): 0.2976064383983612\n",
            "Epoch 6706/10000: L(Train): 0.3193603754043579; L(Test): 0.29737359285354614\n",
            "Epoch 6707/10000: L(Train): 0.3246361315250397; L(Test): 0.2970896363258362\n",
            "Epoch 6708/10000: L(Train): 0.31443145871162415; L(Test): 0.29821720719337463\n",
            "Epoch 6709/10000: L(Train): 0.32275134325027466; L(Test): 0.29877761006355286\n",
            "Epoch 6710/10000: L(Train): 0.32066187262535095; L(Test): 0.2980193793773651\n",
            "Epoch 6711/10000: L(Train): 0.31736403703689575; L(Test): 0.29685303568840027\n",
            "Epoch 6712/10000: L(Train): 0.3203044831752777; L(Test): 0.2959012985229492\n",
            "Epoch 6713/10000: L(Train): 0.32321828603744507; L(Test): 0.29579398036003113\n",
            "Epoch 6714/10000: L(Train): 0.3201387822628021; L(Test): 0.29634183645248413\n",
            "Epoch 6715/10000: L(Train): 0.3166632056236267; L(Test): 0.29636862874031067\n",
            "Epoch 6716/10000: L(Train): 0.3241730332374573; L(Test): 0.2954104244709015\n",
            "Epoch 6717/10000: L(Train): 0.3208368420600891; L(Test): 0.29594284296035767\n",
            "Epoch 6718/10000: L(Train): 0.31575819849967957; L(Test): 0.2967756390571594\n",
            "Epoch 6719/10000: L(Train): 0.3269246220588684; L(Test): 0.2944503128528595\n",
            "Epoch 6720/10000: L(Train): 0.3128502666950226; L(Test): 0.29586854577064514\n",
            "Epoch 6721/10000: L(Train): 0.3220973610877991; L(Test): 0.2959628701210022\n",
            "Epoch 6722/10000: L(Train): 0.31845155358314514; L(Test): 0.29670387506484985\n",
            "Epoch 6723/10000: L(Train): 0.32196152210235596; L(Test): 0.2982325851917267\n",
            "Epoch 6724/10000: L(Train): 0.3170085847377777; L(Test): 0.29730546474456787\n",
            "Epoch 6725/10000: L(Train): 0.3170008361339569; L(Test): 0.29763996601104736\n",
            "Epoch 6726/10000: L(Train): 0.32004788517951965; L(Test): 0.29813045263290405\n",
            "Epoch 6727/10000: L(Train): 0.317852646112442; L(Test): 0.2979104220867157\n",
            "Epoch 6728/10000: L(Train): 0.31299057602882385; L(Test): 0.2969783842563629\n",
            "Epoch 6729/10000: L(Train): 0.31892240047454834; L(Test): 0.29796716570854187\n",
            "Epoch 6730/10000: L(Train): 0.32187187671661377; L(Test): 0.2980952262878418\n",
            "Epoch 6731/10000: L(Train): 0.3206855058670044; L(Test): 0.2966426908969879\n",
            "Epoch 6732/10000: L(Train): 0.3235011100769043; L(Test): 0.29614534974098206\n",
            "Epoch 6733/10000: L(Train): 0.3201802372932434; L(Test): 0.29622551798820496\n",
            "Epoch 6734/10000: L(Train): 0.31282442808151245; L(Test): 0.2959122359752655\n",
            "Epoch 6735/10000: L(Train): 0.320013165473938; L(Test): 0.2968922555446625\n",
            "Epoch 6736/10000: L(Train): 0.3270726203918457; L(Test): 0.29825353622436523\n",
            "Epoch 6737/10000: L(Train): 0.3118027448654175; L(Test): 0.29757726192474365\n",
            "Epoch 6738/10000: L(Train): 0.32747331261634827; L(Test): 0.29683196544647217\n",
            "Epoch 6739/10000: L(Train): 0.3213708996772766; L(Test): 0.2967987060546875\n",
            "Epoch 6740/10000: L(Train): 0.33110886812210083; L(Test): 0.29744186997413635\n",
            "Epoch 6741/10000: L(Train): 0.32943281531333923; L(Test): 0.2958952486515045\n",
            "Epoch 6742/10000: L(Train): 0.32531848549842834; L(Test): 0.29629501700401306\n",
            "Epoch 6743/10000: L(Train): 0.32056134939193726; L(Test): 0.2977744936943054\n",
            "Epoch 6744/10000: L(Train): 0.3177591562271118; L(Test): 0.29910656809806824\n",
            "Epoch 6745/10000: L(Train): 0.3155328631401062; L(Test): 0.2991786301136017\n",
            "Epoch 6746/10000: L(Train): 0.3168776035308838; L(Test): 0.2966564893722534\n",
            "Epoch 6747/10000: L(Train): 0.3191317915916443; L(Test): 0.29556992650032043\n",
            "Epoch 6748/10000: L(Train): 0.32401588559150696; L(Test): 0.29710566997528076\n",
            "Epoch 6749/10000: L(Train): 0.32754430174827576; L(Test): 0.29683631658554077\n",
            "Epoch 6750/10000: L(Train): 0.3205520510673523; L(Test): 0.29570841789245605\n",
            "Epoch 6751/10000: L(Train): 0.3068329691886902; L(Test): 0.2964363992214203\n",
            "Epoch 6752/10000: L(Train): 0.326437771320343; L(Test): 0.29768192768096924\n",
            "Epoch 6753/10000: L(Train): 0.3273734450340271; L(Test): 0.2964346408843994\n",
            "Epoch 6754/10000: L(Train): 0.3171263635158539; L(Test): 0.2962944507598877\n",
            "Epoch 6755/10000: L(Train): 0.31787818670272827; L(Test): 0.29777929186820984\n",
            "Epoch 6756/10000: L(Train): 0.3302439749240875; L(Test): 0.29721909761428833\n",
            "Epoch 6757/10000: L(Train): 0.310486763715744; L(Test): 0.29648950695991516\n",
            "Epoch 6758/10000: L(Train): 0.3144191801548004; L(Test): 0.29707589745521545\n",
            "Epoch 6759/10000: L(Train): 0.31954190135002136; L(Test): 0.2985217273235321\n",
            "Epoch 6760/10000: L(Train): 0.32660630345344543; L(Test): 0.29813501238822937\n",
            "Epoch 6761/10000: L(Train): 0.32819926738739014; L(Test): 0.2965662181377411\n",
            "Epoch 6762/10000: L(Train): 0.31090959906578064; L(Test): 0.2971251904964447\n",
            "Epoch 6763/10000: L(Train): 0.32482412457466125; L(Test): 0.2965334355831146\n",
            "Epoch 6764/10000: L(Train): 0.31854069232940674; L(Test): 0.2959302067756653\n",
            "Epoch 6765/10000: L(Train): 0.3148181736469269; L(Test): 0.295950323343277\n",
            "Epoch 6766/10000: L(Train): 0.3154200613498688; L(Test): 0.2960491478443146\n",
            "Epoch 6767/10000: L(Train): 0.3145626485347748; L(Test): 0.2958102524280548\n",
            "Epoch 6768/10000: L(Train): 0.3210114538669586; L(Test): 0.2953336834907532\n",
            "Epoch 6769/10000: L(Train): 0.3248908519744873; L(Test): 0.2953311502933502\n",
            "Epoch 6770/10000: L(Train): 0.3189410865306854; L(Test): 0.2960040867328644\n",
            "Epoch 6771/10000: L(Train): 0.31284451484680176; L(Test): 0.29595664143562317\n",
            "Epoch 6772/10000: L(Train): 0.32299184799194336; L(Test): 0.29548943042755127\n",
            "Epoch 6773/10000: L(Train): 0.3189280331134796; L(Test): 0.29555073380470276\n",
            "Epoch 6774/10000: L(Train): 0.3246377408504486; L(Test): 0.29555588960647583\n",
            "Epoch 6775/10000: L(Train): 0.31558850407600403; L(Test): 0.29514971375465393\n",
            "Epoch 6776/10000: L(Train): 0.31816551089286804; L(Test): 0.29526573419570923\n",
            "Epoch 6777/10000: L(Train): 0.3158845007419586; L(Test): 0.2952975630760193\n",
            "Epoch 6778/10000: L(Train): 0.31409528851509094; L(Test): 0.2955549955368042\n",
            "Epoch 6779/10000: L(Train): 0.31613388657569885; L(Test): 0.2957506477832794\n",
            "Epoch 6780/10000: L(Train): 0.32648131251335144; L(Test): 0.294626384973526\n",
            "Epoch 6781/10000: L(Train): 0.32026541233062744; L(Test): 0.2950432598590851\n",
            "Epoch 6782/10000: L(Train): 0.3304210901260376; L(Test): 0.295002818107605\n",
            "Epoch 6783/10000: L(Train): 0.31741389632225037; L(Test): 0.29524916410446167\n",
            "Epoch 6784/10000: L(Train): 0.3315053880214691; L(Test): 0.2970275282859802\n",
            "Epoch 6785/10000: L(Train): 0.3271218240261078; L(Test): 0.29801204800605774\n",
            "Epoch 6786/10000: L(Train): 0.32663390040397644; L(Test): 0.2981402277946472\n",
            "Epoch 6787/10000: L(Train): 0.32246240973472595; L(Test): 0.298074334859848\n",
            "Epoch 6788/10000: L(Train): 0.31700363755226135; L(Test): 0.2977135479450226\n",
            "Epoch 6789/10000: L(Train): 0.32155290246009827; L(Test): 0.29797229170799255\n",
            "Epoch 6790/10000: L(Train): 0.3170490860939026; L(Test): 0.2964801788330078\n",
            "Epoch 6791/10000: L(Train): 0.3155466318130493; L(Test): 0.2986990511417389\n",
            "Epoch 6792/10000: L(Train): 0.32638415694236755; L(Test): 0.29706060886383057\n",
            "Epoch 6793/10000: L(Train): 0.3159087002277374; L(Test): 0.29596441984176636\n",
            "Epoch 6794/10000: L(Train): 0.31209173798561096; L(Test): 0.2969224452972412\n",
            "Epoch 6795/10000: L(Train): 0.31627240777015686; L(Test): 0.2989644408226013\n",
            "Epoch 6796/10000: L(Train): 0.33117154240608215; L(Test): 0.30025526881217957\n",
            "Epoch 6797/10000: L(Train): 0.3301454782485962; L(Test): 0.29860711097717285\n",
            "Epoch 6798/10000: L(Train): 0.32469847798347473; L(Test): 0.2977015972137451\n",
            "Epoch 6799/10000: L(Train): 0.32060712575912476; L(Test): 0.29734107851982117\n",
            "Epoch 6800/10000: L(Train): 0.3186388313770294; L(Test): 0.29881271719932556\n",
            "Epoch 6801/10000: L(Train): 0.3230871558189392; L(Test): 0.2982945740222931\n",
            "Epoch 6802/10000: L(Train): 0.31813114881515503; L(Test): 0.2966630756855011\n",
            "Epoch 6803/10000: L(Train): 0.32145389914512634; L(Test): 0.2977200150489807\n",
            "Epoch 6804/10000: L(Train): 0.32309064269065857; L(Test): 0.2988121509552002\n",
            "Epoch 6805/10000: L(Train): 0.33322572708129883; L(Test): 0.29778817296028137\n",
            "Epoch 6806/10000: L(Train): 0.318037211894989; L(Test): 0.29775744676589966\n",
            "Epoch 6807/10000: L(Train): 0.31531956791877747; L(Test): 0.2984837591648102\n",
            "Epoch 6808/10000: L(Train): 0.3153933882713318; L(Test): 0.29785096645355225\n",
            "Epoch 6809/10000: L(Train): 0.32300040125846863; L(Test): 0.2970965504646301\n",
            "Epoch 6810/10000: L(Train): 0.31936192512512207; L(Test): 0.297116756439209\n",
            "Epoch 6811/10000: L(Train): 0.3130497932434082; L(Test): 0.2962052524089813\n",
            "Epoch 6812/10000: L(Train): 0.3284568190574646; L(Test): 0.296789288520813\n",
            "Epoch 6813/10000: L(Train): 0.31887564063072205; L(Test): 0.2965484857559204\n",
            "Epoch 6814/10000: L(Train): 0.3157216012477875; L(Test): 0.2960088551044464\n",
            "Epoch 6815/10000: L(Train): 0.3218095302581787; L(Test): 0.2958625853061676\n",
            "Epoch 6816/10000: L(Train): 0.32437068223953247; L(Test): 0.29517289996147156\n",
            "Epoch 6817/10000: L(Train): 0.3259958028793335; L(Test): 0.29532313346862793\n",
            "Epoch 6818/10000: L(Train): 0.30616480112075806; L(Test): 0.2966754138469696\n",
            "Epoch 6819/10000: L(Train): 0.3185506761074066; L(Test): 0.29639407992362976\n",
            "Epoch 6820/10000: L(Train): 0.31377360224723816; L(Test): 0.29500260949134827\n",
            "Epoch 6821/10000: L(Train): 0.3127363622188568; L(Test): 0.29486334323883057\n",
            "Epoch 6822/10000: L(Train): 0.31581997871398926; L(Test): 0.29485368728637695\n",
            "Epoch 6823/10000: L(Train): 0.32181331515312195; L(Test): 0.29476866126060486\n",
            "Epoch 6824/10000: L(Train): 0.30668535828590393; L(Test): 0.29584062099456787\n",
            "Epoch 6825/10000: L(Train): 0.3163757026195526; L(Test): 0.29514047503471375\n",
            "Epoch 6826/10000: L(Train): 0.3244328200817108; L(Test): 0.29433029890060425\n",
            "Epoch 6827/10000: L(Train): 0.3215804100036621; L(Test): 0.29494959115982056\n",
            "Epoch 6828/10000: L(Train): 0.3233049213886261; L(Test): 0.29422375559806824\n",
            "Epoch 6829/10000: L(Train): 0.3188115954399109; L(Test): 0.2949870824813843\n",
            "Epoch 6830/10000: L(Train): 0.3208974003791809; L(Test): 0.2951904237270355\n",
            "Epoch 6831/10000: L(Train): 0.3191509246826172; L(Test): 0.29374945163726807\n",
            "Epoch 6832/10000: L(Train): 0.3242057263851166; L(Test): 0.2936026155948639\n",
            "Epoch 6833/10000: L(Train): 0.3169199824333191; L(Test): 0.2947463095188141\n",
            "Epoch 6834/10000: L(Train): 0.3177712857723236; L(Test): 0.2948172986507416\n",
            "Epoch 6835/10000: L(Train): 0.32377001643180847; L(Test): 0.2944587171077728\n",
            "Epoch 6836/10000: L(Train): 0.32073974609375; L(Test): 0.2951089143753052\n",
            "Epoch 6837/10000: L(Train): 0.32137078046798706; L(Test): 0.294965922832489\n",
            "Epoch 6838/10000: L(Train): 0.3142049014568329; L(Test): 0.29478347301483154\n",
            "Epoch 6839/10000: L(Train): 0.3096728026866913; L(Test): 0.29493194818496704\n",
            "Epoch 6840/10000: L(Train): 0.3154454231262207; L(Test): 0.29508721828460693\n",
            "Epoch 6841/10000: L(Train): 0.3204275071620941; L(Test): 0.29638975858688354\n",
            "Epoch 6842/10000: L(Train): 0.3251630365848541; L(Test): 0.29607489705085754\n",
            "Epoch 6843/10000: L(Train): 0.3178715109825134; L(Test): 0.29596632719039917\n",
            "Epoch 6844/10000: L(Train): 0.3113858997821808; L(Test): 0.2965921461582184\n",
            "Epoch 6845/10000: L(Train): 0.31233084201812744; L(Test): 0.29536768794059753\n",
            "Epoch 6846/10000: L(Train): 0.3272719979286194; L(Test): 0.2964048385620117\n",
            "Epoch 6847/10000: L(Train): 0.3113463819026947; L(Test): 0.29807353019714355\n",
            "Epoch 6848/10000: L(Train): 0.3289816677570343; L(Test): 0.2958472669124603\n",
            "Epoch 6849/10000: L(Train): 0.31767088174819946; L(Test): 0.2955189049243927\n",
            "Epoch 6850/10000: L(Train): 0.3273561894893646; L(Test): 0.2948266267776489\n",
            "Epoch 6851/10000: L(Train): 0.32422304153442383; L(Test): 0.29477936029434204\n",
            "Epoch 6852/10000: L(Train): 0.32610854506492615; L(Test): 0.2958049178123474\n",
            "Epoch 6853/10000: L(Train): 0.3285541236400604; L(Test): 0.29627659916877747\n",
            "Epoch 6854/10000: L(Train): 0.3110913038253784; L(Test): 0.2958820164203644\n",
            "Epoch 6855/10000: L(Train): 0.31098249554634094; L(Test): 0.29636335372924805\n",
            "Epoch 6856/10000: L(Train): 0.31297948956489563; L(Test): 0.30458223819732666\n",
            "Epoch 6857/10000: L(Train): 0.33500516414642334; L(Test): 0.30538344383239746\n",
            "Epoch 6858/10000: L(Train): 0.3239351511001587; L(Test): 0.30584216117858887\n",
            "Epoch 6859/10000: L(Train): 0.32332444190979004; L(Test): 0.3146238923072815\n",
            "Epoch 6860/10000: L(Train): 0.3328808546066284; L(Test): 0.31146061420440674\n",
            "Epoch 6861/10000: L(Train): 0.3359071612358093; L(Test): 0.3092189431190491\n",
            "Epoch 6862/10000: L(Train): 0.3205844461917877; L(Test): 0.3093658685684204\n",
            "Epoch 6863/10000: L(Train): 0.3202686011791229; L(Test): 0.30983829498291016\n",
            "Epoch 6864/10000: L(Train): 0.33016470074653625; L(Test): 0.30826643109321594\n",
            "Epoch 6865/10000: L(Train): 0.33348482847213745; L(Test): 0.30798810720443726\n",
            "Epoch 6866/10000: L(Train): 0.32719171047210693; L(Test): 0.3067948818206787\n",
            "Epoch 6867/10000: L(Train): 0.31919243931770325; L(Test): 0.3054807186126709\n",
            "Epoch 6868/10000: L(Train): 0.32895514369010925; L(Test): 0.30620893836021423\n",
            "Epoch 6869/10000: L(Train): 0.3374021351337433; L(Test): 0.30677080154418945\n",
            "Epoch 6870/10000: L(Train): 0.32799288630485535; L(Test): 0.3055873215198517\n",
            "Epoch 6871/10000: L(Train): 0.3238607943058014; L(Test): 0.3038145899772644\n",
            "Epoch 6872/10000: L(Train): 0.3275844156742096; L(Test): 0.30389729142189026\n",
            "Epoch 6873/10000: L(Train): 0.3156212568283081; L(Test): 0.3046835958957672\n",
            "Epoch 6874/10000: L(Train): 0.32634419202804565; L(Test): 0.30381810665130615\n",
            "Epoch 6875/10000: L(Train): 0.32102951407432556; L(Test): 0.3037160634994507\n",
            "Epoch 6876/10000: L(Train): 0.3352583944797516; L(Test): 0.30291447043418884\n",
            "Epoch 6877/10000: L(Train): 0.3299928605556488; L(Test): 0.30194151401519775\n",
            "Epoch 6878/10000: L(Train): 0.31478965282440186; L(Test): 0.3025733530521393\n",
            "Epoch 6879/10000: L(Train): 0.3339097201824188; L(Test): 0.30187681317329407\n",
            "Epoch 6880/10000: L(Train): 0.3225712478160858; L(Test): 0.3011104166507721\n",
            "Epoch 6881/10000: L(Train): 0.3217259347438812; L(Test): 0.3031633496284485\n",
            "Epoch 6882/10000: L(Train): 0.32651352882385254; L(Test): 0.3021012246608734\n",
            "Epoch 6883/10000: L(Train): 0.32460132241249084; L(Test): 0.30005955696105957\n",
            "Epoch 6884/10000: L(Train): 0.32505935430526733; L(Test): 0.2991800606250763\n",
            "Epoch 6885/10000: L(Train): 0.32481300830841064; L(Test): 0.2988417148590088\n",
            "Epoch 6886/10000: L(Train): 0.31673941016197205; L(Test): 0.2991436719894409\n",
            "Epoch 6887/10000: L(Train): 0.3265509009361267; L(Test): 0.29918333888053894\n",
            "Epoch 6888/10000: L(Train): 0.3148910403251648; L(Test): 0.30051854252815247\n",
            "Epoch 6889/10000: L(Train): 0.3201969265937805; L(Test): 0.3007359504699707\n",
            "Epoch 6890/10000: L(Train): 0.31601232290267944; L(Test): 0.30106887221336365\n",
            "Epoch 6891/10000: L(Train): 0.3302425444126129; L(Test): 0.3001362383365631\n",
            "Epoch 6892/10000: L(Train): 0.32327955961227417; L(Test): 0.2993420362472534\n",
            "Epoch 6893/10000: L(Train): 0.3213016986846924; L(Test): 0.29986894130706787\n",
            "Epoch 6894/10000: L(Train): 0.3377670645713806; L(Test): 0.29883816838264465\n",
            "Epoch 6895/10000: L(Train): 0.3208218812942505; L(Test): 0.29840290546417236\n",
            "Epoch 6896/10000: L(Train): 0.3237113356590271; L(Test): 0.2984289824962616\n",
            "Epoch 6897/10000: L(Train): 0.3207896947860718; L(Test): 0.2982500493526459\n",
            "Epoch 6898/10000: L(Train): 0.31709668040275574; L(Test): 0.2982427477836609\n",
            "Epoch 6899/10000: L(Train): 0.3315652310848236; L(Test): 0.2978331446647644\n",
            "Epoch 6900/10000: L(Train): 0.3197186291217804; L(Test): 0.2967718541622162\n",
            "Epoch 6901/10000: L(Train): 0.31726565957069397; L(Test): 0.29580193758010864\n",
            "Epoch 6902/10000: L(Train): 0.3206333816051483; L(Test): 0.29554933309555054\n",
            "Epoch 6903/10000: L(Train): 0.31498008966445923; L(Test): 0.29560986161231995\n",
            "Epoch 6904/10000: L(Train): 0.3249405026435852; L(Test): 0.2961539626121521\n",
            "Epoch 6905/10000: L(Train): 0.331712931394577; L(Test): 0.29636672139167786\n",
            "Epoch 6906/10000: L(Train): 0.327901691198349; L(Test): 0.2965339124202728\n",
            "Epoch 6907/10000: L(Train): 0.33110129833221436; L(Test): 0.29552775621414185\n",
            "Epoch 6908/10000: L(Train): 0.3215837776660919; L(Test): 0.2951015830039978\n",
            "Epoch 6909/10000: L(Train): 0.3219960331916809; L(Test): 0.29684728384017944\n",
            "Epoch 6910/10000: L(Train): 0.3334644138813019; L(Test): 0.29698702692985535\n",
            "Epoch 6911/10000: L(Train): 0.32734036445617676; L(Test): 0.29547005891799927\n",
            "Epoch 6912/10000: L(Train): 0.3237519860267639; L(Test): 0.29623112082481384\n",
            "Epoch 6913/10000: L(Train): 0.32846713066101074; L(Test): 0.29621559381484985\n",
            "Epoch 6914/10000: L(Train): 0.3187938630580902; L(Test): 0.29628828167915344\n",
            "Epoch 6915/10000: L(Train): 0.319113552570343; L(Test): 0.2970007658004761\n",
            "Epoch 6916/10000: L(Train): 0.33227601647377014; L(Test): 0.29801273345947266\n",
            "Epoch 6917/10000: L(Train): 0.3217671811580658; L(Test): 0.29905053973197937\n",
            "Epoch 6918/10000: L(Train): 0.3157961070537567; L(Test): 0.29898539185523987\n",
            "Epoch 6919/10000: L(Train): 0.322506844997406; L(Test): 0.29816877841949463\n",
            "Epoch 6920/10000: L(Train): 0.3172745108604431; L(Test): 0.2968663275241852\n",
            "Epoch 6921/10000: L(Train): 0.31383031606674194; L(Test): 0.29585832357406616\n",
            "Epoch 6922/10000: L(Train): 0.31559422612190247; L(Test): 0.29700031876564026\n",
            "Epoch 6923/10000: L(Train): 0.3224792182445526; L(Test): 0.29849380254745483\n",
            "Epoch 6924/10000: L(Train): 0.32968711853027344; L(Test): 0.2978496849536896\n",
            "Epoch 6925/10000: L(Train): 0.3155657649040222; L(Test): 0.29685041308403015\n",
            "Epoch 6926/10000: L(Train): 0.3172551393508911; L(Test): 0.2963394820690155\n",
            "Epoch 6927/10000: L(Train): 0.31802383065223694; L(Test): 0.2972036302089691\n",
            "Epoch 6928/10000: L(Train): 0.3259497284889221; L(Test): 0.29667040705680847\n",
            "Epoch 6929/10000: L(Train): 0.3138454854488373; L(Test): 0.29613909125328064\n",
            "Epoch 6930/10000: L(Train): 0.3239821195602417; L(Test): 0.2964453399181366\n",
            "Epoch 6931/10000: L(Train): 0.3319040536880493; L(Test): 0.29745909571647644\n",
            "Epoch 6932/10000: L(Train): 0.3276160955429077; L(Test): 0.2958417534828186\n",
            "Epoch 6933/10000: L(Train): 0.3123687207698822; L(Test): 0.29704707860946655\n",
            "Epoch 6934/10000: L(Train): 0.3233702778816223; L(Test): 0.29751601815223694\n",
            "Epoch 6935/10000: L(Train): 0.3187871277332306; L(Test): 0.29634690284729004\n",
            "Epoch 6936/10000: L(Train): 0.3285386562347412; L(Test): 0.2965071499347687\n",
            "Epoch 6937/10000: L(Train): 0.3237476944923401; L(Test): 0.2968003749847412\n",
            "Epoch 6938/10000: L(Train): 0.3257642686367035; L(Test): 0.29567214846611023\n",
            "Epoch 6939/10000: L(Train): 0.31332892179489136; L(Test): 0.29617610573768616\n",
            "Epoch 6940/10000: L(Train): 0.3256415128707886; L(Test): 0.2981780171394348\n",
            "Epoch 6941/10000: L(Train): 0.32805338501930237; L(Test): 0.29654431343078613\n",
            "Epoch 6942/10000: L(Train): 0.32334721088409424; L(Test): 0.2953990399837494\n",
            "Epoch 6943/10000: L(Train): 0.3234589099884033; L(Test): 0.29614025354385376\n",
            "Epoch 6944/10000: L(Train): 0.3141782879829407; L(Test): 0.2956233024597168\n",
            "Epoch 6945/10000: L(Train): 0.32966235280036926; L(Test): 0.29888200759887695\n",
            "Epoch 6946/10000: L(Train): 0.32713431119918823; L(Test): 0.301017701625824\n",
            "Epoch 6947/10000: L(Train): 0.32515057921409607; L(Test): 0.2986481785774231\n",
            "Epoch 6948/10000: L(Train): 0.3249662220478058; L(Test): 0.2974821627140045\n",
            "Epoch 6949/10000: L(Train): 0.32256656885147095; L(Test): 0.3001275062561035\n",
            "Epoch 6950/10000: L(Train): 0.3222096860408783; L(Test): 0.2995920479297638\n",
            "Epoch 6951/10000: L(Train): 0.32818061113357544; L(Test): 0.2999321520328522\n",
            "Epoch 6952/10000: L(Train): 0.32899340987205505; L(Test): 0.3036399483680725\n",
            "Epoch 6953/10000: L(Train): 0.31904932856559753; L(Test): 0.30212298035621643\n",
            "Epoch 6954/10000: L(Train): 0.32246097922325134; L(Test): 0.2997678816318512\n",
            "Epoch 6955/10000: L(Train): 0.3286740779876709; L(Test): 0.3008471727371216\n",
            "Epoch 6956/10000: L(Train): 0.32134348154067993; L(Test): 0.2983621060848236\n",
            "Epoch 6957/10000: L(Train): 0.31920814514160156; L(Test): 0.2978168725967407\n",
            "Epoch 6958/10000: L(Train): 0.3288397789001465; L(Test): 0.2986850142478943\n",
            "Epoch 6959/10000: L(Train): 0.32059723138809204; L(Test): 0.29734504222869873\n",
            "Epoch 6960/10000: L(Train): 0.32206982374191284; L(Test): 0.29627278447151184\n",
            "Epoch 6961/10000: L(Train): 0.3229500353336334; L(Test): 0.29657989740371704\n",
            "Epoch 6962/10000: L(Train): 0.31711292266845703; L(Test): 0.2968502640724182\n",
            "Epoch 6963/10000: L(Train): 0.3220793306827545; L(Test): 0.29663947224617004\n",
            "Epoch 6964/10000: L(Train): 0.3231053352355957; L(Test): 0.29730623960494995\n",
            "Epoch 6965/10000: L(Train): 0.3146347105503082; L(Test): 0.2972150444984436\n",
            "Epoch 6966/10000: L(Train): 0.3167174458503723; L(Test): 0.29646801948547363\n",
            "Epoch 6967/10000: L(Train): 0.3263208866119385; L(Test): 0.29592379927635193\n",
            "Epoch 6968/10000: L(Train): 0.3197764456272125; L(Test): 0.29595568776130676\n",
            "Epoch 6969/10000: L(Train): 0.3195751905441284; L(Test): 0.29572758078575134\n",
            "Epoch 6970/10000: L(Train): 0.3197900950908661; L(Test): 0.2954641282558441\n",
            "Epoch 6971/10000: L(Train): 0.32111626863479614; L(Test): 0.29441386461257935\n",
            "Epoch 6972/10000: L(Train): 0.324319988489151; L(Test): 0.2947866916656494\n",
            "Epoch 6973/10000: L(Train): 0.32398340106010437; L(Test): 0.2954760491847992\n",
            "Epoch 6974/10000: L(Train): 0.31890374422073364; L(Test): 0.2945699095726013\n",
            "Epoch 6975/10000: L(Train): 0.29875651001930237; L(Test): 0.29442715644836426\n",
            "Epoch 6976/10000: L(Train): 0.31829842925071716; L(Test): 0.2949567139148712\n",
            "Epoch 6977/10000: L(Train): 0.3202803432941437; L(Test): 0.2942374646663666\n",
            "Epoch 6978/10000: L(Train): 0.3223743438720703; L(Test): 0.2933640480041504\n",
            "Epoch 6979/10000: L(Train): 0.31404909491539; L(Test): 0.29346203804016113\n",
            "Epoch 6980/10000: L(Train): 0.3128211498260498; L(Test): 0.29393622279167175\n",
            "Epoch 6981/10000: L(Train): 0.3168671131134033; L(Test): 0.2936829626560211\n",
            "Epoch 6982/10000: L(Train): 0.3210524618625641; L(Test): 0.29338574409484863\n",
            "Epoch 6983/10000: L(Train): 0.31687262654304504; L(Test): 0.29376867413520813\n",
            "Epoch 6984/10000: L(Train): 0.32681554555892944; L(Test): 0.29338395595550537\n",
            "Epoch 6985/10000: L(Train): 0.31986749172210693; L(Test): 0.29266658425331116\n",
            "Epoch 6986/10000: L(Train): 0.32268670201301575; L(Test): 0.29253360629081726\n",
            "Epoch 6987/10000: L(Train): 0.31627190113067627; L(Test): 0.29208630323410034\n",
            "Epoch 6988/10000: L(Train): 0.3138251304626465; L(Test): 0.2918350398540497\n",
            "Epoch 6989/10000: L(Train): 0.31620165705680847; L(Test): 0.2920278012752533\n",
            "Epoch 6990/10000: L(Train): 0.3159446120262146; L(Test): 0.2922261655330658\n",
            "Epoch 6991/10000: L(Train): 0.3209172189235687; L(Test): 0.292404443025589\n",
            "Epoch 6992/10000: L(Train): 0.3090304732322693; L(Test): 0.29250845313072205\n",
            "Epoch 6993/10000: L(Train): 0.3228663504123688; L(Test): 0.2927790582180023\n",
            "Epoch 6994/10000: L(Train): 0.317269891500473; L(Test): 0.29195308685302734\n",
            "Epoch 6995/10000: L(Train): 0.3224480450153351; L(Test): 0.29217296838760376\n",
            "Epoch 6996/10000: L(Train): 0.3228839635848999; L(Test): 0.2927766442298889\n",
            "Epoch 6997/10000: L(Train): 0.32525017857551575; L(Test): 0.29369044303894043\n",
            "Epoch 6998/10000: L(Train): 0.31727731227874756; L(Test): 0.29466965794563293\n",
            "Epoch 6999/10000: L(Train): 0.31958216428756714; L(Test): 0.29463234543800354\n",
            "Epoch 7000/10000: L(Train): 0.3188154697418213; L(Test): 0.2943568825721741\n",
            "Epoch 7001/10000: L(Train): 0.3140007257461548; L(Test): 0.2947228252887726\n",
            "Epoch 7002/10000: L(Train): 0.31070950627326965; L(Test): 0.2949102520942688\n",
            "Epoch 7003/10000: L(Train): 0.3153296113014221; L(Test): 0.29468420147895813\n",
            "Epoch 7004/10000: L(Train): 0.308409184217453; L(Test): 0.29453080892562866\n",
            "Epoch 7005/10000: L(Train): 0.3236567974090576; L(Test): 0.2939232587814331\n",
            "Epoch 7006/10000: L(Train): 0.3257734775543213; L(Test): 0.29350000619888306\n",
            "Epoch 7007/10000: L(Train): 0.3253757059574127; L(Test): 0.2938055992126465\n",
            "Epoch 7008/10000: L(Train): 0.3229007124900818; L(Test): 0.29484084248542786\n",
            "Epoch 7009/10000: L(Train): 0.31915345788002014; L(Test): 0.29543083906173706\n",
            "Epoch 7010/10000: L(Train): 0.3139016032218933; L(Test): 0.295040488243103\n",
            "Epoch 7011/10000: L(Train): 0.324495792388916; L(Test): 0.2949327528476715\n",
            "Epoch 7012/10000: L(Train): 0.3204023838043213; L(Test): 0.2955349385738373\n",
            "Epoch 7013/10000: L(Train): 0.31002160906791687; L(Test): 0.2963433861732483\n",
            "Epoch 7014/10000: L(Train): 0.3143739104270935; L(Test): 0.29685676097869873\n",
            "Epoch 7015/10000: L(Train): 0.32746192812919617; L(Test): 0.2961881458759308\n",
            "Epoch 7016/10000: L(Train): 0.32272258400917053; L(Test): 0.29687029123306274\n",
            "Epoch 7017/10000: L(Train): 0.32822543382644653; L(Test): 0.2973245680332184\n",
            "Epoch 7018/10000: L(Train): 0.31860652565956116; L(Test): 0.296829491853714\n",
            "Epoch 7019/10000: L(Train): 0.3264564275741577; L(Test): 0.2965203523635864\n",
            "Epoch 7020/10000: L(Train): 0.3224305212497711; L(Test): 0.2978385388851166\n",
            "Epoch 7021/10000: L(Train): 0.3169957399368286; L(Test): 0.2976941168308258\n",
            "Epoch 7022/10000: L(Train): 0.32341742515563965; L(Test): 0.30016282200813293\n",
            "Epoch 7023/10000: L(Train): 0.319457083940506; L(Test): 0.29902663826942444\n",
            "Epoch 7024/10000: L(Train): 0.3219623267650604; L(Test): 0.2972879111766815\n",
            "Epoch 7025/10000: L(Train): 0.32662129402160645; L(Test): 0.30007535219192505\n",
            "Epoch 7026/10000: L(Train): 0.3234037756919861; L(Test): 0.30145585536956787\n",
            "Epoch 7027/10000: L(Train): 0.3330644965171814; L(Test): 0.2982002794742584\n",
            "Epoch 7028/10000: L(Train): 0.32496511936187744; L(Test): 0.29801785945892334\n",
            "Epoch 7029/10000: L(Train): 0.32121723890304565; L(Test): 0.2996205687522888\n",
            "Epoch 7030/10000: L(Train): 0.3178812563419342; L(Test): 0.2973763644695282\n",
            "Epoch 7031/10000: L(Train): 0.32677990198135376; L(Test): 0.29767662286758423\n",
            "Epoch 7032/10000: L(Train): 0.32877933979034424; L(Test): 0.29989713430404663\n",
            "Epoch 7033/10000: L(Train): 0.3304770588874817; L(Test): 0.29880961775779724\n",
            "Epoch 7034/10000: L(Train): 0.32249343395233154; L(Test): 0.2958788573741913\n",
            "Epoch 7035/10000: L(Train): 0.32059475779533386; L(Test): 0.2956068813800812\n",
            "Epoch 7036/10000: L(Train): 0.3146025538444519; L(Test): 0.29696470499038696\n",
            "Epoch 7037/10000: L(Train): 0.32581889629364014; L(Test): 0.29716962575912476\n",
            "Epoch 7038/10000: L(Train): 0.3188263177871704; L(Test): 0.29648357629776\n",
            "Epoch 7039/10000: L(Train): 0.31963875889778137; L(Test): 0.29720497131347656\n",
            "Epoch 7040/10000: L(Train): 0.3167007565498352; L(Test): 0.2978072166442871\n",
            "Epoch 7041/10000: L(Train): 0.31967654824256897; L(Test): 0.29649266600608826\n",
            "Epoch 7042/10000: L(Train): 0.32133403420448303; L(Test): 0.29623284935951233\n",
            "Epoch 7043/10000: L(Train): 0.3162507116794586; L(Test): 0.2969493269920349\n",
            "Epoch 7044/10000: L(Train): 0.3189510107040405; L(Test): 0.2965013384819031\n",
            "Epoch 7045/10000: L(Train): 0.3190276026725769; L(Test): 0.2969360947608948\n",
            "Epoch 7046/10000: L(Train): 0.32606226205825806; L(Test): 0.2963702976703644\n",
            "Epoch 7047/10000: L(Train): 0.3147798478603363; L(Test): 0.29571533203125\n",
            "Epoch 7048/10000: L(Train): 0.31231600046157837; L(Test): 0.29739639163017273\n",
            "Epoch 7049/10000: L(Train): 0.3187454342842102; L(Test): 0.2972393333911896\n",
            "Epoch 7050/10000: L(Train): 0.3186015486717224; L(Test): 0.2973919212818146\n",
            "Epoch 7051/10000: L(Train): 0.31369948387145996; L(Test): 0.29717934131622314\n",
            "Epoch 7052/10000: L(Train): 0.3299216628074646; L(Test): 0.2961297631263733\n",
            "Epoch 7053/10000: L(Train): 0.32215315103530884; L(Test): 0.29544299840927124\n",
            "Epoch 7054/10000: L(Train): 0.3143330514431; L(Test): 0.2956092059612274\n",
            "Epoch 7055/10000: L(Train): 0.3236178457736969; L(Test): 0.2945318818092346\n",
            "Epoch 7056/10000: L(Train): 0.31937354803085327; L(Test): 0.29476654529571533\n",
            "Epoch 7057/10000: L(Train): 0.3144190311431885; L(Test): 0.29647186398506165\n",
            "Epoch 7058/10000: L(Train): 0.32141515612602234; L(Test): 0.29627352952957153\n",
            "Epoch 7059/10000: L(Train): 0.3151018023490906; L(Test): 0.2955036759376526\n",
            "Epoch 7060/10000: L(Train): 0.31981930136680603; L(Test): 0.2956317067146301\n",
            "Epoch 7061/10000: L(Train): 0.325580358505249; L(Test): 0.29466864466667175\n",
            "Epoch 7062/10000: L(Train): 0.3205922842025757; L(Test): 0.29463911056518555\n",
            "Epoch 7063/10000: L(Train): 0.32967114448547363; L(Test): 0.2946986258029938\n",
            "Epoch 7064/10000: L(Train): 0.32531285285949707; L(Test): 0.2946564555168152\n",
            "Epoch 7065/10000: L(Train): 0.31797999143600464; L(Test): 0.294909805059433\n",
            "Epoch 7066/10000: L(Train): 0.32910633087158203; L(Test): 0.29526379704475403\n",
            "Epoch 7067/10000: L(Train): 0.3182998299598694; L(Test): 0.2953614592552185\n",
            "Epoch 7068/10000: L(Train): 0.32852596044540405; L(Test): 0.2959313690662384\n",
            "Epoch 7069/10000: L(Train): 0.3178495168685913; L(Test): 0.2970658838748932\n",
            "Epoch 7070/10000: L(Train): 0.3191416561603546; L(Test): 0.2960565388202667\n",
            "Epoch 7071/10000: L(Train): 0.3156246542930603; L(Test): 0.29659324884414673\n",
            "Epoch 7072/10000: L(Train): 0.31874004006385803; L(Test): 0.2960628569126129\n",
            "Epoch 7073/10000: L(Train): 0.3184647262096405; L(Test): 0.29496073722839355\n",
            "Epoch 7074/10000: L(Train): 0.32214125990867615; L(Test): 0.2960667908191681\n",
            "Epoch 7075/10000: L(Train): 0.3146175146102905; L(Test): 0.29534831643104553\n",
            "Epoch 7076/10000: L(Train): 0.3174425959587097; L(Test): 0.29483309388160706\n",
            "Epoch 7077/10000: L(Train): 0.3212466835975647; L(Test): 0.29497596621513367\n",
            "Epoch 7078/10000: L(Train): 0.32223430275917053; L(Test): 0.29471877217292786\n",
            "Epoch 7079/10000: L(Train): 0.3093841075897217; L(Test): 0.2940933108329773\n",
            "Epoch 7080/10000: L(Train): 0.3260355591773987; L(Test): 0.29489609599113464\n",
            "Epoch 7081/10000: L(Train): 0.3181096911430359; L(Test): 0.2954164445400238\n",
            "Epoch 7082/10000: L(Train): 0.3156795799732208; L(Test): 0.2949265241622925\n",
            "Epoch 7083/10000: L(Train): 0.3166405260562897; L(Test): 0.2951064705848694\n",
            "Epoch 7084/10000: L(Train): 0.32269954681396484; L(Test): 0.2958897650241852\n",
            "Epoch 7085/10000: L(Train): 0.3172813653945923; L(Test): 0.29533231258392334\n",
            "Epoch 7086/10000: L(Train): 0.32325470447540283; L(Test): 0.29545825719833374\n",
            "Epoch 7087/10000: L(Train): 0.32457414269447327; L(Test): 0.29640576243400574\n",
            "Epoch 7088/10000: L(Train): 0.3211993873119354; L(Test): 0.2949376404285431\n",
            "Epoch 7089/10000: L(Train): 0.3145541250705719; L(Test): 0.29583021998405457\n",
            "Epoch 7090/10000: L(Train): 0.3139921724796295; L(Test): 0.2962149977684021\n",
            "Epoch 7091/10000: L(Train): 0.32443568110466003; L(Test): 0.29608190059661865\n",
            "Epoch 7092/10000: L(Train): 0.32387927174568176; L(Test): 0.2957356572151184\n",
            "Epoch 7093/10000: L(Train): 0.312299519777298; L(Test): 0.29677897691726685\n",
            "Epoch 7094/10000: L(Train): 0.3212144076824188; L(Test): 0.2972143888473511\n",
            "Epoch 7095/10000: L(Train): 0.3230705261230469; L(Test): 0.29763349890708923\n",
            "Epoch 7096/10000: L(Train): 0.32504093647003174; L(Test): 0.2972145676612854\n",
            "Epoch 7097/10000: L(Train): 0.3259572386741638; L(Test): 0.2963387668132782\n",
            "Epoch 7098/10000: L(Train): 0.3208552300930023; L(Test): 0.29757124185562134\n",
            "Epoch 7099/10000: L(Train): 0.3194518983364105; L(Test): 0.2974802851676941\n",
            "Epoch 7100/10000: L(Train): 0.31757619976997375; L(Test): 0.2966751158237457\n",
            "Epoch 7101/10000: L(Train): 0.3206745982170105; L(Test): 0.296476274728775\n",
            "Epoch 7102/10000: L(Train): 0.33007287979125977; L(Test): 0.2956335246562958\n",
            "Epoch 7103/10000: L(Train): 0.3151988983154297; L(Test): 0.29541102051734924\n",
            "Epoch 7104/10000: L(Train): 0.3168407380580902; L(Test): 0.2954137623310089\n",
            "Epoch 7105/10000: L(Train): 0.31439948081970215; L(Test): 0.2955184876918793\n",
            "Epoch 7106/10000: L(Train): 0.3187982738018036; L(Test): 0.2957739531993866\n",
            "Epoch 7107/10000: L(Train): 0.31528565287590027; L(Test): 0.29604482650756836\n",
            "Epoch 7108/10000: L(Train): 0.32391995191574097; L(Test): 0.2953476011753082\n",
            "Epoch 7109/10000: L(Train): 0.3233584761619568; L(Test): 0.29459089040756226\n",
            "Epoch 7110/10000: L(Train): 0.3238454759120941; L(Test): 0.2943817377090454\n",
            "Epoch 7111/10000: L(Train): 0.31684035062789917; L(Test): 0.2945265769958496\n",
            "Epoch 7112/10000: L(Train): 0.3177976608276367; L(Test): 0.29523754119873047\n",
            "Epoch 7113/10000: L(Train): 0.31732502579689026; L(Test): 0.2956741452217102\n",
            "Epoch 7114/10000: L(Train): 0.3189045786857605; L(Test): 0.2942080795764923\n",
            "Epoch 7115/10000: L(Train): 0.3230017125606537; L(Test): 0.29428547620773315\n",
            "Epoch 7116/10000: L(Train): 0.3249998688697815; L(Test): 0.2944144308567047\n",
            "Epoch 7117/10000: L(Train): 0.32051917910575867; L(Test): 0.2936359643936157\n",
            "Epoch 7118/10000: L(Train): 0.3220480978488922; L(Test): 0.2940731346607208\n",
            "Epoch 7119/10000: L(Train): 0.30738940834999084; L(Test): 0.29394587874412537\n",
            "Epoch 7120/10000: L(Train): 0.31688085198402405; L(Test): 0.29338303208351135\n",
            "Epoch 7121/10000: L(Train): 0.3201753795146942; L(Test): 0.29451656341552734\n",
            "Epoch 7122/10000: L(Train): 0.3151342272758484; L(Test): 0.2958260178565979\n",
            "Epoch 7123/10000: L(Train): 0.31263014674186707; L(Test): 0.2948084771633148\n",
            "Epoch 7124/10000: L(Train): 0.32108184695243835; L(Test): 0.2943227291107178\n",
            "Epoch 7125/10000: L(Train): 0.3177014887332916; L(Test): 0.2948273718357086\n",
            "Epoch 7126/10000: L(Train): 0.32044869661331177; L(Test): 0.2942226827144623\n",
            "Epoch 7127/10000: L(Train): 0.3149246573448181; L(Test): 0.2946494221687317\n",
            "Epoch 7128/10000: L(Train): 0.3288988769054413; L(Test): 0.29430148005485535\n",
            "Epoch 7129/10000: L(Train): 0.31372910737991333; L(Test): 0.2940816283226013\n",
            "Epoch 7130/10000: L(Train): 0.31704577803611755; L(Test): 0.29611697793006897\n",
            "Epoch 7131/10000: L(Train): 0.32104918360710144; L(Test): 0.295308917760849\n",
            "Epoch 7132/10000: L(Train): 0.3203100860118866; L(Test): 0.2950535714626312\n",
            "Epoch 7133/10000: L(Train): 0.324888676404953; L(Test): 0.29486367106437683\n",
            "Epoch 7134/10000: L(Train): 0.3217238485813141; L(Test): 0.29481709003448486\n",
            "Epoch 7135/10000: L(Train): 0.326759397983551; L(Test): 0.29587477445602417\n",
            "Epoch 7136/10000: L(Train): 0.31772172451019287; L(Test): 0.29509615898132324\n",
            "Epoch 7137/10000: L(Train): 0.32311660051345825; L(Test): 0.2953178286552429\n",
            "Epoch 7138/10000: L(Train): 0.3107846975326538; L(Test): 0.29516324400901794\n",
            "Epoch 7139/10000: L(Train): 0.32292062044143677; L(Test): 0.29481664299964905\n",
            "Epoch 7140/10000: L(Train): 0.3196287751197815; L(Test): 0.2953985333442688\n",
            "Epoch 7141/10000: L(Train): 0.3190211057662964; L(Test): 0.2962466776371002\n",
            "Epoch 7142/10000: L(Train): 0.3283626437187195; L(Test): 0.29491472244262695\n",
            "Epoch 7143/10000: L(Train): 0.31980544328689575; L(Test): 0.2952728867530823\n",
            "Epoch 7144/10000: L(Train): 0.32121962308883667; L(Test): 0.29454243183135986\n",
            "Epoch 7145/10000: L(Train): 0.31088390946388245; L(Test): 0.29444846510887146\n",
            "Epoch 7146/10000: L(Train): 0.3322092890739441; L(Test): 0.29496780037879944\n",
            "Epoch 7147/10000: L(Train): 0.32179927825927734; L(Test): 0.2951677739620209\n",
            "Epoch 7148/10000: L(Train): 0.31183239817619324; L(Test): 0.2948230803012848\n",
            "Epoch 7149/10000: L(Train): 0.3156784176826477; L(Test): 0.2944580018520355\n",
            "Epoch 7150/10000: L(Train): 0.3188498914241791; L(Test): 0.2949618101119995\n",
            "Epoch 7151/10000: L(Train): 0.31726205348968506; L(Test): 0.2947843670845032\n",
            "Epoch 7152/10000: L(Train): 0.31635236740112305; L(Test): 0.2934976816177368\n",
            "Epoch 7153/10000: L(Train): 0.32073765993118286; L(Test): 0.2938605546951294\n",
            "Epoch 7154/10000: L(Train): 0.3156171441078186; L(Test): 0.2954722046852112\n",
            "Epoch 7155/10000: L(Train): 0.3234643340110779; L(Test): 0.2970949113368988\n",
            "Epoch 7156/10000: L(Train): 0.3185211420059204; L(Test): 0.2961038053035736\n",
            "Epoch 7157/10000: L(Train): 0.32276782393455505; L(Test): 0.29490983486175537\n",
            "Epoch 7158/10000: L(Train): 0.324590265750885; L(Test): 0.29638174176216125\n",
            "Epoch 7159/10000: L(Train): 0.3186579644680023; L(Test): 0.2969461679458618\n",
            "Epoch 7160/10000: L(Train): 0.32126352190971375; L(Test): 0.29559192061424255\n",
            "Epoch 7161/10000: L(Train): 0.327973335981369; L(Test): 0.2954552471637726\n",
            "Epoch 7162/10000: L(Train): 0.3284922242164612; L(Test): 0.2964790463447571\n",
            "Epoch 7163/10000: L(Train): 0.3232741355895996; L(Test): 0.29637986421585083\n",
            "Epoch 7164/10000: L(Train): 0.32675570249557495; L(Test): 0.2967020273208618\n",
            "Epoch 7165/10000: L(Train): 0.3181357979774475; L(Test): 0.29742762446403503\n",
            "Epoch 7166/10000: L(Train): 0.3209841549396515; L(Test): 0.2964209318161011\n",
            "Epoch 7167/10000: L(Train): 0.31342238187789917; L(Test): 0.2965492308139801\n",
            "Epoch 7168/10000: L(Train): 0.31877216696739197; L(Test): 0.29530659317970276\n",
            "Epoch 7169/10000: L(Train): 0.31868624687194824; L(Test): 0.2947138845920563\n",
            "Epoch 7170/10000: L(Train): 0.3136308491230011; L(Test): 0.296070396900177\n",
            "Epoch 7171/10000: L(Train): 0.3175334334373474; L(Test): 0.29571276903152466\n",
            "Epoch 7172/10000: L(Train): 0.31602656841278076; L(Test): 0.2955684959888458\n",
            "Epoch 7173/10000: L(Train): 0.31839457154273987; L(Test): 0.296818345785141\n",
            "Epoch 7174/10000: L(Train): 0.32339969277381897; L(Test): 0.29646730422973633\n",
            "Epoch 7175/10000: L(Train): 0.3135852515697479; L(Test): 0.29582300782203674\n",
            "Epoch 7176/10000: L(Train): 0.3160361349582672; L(Test): 0.2960303723812103\n",
            "Epoch 7177/10000: L(Train): 0.3252445459365845; L(Test): 0.2969472110271454\n",
            "Epoch 7178/10000: L(Train): 0.31786203384399414; L(Test): 0.2957184910774231\n",
            "Epoch 7179/10000: L(Train): 0.31728601455688477; L(Test): 0.29470133781433105\n",
            "Epoch 7180/10000: L(Train): 0.3151787221431732; L(Test): 0.2943092882633209\n",
            "Epoch 7181/10000: L(Train): 0.3254106640815735; L(Test): 0.2946348190307617\n",
            "Epoch 7182/10000: L(Train): 0.3167576193809509; L(Test): 0.29585522413253784\n",
            "Epoch 7183/10000: L(Train): 0.31516119837760925; L(Test): 0.2956715226173401\n",
            "Epoch 7184/10000: L(Train): 0.32950058579444885; L(Test): 0.29522913694381714\n",
            "Epoch 7185/10000: L(Train): 0.3171769380569458; L(Test): 0.29552626609802246\n",
            "Epoch 7186/10000: L(Train): 0.31579071283340454; L(Test): 0.293856680393219\n",
            "Epoch 7187/10000: L(Train): 0.3112448453903198; L(Test): 0.29462170600891113\n",
            "Epoch 7188/10000: L(Train): 0.32329028844833374; L(Test): 0.2971481382846832\n",
            "Epoch 7189/10000: L(Train): 0.30712589621543884; L(Test): 0.2957773804664612\n",
            "Epoch 7190/10000: L(Train): 0.315876841545105; L(Test): 0.29380133748054504\n",
            "Epoch 7191/10000: L(Train): 0.31309452652931213; L(Test): 0.2941303253173828\n",
            "Epoch 7192/10000: L(Train): 0.30940210819244385; L(Test): 0.2944711744785309\n",
            "Epoch 7193/10000: L(Train): 0.3229552209377289; L(Test): 0.2932831943035126\n",
            "Epoch 7194/10000: L(Train): 0.31726253032684326; L(Test): 0.29484549164772034\n",
            "Epoch 7195/10000: L(Train): 0.31577596068382263; L(Test): 0.2945522964000702\n",
            "Epoch 7196/10000: L(Train): 0.3253597021102905; L(Test): 0.29356732964515686\n",
            "Epoch 7197/10000: L(Train): 0.30708351731300354; L(Test): 0.2939600646495819\n",
            "Epoch 7198/10000: L(Train): 0.3226507008075714; L(Test): 0.29388344287872314\n",
            "Epoch 7199/10000: L(Train): 0.3245542645454407; L(Test): 0.29340893030166626\n",
            "Epoch 7200/10000: L(Train): 0.3092261552810669; L(Test): 0.2942062020301819\n",
            "Epoch 7201/10000: L(Train): 0.31916481256484985; L(Test): 0.2942934036254883\n",
            "Epoch 7202/10000: L(Train): 0.31107982993125916; L(Test): 0.2937566936016083\n",
            "Epoch 7203/10000: L(Train): 0.3195532560348511; L(Test): 0.2925688326358795\n",
            "Epoch 7204/10000: L(Train): 0.31793221831321716; L(Test): 0.29183030128479004\n",
            "Epoch 7205/10000: L(Train): 0.3143502175807953; L(Test): 0.2919901907444\n",
            "Epoch 7206/10000: L(Train): 0.31611010432243347; L(Test): 0.2923130691051483\n",
            "Epoch 7207/10000: L(Train): 0.32203179597854614; L(Test): 0.29248175024986267\n",
            "Epoch 7208/10000: L(Train): 0.31113937497138977; L(Test): 0.2931535840034485\n",
            "Epoch 7209/10000: L(Train): 0.32577845454216003; L(Test): 0.2933078110218048\n",
            "Epoch 7210/10000: L(Train): 0.33027610182762146; L(Test): 0.2925143837928772\n",
            "Epoch 7211/10000: L(Train): 0.309234619140625; L(Test): 0.2935870587825775\n",
            "Epoch 7212/10000: L(Train): 0.32193106412887573; L(Test): 0.2941179573535919\n",
            "Epoch 7213/10000: L(Train): 0.3183303475379944; L(Test): 0.2939378023147583\n",
            "Epoch 7214/10000: L(Train): 0.3170766532421112; L(Test): 0.29398006200790405\n",
            "Epoch 7215/10000: L(Train): 0.3177095651626587; L(Test): 0.29523470997810364\n",
            "Epoch 7216/10000: L(Train): 0.30726003646850586; L(Test): 0.29618316888809204\n",
            "Epoch 7217/10000: L(Train): 0.32972452044487; L(Test): 0.2952941656112671\n",
            "Epoch 7218/10000: L(Train): 0.3199116289615631; L(Test): 0.29552504420280457\n",
            "Epoch 7219/10000: L(Train): 0.31175464391708374; L(Test): 0.2954660952091217\n",
            "Epoch 7220/10000: L(Train): 0.3146185278892517; L(Test): 0.2945082187652588\n",
            "Epoch 7221/10000: L(Train): 0.3267843425273895; L(Test): 0.29462292790412903\n",
            "Epoch 7222/10000: L(Train): 0.3093312978744507; L(Test): 0.29529082775115967\n",
            "Epoch 7223/10000: L(Train): 0.32335710525512695; L(Test): 0.2961234748363495\n",
            "Epoch 7224/10000: L(Train): 0.322003036737442; L(Test): 0.2949516177177429\n",
            "Epoch 7225/10000: L(Train): 0.32396402955055237; L(Test): 0.2945462465286255\n",
            "Epoch 7226/10000: L(Train): 0.31467577815055847; L(Test): 0.2955949306488037\n",
            "Epoch 7227/10000: L(Train): 0.3232586979866028; L(Test): 0.2960922420024872\n",
            "Epoch 7228/10000: L(Train): 0.3151276707649231; L(Test): 0.2965807616710663\n",
            "Epoch 7229/10000: L(Train): 0.32607460021972656; L(Test): 0.2969392240047455\n",
            "Epoch 7230/10000: L(Train): 0.3209105134010315; L(Test): 0.2969532310962677\n",
            "Epoch 7231/10000: L(Train): 0.32672935724258423; L(Test): 0.2967122197151184\n",
            "Epoch 7232/10000: L(Train): 0.3168855607509613; L(Test): 0.29662707448005676\n",
            "Epoch 7233/10000: L(Train): 0.32200077176094055; L(Test): 0.29654377698898315\n",
            "Epoch 7234/10000: L(Train): 0.30890771746635437; L(Test): 0.2969139516353607\n",
            "Epoch 7235/10000: L(Train): 0.31940895318984985; L(Test): 0.29770392179489136\n",
            "Epoch 7236/10000: L(Train): 0.31914272904396057; L(Test): 0.2975347638130188\n",
            "Epoch 7237/10000: L(Train): 0.31916868686676025; L(Test): 0.2965948283672333\n",
            "Epoch 7238/10000: L(Train): 0.3215371072292328; L(Test): 0.29701322317123413\n",
            "Epoch 7239/10000: L(Train): 0.3206903040409088; L(Test): 0.2953784763813019\n",
            "Epoch 7240/10000: L(Train): 0.30843672156333923; L(Test): 0.2953372895717621\n",
            "Epoch 7241/10000: L(Train): 0.32956796884536743; L(Test): 0.2954499423503876\n",
            "Epoch 7242/10000: L(Train): 0.3205646574497223; L(Test): 0.29502129554748535\n",
            "Epoch 7243/10000: L(Train): 0.317438006401062; L(Test): 0.29556819796562195\n",
            "Epoch 7244/10000: L(Train): 0.3229985535144806; L(Test): 0.2964659333229065\n",
            "Epoch 7245/10000: L(Train): 0.3219800591468811; L(Test): 0.2970173954963684\n",
            "Epoch 7246/10000: L(Train): 0.31806647777557373; L(Test): 0.29647505283355713\n",
            "Epoch 7247/10000: L(Train): 0.33124953508377075; L(Test): 0.29579073190689087\n",
            "Epoch 7248/10000: L(Train): 0.3118141293525696; L(Test): 0.296011745929718\n",
            "Epoch 7249/10000: L(Train): 0.3199160695075989; L(Test): 0.2966110110282898\n",
            "Epoch 7250/10000: L(Train): 0.3214288353919983; L(Test): 0.2952911853790283\n",
            "Epoch 7251/10000: L(Train): 0.3187214136123657; L(Test): 0.2954326868057251\n",
            "Epoch 7252/10000: L(Train): 0.3269403278827667; L(Test): 0.29558417201042175\n",
            "Epoch 7253/10000: L(Train): 0.31662872433662415; L(Test): 0.2949315905570984\n",
            "Epoch 7254/10000: L(Train): 0.31548792123794556; L(Test): 0.29629233479499817\n",
            "Epoch 7255/10000: L(Train): 0.31741762161254883; L(Test): 0.29609042406082153\n",
            "Epoch 7256/10000: L(Train): 0.3159979581832886; L(Test): 0.29473254084587097\n",
            "Epoch 7257/10000: L(Train): 0.3127826750278473; L(Test): 0.2939678728580475\n",
            "Epoch 7258/10000: L(Train): 0.3211882710456848; L(Test): 0.2946797311306\n",
            "Epoch 7259/10000: L(Train): 0.3191520571708679; L(Test): 0.29446542263031006\n",
            "Epoch 7260/10000: L(Train): 0.31826356053352356; L(Test): 0.2947584092617035\n",
            "Epoch 7261/10000: L(Train): 0.3204457461833954; L(Test): 0.2938477396965027\n",
            "Epoch 7262/10000: L(Train): 0.3258135914802551; L(Test): 0.2938336431980133\n",
            "Epoch 7263/10000: L(Train): 0.32233303785324097; L(Test): 0.29435887932777405\n",
            "Epoch 7264/10000: L(Train): 0.3060882091522217; L(Test): 0.29467064142227173\n",
            "Epoch 7265/10000: L(Train): 0.317927747964859; L(Test): 0.29446324706077576\n",
            "Epoch 7266/10000: L(Train): 0.3175250291824341; L(Test): 0.29440173506736755\n",
            "Epoch 7267/10000: L(Train): 0.32072246074676514; L(Test): 0.2936684191226959\n",
            "Epoch 7268/10000: L(Train): 0.3172237277030945; L(Test): 0.29494887590408325\n",
            "Epoch 7269/10000: L(Train): 0.31319093704223633; L(Test): 0.2941570580005646\n",
            "Epoch 7270/10000: L(Train): 0.3145482838153839; L(Test): 0.2937774658203125\n",
            "Epoch 7271/10000: L(Train): 0.31668955087661743; L(Test): 0.2936285436153412\n",
            "Epoch 7272/10000: L(Train): 0.31426572799682617; L(Test): 0.29375696182250977\n",
            "Epoch 7273/10000: L(Train): 0.32390204071998596; L(Test): 0.2938062250614166\n",
            "Epoch 7274/10000: L(Train): 0.3227253556251526; L(Test): 0.29367223381996155\n",
            "Epoch 7275/10000: L(Train): 0.32311511039733887; L(Test): 0.2939023971557617\n",
            "Epoch 7276/10000: L(Train): 0.3209454417228699; L(Test): 0.2930888533592224\n",
            "Epoch 7277/10000: L(Train): 0.31675660610198975; L(Test): 0.2929973602294922\n",
            "Epoch 7278/10000: L(Train): 0.3157561719417572; L(Test): 0.29303404688835144\n",
            "Epoch 7279/10000: L(Train): 0.32561779022216797; L(Test): 0.29332491755485535\n",
            "Epoch 7280/10000: L(Train): 0.31479060649871826; L(Test): 0.29340484738349915\n",
            "Epoch 7281/10000: L(Train): 0.3206884264945984; L(Test): 0.29366081953048706\n",
            "Epoch 7282/10000: L(Train): 0.3138794004917145; L(Test): 0.2935641407966614\n",
            "Epoch 7283/10000: L(Train): 0.32018449902534485; L(Test): 0.2930833101272583\n",
            "Epoch 7284/10000: L(Train): 0.3218848407268524; L(Test): 0.2936546206474304\n",
            "Epoch 7285/10000: L(Train): 0.32617753744125366; L(Test): 0.29333406686782837\n",
            "Epoch 7286/10000: L(Train): 0.3217499256134033; L(Test): 0.29308974742889404\n",
            "Epoch 7287/10000: L(Train): 0.3229367136955261; L(Test): 0.29262927174568176\n",
            "Epoch 7288/10000: L(Train): 0.3141636848449707; L(Test): 0.29238221049308777\n",
            "Epoch 7289/10000: L(Train): 0.32079067826271057; L(Test): 0.29308682680130005\n",
            "Epoch 7290/10000: L(Train): 0.32596319913864136; L(Test): 0.29288041591644287\n",
            "Epoch 7291/10000: L(Train): 0.3207734525203705; L(Test): 0.2939676344394684\n",
            "Epoch 7292/10000: L(Train): 0.32565128803253174; L(Test): 0.2939050793647766\n",
            "Epoch 7293/10000: L(Train): 0.32689833641052246; L(Test): 0.29382041096687317\n",
            "Epoch 7294/10000: L(Train): 0.3072051703929901; L(Test): 0.2951212525367737\n",
            "Epoch 7295/10000: L(Train): 0.3160804510116577; L(Test): 0.2948724925518036\n",
            "Epoch 7296/10000: L(Train): 0.3236713409423828; L(Test): 0.2952677309513092\n",
            "Epoch 7297/10000: L(Train): 0.3097562789916992; L(Test): 0.29575836658477783\n",
            "Epoch 7298/10000: L(Train): 0.32057660818099976; L(Test): 0.2948402166366577\n",
            "Epoch 7299/10000: L(Train): 0.3197343945503235; L(Test): 0.2941705882549286\n",
            "Epoch 7300/10000: L(Train): 0.3195366859436035; L(Test): 0.29493990540504456\n",
            "Epoch 7301/10000: L(Train): 0.31650033593177795; L(Test): 0.2949094772338867\n",
            "Epoch 7302/10000: L(Train): 0.3192936182022095; L(Test): 0.29428955912590027\n",
            "Epoch 7303/10000: L(Train): 0.3267239034175873; L(Test): 0.2950216829776764\n",
            "Epoch 7304/10000: L(Train): 0.31843850016593933; L(Test): 0.2946566343307495\n",
            "Epoch 7305/10000: L(Train): 0.3119082748889923; L(Test): 0.29398104548454285\n",
            "Epoch 7306/10000: L(Train): 0.31973740458488464; L(Test): 0.29545778036117554\n",
            "Epoch 7307/10000: L(Train): 0.32495617866516113; L(Test): 0.29737547039985657\n",
            "Epoch 7308/10000: L(Train): 0.3222121298313141; L(Test): 0.296080082654953\n",
            "Epoch 7309/10000: L(Train): 0.32182392477989197; L(Test): 0.29411834478378296\n",
            "Epoch 7310/10000: L(Train): 0.3237651586532593; L(Test): 0.2950405180454254\n",
            "Epoch 7311/10000: L(Train): 0.31935662031173706; L(Test): 0.2940279245376587\n",
            "Epoch 7312/10000: L(Train): 0.3215288519859314; L(Test): 0.29470133781433105\n",
            "Epoch 7313/10000: L(Train): 0.32268017530441284; L(Test): 0.2959512770175934\n",
            "Epoch 7314/10000: L(Train): 0.31634828448295593; L(Test): 0.2962293326854706\n",
            "Epoch 7315/10000: L(Train): 0.3235439360141754; L(Test): 0.29512181878089905\n",
            "Epoch 7316/10000: L(Train): 0.31858542561531067; L(Test): 0.29696914553642273\n",
            "Epoch 7317/10000: L(Train): 0.3190425634384155; L(Test): 0.30497264862060547\n",
            "Epoch 7318/10000: L(Train): 0.3305646777153015; L(Test): 0.3034943640232086\n",
            "Epoch 7319/10000: L(Train): 0.31815415620803833; L(Test): 0.2996046245098114\n",
            "Epoch 7320/10000: L(Train): 0.32916536927223206; L(Test): 0.303230345249176\n",
            "Epoch 7321/10000: L(Train): 0.32585379481315613; L(Test): 0.3035576343536377\n",
            "Epoch 7322/10000: L(Train): 0.32997581362724304; L(Test): 0.3010965883731842\n",
            "Epoch 7323/10000: L(Train): 0.3194081485271454; L(Test): 0.3001005947589874\n",
            "Epoch 7324/10000: L(Train): 0.32648324966430664; L(Test): 0.3015063405036926\n",
            "Epoch 7325/10000: L(Train): 0.3263534903526306; L(Test): 0.30138731002807617\n",
            "Epoch 7326/10000: L(Train): 0.32205793261528015; L(Test): 0.2989078462123871\n",
            "Epoch 7327/10000: L(Train): 0.32280853390693665; L(Test): 0.30015698075294495\n",
            "Epoch 7328/10000: L(Train): 0.3164128363132477; L(Test): 0.3016063868999481\n",
            "Epoch 7329/10000: L(Train): 0.3339274823665619; L(Test): 0.2995745837688446\n",
            "Epoch 7330/10000: L(Train): 0.32145029306411743; L(Test): 0.29835525155067444\n",
            "Epoch 7331/10000: L(Train): 0.32511192560195923; L(Test): 0.2985632121562958\n",
            "Epoch 7332/10000: L(Train): 0.3152478039264679; L(Test): 0.3003329932689667\n",
            "Epoch 7333/10000: L(Train): 0.3265377879142761; L(Test): 0.29858019948005676\n",
            "Epoch 7334/10000: L(Train): 0.3186797797679901; L(Test): 0.2998059391975403\n",
            "Epoch 7335/10000: L(Train): 0.3310086727142334; L(Test): 0.3014833331108093\n",
            "Epoch 7336/10000: L(Train): 0.3226737082004547; L(Test): 0.3015809655189514\n",
            "Epoch 7337/10000: L(Train): 0.32188519835472107; L(Test): 0.3009883463382721\n",
            "Epoch 7338/10000: L(Train): 0.32396090030670166; L(Test): 0.30028486251831055\n",
            "Epoch 7339/10000: L(Train): 0.32116270065307617; L(Test): 0.30078762769699097\n",
            "Epoch 7340/10000: L(Train): 0.32303187251091003; L(Test): 0.30076125264167786\n",
            "Epoch 7341/10000: L(Train): 0.3203262388706207; L(Test): 0.29947617650032043\n",
            "Epoch 7342/10000: L(Train): 0.3189553916454315; L(Test): 0.30043280124664307\n",
            "Epoch 7343/10000: L(Train): 0.31982704997062683; L(Test): 0.2996821403503418\n",
            "Epoch 7344/10000: L(Train): 0.3271397650241852; L(Test): 0.2984939217567444\n",
            "Epoch 7345/10000: L(Train): 0.3242144286632538; L(Test): 0.29806527495384216\n",
            "Epoch 7346/10000: L(Train): 0.32475990056991577; L(Test): 0.2964298725128174\n",
            "Epoch 7347/10000: L(Train): 0.3212129473686218; L(Test): 0.2971378266811371\n",
            "Epoch 7348/10000: L(Train): 0.32761943340301514; L(Test): 0.2983841300010681\n",
            "Epoch 7349/10000: L(Train): 0.311769038438797; L(Test): 0.29717904329299927\n",
            "Epoch 7350/10000: L(Train): 0.32246583700180054; L(Test): 0.29749011993408203\n",
            "Epoch 7351/10000: L(Train): 0.31544429063796997; L(Test): 0.2985326051712036\n",
            "Epoch 7352/10000: L(Train): 0.32000964879989624; L(Test): 0.2966994643211365\n",
            "Epoch 7353/10000: L(Train): 0.31832924485206604; L(Test): 0.2953711152076721\n",
            "Epoch 7354/10000: L(Train): 0.32299262285232544; L(Test): 0.29569607973098755\n",
            "Epoch 7355/10000: L(Train): 0.3237731456756592; L(Test): 0.294140487909317\n",
            "Epoch 7356/10000: L(Train): 0.3112160861492157; L(Test): 0.29464760422706604\n",
            "Epoch 7357/10000: L(Train): 0.32124465703964233; L(Test): 0.29528185725212097\n",
            "Epoch 7358/10000: L(Train): 0.3150482177734375; L(Test): 0.29557257890701294\n",
            "Epoch 7359/10000: L(Train): 0.321157306432724; L(Test): 0.2964169681072235\n",
            "Epoch 7360/10000: L(Train): 0.3269171416759491; L(Test): 0.2958904206752777\n",
            "Epoch 7361/10000: L(Train): 0.3235141932964325; L(Test): 0.29458802938461304\n",
            "Epoch 7362/10000: L(Train): 0.3168531358242035; L(Test): 0.2957030236721039\n",
            "Epoch 7363/10000: L(Train): 0.31937724351882935; L(Test): 0.2960628867149353\n",
            "Epoch 7364/10000: L(Train): 0.31462883949279785; L(Test): 0.29620635509490967\n",
            "Epoch 7365/10000: L(Train): 0.3269551694393158; L(Test): 0.29390379786491394\n",
            "Epoch 7366/10000: L(Train): 0.3207901120185852; L(Test): 0.29382696747779846\n",
            "Epoch 7367/10000: L(Train): 0.3214070498943329; L(Test): 0.2941141426563263\n",
            "Epoch 7368/10000: L(Train): 0.3260014057159424; L(Test): 0.29424595832824707\n",
            "Epoch 7369/10000: L(Train): 0.3273138999938965; L(Test): 0.2965394854545593\n",
            "Epoch 7370/10000: L(Train): 0.324186772108078; L(Test): 0.2946891188621521\n",
            "Epoch 7371/10000: L(Train): 0.31524187326431274; L(Test): 0.29392361640930176\n",
            "Epoch 7372/10000: L(Train): 0.3152683675289154; L(Test): 0.2943417429924011\n",
            "Epoch 7373/10000: L(Train): 0.3180426359176636; L(Test): 0.2951323390007019\n",
            "Epoch 7374/10000: L(Train): 0.3208916187286377; L(Test): 0.2964993119239807\n",
            "Epoch 7375/10000: L(Train): 0.31982699036598206; L(Test): 0.2961846590042114\n",
            "Epoch 7376/10000: L(Train): 0.3248370289802551; L(Test): 0.2933007478713989\n",
            "Epoch 7377/10000: L(Train): 0.3267166316509247; L(Test): 0.2951735854148865\n",
            "Epoch 7378/10000: L(Train): 0.32196322083473206; L(Test): 0.29569000005722046\n",
            "Epoch 7379/10000: L(Train): 0.3242298364639282; L(Test): 0.2951935827732086\n",
            "Epoch 7380/10000: L(Train): 0.31543639302253723; L(Test): 0.296353816986084\n",
            "Epoch 7381/10000: L(Train): 0.31129854917526245; L(Test): 0.2945101857185364\n",
            "Epoch 7382/10000: L(Train): 0.31481823325157166; L(Test): 0.293056845664978\n",
            "Epoch 7383/10000: L(Train): 0.32612696290016174; L(Test): 0.2948845624923706\n",
            "Epoch 7384/10000: L(Train): 0.3269520401954651; L(Test): 0.29648685455322266\n",
            "Epoch 7385/10000: L(Train): 0.3113170862197876; L(Test): 0.2954261898994446\n",
            "Epoch 7386/10000: L(Train): 0.3132041096687317; L(Test): 0.2958497107028961\n",
            "Epoch 7387/10000: L(Train): 0.3201065957546234; L(Test): 0.2978711724281311\n",
            "Epoch 7388/10000: L(Train): 0.3174839913845062; L(Test): 0.29669666290283203\n",
            "Epoch 7389/10000: L(Train): 0.3271217942237854; L(Test): 0.29561325907707214\n",
            "Epoch 7390/10000: L(Train): 0.3099890351295471; L(Test): 0.29552435874938965\n",
            "Epoch 7391/10000: L(Train): 0.3231236934661865; L(Test): 0.2958031892776489\n",
            "Epoch 7392/10000: L(Train): 0.3183742165565491; L(Test): 0.29525813460350037\n",
            "Epoch 7393/10000: L(Train): 0.317333847284317; L(Test): 0.2943558096885681\n",
            "Epoch 7394/10000: L(Train): 0.30626946687698364; L(Test): 0.295601487159729\n",
            "Epoch 7395/10000: L(Train): 0.325050950050354; L(Test): 0.29550477862358093\n",
            "Epoch 7396/10000: L(Train): 0.3282069265842438; L(Test): 0.2944006621837616\n",
            "Epoch 7397/10000: L(Train): 0.3158196210861206; L(Test): 0.2945222854614258\n",
            "Epoch 7398/10000: L(Train): 0.3161604702472687; L(Test): 0.2942696809768677\n",
            "Epoch 7399/10000: L(Train): 0.318765252828598; L(Test): 0.2937493920326233\n",
            "Epoch 7400/10000: L(Train): 0.3162461221218109; L(Test): 0.29492297768592834\n",
            "Epoch 7401/10000: L(Train): 0.3179721534252167; L(Test): 0.2950867712497711\n",
            "Epoch 7402/10000: L(Train): 0.3114188611507416; L(Test): 0.2942219078540802\n",
            "Epoch 7403/10000: L(Train): 0.32081642746925354; L(Test): 0.2931431233882904\n",
            "Epoch 7404/10000: L(Train): 0.319892555475235; L(Test): 0.2926041781902313\n",
            "Epoch 7405/10000: L(Train): 0.3237002491950989; L(Test): 0.29232946038246155\n",
            "Epoch 7406/10000: L(Train): 0.3194243013858795; L(Test): 0.2920166850090027\n",
            "Epoch 7407/10000: L(Train): 0.31427624821662903; L(Test): 0.2919492721557617\n",
            "Epoch 7408/10000: L(Train): 0.3235680162906647; L(Test): 0.29244303703308105\n",
            "Epoch 7409/10000: L(Train): 0.3101505637168884; L(Test): 0.2931615710258484\n",
            "Epoch 7410/10000: L(Train): 0.3131979703903198; L(Test): 0.29292914271354675\n",
            "Epoch 7411/10000: L(Train): 0.31677141785621643; L(Test): 0.2927997410297394\n",
            "Epoch 7412/10000: L(Train): 0.313678503036499; L(Test): 0.292141318321228\n",
            "Epoch 7413/10000: L(Train): 0.32010984420776367; L(Test): 0.2926846742630005\n",
            "Epoch 7414/10000: L(Train): 0.31753459572792053; L(Test): 0.29304239153862\n",
            "Epoch 7415/10000: L(Train): 0.3215394616127014; L(Test): 0.29307398200035095\n",
            "Epoch 7416/10000: L(Train): 0.32040905952453613; L(Test): 0.293282151222229\n",
            "Epoch 7417/10000: L(Train): 0.31958287954330444; L(Test): 0.29369229078292847\n",
            "Epoch 7418/10000: L(Train): 0.3228125274181366; L(Test): 0.29349958896636963\n",
            "Epoch 7419/10000: L(Train): 0.3208680748939514; L(Test): 0.29319167137145996\n",
            "Epoch 7420/10000: L(Train): 0.32474246621131897; L(Test): 0.29357224702835083\n",
            "Epoch 7421/10000: L(Train): 0.32274961471557617; L(Test): 0.2938134968280792\n",
            "Epoch 7422/10000: L(Train): 0.3244413137435913; L(Test): 0.29468071460723877\n",
            "Epoch 7423/10000: L(Train): 0.33472883701324463; L(Test): 0.294622004032135\n",
            "Epoch 7424/10000: L(Train): 0.3201211392879486; L(Test): 0.29562512040138245\n",
            "Epoch 7425/10000: L(Train): 0.3111758828163147; L(Test): 0.29588791728019714\n",
            "Epoch 7426/10000: L(Train): 0.33041417598724365; L(Test): 0.29441869258880615\n",
            "Epoch 7427/10000: L(Train): 0.3148074448108673; L(Test): 0.2941199541091919\n",
            "Epoch 7428/10000: L(Train): 0.31860285997390747; L(Test): 0.294916570186615\n",
            "Epoch 7429/10000: L(Train): 0.31131452322006226; L(Test): 0.2955133616924286\n",
            "Epoch 7430/10000: L(Train): 0.32054275274276733; L(Test): 0.29532477259635925\n",
            "Epoch 7431/10000: L(Train): 0.3144718408584595; L(Test): 0.2967011630535126\n",
            "Epoch 7432/10000: L(Train): 0.32418859004974365; L(Test): 0.2964570224285126\n",
            "Epoch 7433/10000: L(Train): 0.321323037147522; L(Test): 0.2951697111129761\n",
            "Epoch 7434/10000: L(Train): 0.31816422939300537; L(Test): 0.2954983413219452\n",
            "Epoch 7435/10000: L(Train): 0.3252522051334381; L(Test): 0.2960469424724579\n",
            "Epoch 7436/10000: L(Train): 0.3304305970668793; L(Test): 0.29580387473106384\n",
            "Epoch 7437/10000: L(Train): 0.3266318142414093; L(Test): 0.2957318425178528\n",
            "Epoch 7438/10000: L(Train): 0.3154527246952057; L(Test): 0.2959898114204407\n",
            "Epoch 7439/10000: L(Train): 0.32525116205215454; L(Test): 0.2949012517929077\n",
            "Epoch 7440/10000: L(Train): 0.32072874903678894; L(Test): 0.2940334379673004\n",
            "Epoch 7441/10000: L(Train): 0.3232250511646271; L(Test): 0.2939051687717438\n",
            "Epoch 7442/10000: L(Train): 0.314567893743515; L(Test): 0.29443642497062683\n",
            "Epoch 7443/10000: L(Train): 0.31565067172050476; L(Test): 0.29431968927383423\n",
            "Epoch 7444/10000: L(Train): 0.320667564868927; L(Test): 0.29484251141548157\n",
            "Epoch 7445/10000: L(Train): 0.32038506865501404; L(Test): 0.2961091101169586\n",
            "Epoch 7446/10000: L(Train): 0.30310124158859253; L(Test): 0.2953302264213562\n",
            "Epoch 7447/10000: L(Train): 0.31909218430519104; L(Test): 0.2949744760990143\n",
            "Epoch 7448/10000: L(Train): 0.319951593875885; L(Test): 0.29484817385673523\n",
            "Epoch 7449/10000: L(Train): 0.31508904695510864; L(Test): 0.29432401061058044\n",
            "Epoch 7450/10000: L(Train): 0.3223896324634552; L(Test): 0.2937886118888855\n",
            "Epoch 7451/10000: L(Train): 0.3232111930847168; L(Test): 0.2937401235103607\n",
            "Epoch 7452/10000: L(Train): 0.3224547207355499; L(Test): 0.2944045960903168\n",
            "Epoch 7453/10000: L(Train): 0.3126812279224396; L(Test): 0.29500812292099\n",
            "Epoch 7454/10000: L(Train): 0.3169230818748474; L(Test): 0.2941482365131378\n",
            "Epoch 7455/10000: L(Train): 0.31984779238700867; L(Test): 0.2944410741329193\n",
            "Epoch 7456/10000: L(Train): 0.31477445363998413; L(Test): 0.2943466007709503\n",
            "Epoch 7457/10000: L(Train): 0.31856992840766907; L(Test): 0.29395437240600586\n",
            "Epoch 7458/10000: L(Train): 0.3222046494483948; L(Test): 0.2939070463180542\n",
            "Epoch 7459/10000: L(Train): 0.31697168946266174; L(Test): 0.29425719380378723\n",
            "Epoch 7460/10000: L(Train): 0.3091334104537964; L(Test): 0.2932332754135132\n",
            "Epoch 7461/10000: L(Train): 0.324611634016037; L(Test): 0.29373639822006226\n",
            "Epoch 7462/10000: L(Train): 0.31887683272361755; L(Test): 0.2945421040058136\n",
            "Epoch 7463/10000: L(Train): 0.3141058683395386; L(Test): 0.29506587982177734\n",
            "Epoch 7464/10000: L(Train): 0.31696584820747375; L(Test): 0.2951941192150116\n",
            "Epoch 7465/10000: L(Train): 0.31854763627052307; L(Test): 0.29516905546188354\n",
            "Epoch 7466/10000: L(Train): 0.3201897442340851; L(Test): 0.29494044184684753\n",
            "Epoch 7467/10000: L(Train): 0.32698726654052734; L(Test): 0.296070396900177\n",
            "Epoch 7468/10000: L(Train): 0.3203883171081543; L(Test): 0.29865771532058716\n",
            "Epoch 7469/10000: L(Train): 0.33508020639419556; L(Test): 0.2972774803638458\n",
            "Epoch 7470/10000: L(Train): 0.3199310302734375; L(Test): 0.3000962436199188\n",
            "Epoch 7471/10000: L(Train): 0.33316275477409363; L(Test): 0.2997414171695709\n",
            "Epoch 7472/10000: L(Train): 0.32073116302490234; L(Test): 0.2993154227733612\n",
            "Epoch 7473/10000: L(Train): 0.3263421654701233; L(Test): 0.30389031767845154\n",
            "Epoch 7474/10000: L(Train): 0.32702866196632385; L(Test): 0.30327579379081726\n",
            "Epoch 7475/10000: L(Train): 0.32840481400489807; L(Test): 0.30182719230651855\n",
            "Epoch 7476/10000: L(Train): 0.3275983929634094; L(Test): 0.30106163024902344\n",
            "Epoch 7477/10000: L(Train): 0.32541489601135254; L(Test): 0.30052557587623596\n",
            "Epoch 7478/10000: L(Train): 0.31446725130081177; L(Test): 0.29893240332603455\n",
            "Epoch 7479/10000: L(Train): 0.32205730676651; L(Test): 0.3005804419517517\n",
            "Epoch 7480/10000: L(Train): 0.3227788805961609; L(Test): 0.30204981565475464\n",
            "Epoch 7481/10000: L(Train): 0.3304348886013031; L(Test): 0.3013835847377777\n",
            "Epoch 7482/10000: L(Train): 0.320829302072525; L(Test): 0.29941266775131226\n",
            "Epoch 7483/10000: L(Train): 0.3196318745613098; L(Test): 0.29985156655311584\n",
            "Epoch 7484/10000: L(Train): 0.32042333483695984; L(Test): 0.30094584822654724\n",
            "Epoch 7485/10000: L(Train): 0.32373347878456116; L(Test): 0.2991817891597748\n",
            "Epoch 7486/10000: L(Train): 0.3154911398887634; L(Test): 0.29775816202163696\n",
            "Epoch 7487/10000: L(Train): 0.32837361097335815; L(Test): 0.2982672452926636\n",
            "Epoch 7488/10000: L(Train): 0.3199703097343445; L(Test): 0.2987506687641144\n",
            "Epoch 7489/10000: L(Train): 0.32846584916114807; L(Test): 0.29740604758262634\n",
            "Epoch 7490/10000: L(Train): 0.3137947618961334; L(Test): 0.2972603738307953\n",
            "Epoch 7491/10000: L(Train): 0.3212401866912842; L(Test): 0.29763495922088623\n",
            "Epoch 7492/10000: L(Train): 0.3231738805770874; L(Test): 0.2993798553943634\n",
            "Epoch 7493/10000: L(Train): 0.33109280467033386; L(Test): 0.29736801981925964\n",
            "Epoch 7494/10000: L(Train): 0.3185408413410187; L(Test): 0.29724931716918945\n",
            "Epoch 7495/10000: L(Train): 0.32466021180152893; L(Test): 0.2980734407901764\n",
            "Epoch 7496/10000: L(Train): 0.32262250781059265; L(Test): 0.29692140221595764\n",
            "Epoch 7497/10000: L(Train): 0.3242470920085907; L(Test): 0.2964193522930145\n",
            "Epoch 7498/10000: L(Train): 0.3175640106201172; L(Test): 0.29713118076324463\n",
            "Epoch 7499/10000: L(Train): 0.3249216675758362; L(Test): 0.2974421977996826\n",
            "Epoch 7500/10000: L(Train): 0.3208145201206207; L(Test): 0.29754775762557983\n",
            "Epoch 7501/10000: L(Train): 0.3245967924594879; L(Test): 0.2967569828033447\n",
            "Epoch 7502/10000: L(Train): 0.3284968137741089; L(Test): 0.295714408159256\n",
            "Epoch 7503/10000: L(Train): 0.3205626308917999; L(Test): 0.2963360548019409\n",
            "Epoch 7504/10000: L(Train): 0.31812384724617004; L(Test): 0.2954300045967102\n",
            "Epoch 7505/10000: L(Train): 0.3141384422779083; L(Test): 0.2947932183742523\n",
            "Epoch 7506/10000: L(Train): 0.32494813203811646; L(Test): 0.29586923122406006\n",
            "Epoch 7507/10000: L(Train): 0.31988582015037537; L(Test): 0.29589569568634033\n",
            "Epoch 7508/10000: L(Train): 0.3152599036693573; L(Test): 0.2953435778617859\n",
            "Epoch 7509/10000: L(Train): 0.3224655091762543; L(Test): 0.2959124743938446\n",
            "Epoch 7510/10000: L(Train): 0.32526761293411255; L(Test): 0.29717788100242615\n",
            "Epoch 7511/10000: L(Train): 0.32029107213020325; L(Test): 0.29674044251441956\n",
            "Epoch 7512/10000: L(Train): 0.3165915310382843; L(Test): 0.29544347524642944\n",
            "Epoch 7513/10000: L(Train): 0.31602486968040466; L(Test): 0.29531794786453247\n",
            "Epoch 7514/10000: L(Train): 0.3176419138908386; L(Test): 0.295296847820282\n",
            "Epoch 7515/10000: L(Train): 0.3174322545528412; L(Test): 0.29487961530685425\n",
            "Epoch 7516/10000: L(Train): 0.32401418685913086; L(Test): 0.2940003573894501\n",
            "Epoch 7517/10000: L(Train): 0.31856203079223633; L(Test): 0.29513615369796753\n",
            "Epoch 7518/10000: L(Train): 0.31656283140182495; L(Test): 0.29527753591537476\n",
            "Epoch 7519/10000: L(Train): 0.3167976140975952; L(Test): 0.2942846119403839\n",
            "Epoch 7520/10000: L(Train): 0.31921759247779846; L(Test): 0.2959524095058441\n",
            "Epoch 7521/10000: L(Train): 0.3153313994407654; L(Test): 0.2978876531124115\n",
            "Epoch 7522/10000: L(Train): 0.3150010108947754; L(Test): 0.29623180627822876\n",
            "Epoch 7523/10000: L(Train): 0.32008928060531616; L(Test): 0.2949710488319397\n",
            "Epoch 7524/10000: L(Train): 0.3185444474220276; L(Test): 0.2968820333480835\n",
            "Epoch 7525/10000: L(Train): 0.31910839676856995; L(Test): 0.29677146673202515\n",
            "Epoch 7526/10000: L(Train): 0.3243321478366852; L(Test): 0.2967100739479065\n",
            "Epoch 7527/10000: L(Train): 0.31416887044906616; L(Test): 0.29591134190559387\n",
            "Epoch 7528/10000: L(Train): 0.317743182182312; L(Test): 0.29583850502967834\n",
            "Epoch 7529/10000: L(Train): 0.3231239318847656; L(Test): 0.29577234387397766\n",
            "Epoch 7530/10000: L(Train): 0.31717008352279663; L(Test): 0.2942254841327667\n",
            "Epoch 7531/10000: L(Train): 0.3172316551208496; L(Test): 0.29398974776268005\n",
            "Epoch 7532/10000: L(Train): 0.32520243525505066; L(Test): 0.2938704490661621\n",
            "Epoch 7533/10000: L(Train): 0.3234340250492096; L(Test): 0.29389306902885437\n",
            "Epoch 7534/10000: L(Train): 0.3171326816082001; L(Test): 0.29507774114608765\n",
            "Epoch 7535/10000: L(Train): 0.3173709213733673; L(Test): 0.29574868083000183\n",
            "Epoch 7536/10000: L(Train): 0.3193924129009247; L(Test): 0.29445868730545044\n",
            "Epoch 7537/10000: L(Train): 0.3192465007305145; L(Test): 0.2959323525428772\n",
            "Epoch 7538/10000: L(Train): 0.32294487953186035; L(Test): 0.3016447424888611\n",
            "Epoch 7539/10000: L(Train): 0.3238822817802429; L(Test): 0.3002311587333679\n",
            "Epoch 7540/10000: L(Train): 0.323257714509964; L(Test): 0.29688480496406555\n",
            "Epoch 7541/10000: L(Train): 0.32687288522720337; L(Test): 0.30115190148353577\n",
            "Epoch 7542/10000: L(Train): 0.3322325646877289; L(Test): 0.30134350061416626\n",
            "Epoch 7543/10000: L(Train): 0.32234758138656616; L(Test): 0.2998998165130615\n",
            "Epoch 7544/10000: L(Train): 0.31965458393096924; L(Test): 0.29863518476486206\n",
            "Epoch 7545/10000: L(Train): 0.326693594455719; L(Test): 0.30042871832847595\n",
            "Epoch 7546/10000: L(Train): 0.3293403089046478; L(Test): 0.3001537024974823\n",
            "Epoch 7547/10000: L(Train): 0.32056155800819397; L(Test): 0.2986043691635132\n",
            "Epoch 7548/10000: L(Train): 0.3262860178947449; L(Test): 0.2999279797077179\n",
            "Epoch 7549/10000: L(Train): 0.3221910297870636; L(Test): 0.30005547404289246\n",
            "Epoch 7550/10000: L(Train): 0.31978511810302734; L(Test): 0.297488272190094\n",
            "Epoch 7551/10000: L(Train): 0.3162682354450226; L(Test): 0.29705074429512024\n",
            "Epoch 7552/10000: L(Train): 0.32927078008651733; L(Test): 0.2976722717285156\n",
            "Epoch 7553/10000: L(Train): 0.32711875438690186; L(Test): 0.29723432660102844\n",
            "Epoch 7554/10000: L(Train): 0.31826892495155334; L(Test): 0.2976686954498291\n",
            "Epoch 7555/10000: L(Train): 0.32680848240852356; L(Test): 0.2975276708602905\n",
            "Epoch 7556/10000: L(Train): 0.3197689652442932; L(Test): 0.29698091745376587\n",
            "Epoch 7557/10000: L(Train): 0.3252907991409302; L(Test): 0.29767492413520813\n",
            "Epoch 7558/10000: L(Train): 0.3183107376098633; L(Test): 0.2973286211490631\n",
            "Epoch 7559/10000: L(Train): 0.32987067103385925; L(Test): 0.2961582541465759\n",
            "Epoch 7560/10000: L(Train): 0.3176238536834717; L(Test): 0.2954616844654083\n",
            "Epoch 7561/10000: L(Train): 0.3197789490222931; L(Test): 0.2964245080947876\n",
            "Epoch 7562/10000: L(Train): 0.32268768548965454; L(Test): 0.2965667247772217\n",
            "Epoch 7563/10000: L(Train): 0.3166804015636444; L(Test): 0.2967722415924072\n",
            "Epoch 7564/10000: L(Train): 0.3219746947288513; L(Test): 0.29548272490501404\n",
            "Epoch 7565/10000: L(Train): 0.30823400616645813; L(Test): 0.2961500287055969\n",
            "Epoch 7566/10000: L(Train): 0.30920833349227905; L(Test): 0.2977830469608307\n",
            "Epoch 7567/10000: L(Train): 0.31092727184295654; L(Test): 0.29769065976142883\n",
            "Epoch 7568/10000: L(Train): 0.31674298644065857; L(Test): 0.2950893044471741\n",
            "Epoch 7569/10000: L(Train): 0.31999942660331726; L(Test): 0.2962101995944977\n",
            "Epoch 7570/10000: L(Train): 0.32015708088874817; L(Test): 0.2964693307876587\n",
            "Epoch 7571/10000: L(Train): 0.32307514548301697; L(Test): 0.2959379255771637\n",
            "Epoch 7572/10000: L(Train): 0.32461032271385193; L(Test): 0.29619070887565613\n",
            "Epoch 7573/10000: L(Train): 0.320126473903656; L(Test): 0.2954086661338806\n",
            "Epoch 7574/10000: L(Train): 0.31606951355934143; L(Test): 0.2961096167564392\n",
            "Epoch 7575/10000: L(Train): 0.3195468485355377; L(Test): 0.29566940665245056\n",
            "Epoch 7576/10000: L(Train): 0.3072231709957123; L(Test): 0.295023113489151\n",
            "Epoch 7577/10000: L(Train): 0.32653358578681946; L(Test): 0.29523390531539917\n",
            "Epoch 7578/10000: L(Train): 0.3251129686832428; L(Test): 0.29405057430267334\n",
            "Epoch 7579/10000: L(Train): 0.30702486634254456; L(Test): 0.2940692901611328\n",
            "Epoch 7580/10000: L(Train): 0.31731730699539185; L(Test): 0.29401135444641113\n",
            "Epoch 7581/10000: L(Train): 0.3240913450717926; L(Test): 0.2945321798324585\n",
            "Epoch 7582/10000: L(Train): 0.3195117712020874; L(Test): 0.29427772760391235\n",
            "Epoch 7583/10000: L(Train): 0.3152477443218231; L(Test): 0.29409319162368774\n",
            "Epoch 7584/10000: L(Train): 0.3229404389858246; L(Test): 0.29537779092788696\n",
            "Epoch 7585/10000: L(Train): 0.3141297698020935; L(Test): 0.294869601726532\n",
            "Epoch 7586/10000: L(Train): 0.32197311520576477; L(Test): 0.2935712933540344\n",
            "Epoch 7587/10000: L(Train): 0.3202682137489319; L(Test): 0.2931514382362366\n",
            "Epoch 7588/10000: L(Train): 0.30836769938468933; L(Test): 0.29374805092811584\n",
            "Epoch 7589/10000: L(Train): 0.32345300912857056; L(Test): 0.2937873303890228\n",
            "Epoch 7590/10000: L(Train): 0.3202923834323883; L(Test): 0.29328271746635437\n",
            "Epoch 7591/10000: L(Train): 0.31733402609825134; L(Test): 0.2922748327255249\n",
            "Epoch 7592/10000: L(Train): 0.3222065567970276; L(Test): 0.29205840826034546\n",
            "Epoch 7593/10000: L(Train): 0.3208532929420471; L(Test): 0.2916596829891205\n",
            "Epoch 7594/10000: L(Train): 0.32299092411994934; L(Test): 0.29254063963890076\n",
            "Epoch 7595/10000: L(Train): 0.3146545886993408; L(Test): 0.294007271528244\n",
            "Epoch 7596/10000: L(Train): 0.3109440207481384; L(Test): 0.2940378785133362\n",
            "Epoch 7597/10000: L(Train): 0.32036274671554565; L(Test): 0.2934052348136902\n",
            "Epoch 7598/10000: L(Train): 0.31590908765792847; L(Test): 0.29265275597572327\n",
            "Epoch 7599/10000: L(Train): 0.31449249386787415; L(Test): 0.29310154914855957\n",
            "Epoch 7600/10000: L(Train): 0.32019850611686707; L(Test): 0.2925136089324951\n",
            "Epoch 7601/10000: L(Train): 0.32080501317977905; L(Test): 0.29212358593940735\n",
            "Epoch 7602/10000: L(Train): 0.3219314515590668; L(Test): 0.2929215431213379\n",
            "Epoch 7603/10000: L(Train): 0.3206171691417694; L(Test): 0.2938854396343231\n",
            "Epoch 7604/10000: L(Train): 0.3154815435409546; L(Test): 0.29399552941322327\n",
            "Epoch 7605/10000: L(Train): 0.317340224981308; L(Test): 0.29319655895233154\n",
            "Epoch 7606/10000: L(Train): 0.3237523138523102; L(Test): 0.29336661100387573\n",
            "Epoch 7607/10000: L(Train): 0.3241819143295288; L(Test): 0.29358842968940735\n",
            "Epoch 7608/10000: L(Train): 0.3200482428073883; L(Test): 0.29318001866340637\n",
            "Epoch 7609/10000: L(Train): 0.31467708945274353; L(Test): 0.2929079830646515\n",
            "Epoch 7610/10000: L(Train): 0.3183397352695465; L(Test): 0.29299023747444153\n",
            "Epoch 7611/10000: L(Train): 0.3102628290653229; L(Test): 0.29261115193367004\n",
            "Epoch 7612/10000: L(Train): 0.31236889958381653; L(Test): 0.292876273393631\n",
            "Epoch 7613/10000: L(Train): 0.31565383076667786; L(Test): 0.29344555735588074\n",
            "Epoch 7614/10000: L(Train): 0.3139595091342926; L(Test): 0.293178915977478\n",
            "Epoch 7615/10000: L(Train): 0.3178964853286743; L(Test): 0.2927601933479309\n",
            "Epoch 7616/10000: L(Train): 0.3205835819244385; L(Test): 0.2929217517375946\n",
            "Epoch 7617/10000: L(Train): 0.31849348545074463; L(Test): 0.29226747155189514\n",
            "Epoch 7618/10000: L(Train): 0.32671117782592773; L(Test): 0.29247593879699707\n",
            "Epoch 7619/10000: L(Train): 0.31995856761932373; L(Test): 0.2922627925872803\n",
            "Epoch 7620/10000: L(Train): 0.3171158730983734; L(Test): 0.29376113414764404\n",
            "Epoch 7621/10000: L(Train): 0.3153344988822937; L(Test): 0.2937146723270416\n",
            "Epoch 7622/10000: L(Train): 0.32739749550819397; L(Test): 0.2926328778266907\n",
            "Epoch 7623/10000: L(Train): 0.31984275579452515; L(Test): 0.2925724685192108\n",
            "Epoch 7624/10000: L(Train): 0.32199621200561523; L(Test): 0.29207688570022583\n",
            "Epoch 7625/10000: L(Train): 0.3143311142921448; L(Test): 0.2920650541782379\n",
            "Epoch 7626/10000: L(Train): 0.3217127323150635; L(Test): 0.2928485572338104\n",
            "Epoch 7627/10000: L(Train): 0.3078782856464386; L(Test): 0.29372236132621765\n",
            "Epoch 7628/10000: L(Train): 0.32563650608062744; L(Test): 0.29374268651008606\n",
            "Epoch 7629/10000: L(Train): 0.3187737762928009; L(Test): 0.292904794216156\n",
            "Epoch 7630/10000: L(Train): 0.3169809877872467; L(Test): 0.2941918671131134\n",
            "Epoch 7631/10000: L(Train): 0.31977978348731995; L(Test): 0.29505854845046997\n",
            "Epoch 7632/10000: L(Train): 0.31649646162986755; L(Test): 0.2956971526145935\n",
            "Epoch 7633/10000: L(Train): 0.3190898299217224; L(Test): 0.29457956552505493\n",
            "Epoch 7634/10000: L(Train): 0.3117188811302185; L(Test): 0.2946377694606781\n",
            "Epoch 7635/10000: L(Train): 0.3202976882457733; L(Test): 0.2938993573188782\n",
            "Epoch 7636/10000: L(Train): 0.31826043128967285; L(Test): 0.2936995029449463\n",
            "Epoch 7637/10000: L(Train): 0.32074660062789917; L(Test): 0.2946556508541107\n",
            "Epoch 7638/10000: L(Train): 0.32486894726753235; L(Test): 0.2948148548603058\n",
            "Epoch 7639/10000: L(Train): 0.32434943318367004; L(Test): 0.29476332664489746\n",
            "Epoch 7640/10000: L(Train): 0.3172590136528015; L(Test): 0.2972213625907898\n",
            "Epoch 7641/10000: L(Train): 0.31355902552604675; L(Test): 0.3000902533531189\n",
            "Epoch 7642/10000: L(Train): 0.32703104615211487; L(Test): 0.2974186837673187\n",
            "Epoch 7643/10000: L(Train): 0.32928791642189026; L(Test): 0.30023109912872314\n",
            "Epoch 7644/10000: L(Train): 0.3164728283882141; L(Test): 0.30022990703582764\n",
            "Epoch 7645/10000: L(Train): 0.32642534375190735; L(Test): 0.2998739778995514\n",
            "Epoch 7646/10000: L(Train): 0.3318195044994354; L(Test): 0.2995988428592682\n",
            "Epoch 7647/10000: L(Train): 0.32736945152282715; L(Test): 0.29874899983406067\n",
            "Epoch 7648/10000: L(Train): 0.32875344157218933; L(Test): 0.3007175922393799\n",
            "Epoch 7649/10000: L(Train): 0.3288666009902954; L(Test): 0.30079418420791626\n",
            "Epoch 7650/10000: L(Train): 0.33453112840652466; L(Test): 0.2999596893787384\n",
            "Epoch 7651/10000: L(Train): 0.31452569365501404; L(Test): 0.29967814683914185\n",
            "Epoch 7652/10000: L(Train): 0.3212369680404663; L(Test): 0.29954004287719727\n",
            "Epoch 7653/10000: L(Train): 0.3184569776058197; L(Test): 0.30058008432388306\n",
            "Epoch 7654/10000: L(Train): 0.3173314929008484; L(Test): 0.30047154426574707\n",
            "Epoch 7655/10000: L(Train): 0.32524436712265015; L(Test): 0.299495667219162\n",
            "Epoch 7656/10000: L(Train): 0.3227694630622864; L(Test): 0.30018576979637146\n",
            "Epoch 7657/10000: L(Train): 0.32133927941322327; L(Test): 0.2997768223285675\n",
            "Epoch 7658/10000: L(Train): 0.3177485764026642; L(Test): 0.2990671694278717\n",
            "Epoch 7659/10000: L(Train): 0.3197777271270752; L(Test): 0.29976603388786316\n",
            "Epoch 7660/10000: L(Train): 0.31936943531036377; L(Test): 0.2991829216480255\n",
            "Epoch 7661/10000: L(Train): 0.31333687901496887; L(Test): 0.29795172810554504\n",
            "Epoch 7662/10000: L(Train): 0.3233824670314789; L(Test): 0.297670841217041\n",
            "Epoch 7663/10000: L(Train): 0.31714341044425964; L(Test): 0.29768115282058716\n",
            "Epoch 7664/10000: L(Train): 0.3205898106098175; L(Test): 0.2970542013645172\n",
            "Epoch 7665/10000: L(Train): 0.32892078161239624; L(Test): 0.2963044047355652\n",
            "Epoch 7666/10000: L(Train): 0.31823575496673584; L(Test): 0.29545167088508606\n",
            "Epoch 7667/10000: L(Train): 0.31522613763809204; L(Test): 0.2959846556186676\n",
            "Epoch 7668/10000: L(Train): 0.31772753596305847; L(Test): 0.29631662368774414\n",
            "Epoch 7669/10000: L(Train): 0.3185688555240631; L(Test): 0.29522064328193665\n",
            "Epoch 7670/10000: L(Train): 0.3222580850124359; L(Test): 0.294210821390152\n",
            "Epoch 7671/10000: L(Train): 0.3240833282470703; L(Test): 0.29612213373184204\n",
            "Epoch 7672/10000: L(Train): 0.3273799419403076; L(Test): 0.2961369752883911\n",
            "Epoch 7673/10000: L(Train): 0.3158743381500244; L(Test): 0.29544153809547424\n",
            "Epoch 7674/10000: L(Train): 0.3151344060897827; L(Test): 0.29710617661476135\n",
            "Epoch 7675/10000: L(Train): 0.33434370160102844; L(Test): 0.2955910563468933\n",
            "Epoch 7676/10000: L(Train): 0.3226293921470642; L(Test): 0.2934718430042267\n",
            "Epoch 7677/10000: L(Train): 0.32318466901779175; L(Test): 0.2952445447444916\n",
            "Epoch 7678/10000: L(Train): 0.324349969625473; L(Test): 0.29489216208457947\n",
            "Epoch 7679/10000: L(Train): 0.3228396475315094; L(Test): 0.293515145778656\n",
            "Epoch 7680/10000: L(Train): 0.3247895836830139; L(Test): 0.29406076669692993\n",
            "Epoch 7681/10000: L(Train): 0.3211705982685089; L(Test): 0.29497331380844116\n",
            "Epoch 7682/10000: L(Train): 0.3136454224586487; L(Test): 0.2948794662952423\n",
            "Epoch 7683/10000: L(Train): 0.32277345657348633; L(Test): 0.3007962703704834\n",
            "Epoch 7684/10000: L(Train): 0.3230460286140442; L(Test): 0.3041481375694275\n",
            "Epoch 7685/10000: L(Train): 0.32606422901153564; L(Test): 0.30181390047073364\n",
            "Epoch 7686/10000: L(Train): 0.32635796070098877; L(Test): 0.3016606271266937\n",
            "Epoch 7687/10000: L(Train): 0.33380448818206787; L(Test): 0.3054547607898712\n",
            "Epoch 7688/10000: L(Train): 0.32322484254837036; L(Test): 0.3043700158596039\n",
            "Epoch 7689/10000: L(Train): 0.32756373286247253; L(Test): 0.30347079038619995\n",
            "Epoch 7690/10000: L(Train): 0.3220254182815552; L(Test): 0.30283254384994507\n",
            "Epoch 7691/10000: L(Train): 0.3158246874809265; L(Test): 0.3045431673526764\n",
            "Epoch 7692/10000: L(Train): 0.3353903591632843; L(Test): 0.3052281439304352\n",
            "Epoch 7693/10000: L(Train): 0.3292231857776642; L(Test): 0.3035357594490051\n",
            "Epoch 7694/10000: L(Train): 0.328228235244751; L(Test): 0.3025035858154297\n",
            "Epoch 7695/10000: L(Train): 0.32386621832847595; L(Test): 0.30423545837402344\n",
            "Epoch 7696/10000: L(Train): 0.33048513531684875; L(Test): 0.30353620648384094\n",
            "Epoch 7697/10000: L(Train): 0.3290727436542511; L(Test): 0.301141619682312\n",
            "Epoch 7698/10000: L(Train): 0.31879040598869324; L(Test): 0.30137911438941956\n",
            "Epoch 7699/10000: L(Train): 0.3269612193107605; L(Test): 0.30138519406318665\n",
            "Epoch 7700/10000: L(Train): 0.3170909881591797; L(Test): 0.3003166913986206\n",
            "Epoch 7701/10000: L(Train): 0.3276209533214569; L(Test): 0.29999732971191406\n",
            "Epoch 7702/10000: L(Train): 0.330003559589386; L(Test): 0.300507515668869\n",
            "Epoch 7703/10000: L(Train): 0.3341749310493469; L(Test): 0.3000515401363373\n",
            "Epoch 7704/10000: L(Train): 0.31330639123916626; L(Test): 0.29985475540161133\n",
            "Epoch 7705/10000: L(Train): 0.3253220319747925; L(Test): 0.2989695966243744\n",
            "Epoch 7706/10000: L(Train): 0.327346533536911; L(Test): 0.2991909682750702\n",
            "Epoch 7707/10000: L(Train): 0.32724928855895996; L(Test): 0.2988710403442383\n",
            "Epoch 7708/10000: L(Train): 0.3196018636226654; L(Test): 0.29746881127357483\n",
            "Epoch 7709/10000: L(Train): 0.31606325507164; L(Test): 0.29725322127342224\n",
            "Epoch 7710/10000: L(Train): 0.3091106712818146; L(Test): 0.2972126305103302\n",
            "Epoch 7711/10000: L(Train): 0.3149973750114441; L(Test): 0.29670611023902893\n",
            "Epoch 7712/10000: L(Train): 0.318714439868927; L(Test): 0.296979159116745\n",
            "Epoch 7713/10000: L(Train): 0.3275526463985443; L(Test): 0.29661256074905396\n",
            "Epoch 7714/10000: L(Train): 0.3118421733379364; L(Test): 0.2955414950847626\n",
            "Epoch 7715/10000: L(Train): 0.3316458761692047; L(Test): 0.2955346405506134\n",
            "Epoch 7716/10000: L(Train): 0.3169157803058624; L(Test): 0.2963692545890808\n",
            "Epoch 7717/10000: L(Train): 0.3233169913291931; L(Test): 0.29635173082351685\n",
            "Epoch 7718/10000: L(Train): 0.3258139193058014; L(Test): 0.29570305347442627\n",
            "Epoch 7719/10000: L(Train): 0.3279358744621277; L(Test): 0.29638177156448364\n",
            "Epoch 7720/10000: L(Train): 0.32070818543434143; L(Test): 0.29842516779899597\n",
            "Epoch 7721/10000: L(Train): 0.31920501589775085; L(Test): 0.2991079092025757\n",
            "Epoch 7722/10000: L(Train): 0.3205975592136383; L(Test): 0.3036036491394043\n",
            "Epoch 7723/10000: L(Train): 0.3253936469554901; L(Test): 0.30643126368522644\n",
            "Epoch 7724/10000: L(Train): 0.3345186114311218; L(Test): 0.3027365207672119\n",
            "Epoch 7725/10000: L(Train): 0.333600789308548; L(Test): 0.3015460669994354\n",
            "Epoch 7726/10000: L(Train): 0.3222993314266205; L(Test): 0.30145028233528137\n",
            "Epoch 7727/10000: L(Train): 0.3204834461212158; L(Test): 0.30157509446144104\n",
            "Epoch 7728/10000: L(Train): 0.3251081109046936; L(Test): 0.30252525210380554\n",
            "Epoch 7729/10000: L(Train): 0.3263639509677887; L(Test): 0.3036097586154938\n",
            "Epoch 7730/10000: L(Train): 0.3231748640537262; L(Test): 0.30404749512672424\n",
            "Epoch 7731/10000: L(Train): 0.3195207417011261; L(Test): 0.302992045879364\n",
            "Epoch 7732/10000: L(Train): 0.3197154104709625; L(Test): 0.30219051241874695\n",
            "Epoch 7733/10000: L(Train): 0.32307857275009155; L(Test): 0.30248910188674927\n",
            "Epoch 7734/10000: L(Train): 0.32872241735458374; L(Test): 0.30333396792411804\n",
            "Epoch 7735/10000: L(Train): 0.3224492073059082; L(Test): 0.30322128534317017\n",
            "Epoch 7736/10000: L(Train): 0.32595089077949524; L(Test): 0.3031753897666931\n",
            "Epoch 7737/10000: L(Train): 0.31342339515686035; L(Test): 0.30356496572494507\n",
            "Epoch 7738/10000: L(Train): 0.3234613537788391; L(Test): 0.30227190256118774\n",
            "Epoch 7739/10000: L(Train): 0.3239339292049408; L(Test): 0.302264541387558\n",
            "Epoch 7740/10000: L(Train): 0.3304067850112915; L(Test): 0.299620121717453\n",
            "Epoch 7741/10000: L(Train): 0.323268860578537; L(Test): 0.2987317740917206\n",
            "Epoch 7742/10000: L(Train): 0.32751762866973877; L(Test): 0.30047503113746643\n",
            "Epoch 7743/10000: L(Train): 0.32390686869621277; L(Test): 0.3011448085308075\n",
            "Epoch 7744/10000: L(Train): 0.32011163234710693; L(Test): 0.2988343834877014\n",
            "Epoch 7745/10000: L(Train): 0.32696932554244995; L(Test): 0.29865726828575134\n",
            "Epoch 7746/10000: L(Train): 0.30937039852142334; L(Test): 0.30083879828453064\n",
            "Epoch 7747/10000: L(Train): 0.3373904526233673; L(Test): 0.2989893853664398\n",
            "Epoch 7748/10000: L(Train): 0.3205767869949341; L(Test): 0.29613587260246277\n",
            "Epoch 7749/10000: L(Train): 0.32477107644081116; L(Test): 0.29610177874565125\n",
            "Epoch 7750/10000: L(Train): 0.3148118555545807; L(Test): 0.29662084579467773\n",
            "Epoch 7751/10000: L(Train): 0.30858713388442993; L(Test): 0.2963476777076721\n",
            "Epoch 7752/10000: L(Train): 0.32252442836761475; L(Test): 0.2967052161693573\n",
            "Epoch 7753/10000: L(Train): 0.31725046038627625; L(Test): 0.29661256074905396\n",
            "Epoch 7754/10000: L(Train): 0.31933656334877014; L(Test): 0.29636088013648987\n",
            "Epoch 7755/10000: L(Train): 0.32642069458961487; L(Test): 0.29626914858818054\n",
            "Epoch 7756/10000: L(Train): 0.3222923278808594; L(Test): 0.29493361711502075\n",
            "Epoch 7757/10000: L(Train): 0.32007473707199097; L(Test): 0.29575201869010925\n",
            "Epoch 7758/10000: L(Train): 0.3219606876373291; L(Test): 0.29618075489997864\n",
            "Epoch 7759/10000: L(Train): 0.32187968492507935; L(Test): 0.29508864879608154\n",
            "Epoch 7760/10000: L(Train): 0.32418668270111084; L(Test): 0.29546070098876953\n",
            "Epoch 7761/10000: L(Train): 0.3158266842365265; L(Test): 0.295486181974411\n",
            "Epoch 7762/10000: L(Train): 0.31526729464530945; L(Test): 0.29414254426956177\n",
            "Epoch 7763/10000: L(Train): 0.31004780530929565; L(Test): 0.295103520154953\n",
            "Epoch 7764/10000: L(Train): 0.31692609190940857; L(Test): 0.29561182856559753\n",
            "Epoch 7765/10000: L(Train): 0.3202054798603058; L(Test): 0.29433712363243103\n",
            "Epoch 7766/10000: L(Train): 0.31333449482917786; L(Test): 0.29689517617225647\n",
            "Epoch 7767/10000: L(Train): 0.3267139196395874; L(Test): 0.2968076765537262\n",
            "Epoch 7768/10000: L(Train): 0.3157919943332672; L(Test): 0.2951173782348633\n",
            "Epoch 7769/10000: L(Train): 0.3223993480205536; L(Test): 0.29491525888442993\n",
            "Epoch 7770/10000: L(Train): 0.32024019956588745; L(Test): 0.29482975602149963\n",
            "Epoch 7771/10000: L(Train): 0.3202071487903595; L(Test): 0.2939092814922333\n",
            "Epoch 7772/10000: L(Train): 0.32165440917015076; L(Test): 0.29529714584350586\n",
            "Epoch 7773/10000: L(Train): 0.3206101059913635; L(Test): 0.2974839210510254\n",
            "Epoch 7774/10000: L(Train): 0.3103131949901581; L(Test): 0.296133428812027\n",
            "Epoch 7775/10000: L(Train): 0.3218936324119568; L(Test): 0.29450511932373047\n",
            "Epoch 7776/10000: L(Train): 0.3172665536403656; L(Test): 0.2962073087692261\n",
            "Epoch 7777/10000: L(Train): 0.31556129455566406; L(Test): 0.29725244641304016\n",
            "Epoch 7778/10000: L(Train): 0.32772958278656006; L(Test): 0.29620805382728577\n",
            "Epoch 7779/10000: L(Train): 0.32506054639816284; L(Test): 0.29574909806251526\n",
            "Epoch 7780/10000: L(Train): 0.3200056254863739; L(Test): 0.2944095730781555\n",
            "Epoch 7781/10000: L(Train): 0.3248579502105713; L(Test): 0.29447081685066223\n",
            "Epoch 7782/10000: L(Train): 0.3197096288204193; L(Test): 0.29501721262931824\n",
            "Epoch 7783/10000: L(Train): 0.32406458258628845; L(Test): 0.29495689272880554\n",
            "Epoch 7784/10000: L(Train): 0.31751006841659546; L(Test): 0.2950758934020996\n",
            "Epoch 7785/10000: L(Train): 0.3117939829826355; L(Test): 0.2957283556461334\n",
            "Epoch 7786/10000: L(Train): 0.3155421018600464; L(Test): 0.2950849235057831\n",
            "Epoch 7787/10000: L(Train): 0.3157103955745697; L(Test): 0.2949998080730438\n",
            "Epoch 7788/10000: L(Train): 0.3201717734336853; L(Test): 0.2945219576358795\n",
            "Epoch 7789/10000: L(Train): 0.3175140619277954; L(Test): 0.2959553897380829\n",
            "Epoch 7790/10000: L(Train): 0.33788034319877625; L(Test): 0.2937477231025696\n",
            "Epoch 7791/10000: L(Train): 0.3123043477535248; L(Test): 0.29415908455848694\n",
            "Epoch 7792/10000: L(Train): 0.32304659485816956; L(Test): 0.29513487219810486\n",
            "Epoch 7793/10000: L(Train): 0.3224560618400574; L(Test): 0.29383596777915955\n",
            "Epoch 7794/10000: L(Train): 0.3284735381603241; L(Test): 0.29325324296951294\n",
            "Epoch 7795/10000: L(Train): 0.30551639199256897; L(Test): 0.2942522466182709\n",
            "Epoch 7796/10000: L(Train): 0.32045236229896545; L(Test): 0.29564744234085083\n",
            "Epoch 7797/10000: L(Train): 0.3123011291027069; L(Test): 0.29633188247680664\n",
            "Epoch 7798/10000: L(Train): 0.32445475459098816; L(Test): 0.2955762445926666\n",
            "Epoch 7799/10000: L(Train): 0.3292214572429657; L(Test): 0.2948184609413147\n",
            "Epoch 7800/10000: L(Train): 0.32548192143440247; L(Test): 0.2940722405910492\n",
            "Epoch 7801/10000: L(Train): 0.3178960978984833; L(Test): 0.29572880268096924\n",
            "Epoch 7802/10000: L(Train): 0.3205811083316803; L(Test): 0.2950074076652527\n",
            "Epoch 7803/10000: L(Train): 0.3148258626461029; L(Test): 0.29541367292404175\n",
            "Epoch 7804/10000: L(Train): 0.32285401225090027; L(Test): 0.29670450091362\n",
            "Epoch 7805/10000: L(Train): 0.3268437683582306; L(Test): 0.29677093029022217\n",
            "Epoch 7806/10000: L(Train): 0.31481990218162537; L(Test): 0.2953253388404846\n",
            "Epoch 7807/10000: L(Train): 0.3240430951118469; L(Test): 0.2954810857772827\n",
            "Epoch 7808/10000: L(Train): 0.31542035937309265; L(Test): 0.2974030673503876\n",
            "Epoch 7809/10000: L(Train): 0.32709580659866333; L(Test): 0.2991484999656677\n",
            "Epoch 7810/10000: L(Train): 0.33110231161117554; L(Test): 0.29617318511009216\n",
            "Epoch 7811/10000: L(Train): 0.3221350908279419; L(Test): 0.296159952878952\n",
            "Epoch 7812/10000: L(Train): 0.3205568790435791; L(Test): 0.2984192371368408\n",
            "Epoch 7813/10000: L(Train): 0.32450157403945923; L(Test): 0.29621079564094543\n",
            "Epoch 7814/10000: L(Train): 0.3215063810348511; L(Test): 0.2948390543460846\n",
            "Epoch 7815/10000: L(Train): 0.3236137330532074; L(Test): 0.29615679383277893\n",
            "Epoch 7816/10000: L(Train): 0.31896471977233887; L(Test): 0.2966151833534241\n",
            "Epoch 7817/10000: L(Train): 0.32342955470085144; L(Test): 0.2955966889858246\n",
            "Epoch 7818/10000: L(Train): 0.3167037069797516; L(Test): 0.29513248801231384\n",
            "Epoch 7819/10000: L(Train): 0.31833401322364807; L(Test): 0.2957628071308136\n",
            "Epoch 7820/10000: L(Train): 0.33099040389060974; L(Test): 0.29511427879333496\n",
            "Epoch 7821/10000: L(Train): 0.3186902701854706; L(Test): 0.29535186290740967\n",
            "Epoch 7822/10000: L(Train): 0.33228132128715515; L(Test): 0.2965277433395386\n",
            "Epoch 7823/10000: L(Train): 0.3218690752983093; L(Test): 0.2951483428478241\n",
            "Epoch 7824/10000: L(Train): 0.3278026580810547; L(Test): 0.2947455942630768\n",
            "Epoch 7825/10000: L(Train): 0.3038066327571869; L(Test): 0.29474353790283203\n",
            "Epoch 7826/10000: L(Train): 0.31941917538642883; L(Test): 0.29454827308654785\n",
            "Epoch 7827/10000: L(Train): 0.32525211572647095; L(Test): 0.2944439649581909\n",
            "Epoch 7828/10000: L(Train): 0.32572340965270996; L(Test): 0.29479557275772095\n",
            "Epoch 7829/10000: L(Train): 0.32631129026412964; L(Test): 0.29554635286331177\n",
            "Epoch 7830/10000: L(Train): 0.3291611075401306; L(Test): 0.29689350724220276\n",
            "Epoch 7831/10000: L(Train): 0.3228922188282013; L(Test): 0.3017086684703827\n",
            "Epoch 7832/10000: L(Train): 0.3258468806743622; L(Test): 0.3042311668395996\n",
            "Epoch 7833/10000: L(Train): 0.3257884383201599; L(Test): 0.3043089807033539\n",
            "Epoch 7834/10000: L(Train): 0.33797988295555115; L(Test): 0.3042985796928406\n",
            "Epoch 7835/10000: L(Train): 0.32544422149658203; L(Test): 0.30401575565338135\n",
            "Epoch 7836/10000: L(Train): 0.33144158124923706; L(Test): 0.30444228649139404\n",
            "Epoch 7837/10000: L(Train): 0.3269520103931427; L(Test): 0.3060798645019531\n",
            "Epoch 7838/10000: L(Train): 0.3247515857219696; L(Test): 0.3065527081489563\n",
            "Epoch 7839/10000: L(Train): 0.3251684010028839; L(Test): 0.3047204315662384\n",
            "Epoch 7840/10000: L(Train): 0.3269615173339844; L(Test): 0.3053092062473297\n",
            "Epoch 7841/10000: L(Train): 0.3290117681026459; L(Test): 0.3065033555030823\n",
            "Epoch 7842/10000: L(Train): 0.3360110819339752; L(Test): 0.3062281310558319\n",
            "Epoch 7843/10000: L(Train): 0.3177454471588135; L(Test): 0.30492886900901794\n",
            "Epoch 7844/10000: L(Train): 0.3178211450576782; L(Test): 0.3040139079093933\n",
            "Epoch 7845/10000: L(Train): 0.33084961771965027; L(Test): 0.3036292493343353\n",
            "Epoch 7846/10000: L(Train): 0.3215841054916382; L(Test): 0.3030408024787903\n",
            "Epoch 7847/10000: L(Train): 0.3280729651451111; L(Test): 0.302001953125\n",
            "Epoch 7848/10000: L(Train): 0.32653677463531494; L(Test): 0.3020022213459015\n",
            "Epoch 7849/10000: L(Train): 0.3221161365509033; L(Test): 0.3029472231864929\n",
            "Epoch 7850/10000: L(Train): 0.32665538787841797; L(Test): 0.30145713686943054\n",
            "Epoch 7851/10000: L(Train): 0.31619203090667725; L(Test): 0.3029429316520691\n",
            "Epoch 7852/10000: L(Train): 0.33029285073280334; L(Test): 0.30405089259147644\n",
            "Epoch 7853/10000: L(Train): 0.326778382062912; L(Test): 0.30405378341674805\n",
            "Epoch 7854/10000: L(Train): 0.32441940903663635; L(Test): 0.30345281958580017\n",
            "Epoch 7855/10000: L(Train): 0.32330429553985596; L(Test): 0.3031022250652313\n",
            "Epoch 7856/10000: L(Train): 0.3215802311897278; L(Test): 0.3028583824634552\n",
            "Epoch 7857/10000: L(Train): 0.3322324752807617; L(Test): 0.30270901322364807\n",
            "Epoch 7858/10000: L(Train): 0.3242433965206146; L(Test): 0.30471959710121155\n",
            "Epoch 7859/10000: L(Train): 0.3226225674152374; L(Test): 0.30779677629470825\n",
            "Epoch 7860/10000: L(Train): 0.32210639119148254; L(Test): 0.3081854283809662\n",
            "Epoch 7861/10000: L(Train): 0.3358800709247589; L(Test): 0.30896902084350586\n",
            "Epoch 7862/10000: L(Train): 0.32851454615592957; L(Test): 0.3067752420902252\n",
            "Epoch 7863/10000: L(Train): 0.3252110481262207; L(Test): 0.30507397651672363\n",
            "Epoch 7864/10000: L(Train): 0.3188800811767578; L(Test): 0.3060678541660309\n",
            "Epoch 7865/10000: L(Train): 0.33169475197792053; L(Test): 0.3057973086833954\n",
            "Epoch 7866/10000: L(Train): 0.32157397270202637; L(Test): 0.3039529323577881\n",
            "Epoch 7867/10000: L(Train): 0.3272934854030609; L(Test): 0.303286075592041\n",
            "Epoch 7868/10000: L(Train): 0.3300478160381317; L(Test): 0.30316370725631714\n",
            "Epoch 7869/10000: L(Train): 0.32193052768707275; L(Test): 0.30294084548950195\n",
            "Epoch 7870/10000: L(Train): 0.32789137959480286; L(Test): 0.3023623526096344\n",
            "Epoch 7871/10000: L(Train): 0.32829028367996216; L(Test): 0.30183306336402893\n",
            "Epoch 7872/10000: L(Train): 0.31641849875450134; L(Test): 0.30210354924201965\n",
            "Epoch 7873/10000: L(Train): 0.31554436683654785; L(Test): 0.30253925919532776\n",
            "Epoch 7874/10000: L(Train): 0.32305148243904114; L(Test): 0.3014358580112457\n",
            "Epoch 7875/10000: L(Train): 0.3316859304904938; L(Test): 0.3007793128490448\n",
            "Epoch 7876/10000: L(Train): 0.32946017384529114; L(Test): 0.3005756437778473\n",
            "Epoch 7877/10000: L(Train): 0.32085853815078735; L(Test): 0.300991415977478\n",
            "Epoch 7878/10000: L(Train): 0.3209368586540222; L(Test): 0.3009442985057831\n",
            "Epoch 7879/10000: L(Train): 0.3261665105819702; L(Test): 0.30054253339767456\n",
            "Epoch 7880/10000: L(Train): 0.32091760635375977; L(Test): 0.2999067008495331\n",
            "Epoch 7881/10000: L(Train): 0.3201957643032074; L(Test): 0.2989081144332886\n",
            "Epoch 7882/10000: L(Train): 0.3248456120491028; L(Test): 0.29795876145362854\n",
            "Epoch 7883/10000: L(Train): 0.31639620661735535; L(Test): 0.2979014217853546\n",
            "Epoch 7884/10000: L(Train): 0.3137434124946594; L(Test): 0.29749205708503723\n",
            "Epoch 7885/10000: L(Train): 0.32517144083976746; L(Test): 0.2975965440273285\n",
            "Epoch 7886/10000: L(Train): 0.32849401235580444; L(Test): 0.29734179377555847\n",
            "Epoch 7887/10000: L(Train): 0.3272222876548767; L(Test): 0.2964993119239807\n",
            "Epoch 7888/10000: L(Train): 0.32023119926452637; L(Test): 0.29602935910224915\n",
            "Epoch 7889/10000: L(Train): 0.3219267725944519; L(Test): 0.29674363136291504\n",
            "Epoch 7890/10000: L(Train): 0.31664004921913147; L(Test): 0.2965479791164398\n",
            "Epoch 7891/10000: L(Train): 0.31935444474220276; L(Test): 0.29716870188713074\n",
            "Epoch 7892/10000: L(Train): 0.3098127543926239; L(Test): 0.29814475774765015\n",
            "Epoch 7893/10000: L(Train): 0.31846100091934204; L(Test): 0.2965352535247803\n",
            "Epoch 7894/10000: L(Train): 0.3181245028972626; L(Test): 0.2965258061885834\n",
            "Epoch 7895/10000: L(Train): 0.30437928438186646; L(Test): 0.29744982719421387\n",
            "Epoch 7896/10000: L(Train): 0.3227842152118683; L(Test): 0.2979309558868408\n",
            "Epoch 7897/10000: L(Train): 0.31834676861763; L(Test): 0.2984376847743988\n",
            "Epoch 7898/10000: L(Train): 0.3193884491920471; L(Test): 0.2976799011230469\n",
            "Epoch 7899/10000: L(Train): 0.3245311975479126; L(Test): 0.2966003119945526\n",
            "Epoch 7900/10000: L(Train): 0.31383153796195984; L(Test): 0.2958527207374573\n",
            "Epoch 7901/10000: L(Train): 0.32059064507484436; L(Test): 0.2952151894569397\n",
            "Epoch 7902/10000: L(Train): 0.3137267231941223; L(Test): 0.2955736815929413\n",
            "Epoch 7903/10000: L(Train): 0.3315790891647339; L(Test): 0.2957988679409027\n",
            "Epoch 7904/10000: L(Train): 0.3194601833820343; L(Test): 0.2954629957675934\n",
            "Epoch 7905/10000: L(Train): 0.3196735680103302; L(Test): 0.2952233850955963\n",
            "Epoch 7906/10000: L(Train): 0.3118728697299957; L(Test): 0.29470551013946533\n",
            "Epoch 7907/10000: L(Train): 0.3138677179813385; L(Test): 0.2944197952747345\n",
            "Epoch 7908/10000: L(Train): 0.3162197768688202; L(Test): 0.2943999767303467\n",
            "Epoch 7909/10000: L(Train): 0.3178459107875824; L(Test): 0.29486194252967834\n",
            "Epoch 7910/10000: L(Train): 0.31696152687072754; L(Test): 0.29478326439857483\n",
            "Epoch 7911/10000: L(Train): 0.3090593218803406; L(Test): 0.294573575258255\n",
            "Epoch 7912/10000: L(Train): 0.32503074407577515; L(Test): 0.29398566484451294\n",
            "Epoch 7913/10000: L(Train): 0.3256934881210327; L(Test): 0.2936953902244568\n",
            "Epoch 7914/10000: L(Train): 0.32023119926452637; L(Test): 0.2933000922203064\n",
            "Epoch 7915/10000: L(Train): 0.31782010197639465; L(Test): 0.293216735124588\n",
            "Epoch 7916/10000: L(Train): 0.31816935539245605; L(Test): 0.2935181260108948\n",
            "Epoch 7917/10000: L(Train): 0.31652796268463135; L(Test): 0.2940519452095032\n",
            "Epoch 7918/10000: L(Train): 0.31645798683166504; L(Test): 0.2937147319316864\n",
            "Epoch 7919/10000: L(Train): 0.3196783661842346; L(Test): 0.2933942973613739\n",
            "Epoch 7920/10000: L(Train): 0.3266719877719879; L(Test): 0.29369425773620605\n",
            "Epoch 7921/10000: L(Train): 0.3079257309436798; L(Test): 0.2935943901538849\n",
            "Epoch 7922/10000: L(Train): 0.31693384051322937; L(Test): 0.2931838631629944\n",
            "Epoch 7923/10000: L(Train): 0.3275887668132782; L(Test): 0.29337695240974426\n",
            "Epoch 7924/10000: L(Train): 0.3157472610473633; L(Test): 0.29459062218666077\n",
            "Epoch 7925/10000: L(Train): 0.31086328625679016; L(Test): 0.29485660791397095\n",
            "Epoch 7926/10000: L(Train): 0.3209856450557709; L(Test): 0.29388517141342163\n",
            "Epoch 7927/10000: L(Train): 0.32361599802970886; L(Test): 0.29336807131767273\n",
            "Epoch 7928/10000: L(Train): 0.31197497248649597; L(Test): 0.29378169775009155\n",
            "Epoch 7929/10000: L(Train): 0.3140389621257782; L(Test): 0.29365646839141846\n",
            "Epoch 7930/10000: L(Train): 0.321363627910614; L(Test): 0.29272976517677307\n",
            "Epoch 7931/10000: L(Train): 0.32080355286598206; L(Test): 0.29269546270370483\n",
            "Epoch 7932/10000: L(Train): 0.3170569837093353; L(Test): 0.29255807399749756\n",
            "Epoch 7933/10000: L(Train): 0.3170517683029175; L(Test): 0.29247230291366577\n",
            "Epoch 7934/10000: L(Train): 0.31633076071739197; L(Test): 0.29244089126586914\n",
            "Epoch 7935/10000: L(Train): 0.31694164872169495; L(Test): 0.29201310873031616\n",
            "Epoch 7936/10000: L(Train): 0.30637598037719727; L(Test): 0.2917238473892212\n",
            "Epoch 7937/10000: L(Train): 0.31799614429473877; L(Test): 0.2925003170967102\n",
            "Epoch 7938/10000: L(Train): 0.3251829743385315; L(Test): 0.29292842745780945\n",
            "Epoch 7939/10000: L(Train): 0.3175680935382843; L(Test): 0.2924451529979706\n",
            "Epoch 7940/10000: L(Train): 0.30493631958961487; L(Test): 0.29242995381355286\n",
            "Epoch 7941/10000: L(Train): 0.31845197081565857; L(Test): 0.29207634925842285\n",
            "Epoch 7942/10000: L(Train): 0.31840357184410095; L(Test): 0.29329347610473633\n",
            "Epoch 7943/10000: L(Train): 0.3107665181159973; L(Test): 0.2928658425807953\n",
            "Epoch 7944/10000: L(Train): 0.3271160125732422; L(Test): 0.2926442325115204\n",
            "Epoch 7945/10000: L(Train): 0.3225249946117401; L(Test): 0.2942103147506714\n",
            "Epoch 7946/10000: L(Train): 0.32075950503349304; L(Test): 0.2936139702796936\n",
            "Epoch 7947/10000: L(Train): 0.3202321529388428; L(Test): 0.2961725890636444\n",
            "Epoch 7948/10000: L(Train): 0.32390135526657104; L(Test): 0.29839402437210083\n",
            "Epoch 7949/10000: L(Train): 0.3316551148891449; L(Test): 0.2951834797859192\n",
            "Epoch 7950/10000: L(Train): 0.31182852387428284; L(Test): 0.2963123619556427\n",
            "Epoch 7951/10000: L(Train): 0.3199993669986725; L(Test): 0.2984172999858856\n",
            "Epoch 7952/10000: L(Train): 0.3289586901664734; L(Test): 0.296550452709198\n",
            "Epoch 7953/10000: L(Train): 0.31669294834136963; L(Test): 0.29914090037345886\n",
            "Epoch 7954/10000: L(Train): 0.33456870913505554; L(Test): 0.299715131521225\n",
            "Epoch 7955/10000: L(Train): 0.3280223608016968; L(Test): 0.29845181107521057\n",
            "Epoch 7956/10000: L(Train): 0.3191744089126587; L(Test): 0.2971767783164978\n",
            "Epoch 7957/10000: L(Train): 0.32867929339408875; L(Test): 0.2970506250858307\n",
            "Epoch 7958/10000: L(Train): 0.323764830827713; L(Test): 0.29882195591926575\n",
            "Epoch 7959/10000: L(Train): 0.3215370774269104; L(Test): 0.2987370789051056\n",
            "Epoch 7960/10000: L(Train): 0.32142874598503113; L(Test): 0.29743415117263794\n",
            "Epoch 7961/10000: L(Train): 0.32245364785194397; L(Test): 0.29774585366249084\n",
            "Epoch 7962/10000: L(Train): 0.33045294880867004; L(Test): 0.29679936170578003\n",
            "Epoch 7963/10000: L(Train): 0.32238513231277466; L(Test): 0.29665157198905945\n",
            "Epoch 7964/10000: L(Train): 0.3248338997364044; L(Test): 0.2985035479068756\n",
            "Epoch 7965/10000: L(Train): 0.3229830861091614; L(Test): 0.298701673746109\n",
            "Epoch 7966/10000: L(Train): 0.3221736252307892; L(Test): 0.2979121506214142\n",
            "Epoch 7967/10000: L(Train): 0.3095386326313019; L(Test): 0.2982530891895294\n",
            "Epoch 7968/10000: L(Train): 0.327044278383255; L(Test): 0.2962770462036133\n",
            "Epoch 7969/10000: L(Train): 0.32613611221313477; L(Test): 0.294996440410614\n",
            "Epoch 7970/10000: L(Train): 0.3145352900028229; L(Test): 0.2970372438430786\n",
            "Epoch 7971/10000: L(Train): 0.32987284660339355; L(Test): 0.2966347932815552\n",
            "Epoch 7972/10000: L(Train): 0.3216443657875061; L(Test): 0.29539933800697327\n",
            "Epoch 7973/10000: L(Train): 0.3247784674167633; L(Test): 0.295314222574234\n",
            "Epoch 7974/10000: L(Train): 0.323768675327301; L(Test): 0.29508256912231445\n",
            "Epoch 7975/10000: L(Train): 0.32044655084609985; L(Test): 0.2967183589935303\n",
            "Epoch 7976/10000: L(Train): 0.32930952310562134; L(Test): 0.2954183518886566\n",
            "Epoch 7977/10000: L(Train): 0.32955414056777954; L(Test): 0.2945252060890198\n",
            "Epoch 7978/10000: L(Train): 0.30928531289100647; L(Test): 0.2945552170276642\n",
            "Epoch 7979/10000: L(Train): 0.3263119161128998; L(Test): 0.2939647138118744\n",
            "Epoch 7980/10000: L(Train): 0.317684531211853; L(Test): 0.29470959305763245\n",
            "Epoch 7981/10000: L(Train): 0.31417202949523926; L(Test): 0.2950175404548645\n",
            "Epoch 7982/10000: L(Train): 0.3164565861225128; L(Test): 0.2936197519302368\n",
            "Epoch 7983/10000: L(Train): 0.31848013401031494; L(Test): 0.29408377408981323\n",
            "Epoch 7984/10000: L(Train): 0.3313063681125641; L(Test): 0.2943120300769806\n",
            "Epoch 7985/10000: L(Train): 0.3220788836479187; L(Test): 0.29371240735054016\n",
            "Epoch 7986/10000: L(Train): 0.3156315088272095; L(Test): 0.2946797013282776\n",
            "Epoch 7987/10000: L(Train): 0.30978843569755554; L(Test): 0.29466569423675537\n",
            "Epoch 7988/10000: L(Train): 0.3232192099094391; L(Test): 0.29366636276245117\n",
            "Epoch 7989/10000: L(Train): 0.3160177171230316; L(Test): 0.29589197039604187\n",
            "Epoch 7990/10000: L(Train): 0.33262160420417786; L(Test): 0.29461270570755005\n",
            "Epoch 7991/10000: L(Train): 0.3119651675224304; L(Test): 0.2942744791507721\n",
            "Epoch 7992/10000: L(Train): 0.3226756155490875; L(Test): 0.2960982620716095\n",
            "Epoch 7993/10000: L(Train): 0.3238329589366913; L(Test): 0.29533255100250244\n",
            "Epoch 7994/10000: L(Train): 0.31495586037635803; L(Test): 0.2930184006690979\n",
            "Epoch 7995/10000: L(Train): 0.31022098660469055; L(Test): 0.2951461970806122\n",
            "Epoch 7996/10000: L(Train): 0.31970682740211487; L(Test): 0.2932100296020508\n",
            "Epoch 7997/10000: L(Train): 0.3162723481655121; L(Test): 0.29532167315483093\n",
            "Epoch 7998/10000: L(Train): 0.32263991236686707; L(Test): 0.29967042803764343\n",
            "Epoch 7999/10000: L(Train): 0.32977184653282166; L(Test): 0.2975178360939026\n",
            "Epoch 8000/10000: L(Train): 0.3243381679058075; L(Test): 0.2959025204181671\n",
            "Epoch 8001/10000: L(Train): 0.31773045659065247; L(Test): 0.29587069153785706\n",
            "Epoch 8002/10000: L(Train): 0.3241957724094391; L(Test): 0.29648640751838684\n",
            "Epoch 8003/10000: L(Train): 0.32886379957199097; L(Test): 0.2972125709056854\n",
            "Epoch 8004/10000: L(Train): 0.3228476643562317; L(Test): 0.2962284982204437\n",
            "Epoch 8005/10000: L(Train): 0.3200432062149048; L(Test): 0.2957490086555481\n",
            "Epoch 8006/10000: L(Train): 0.32119882106781006; L(Test): 0.296165406703949\n",
            "Epoch 8007/10000: L(Train): 0.31053245067596436; L(Test): 0.29751935601234436\n",
            "Epoch 8008/10000: L(Train): 0.325758159160614; L(Test): 0.29725030064582825\n",
            "Epoch 8009/10000: L(Train): 0.3197525143623352; L(Test): 0.29642602801322937\n",
            "Epoch 8010/10000: L(Train): 0.3159855902194977; L(Test): 0.2965913414955139\n",
            "Epoch 8011/10000: L(Train): 0.3288649618625641; L(Test): 0.2955498993396759\n",
            "Epoch 8012/10000: L(Train): 0.3215778172016144; L(Test): 0.295270711183548\n",
            "Epoch 8013/10000: L(Train): 0.326528400182724; L(Test): 0.2956700325012207\n",
            "Epoch 8014/10000: L(Train): 0.32187676429748535; L(Test): 0.2955378293991089\n",
            "Epoch 8015/10000: L(Train): 0.3117683529853821; L(Test): 0.2942773103713989\n",
            "Epoch 8016/10000: L(Train): 0.32365870475769043; L(Test): 0.2952192723751068\n",
            "Epoch 8017/10000: L(Train): 0.31750673055648804; L(Test): 0.29435425996780396\n",
            "Epoch 8018/10000: L(Train): 0.32497796416282654; L(Test): 0.29537278413772583\n",
            "Epoch 8019/10000: L(Train): 0.3166355490684509; L(Test): 0.29734888672828674\n",
            "Epoch 8020/10000: L(Train): 0.32154935598373413; L(Test): 0.2947614789009094\n",
            "Epoch 8021/10000: L(Train): 0.3116317689418793; L(Test): 0.29527968168258667\n",
            "Epoch 8022/10000: L(Train): 0.31829333305358887; L(Test): 0.2950388193130493\n",
            "Epoch 8023/10000: L(Train): 0.3267352283000946; L(Test): 0.2939814031124115\n",
            "Epoch 8024/10000: L(Train): 0.318672239780426; L(Test): 0.29610323905944824\n",
            "Epoch 8025/10000: L(Train): 0.332089364528656; L(Test): 0.2962609529495239\n",
            "Epoch 8026/10000: L(Train): 0.3197859227657318; L(Test): 0.295148640871048\n",
            "Epoch 8027/10000: L(Train): 0.32455506920814514; L(Test): 0.29520320892333984\n",
            "Epoch 8028/10000: L(Train): 0.3150438368320465; L(Test): 0.29442453384399414\n",
            "Epoch 8029/10000: L(Train): 0.3261411488056183; L(Test): 0.2965376079082489\n",
            "Epoch 8030/10000: L(Train): 0.3266264796257019; L(Test): 0.29705125093460083\n",
            "Epoch 8031/10000: L(Train): 0.3124384880065918; L(Test): 0.2947198748588562\n",
            "Epoch 8032/10000: L(Train): 0.32160791754722595; L(Test): 0.2952120304107666\n",
            "Epoch 8033/10000: L(Train): 0.3246079385280609; L(Test): 0.2949393689632416\n",
            "Epoch 8034/10000: L(Train): 0.32137733697891235; L(Test): 0.2935173511505127\n",
            "Epoch 8035/10000: L(Train): 0.32142046093940735; L(Test): 0.29550570249557495\n",
            "Epoch 8036/10000: L(Train): 0.333217054605484; L(Test): 0.294666588306427\n",
            "Epoch 8037/10000: L(Train): 0.31313616037368774; L(Test): 0.29299017786979675\n",
            "Epoch 8038/10000: L(Train): 0.32355940341949463; L(Test): 0.294141560792923\n",
            "Epoch 8039/10000: L(Train): 0.3269268274307251; L(Test): 0.2943301796913147\n",
            "Epoch 8040/10000: L(Train): 0.311654269695282; L(Test): 0.29306483268737793\n",
            "Epoch 8041/10000: L(Train): 0.32230523228645325; L(Test): 0.29492318630218506\n",
            "Epoch 8042/10000: L(Train): 0.3248078227043152; L(Test): 0.29394829273223877\n",
            "Epoch 8043/10000: L(Train): 0.32071489095687866; L(Test): 0.2930362820625305\n",
            "Epoch 8044/10000: L(Train): 0.31491127610206604; L(Test): 0.2941370904445648\n",
            "Epoch 8045/10000: L(Train): 0.3178417980670929; L(Test): 0.2928512394428253\n",
            "Epoch 8046/10000: L(Train): 0.3181460201740265; L(Test): 0.29194048047065735\n",
            "Epoch 8047/10000: L(Train): 0.31839197874069214; L(Test): 0.2927049994468689\n",
            "Epoch 8048/10000: L(Train): 0.3113952875137329; L(Test): 0.2929213047027588\n",
            "Epoch 8049/10000: L(Train): 0.31992828845977783; L(Test): 0.29274651408195496\n",
            "Epoch 8050/10000: L(Train): 0.32937154173851013; L(Test): 0.29325366020202637\n",
            "Epoch 8051/10000: L(Train): 0.3215828835964203; L(Test): 0.2927399277687073\n",
            "Epoch 8052/10000: L(Train): 0.33457237482070923; L(Test): 0.2929728627204895\n",
            "Epoch 8053/10000: L(Train): 0.3232647776603699; L(Test): 0.29259228706359863\n",
            "Epoch 8054/10000: L(Train): 0.3248487114906311; L(Test): 0.29248717427253723\n",
            "Epoch 8055/10000: L(Train): 0.3111599385738373; L(Test): 0.29267072677612305\n",
            "Epoch 8056/10000: L(Train): 0.3156905770301819; L(Test): 0.2929547131061554\n",
            "Epoch 8057/10000: L(Train): 0.32452866435050964; L(Test): 0.29280391335487366\n",
            "Epoch 8058/10000: L(Train): 0.3138648569583893; L(Test): 0.29242268204689026\n",
            "Epoch 8059/10000: L(Train): 0.32286006212234497; L(Test): 0.2919944226741791\n",
            "Epoch 8060/10000: L(Train): 0.3268543481826782; L(Test): 0.2919473350048065\n",
            "Epoch 8061/10000: L(Train): 0.32065728306770325; L(Test): 0.29246240854263306\n",
            "Epoch 8062/10000: L(Train): 0.32029104232788086; L(Test): 0.29288703203201294\n",
            "Epoch 8063/10000: L(Train): 0.32280462980270386; L(Test): 0.2918277084827423\n",
            "Epoch 8064/10000: L(Train): 0.315628319978714; L(Test): 0.29212719202041626\n",
            "Epoch 8065/10000: L(Train): 0.3106626272201538; L(Test): 0.292591392993927\n",
            "Epoch 8066/10000: L(Train): 0.3140115439891815; L(Test): 0.2954028248786926\n",
            "Epoch 8067/10000: L(Train): 0.3121853768825531; L(Test): 0.297249972820282\n",
            "Epoch 8068/10000: L(Train): 0.3266545832157135; L(Test): 0.2979157567024231\n",
            "Epoch 8069/10000: L(Train): 0.31705033779144287; L(Test): 0.29720258712768555\n",
            "Epoch 8070/10000: L(Train): 0.31897789239883423; L(Test): 0.29654014110565186\n",
            "Epoch 8071/10000: L(Train): 0.33425360918045044; L(Test): 0.29631757736206055\n",
            "Epoch 8072/10000: L(Train): 0.3202715218067169; L(Test): 0.2948087751865387\n",
            "Epoch 8073/10000: L(Train): 0.32122549414634705; L(Test): 0.29581689834594727\n",
            "Epoch 8074/10000: L(Train): 0.3212810754776001; L(Test): 0.2970416843891144\n",
            "Epoch 8075/10000: L(Train): 0.3156798183917999; L(Test): 0.2958761155605316\n",
            "Epoch 8076/10000: L(Train): 0.3223320543766022; L(Test): 0.2943710386753082\n",
            "Epoch 8077/10000: L(Train): 0.323413610458374; L(Test): 0.29631292819976807\n",
            "Epoch 8078/10000: L(Train): 0.32187753915786743; L(Test): 0.2954946756362915\n",
            "Epoch 8079/10000: L(Train): 0.33052492141723633; L(Test): 0.2955126464366913\n",
            "Epoch 8080/10000: L(Train): 0.31910204887390137; L(Test): 0.29643821716308594\n",
            "Epoch 8081/10000: L(Train): 0.3204692006111145; L(Test): 0.29519957304000854\n",
            "Epoch 8082/10000: L(Train): 0.32502955198287964; L(Test): 0.2952549159526825\n",
            "Epoch 8083/10000: L(Train): 0.31951966881752014; L(Test): 0.2948755919933319\n",
            "Epoch 8084/10000: L(Train): 0.3202490210533142; L(Test): 0.29551851749420166\n",
            "Epoch 8085/10000: L(Train): 0.32539868354797363; L(Test): 0.2955667972564697\n",
            "Epoch 8086/10000: L(Train): 0.31792327761650085; L(Test): 0.29618582129478455\n",
            "Epoch 8087/10000: L(Train): 0.33237311244010925; L(Test): 0.29653993248939514\n",
            "Epoch 8088/10000: L(Train): 0.3202625513076782; L(Test): 0.29564106464385986\n",
            "Epoch 8089/10000: L(Train): 0.3234139084815979; L(Test): 0.294636607170105\n",
            "Epoch 8090/10000: L(Train): 0.3165944814682007; L(Test): 0.2949405312538147\n",
            "Epoch 8091/10000: L(Train): 0.31580379605293274; L(Test): 0.29597997665405273\n",
            "Epoch 8092/10000: L(Train): 0.3284628689289093; L(Test): 0.29624736309051514\n",
            "Epoch 8093/10000: L(Train): 0.3209854066371918; L(Test): 0.2954373061656952\n",
            "Epoch 8094/10000: L(Train): 0.31559717655181885; L(Test): 0.29440075159072876\n",
            "Epoch 8095/10000: L(Train): 0.3188105523586273; L(Test): 0.2955765426158905\n",
            "Epoch 8096/10000: L(Train): 0.3216465413570404; L(Test): 0.2943393886089325\n",
            "Epoch 8097/10000: L(Train): 0.31716054677963257; L(Test): 0.29562246799468994\n",
            "Epoch 8098/10000: L(Train): 0.3064546585083008; L(Test): 0.2972165644168854\n",
            "Epoch 8099/10000: L(Train): 0.3107353150844574; L(Test): 0.2970137596130371\n",
            "Epoch 8100/10000: L(Train): 0.319305419921875; L(Test): 0.2946661710739136\n",
            "Epoch 8101/10000: L(Train): 0.31584152579307556; L(Test): 0.29493021965026855\n",
            "Epoch 8102/10000: L(Train): 0.3259285092353821; L(Test): 0.2965814769268036\n",
            "Epoch 8103/10000: L(Train): 0.318459689617157; L(Test): 0.29500800371170044\n",
            "Epoch 8104/10000: L(Train): 0.32388484477996826; L(Test): 0.29610639810562134\n",
            "Epoch 8105/10000: L(Train): 0.3239562511444092; L(Test): 0.29588645696640015\n",
            "Epoch 8106/10000: L(Train): 0.3115629553794861; L(Test): 0.29501232504844666\n",
            "Epoch 8107/10000: L(Train): 0.32121914625167847; L(Test): 0.29439666867256165\n",
            "Epoch 8108/10000: L(Train): 0.3160974383354187; L(Test): 0.29460635781288147\n",
            "Epoch 8109/10000: L(Train): 0.3112860321998596; L(Test): 0.29362720251083374\n",
            "Epoch 8110/10000: L(Train): 0.32686182856559753; L(Test): 0.29392412304878235\n",
            "Epoch 8111/10000: L(Train): 0.3166419565677643; L(Test): 0.2939341366291046\n",
            "Epoch 8112/10000: L(Train): 0.32234033942222595; L(Test): 0.29277318716049194\n",
            "Epoch 8113/10000: L(Train): 0.32904741168022156; L(Test): 0.2928619086742401\n",
            "Epoch 8114/10000: L(Train): 0.3144471049308777; L(Test): 0.29329442977905273\n",
            "Epoch 8115/10000: L(Train): 0.3170844316482544; L(Test): 0.2938714027404785\n",
            "Epoch 8116/10000: L(Train): 0.3175356090068817; L(Test): 0.2958752512931824\n",
            "Epoch 8117/10000: L(Train): 0.30976101756095886; L(Test): 0.29546090960502625\n",
            "Epoch 8118/10000: L(Train): 0.32622724771499634; L(Test): 0.2950056493282318\n",
            "Epoch 8119/10000: L(Train): 0.3261150121688843; L(Test): 0.2961218059062958\n",
            "Epoch 8120/10000: L(Train): 0.3144553601741791; L(Test): 0.29504331946372986\n",
            "Epoch 8121/10000: L(Train): 0.31491225957870483; L(Test): 0.29432934522628784\n",
            "Epoch 8122/10000: L(Train): 0.3170824348926544; L(Test): 0.29451996088027954\n",
            "Epoch 8123/10000: L(Train): 0.31494250893592834; L(Test): 0.2951015532016754\n",
            "Epoch 8124/10000: L(Train): 0.3209229111671448; L(Test): 0.29416704177856445\n",
            "Epoch 8125/10000: L(Train): 0.30499324202537537; L(Test): 0.2938704490661621\n",
            "Epoch 8126/10000: L(Train): 0.32410097122192383; L(Test): 0.29403436183929443\n",
            "Epoch 8127/10000: L(Train): 0.33148106932640076; L(Test): 0.29357221722602844\n",
            "Epoch 8128/10000: L(Train): 0.3214506506919861; L(Test): 0.29433706402778625\n",
            "Epoch 8129/10000: L(Train): 0.3138522207736969; L(Test): 0.2939867377281189\n",
            "Epoch 8130/10000: L(Train): 0.32092541456222534; L(Test): 0.293554425239563\n",
            "Epoch 8131/10000: L(Train): 0.3193817436695099; L(Test): 0.29451921582221985\n",
            "Epoch 8132/10000: L(Train): 0.3197754919528961; L(Test): 0.29453951120376587\n",
            "Epoch 8133/10000: L(Train): 0.3205762505531311; L(Test): 0.29408255219459534\n",
            "Epoch 8134/10000: L(Train): 0.32302767038345337; L(Test): 0.29442664980888367\n",
            "Epoch 8135/10000: L(Train): 0.3256814181804657; L(Test): 0.2940402328968048\n",
            "Epoch 8136/10000: L(Train): 0.32235807180404663; L(Test): 0.2934640347957611\n",
            "Epoch 8137/10000: L(Train): 0.3317492604255676; L(Test): 0.29482242465019226\n",
            "Epoch 8138/10000: L(Train): 0.32251083850860596; L(Test): 0.2950611710548401\n",
            "Epoch 8139/10000: L(Train): 0.3222060203552246; L(Test): 0.2943463623523712\n",
            "Epoch 8140/10000: L(Train): 0.32382267713546753; L(Test): 0.29681453108787537\n",
            "Epoch 8141/10000: L(Train): 0.3194275200366974; L(Test): 0.29614055156707764\n",
            "Epoch 8142/10000: L(Train): 0.3167645335197449; L(Test): 0.2962878346443176\n",
            "Epoch 8143/10000: L(Train): 0.31736376881599426; L(Test): 0.2941059172153473\n",
            "Epoch 8144/10000: L(Train): 0.3170343041419983; L(Test): 0.29350003600120544\n",
            "Epoch 8145/10000: L(Train): 0.3148566484451294; L(Test): 0.2941190302371979\n",
            "Epoch 8146/10000: L(Train): 0.3266426622867584; L(Test): 0.2934156060218811\n",
            "Epoch 8147/10000: L(Train): 0.3203124701976776; L(Test): 0.2941272556781769\n",
            "Epoch 8148/10000: L(Train): 0.3227182924747467; L(Test): 0.29608118534088135\n",
            "Epoch 8149/10000: L(Train): 0.31501874327659607; L(Test): 0.2956944704055786\n",
            "Epoch 8150/10000: L(Train): 0.31970810890197754; L(Test): 0.29422804713249207\n",
            "Epoch 8151/10000: L(Train): 0.32184046506881714; L(Test): 0.2956138849258423\n",
            "Epoch 8152/10000: L(Train): 0.3209320604801178; L(Test): 0.29479119181632996\n",
            "Epoch 8153/10000: L(Train): 0.3129618763923645; L(Test): 0.2940027415752411\n",
            "Epoch 8154/10000: L(Train): 0.3122698962688446; L(Test): 0.29442089796066284\n",
            "Epoch 8155/10000: L(Train): 0.3148864805698395; L(Test): 0.29586121439933777\n",
            "Epoch 8156/10000: L(Train): 0.3165206015110016; L(Test): 0.2956819534301758\n",
            "Epoch 8157/10000: L(Train): 0.3312370777130127; L(Test): 0.2944727838039398\n",
            "Epoch 8158/10000: L(Train): 0.3198789954185486; L(Test): 0.2957395315170288\n",
            "Epoch 8159/10000: L(Train): 0.31152650713920593; L(Test): 0.29517990350723267\n",
            "Epoch 8160/10000: L(Train): 0.31489571928977966; L(Test): 0.2939153015613556\n",
            "Epoch 8161/10000: L(Train): 0.31892088055610657; L(Test): 0.2945573329925537\n",
            "Epoch 8162/10000: L(Train): 0.31543153524398804; L(Test): 0.29383403062820435\n",
            "Epoch 8163/10000: L(Train): 0.3228379786014557; L(Test): 0.2938438355922699\n",
            "Epoch 8164/10000: L(Train): 0.31449565291404724; L(Test): 0.2939648926258087\n",
            "Epoch 8165/10000: L(Train): 0.3157845139503479; L(Test): 0.2940366864204407\n",
            "Epoch 8166/10000: L(Train): 0.3163493871688843; L(Test): 0.29585394263267517\n",
            "Epoch 8167/10000: L(Train): 0.328328013420105; L(Test): 0.2958914339542389\n",
            "Epoch 8168/10000: L(Train): 0.3214244544506073; L(Test): 0.29401299357414246\n",
            "Epoch 8169/10000: L(Train): 0.31525471806526184; L(Test): 0.2938261926174164\n",
            "Epoch 8170/10000: L(Train): 0.31612277030944824; L(Test): 0.2944427728652954\n",
            "Epoch 8171/10000: L(Train): 0.32619157433509827; L(Test): 0.2939053475856781\n",
            "Epoch 8172/10000: L(Train): 0.311512291431427; L(Test): 0.29504796862602234\n",
            "Epoch 8173/10000: L(Train): 0.3244115114212036; L(Test): 0.2963491380214691\n",
            "Epoch 8174/10000: L(Train): 0.33597657084465027; L(Test): 0.2949509620666504\n",
            "Epoch 8175/10000: L(Train): 0.3197287917137146; L(Test): 0.2957887649536133\n",
            "Epoch 8176/10000: L(Train): 0.31505000591278076; L(Test): 0.2965967357158661\n",
            "Epoch 8177/10000: L(Train): 0.324001282453537; L(Test): 0.2937126159667969\n",
            "Epoch 8178/10000: L(Train): 0.313724547624588; L(Test): 0.29560011625289917\n",
            "Epoch 8179/10000: L(Train): 0.31078335642814636; L(Test): 0.2986661493778229\n",
            "Epoch 8180/10000: L(Train): 0.31744900345802307; L(Test): 0.29615354537963867\n",
            "Epoch 8181/10000: L(Train): 0.32374444603919983; L(Test): 0.29572850465774536\n",
            "Epoch 8182/10000: L(Train): 0.31576067209243774; L(Test): 0.2950497269630432\n",
            "Epoch 8183/10000: L(Train): 0.3123997747898102; L(Test): 0.29383111000061035\n",
            "Epoch 8184/10000: L(Train): 0.3220175504684448; L(Test): 0.2933025658130646\n",
            "Epoch 8185/10000: L(Train): 0.3218063712120056; L(Test): 0.2948928475379944\n",
            "Epoch 8186/10000: L(Train): 0.3109532296657562; L(Test): 0.2962223291397095\n",
            "Epoch 8187/10000: L(Train): 0.3132498860359192; L(Test): 0.295429527759552\n",
            "Epoch 8188/10000: L(Train): 0.3215568959712982; L(Test): 0.29468056559562683\n",
            "Epoch 8189/10000: L(Train): 0.3136788308620453; L(Test): 0.2932620942592621\n",
            "Epoch 8190/10000: L(Train): 0.32584846019744873; L(Test): 0.2944278419017792\n",
            "Epoch 8191/10000: L(Train): 0.32445210218429565; L(Test): 0.29509761929512024\n",
            "Epoch 8192/10000: L(Train): 0.31705448031425476; L(Test): 0.29503411054611206\n",
            "Epoch 8193/10000: L(Train): 0.32236504554748535; L(Test): 0.29429471492767334\n",
            "Epoch 8194/10000: L(Train): 0.32509586215019226; L(Test): 0.2949275076389313\n",
            "Epoch 8195/10000: L(Train): 0.3253512978553772; L(Test): 0.2946803867816925\n",
            "Epoch 8196/10000: L(Train): 0.31885826587677; L(Test): 0.2941436469554901\n",
            "Epoch 8197/10000: L(Train): 0.327554851770401; L(Test): 0.2950897514820099\n",
            "Epoch 8198/10000: L(Train): 0.3076879382133484; L(Test): 0.29563188552856445\n",
            "Epoch 8199/10000: L(Train): 0.318973571062088; L(Test): 0.296047180891037\n",
            "Epoch 8200/10000: L(Train): 0.32768121361732483; L(Test): 0.2952326536178589\n",
            "Epoch 8201/10000: L(Train): 0.3193862736225128; L(Test): 0.296094685792923\n",
            "Epoch 8202/10000: L(Train): 0.31869226694107056; L(Test): 0.29692092537879944\n",
            "Epoch 8203/10000: L(Train): 0.3141302764415741; L(Test): 0.2947867512702942\n",
            "Epoch 8204/10000: L(Train): 0.3147183358669281; L(Test): 0.2947843372821808\n",
            "Epoch 8205/10000: L(Train): 0.32181647419929504; L(Test): 0.29671138525009155\n",
            "Epoch 8206/10000: L(Train): 0.3190252482891083; L(Test): 0.29555970430374146\n",
            "Epoch 8207/10000: L(Train): 0.3200170397758484; L(Test): 0.2950419485569\n",
            "Epoch 8208/10000: L(Train): 0.319398432970047; L(Test): 0.29584047198295593\n",
            "Epoch 8209/10000: L(Train): 0.3046374022960663; L(Test): 0.2964760959148407\n",
            "Epoch 8210/10000: L(Train): 0.31710365414619446; L(Test): 0.29487377405166626\n",
            "Epoch 8211/10000: L(Train): 0.3209138512611389; L(Test): 0.2951952815055847\n",
            "Epoch 8212/10000: L(Train): 0.3106791377067566; L(Test): 0.2964169681072235\n",
            "Epoch 8213/10000: L(Train): 0.3175438344478607; L(Test): 0.29543277621269226\n",
            "Epoch 8214/10000: L(Train): 0.3199370205402374; L(Test): 0.29633796215057373\n",
            "Epoch 8215/10000: L(Train): 0.3195229172706604; L(Test): 0.2963527739048004\n",
            "Epoch 8216/10000: L(Train): 0.32382676005363464; L(Test): 0.29619649052619934\n",
            "Epoch 8217/10000: L(Train): 0.3161962032318115; L(Test): 0.29532140493392944\n",
            "Epoch 8218/10000: L(Train): 0.3226977586746216; L(Test): 0.2944071888923645\n",
            "Epoch 8219/10000: L(Train): 0.3173503875732422; L(Test): 0.2939571142196655\n",
            "Epoch 8220/10000: L(Train): 0.3190295994281769; L(Test): 0.29465585947036743\n",
            "Epoch 8221/10000: L(Train): 0.32013607025146484; L(Test): 0.2945272922515869\n",
            "Epoch 8222/10000: L(Train): 0.32165223360061646; L(Test): 0.29410287737846375\n",
            "Epoch 8223/10000: L(Train): 0.3244950473308563; L(Test): 0.29573529958724976\n",
            "Epoch 8224/10000: L(Train): 0.31681331992149353; L(Test): 0.2964276969432831\n",
            "Epoch 8225/10000: L(Train): 0.3244856297969818; L(Test): 0.29495424032211304\n",
            "Epoch 8226/10000: L(Train): 0.3145330250263214; L(Test): 0.2939684987068176\n",
            "Epoch 8227/10000: L(Train): 0.32174059748649597; L(Test): 0.2938386797904968\n",
            "Epoch 8228/10000: L(Train): 0.317064106464386; L(Test): 0.2947732210159302\n",
            "Epoch 8229/10000: L(Train): 0.31692102551460266; L(Test): 0.29439976811408997\n",
            "Epoch 8230/10000: L(Train): 0.32220137119293213; L(Test): 0.29596737027168274\n",
            "Epoch 8231/10000: L(Train): 0.31683701276779175; L(Test): 0.29832926392555237\n",
            "Epoch 8232/10000: L(Train): 0.32864511013031006; L(Test): 0.2977350354194641\n",
            "Epoch 8233/10000: L(Train): 0.3176749646663666; L(Test): 0.2950553596019745\n",
            "Epoch 8234/10000: L(Train): 0.3314036726951599; L(Test): 0.29670020937919617\n",
            "Epoch 8235/10000: L(Train): 0.3253580629825592; L(Test): 0.2980785369873047\n",
            "Epoch 8236/10000: L(Train): 0.3204870820045471; L(Test): 0.29705989360809326\n",
            "Epoch 8237/10000: L(Train): 0.3185560703277588; L(Test): 0.29550597071647644\n",
            "Epoch 8238/10000: L(Train): 0.32085534930229187; L(Test): 0.296379029750824\n",
            "Epoch 8239/10000: L(Train): 0.31510433554649353; L(Test): 0.29855281114578247\n",
            "Epoch 8240/10000: L(Train): 0.32207953929901123; L(Test): 0.297487735748291\n",
            "Epoch 8241/10000: L(Train): 0.3303404748439789; L(Test): 0.2966693639755249\n",
            "Epoch 8242/10000: L(Train): 0.31855982542037964; L(Test): 0.2961624264717102\n",
            "Epoch 8243/10000: L(Train): 0.3224948048591614; L(Test): 0.29475605487823486\n",
            "Epoch 8244/10000: L(Train): 0.3168107569217682; L(Test): 0.29683050513267517\n",
            "Epoch 8245/10000: L(Train): 0.3244628310203552; L(Test): 0.29840734601020813\n",
            "Epoch 8246/10000: L(Train): 0.32616329193115234; L(Test): 0.29966607689857483\n",
            "Epoch 8247/10000: L(Train): 0.3268705904483795; L(Test): 0.29703468084335327\n",
            "Epoch 8248/10000: L(Train): 0.33069169521331787; L(Test): 0.3002426028251648\n",
            "Epoch 8249/10000: L(Train): 0.3213318884372711; L(Test): 0.3011375665664673\n",
            "Epoch 8250/10000: L(Train): 0.32604509592056274; L(Test): 0.29840490221977234\n",
            "Epoch 8251/10000: L(Train): 0.32583802938461304; L(Test): 0.29774171113967896\n",
            "Epoch 8252/10000: L(Train): 0.33449846506118774; L(Test): 0.29948690533638\n",
            "Epoch 8253/10000: L(Train): 0.3202952444553375; L(Test): 0.29886430501937866\n",
            "Epoch 8254/10000: L(Train): 0.32055434584617615; L(Test): 0.2972647249698639\n",
            "Epoch 8255/10000: L(Train): 0.32303205132484436; L(Test): 0.29645878076553345\n",
            "Epoch 8256/10000: L(Train): 0.3287965953350067; L(Test): 0.2965676188468933\n",
            "Epoch 8257/10000: L(Train): 0.32222676277160645; L(Test): 0.2965584099292755\n",
            "Epoch 8258/10000: L(Train): 0.31630414724349976; L(Test): 0.29690903425216675\n",
            "Epoch 8259/10000: L(Train): 0.3107868432998657; L(Test): 0.2968841791152954\n",
            "Epoch 8260/10000: L(Train): 0.3242857754230499; L(Test): 0.2971029579639435\n",
            "Epoch 8261/10000: L(Train): 0.32087230682373047; L(Test): 0.296676367521286\n",
            "Epoch 8262/10000: L(Train): 0.3189062476158142; L(Test): 0.2967204749584198\n",
            "Epoch 8263/10000: L(Train): 0.3212074637413025; L(Test): 0.29608866572380066\n",
            "Epoch 8264/10000: L(Train): 0.31314578652381897; L(Test): 0.29490190744400024\n",
            "Epoch 8265/10000: L(Train): 0.31730109453201294; L(Test): 0.2963721752166748\n",
            "Epoch 8266/10000: L(Train): 0.3113952577114105; L(Test): 0.29524677991867065\n",
            "Epoch 8267/10000: L(Train): 0.31648772954940796; L(Test): 0.2944243848323822\n",
            "Epoch 8268/10000: L(Train): 0.3159261643886566; L(Test): 0.29424330592155457\n",
            "Epoch 8269/10000: L(Train): 0.31308671832084656; L(Test): 0.29499584436416626\n",
            "Epoch 8270/10000: L(Train): 0.31278157234191895; L(Test): 0.296587735414505\n",
            "Epoch 8271/10000: L(Train): 0.3300846517086029; L(Test): 0.295480340719223\n",
            "Epoch 8272/10000: L(Train): 0.3259715139865875; L(Test): 0.2956618070602417\n",
            "Epoch 8273/10000: L(Train): 0.323863685131073; L(Test): 0.2967175543308258\n",
            "Epoch 8274/10000: L(Train): 0.3272261619567871; L(Test): 0.29482710361480713\n",
            "Epoch 8275/10000: L(Train): 0.3144831955432892; L(Test): 0.2948949635028839\n",
            "Epoch 8276/10000: L(Train): 0.3186452388763428; L(Test): 0.29435673356056213\n",
            "Epoch 8277/10000: L(Train): 0.32427704334259033; L(Test): 0.2935161888599396\n",
            "Epoch 8278/10000: L(Train): 0.30917471647262573; L(Test): 0.2944197654724121\n",
            "Epoch 8279/10000: L(Train): 0.3199826776981354; L(Test): 0.29547742009162903\n",
            "Epoch 8280/10000: L(Train): 0.32396888732910156; L(Test): 0.2949031889438629\n",
            "Epoch 8281/10000: L(Train): 0.32036808133125305; L(Test): 0.29413294792175293\n",
            "Epoch 8282/10000: L(Train): 0.322846919298172; L(Test): 0.29335328936576843\n",
            "Epoch 8283/10000: L(Train): 0.304262638092041; L(Test): 0.29372453689575195\n",
            "Epoch 8284/10000: L(Train): 0.3181250989437103; L(Test): 0.2934468984603882\n",
            "Epoch 8285/10000: L(Train): 0.312841534614563; L(Test): 0.2943442463874817\n",
            "Epoch 8286/10000: L(Train): 0.32612496614456177; L(Test): 0.2946620285511017\n",
            "Epoch 8287/10000: L(Train): 0.32149940729141235; L(Test): 0.2943248152732849\n",
            "Epoch 8288/10000: L(Train): 0.32518449425697327; L(Test): 0.2942028045654297\n",
            "Epoch 8289/10000: L(Train): 0.31804123520851135; L(Test): 0.2936764061450958\n",
            "Epoch 8290/10000: L(Train): 0.32168102264404297; L(Test): 0.2939186096191406\n",
            "Epoch 8291/10000: L(Train): 0.3142508864402771; L(Test): 0.29408153891563416\n",
            "Epoch 8292/10000: L(Train): 0.31933072209358215; L(Test): 0.2944507598876953\n",
            "Epoch 8293/10000: L(Train): 0.32261472940444946; L(Test): 0.29360195994377136\n",
            "Epoch 8294/10000: L(Train): 0.31733331084251404; L(Test): 0.2941690981388092\n",
            "Epoch 8295/10000: L(Train): 0.3114301860332489; L(Test): 0.29450905323028564\n",
            "Epoch 8296/10000: L(Train): 0.3284425735473633; L(Test): 0.29414233565330505\n",
            "Epoch 8297/10000: L(Train): 0.31092730164527893; L(Test): 0.2935054302215576\n",
            "Epoch 8298/10000: L(Train): 0.3157813251018524; L(Test): 0.29473698139190674\n",
            "Epoch 8299/10000: L(Train): 0.31044912338256836; L(Test): 0.29415157437324524\n",
            "Epoch 8300/10000: L(Train): 0.3160722553730011; L(Test): 0.29222673177719116\n",
            "Epoch 8301/10000: L(Train): 0.3224136233329773; L(Test): 0.29309505224227905\n",
            "Epoch 8302/10000: L(Train): 0.3151015043258667; L(Test): 0.2940689027309418\n",
            "Epoch 8303/10000: L(Train): 0.31475719809532166; L(Test): 0.2921752631664276\n",
            "Epoch 8304/10000: L(Train): 0.3252429962158203; L(Test): 0.29357004165649414\n",
            "Epoch 8305/10000: L(Train): 0.3187519907951355; L(Test): 0.29415762424468994\n",
            "Epoch 8306/10000: L(Train): 0.3203963339328766; L(Test): 0.2929922640323639\n",
            "Epoch 8307/10000: L(Train): 0.31505879759788513; L(Test): 0.2931753098964691\n",
            "Epoch 8308/10000: L(Train): 0.3146418035030365; L(Test): 0.29339155554771423\n",
            "Epoch 8309/10000: L(Train): 0.31857219338417053; L(Test): 0.29363635182380676\n",
            "Epoch 8310/10000: L(Train): 0.32009997963905334; L(Test): 0.2940160930156708\n",
            "Epoch 8311/10000: L(Train): 0.3294847309589386; L(Test): 0.2929404675960541\n",
            "Epoch 8312/10000: L(Train): 0.3208523988723755; L(Test): 0.2922089099884033\n",
            "Epoch 8313/10000: L(Train): 0.31216439604759216; L(Test): 0.29230475425720215\n",
            "Epoch 8314/10000: L(Train): 0.31740662455558777; L(Test): 0.2925119996070862\n",
            "Epoch 8315/10000: L(Train): 0.3239421248435974; L(Test): 0.29304054379463196\n",
            "Epoch 8316/10000: L(Train): 0.31496354937553406; L(Test): 0.2933931350708008\n",
            "Epoch 8317/10000: L(Train): 0.3176111876964569; L(Test): 0.292826771736145\n",
            "Epoch 8318/10000: L(Train): 0.3123147487640381; L(Test): 0.2917253375053406\n",
            "Epoch 8319/10000: L(Train): 0.31456324458122253; L(Test): 0.29130712151527405\n",
            "Epoch 8320/10000: L(Train): 0.3175351321697235; L(Test): 0.2924196720123291\n",
            "Epoch 8321/10000: L(Train): 0.3242858052253723; L(Test): 0.29177170991897583\n",
            "Epoch 8322/10000: L(Train): 0.3166196644306183; L(Test): 0.2923561632633209\n",
            "Epoch 8323/10000: L(Train): 0.31937241554260254; L(Test): 0.29301145672798157\n",
            "Epoch 8324/10000: L(Train): 0.3219675123691559; L(Test): 0.2921924591064453\n",
            "Epoch 8325/10000: L(Train): 0.3164709508419037; L(Test): 0.29161882400512695\n",
            "Epoch 8326/10000: L(Train): 0.31219038367271423; L(Test): 0.29152053594589233\n",
            "Epoch 8327/10000: L(Train): 0.3117717504501343; L(Test): 0.2913035750389099\n",
            "Epoch 8328/10000: L(Train): 0.3148021996021271; L(Test): 0.29168784618377686\n",
            "Epoch 8329/10000: L(Train): 0.3208499848842621; L(Test): 0.2916279733181\n",
            "Epoch 8330/10000: L(Train): 0.31431251764297485; L(Test): 0.2920568287372589\n",
            "Epoch 8331/10000: L(Train): 0.31435054540634155; L(Test): 0.292641282081604\n",
            "Epoch 8332/10000: L(Train): 0.31516024470329285; L(Test): 0.292267769575119\n",
            "Epoch 8333/10000: L(Train): 0.32086876034736633; L(Test): 0.29262489080429077\n",
            "Epoch 8334/10000: L(Train): 0.3108924627304077; L(Test): 0.29256710410118103\n",
            "Epoch 8335/10000: L(Train): 0.3201543986797333; L(Test): 0.29247117042541504\n",
            "Epoch 8336/10000: L(Train): 0.32206782698631287; L(Test): 0.29266414046287537\n",
            "Epoch 8337/10000: L(Train): 0.3228664994239807; L(Test): 0.2931651473045349\n",
            "Epoch 8338/10000: L(Train): 0.3198837637901306; L(Test): 0.29358139634132385\n",
            "Epoch 8339/10000: L(Train): 0.32034367322921753; L(Test): 0.29358533024787903\n",
            "Epoch 8340/10000: L(Train): 0.3245926797389984; L(Test): 0.2926405966281891\n",
            "Epoch 8341/10000: L(Train): 0.30667203664779663; L(Test): 0.2933117747306824\n",
            "Epoch 8342/10000: L(Train): 0.31758925318717957; L(Test): 0.29345226287841797\n",
            "Epoch 8343/10000: L(Train): 0.3215180039405823; L(Test): 0.29293033480644226\n",
            "Epoch 8344/10000: L(Train): 0.32101255655288696; L(Test): 0.2931690812110901\n",
            "Epoch 8345/10000: L(Train): 0.32235971093177795; L(Test): 0.2941431999206543\n",
            "Epoch 8346/10000: L(Train): 0.31999319791793823; L(Test): 0.2935144007205963\n",
            "Epoch 8347/10000: L(Train): 0.32539990544319153; L(Test): 0.2932489514350891\n",
            "Epoch 8348/10000: L(Train): 0.31776708364486694; L(Test): 0.2934514284133911\n",
            "Epoch 8349/10000: L(Train): 0.32142823934555054; L(Test): 0.2928563952445984\n",
            "Epoch 8350/10000: L(Train): 0.32554492354393005; L(Test): 0.29281899333000183\n",
            "Epoch 8351/10000: L(Train): 0.32040542364120483; L(Test): 0.293252557516098\n",
            "Epoch 8352/10000: L(Train): 0.323672890663147; L(Test): 0.29319050908088684\n",
            "Epoch 8353/10000: L(Train): 0.31676745414733887; L(Test): 0.2940710783004761\n",
            "Epoch 8354/10000: L(Train): 0.32314079999923706; L(Test): 0.2927418053150177\n",
            "Epoch 8355/10000: L(Train): 0.3279431164264679; L(Test): 0.2933216691017151\n",
            "Epoch 8356/10000: L(Train): 0.32130903005599976; L(Test): 0.2968401610851288\n",
            "Epoch 8357/10000: L(Train): 0.3248352110385895; L(Test): 0.2936670184135437\n",
            "Epoch 8358/10000: L(Train): 0.32035359740257263; L(Test): 0.29336482286453247\n",
            "Epoch 8359/10000: L(Train): 0.3239824175834656; L(Test): 0.29356908798217773\n",
            "Epoch 8360/10000: L(Train): 0.3093448877334595; L(Test): 0.2928503155708313\n",
            "Epoch 8361/10000: L(Train): 0.3175494968891144; L(Test): 0.295475035905838\n",
            "Epoch 8362/10000: L(Train): 0.3134710192680359; L(Test): 0.29557517170906067\n",
            "Epoch 8363/10000: L(Train): 0.313679575920105; L(Test): 0.29374733567237854\n",
            "Epoch 8364/10000: L(Train): 0.3242371678352356; L(Test): 0.29380902647972107\n",
            "Epoch 8365/10000: L(Train): 0.3131442368030548; L(Test): 0.2942502200603485\n",
            "Epoch 8366/10000: L(Train): 0.3172883093357086; L(Test): 0.29245489835739136\n",
            "Epoch 8367/10000: L(Train): 0.31348124146461487; L(Test): 0.29389262199401855\n",
            "Epoch 8368/10000: L(Train): 0.3153756856918335; L(Test): 0.29410430788993835\n",
            "Epoch 8369/10000: L(Train): 0.3255424201488495; L(Test): 0.2930660545825958\n",
            "Epoch 8370/10000: L(Train): 0.32073745131492615; L(Test): 0.2942982017993927\n",
            "Epoch 8371/10000: L(Train): 0.321863055229187; L(Test): 0.2936723232269287\n",
            "Epoch 8372/10000: L(Train): 0.32242655754089355; L(Test): 0.29315468668937683\n",
            "Epoch 8373/10000: L(Train): 0.3205992579460144; L(Test): 0.2937999665737152\n",
            "Epoch 8374/10000: L(Train): 0.3152339458465576; L(Test): 0.29348406195640564\n",
            "Epoch 8375/10000: L(Train): 0.32283636927604675; L(Test): 0.2960467040538788\n",
            "Epoch 8376/10000: L(Train): 0.3273891508579254; L(Test): 0.29528918862342834\n",
            "Epoch 8377/10000: L(Train): 0.33010995388031006; L(Test): 0.29476866126060486\n",
            "Epoch 8378/10000: L(Train): 0.31998956203460693; L(Test): 0.29628661274909973\n",
            "Epoch 8379/10000: L(Train): 0.32557618618011475; L(Test): 0.2944374680519104\n",
            "Epoch 8380/10000: L(Train): 0.32625797390937805; L(Test): 0.29487425088882446\n",
            "Epoch 8381/10000: L(Train): 0.32579776644706726; L(Test): 0.2944171130657196\n",
            "Epoch 8382/10000: L(Train): 0.31824496388435364; L(Test): 0.293251097202301\n",
            "Epoch 8383/10000: L(Train): 0.3139289319515228; L(Test): 0.2938939929008484\n",
            "Epoch 8384/10000: L(Train): 0.3229031264781952; L(Test): 0.2935087978839874\n",
            "Epoch 8385/10000: L(Train): 0.3168940842151642; L(Test): 0.29327842593193054\n",
            "Epoch 8386/10000: L(Train): 0.3237314820289612; L(Test): 0.29450908303260803\n",
            "Epoch 8387/10000: L(Train): 0.32631421089172363; L(Test): 0.2949766218662262\n",
            "Epoch 8388/10000: L(Train): 0.3223666548728943; L(Test): 0.29490989446640015\n",
            "Epoch 8389/10000: L(Train): 0.3224971294403076; L(Test): 0.29539960622787476\n",
            "Epoch 8390/10000: L(Train): 0.3256324529647827; L(Test): 0.29755136370658875\n",
            "Epoch 8391/10000: L(Train): 0.31668806076049805; L(Test): 0.29930493235588074\n",
            "Epoch 8392/10000: L(Train): 0.32654058933258057; L(Test): 0.2966271638870239\n",
            "Epoch 8393/10000: L(Train): 0.31470614671707153; L(Test): 0.2960350811481476\n",
            "Epoch 8394/10000: L(Train): 0.3252101540565491; L(Test): 0.2957911789417267\n",
            "Epoch 8395/10000: L(Train): 0.32137370109558105; L(Test): 0.29470258951187134\n",
            "Epoch 8396/10000: L(Train): 0.32114019989967346; L(Test): 0.2948662340641022\n",
            "Epoch 8397/10000: L(Train): 0.3238573372364044; L(Test): 0.2956966161727905\n",
            "Epoch 8398/10000: L(Train): 0.31374335289001465; L(Test): 0.29652103781700134\n",
            "Epoch 8399/10000: L(Train): 0.31943073868751526; L(Test): 0.294914186000824\n",
            "Epoch 8400/10000: L(Train): 0.3230828642845154; L(Test): 0.2940520942211151\n",
            "Epoch 8401/10000: L(Train): 0.31515541672706604; L(Test): 0.29481804370880127\n",
            "Epoch 8402/10000: L(Train): 0.3250989615917206; L(Test): 0.2942143380641937\n",
            "Epoch 8403/10000: L(Train): 0.3134143650531769; L(Test): 0.29454511404037476\n",
            "Epoch 8404/10000: L(Train): 0.31341588497161865; L(Test): 0.2956201732158661\n",
            "Epoch 8405/10000: L(Train): 0.31982290744781494; L(Test): 0.2940405011177063\n",
            "Epoch 8406/10000: L(Train): 0.318282812833786; L(Test): 0.2933945655822754\n",
            "Epoch 8407/10000: L(Train): 0.32510247826576233; L(Test): 0.29336413741111755\n",
            "Epoch 8408/10000: L(Train): 0.3140642046928406; L(Test): 0.2931273281574249\n",
            "Epoch 8409/10000: L(Train): 0.3133133053779602; L(Test): 0.29300543665885925\n",
            "Epoch 8410/10000: L(Train): 0.3182818293571472; L(Test): 0.2937123775482178\n",
            "Epoch 8411/10000: L(Train): 0.31668221950531006; L(Test): 0.2938615679740906\n",
            "Epoch 8412/10000: L(Train): 0.3127419948577881; L(Test): 0.2937384247779846\n",
            "Epoch 8413/10000: L(Train): 0.3203701078891754; L(Test): 0.29288816452026367\n",
            "Epoch 8414/10000: L(Train): 0.3102795481681824; L(Test): 0.2923858165740967\n",
            "Epoch 8415/10000: L(Train): 0.3180767297744751; L(Test): 0.2927388548851013\n",
            "Epoch 8416/10000: L(Train): 0.30993199348449707; L(Test): 0.29325172305107117\n",
            "Epoch 8417/10000: L(Train): 0.31574225425720215; L(Test): 0.2929043173789978\n",
            "Epoch 8418/10000: L(Train): 0.3153838813304901; L(Test): 0.2938232719898224\n",
            "Epoch 8419/10000: L(Train): 0.31040701270103455; L(Test): 0.2956080436706543\n",
            "Epoch 8420/10000: L(Train): 0.3227643072605133; L(Test): 0.2947433590888977\n",
            "Epoch 8421/10000: L(Train): 0.32349538803100586; L(Test): 0.29445895552635193\n",
            "Epoch 8422/10000: L(Train): 0.31835290789604187; L(Test): 0.2960989475250244\n",
            "Epoch 8423/10000: L(Train): 0.32262158393859863; L(Test): 0.2962448000907898\n",
            "Epoch 8424/10000: L(Train): 0.31624627113342285; L(Test): 0.2963564097881317\n",
            "Epoch 8425/10000: L(Train): 0.3199668824672699; L(Test): 0.2980806231498718\n",
            "Epoch 8426/10000: L(Train): 0.32067230343818665; L(Test): 0.2988794147968292\n",
            "Epoch 8427/10000: L(Train): 0.3256795108318329; L(Test): 0.2988549470901489\n",
            "Epoch 8428/10000: L(Train): 0.3329887390136719; L(Test): 0.2958819270133972\n",
            "Epoch 8429/10000: L(Train): 0.3266180157661438; L(Test): 0.29660850763320923\n",
            "Epoch 8430/10000: L(Train): 0.32093408703804016; L(Test): 0.2992437481880188\n",
            "Epoch 8431/10000: L(Train): 0.3210742175579071; L(Test): 0.30043089389801025\n",
            "Epoch 8432/10000: L(Train): 0.3240593671798706; L(Test): 0.300942599773407\n",
            "Epoch 8433/10000: L(Train): 0.3239577114582062; L(Test): 0.2998661696910858\n",
            "Epoch 8434/10000: L(Train): 0.3168031871318817; L(Test): 0.29982468485832214\n",
            "Epoch 8435/10000: L(Train): 0.324240118265152; L(Test): 0.30065247416496277\n",
            "Epoch 8436/10000: L(Train): 0.33371493220329285; L(Test): 0.2986958622932434\n",
            "Epoch 8437/10000: L(Train): 0.3209693431854248; L(Test): 0.2996475100517273\n",
            "Epoch 8438/10000: L(Train): 0.31839051842689514; L(Test): 0.3021811842918396\n",
            "Epoch 8439/10000: L(Train): 0.31573277711868286; L(Test): 0.29949653148651123\n",
            "Epoch 8440/10000: L(Train): 0.3285633623600006; L(Test): 0.2980714440345764\n",
            "Epoch 8441/10000: L(Train): 0.3244861662387848; L(Test): 0.30116069316864014\n",
            "Epoch 8442/10000: L(Train): 0.31422659754753113; L(Test): 0.30079391598701477\n",
            "Epoch 8443/10000: L(Train): 0.32861584424972534; L(Test): 0.2987561523914337\n",
            "Epoch 8444/10000: L(Train): 0.3196294903755188; L(Test): 0.29813528060913086\n",
            "Epoch 8445/10000: L(Train): 0.32031872868537903; L(Test): 0.29925107955932617\n",
            "Epoch 8446/10000: L(Train): 0.32857197523117065; L(Test): 0.2989959418773651\n",
            "Epoch 8447/10000: L(Train): 0.3273753225803375; L(Test): 0.2964142858982086\n",
            "Epoch 8448/10000: L(Train): 0.3131120204925537; L(Test): 0.29597318172454834\n",
            "Epoch 8449/10000: L(Train): 0.31197527050971985; L(Test): 0.29748302698135376\n",
            "Epoch 8450/10000: L(Train): 0.3182308077812195; L(Test): 0.2968848943710327\n",
            "Epoch 8451/10000: L(Train): 0.31834477186203003; L(Test): 0.2962935268878937\n",
            "Epoch 8452/10000: L(Train): 0.3070402145385742; L(Test): 0.2972509562969208\n",
            "Epoch 8453/10000: L(Train): 0.32537734508514404; L(Test): 0.29538530111312866\n",
            "Epoch 8454/10000: L(Train): 0.3175968527793884; L(Test): 0.29456961154937744\n",
            "Epoch 8455/10000: L(Train): 0.32750505208969116; L(Test): 0.29498815536499023\n",
            "Epoch 8456/10000: L(Train): 0.32822468876838684; L(Test): 0.29462671279907227\n",
            "Epoch 8457/10000: L(Train): 0.3137076497077942; L(Test): 0.2946595549583435\n",
            "Epoch 8458/10000: L(Train): 0.3174389898777008; L(Test): 0.2941116392612457\n",
            "Epoch 8459/10000: L(Train): 0.3184058368206024; L(Test): 0.2933460772037506\n",
            "Epoch 8460/10000: L(Train): 0.31857454776763916; L(Test): 0.2924661338329315\n",
            "Epoch 8461/10000: L(Train): 0.3185272216796875; L(Test): 0.2927154302597046\n",
            "Epoch 8462/10000: L(Train): 0.3093424439430237; L(Test): 0.29335981607437134\n",
            "Epoch 8463/10000: L(Train): 0.3161739706993103; L(Test): 0.2930591404438019\n",
            "Epoch 8464/10000: L(Train): 0.319208562374115; L(Test): 0.2930659353733063\n",
            "Epoch 8465/10000: L(Train): 0.317600280046463; L(Test): 0.2925044596195221\n",
            "Epoch 8466/10000: L(Train): 0.32260236144065857; L(Test): 0.2918489873409271\n",
            "Epoch 8467/10000: L(Train): 0.30969274044036865; L(Test): 0.29198896884918213\n",
            "Epoch 8468/10000: L(Train): 0.3168068826198578; L(Test): 0.2920013666152954\n",
            "Epoch 8469/10000: L(Train): 0.32487955689430237; L(Test): 0.29151204228401184\n",
            "Epoch 8470/10000: L(Train): 0.31261417269706726; L(Test): 0.29179632663726807\n",
            "Epoch 8471/10000: L(Train): 0.3180651068687439; L(Test): 0.2928276062011719\n",
            "Epoch 8472/10000: L(Train): 0.31698545813560486; L(Test): 0.2931247353553772\n",
            "Epoch 8473/10000: L(Train): 0.32647278904914856; L(Test): 0.293124258518219\n",
            "Epoch 8474/10000: L(Train): 0.31918206810951233; L(Test): 0.2930988371372223\n",
            "Epoch 8475/10000: L(Train): 0.3225778043270111; L(Test): 0.2926221191883087\n",
            "Epoch 8476/10000: L(Train): 0.32557305693626404; L(Test): 0.29237985610961914\n",
            "Epoch 8477/10000: L(Train): 0.3171205520629883; L(Test): 0.29279786348342896\n",
            "Epoch 8478/10000: L(Train): 0.3117690682411194; L(Test): 0.2931164503097534\n",
            "Epoch 8479/10000: L(Train): 0.3079738914966583; L(Test): 0.29364335536956787\n",
            "Epoch 8480/10000: L(Train): 0.3251858353614807; L(Test): 0.2949007749557495\n",
            "Epoch 8481/10000: L(Train): 0.3167376220226288; L(Test): 0.2944628596305847\n",
            "Epoch 8482/10000: L(Train): 0.3134422302246094; L(Test): 0.29396310448646545\n",
            "Epoch 8483/10000: L(Train): 0.3203808665275574; L(Test): 0.2936592400074005\n",
            "Epoch 8484/10000: L(Train): 0.3190171718597412; L(Test): 0.29361844062805176\n",
            "Epoch 8485/10000: L(Train): 0.3215104341506958; L(Test): 0.2945309579372406\n",
            "Epoch 8486/10000: L(Train): 0.3224239945411682; L(Test): 0.2938162088394165\n",
            "Epoch 8487/10000: L(Train): 0.3216021656990051; L(Test): 0.29344409704208374\n",
            "Epoch 8488/10000: L(Train): 0.3273391127586365; L(Test): 0.29311510920524597\n",
            "Epoch 8489/10000: L(Train): 0.3220093846321106; L(Test): 0.29379141330718994\n",
            "Epoch 8490/10000: L(Train): 0.32076209783554077; L(Test): 0.29315242171287537\n",
            "Epoch 8491/10000: L(Train): 0.3173399567604065; L(Test): 0.2936513423919678\n",
            "Epoch 8492/10000: L(Train): 0.317491739988327; L(Test): 0.29436907172203064\n",
            "Epoch 8493/10000: L(Train): 0.3080635368824005; L(Test): 0.2947829067707062\n",
            "Epoch 8494/10000: L(Train): 0.3268633782863617; L(Test): 0.29539254307746887\n",
            "Epoch 8495/10000: L(Train): 0.32776379585266113; L(Test): 0.2965393662452698\n",
            "Epoch 8496/10000: L(Train): 0.3258213698863983; L(Test): 0.295709490776062\n",
            "Epoch 8497/10000: L(Train): 0.3195649981498718; L(Test): 0.29524296522140503\n",
            "Epoch 8498/10000: L(Train): 0.3221852779388428; L(Test): 0.29452794790267944\n",
            "Epoch 8499/10000: L(Train): 0.32272765040397644; L(Test): 0.29354962706565857\n",
            "Epoch 8500/10000: L(Train): 0.3219742476940155; L(Test): 0.29454317688941956\n",
            "Epoch 8501/10000: L(Train): 0.32438889145851135; L(Test): 0.2946610152721405\n",
            "Epoch 8502/10000: L(Train): 0.3155900835990906; L(Test): 0.29386645555496216\n",
            "Epoch 8503/10000: L(Train): 0.31820449233055115; L(Test): 0.2937658131122589\n",
            "Epoch 8504/10000: L(Train): 0.3202988803386688; L(Test): 0.2943207919597626\n",
            "Epoch 8505/10000: L(Train): 0.31789103150367737; L(Test): 0.2945985198020935\n",
            "Epoch 8506/10000: L(Train): 0.31531408429145813; L(Test): 0.29443079233169556\n",
            "Epoch 8507/10000: L(Train): 0.31465667486190796; L(Test): 0.2939032316207886\n",
            "Epoch 8508/10000: L(Train): 0.3138423264026642; L(Test): 0.29382607340812683\n",
            "Epoch 8509/10000: L(Train): 0.32106560468673706; L(Test): 0.2946085035800934\n",
            "Epoch 8510/10000: L(Train): 0.31947147846221924; L(Test): 0.29349297285079956\n",
            "Epoch 8511/10000: L(Train): 0.32245326042175293; L(Test): 0.29363614320755005\n",
            "Epoch 8512/10000: L(Train): 0.3244868218898773; L(Test): 0.2942315340042114\n",
            "Epoch 8513/10000: L(Train): 0.320087194442749; L(Test): 0.2932298183441162\n",
            "Epoch 8514/10000: L(Train): 0.32262784242630005; L(Test): 0.29392868280410767\n",
            "Epoch 8515/10000: L(Train): 0.3140681982040405; L(Test): 0.29511356353759766\n",
            "Epoch 8516/10000: L(Train): 0.32109060883522034; L(Test): 0.2948952615261078\n",
            "Epoch 8517/10000: L(Train): 0.30987611413002014; L(Test): 0.2939617335796356\n",
            "Epoch 8518/10000: L(Train): 0.31788694858551025; L(Test): 0.2928166389465332\n",
            "Epoch 8519/10000: L(Train): 0.3159630298614502; L(Test): 0.2929456830024719\n",
            "Epoch 8520/10000: L(Train): 0.3165058195590973; L(Test): 0.29323747754096985\n",
            "Epoch 8521/10000: L(Train): 0.31869515776634216; L(Test): 0.29425981640815735\n",
            "Epoch 8522/10000: L(Train): 0.32216140627861023; L(Test): 0.2929796576499939\n",
            "Epoch 8523/10000: L(Train): 0.3108613193035126; L(Test): 0.29356318712234497\n",
            "Epoch 8524/10000: L(Train): 0.3100735545158386; L(Test): 0.293363481760025\n",
            "Epoch 8525/10000: L(Train): 0.31829148530960083; L(Test): 0.2930760383605957\n",
            "Epoch 8526/10000: L(Train): 0.30851224064826965; L(Test): 0.29225975275039673\n",
            "Epoch 8527/10000: L(Train): 0.322829931974411; L(Test): 0.29178157448768616\n",
            "Epoch 8528/10000: L(Train): 0.31662115454673767; L(Test): 0.29304519295692444\n",
            "Epoch 8529/10000: L(Train): 0.31957122683525085; L(Test): 0.2944943606853485\n",
            "Epoch 8530/10000: L(Train): 0.31774094700813293; L(Test): 0.2931353747844696\n",
            "Epoch 8531/10000: L(Train): 0.3210693299770355; L(Test): 0.29327115416526794\n",
            "Epoch 8532/10000: L(Train): 0.31840407848358154; L(Test): 0.29384976625442505\n",
            "Epoch 8533/10000: L(Train): 0.3177569508552551; L(Test): 0.29272210597991943\n",
            "Epoch 8534/10000: L(Train): 0.3229602873325348; L(Test): 0.29412534832954407\n",
            "Epoch 8535/10000: L(Train): 0.32091405987739563; L(Test): 0.2937599718570709\n",
            "Epoch 8536/10000: L(Train): 0.32977116107940674; L(Test): 0.29346606135368347\n",
            "Epoch 8537/10000: L(Train): 0.32407495379447937; L(Test): 0.2945782244205475\n",
            "Epoch 8538/10000: L(Train): 0.3208356499671936; L(Test): 0.29298824071884155\n",
            "Epoch 8539/10000: L(Train): 0.3247245252132416; L(Test): 0.2942195534706116\n",
            "Epoch 8540/10000: L(Train): 0.32558196783065796; L(Test): 0.29457157850265503\n",
            "Epoch 8541/10000: L(Train): 0.3226146996021271; L(Test): 0.292829304933548\n",
            "Epoch 8542/10000: L(Train): 0.3160459101200104; L(Test): 0.2924875020980835\n",
            "Epoch 8543/10000: L(Train): 0.31471481919288635; L(Test): 0.29293322563171387\n",
            "Epoch 8544/10000: L(Train): 0.3237472474575043; L(Test): 0.2926601767539978\n",
            "Epoch 8545/10000: L(Train): 0.3141159415245056; L(Test): 0.29258328676223755\n",
            "Epoch 8546/10000: L(Train): 0.314206600189209; L(Test): 0.29243701696395874\n",
            "Epoch 8547/10000: L(Train): 0.3215246796607971; L(Test): 0.2931089401245117\n",
            "Epoch 8548/10000: L(Train): 0.3112438917160034; L(Test): 0.29391127824783325\n",
            "Epoch 8549/10000: L(Train): 0.3209514319896698; L(Test): 0.2936150133609772\n",
            "Epoch 8550/10000: L(Train): 0.3147105574607849; L(Test): 0.29347947239875793\n",
            "Epoch 8551/10000: L(Train): 0.3161940574645996; L(Test): 0.2929186522960663\n",
            "Epoch 8552/10000: L(Train): 0.3332468867301941; L(Test): 0.29301294684410095\n",
            "Epoch 8553/10000: L(Train): 0.31879860162734985; L(Test): 0.2929783761501312\n",
            "Epoch 8554/10000: L(Train): 0.31530606746673584; L(Test): 0.29227328300476074\n",
            "Epoch 8555/10000: L(Train): 0.32289379835128784; L(Test): 0.29171016812324524\n",
            "Epoch 8556/10000: L(Train): 0.3077293932437897; L(Test): 0.29221978783607483\n",
            "Epoch 8557/10000: L(Train): 0.3170543611049652; L(Test): 0.2936244606971741\n",
            "Epoch 8558/10000: L(Train): 0.31246548891067505; L(Test): 0.29475873708724976\n",
            "Epoch 8559/10000: L(Train): 0.31957578659057617; L(Test): 0.2936258614063263\n",
            "Epoch 8560/10000: L(Train): 0.3224550187587738; L(Test): 0.2921215295791626\n",
            "Epoch 8561/10000: L(Train): 0.319324791431427; L(Test): 0.29265958070755005\n",
            "Epoch 8562/10000: L(Train): 0.31353914737701416; L(Test): 0.2940622568130493\n",
            "Epoch 8563/10000: L(Train): 0.31486746668815613; L(Test): 0.29369857907295227\n",
            "Epoch 8564/10000: L(Train): 0.3198406398296356; L(Test): 0.29290077090263367\n",
            "Epoch 8565/10000: L(Train): 0.32230088114738464; L(Test): 0.29411301016807556\n",
            "Epoch 8566/10000: L(Train): 0.3185638189315796; L(Test): 0.29394522309303284\n",
            "Epoch 8567/10000: L(Train): 0.3156777024269104; L(Test): 0.29346388578414917\n",
            "Epoch 8568/10000: L(Train): 0.3208372890949249; L(Test): 0.29369884729385376\n",
            "Epoch 8569/10000: L(Train): 0.3187834322452545; L(Test): 0.29329219460487366\n",
            "Epoch 8570/10000: L(Train): 0.3154585361480713; L(Test): 0.2921992838382721\n",
            "Epoch 8571/10000: L(Train): 0.3161575496196747; L(Test): 0.2924814820289612\n",
            "Epoch 8572/10000: L(Train): 0.31296226382255554; L(Test): 0.2923761010169983\n",
            "Epoch 8573/10000: L(Train): 0.3153058886528015; L(Test): 0.2933729887008667\n",
            "Epoch 8574/10000: L(Train): 0.3164190948009491; L(Test): 0.2945115864276886\n",
            "Epoch 8575/10000: L(Train): 0.32486647367477417; L(Test): 0.29453614354133606\n",
            "Epoch 8576/10000: L(Train): 0.3176664113998413; L(Test): 0.2949775159358978\n",
            "Epoch 8577/10000: L(Train): 0.3166465163230896; L(Test): 0.29499542713165283\n",
            "Epoch 8578/10000: L(Train): 0.3116127848625183; L(Test): 0.2932627499103546\n",
            "Epoch 8579/10000: L(Train): 0.3128378093242645; L(Test): 0.29355573654174805\n",
            "Epoch 8580/10000: L(Train): 0.31723472476005554; L(Test): 0.2954859435558319\n",
            "Epoch 8581/10000: L(Train): 0.32748815417289734; L(Test): 0.2946839928627014\n",
            "Epoch 8582/10000: L(Train): 0.30999860167503357; L(Test): 0.2944330871105194\n",
            "Epoch 8583/10000: L(Train): 0.3180526793003082; L(Test): 0.2943577468395233\n",
            "Epoch 8584/10000: L(Train): 0.3184772729873657; L(Test): 0.2935282289981842\n",
            "Epoch 8585/10000: L(Train): 0.3272143602371216; L(Test): 0.2933863401412964\n",
            "Epoch 8586/10000: L(Train): 0.3179785907268524; L(Test): 0.2939818203449249\n",
            "Epoch 8587/10000: L(Train): 0.3279823660850525; L(Test): 0.2948894500732422\n",
            "Epoch 8588/10000: L(Train): 0.32345688343048096; L(Test): 0.29559066891670227\n",
            "Epoch 8589/10000: L(Train): 0.32231375575065613; L(Test): 0.2952442169189453\n",
            "Epoch 8590/10000: L(Train): 0.3150101602077484; L(Test): 0.29434189200401306\n",
            "Epoch 8591/10000: L(Train): 0.32184842228889465; L(Test): 0.29421988129615784\n",
            "Epoch 8592/10000: L(Train): 0.31513530015945435; L(Test): 0.29408711194992065\n",
            "Epoch 8593/10000: L(Train): 0.3195149004459381; L(Test): 0.29615136981010437\n",
            "Epoch 8594/10000: L(Train): 0.32687094807624817; L(Test): 0.30203115940093994\n",
            "Epoch 8595/10000: L(Train): 0.32966384291648865; L(Test): 0.29986077547073364\n",
            "Epoch 8596/10000: L(Train): 0.32262250781059265; L(Test): 0.2959745228290558\n",
            "Epoch 8597/10000: L(Train): 0.3135836720466614; L(Test): 0.2960153818130493\n",
            "Epoch 8598/10000: L(Train): 0.3155885338783264; L(Test): 0.29944106936454773\n",
            "Epoch 8599/10000: L(Train): 0.3293793797492981; L(Test): 0.3005048632621765\n",
            "Epoch 8600/10000: L(Train): 0.32722166180610657; L(Test): 0.301815927028656\n",
            "Epoch 8601/10000: L(Train): 0.3268873393535614; L(Test): 0.3003692328929901\n",
            "Epoch 8602/10000: L(Train): 0.30802372097969055; L(Test): 0.2984231412410736\n",
            "Epoch 8603/10000: L(Train): 0.3265330195426941; L(Test): 0.29977521300315857\n",
            "Epoch 8604/10000: L(Train): 0.3330363929271698; L(Test): 0.29707643389701843\n",
            "Epoch 8605/10000: L(Train): 0.31234174966812134; L(Test): 0.29759326577186584\n",
            "Epoch 8606/10000: L(Train): 0.3189319670200348; L(Test): 0.3000084161758423\n",
            "Epoch 8607/10000: L(Train): 0.3189335763454437; L(Test): 0.2997482419013977\n",
            "Epoch 8608/10000: L(Train): 0.32938140630722046; L(Test): 0.2979613244533539\n",
            "Epoch 8609/10000: L(Train): 0.31898054480552673; L(Test): 0.2997211217880249\n",
            "Epoch 8610/10000: L(Train): 0.32141220569610596; L(Test): 0.2970017194747925\n",
            "Epoch 8611/10000: L(Train): 0.32585814595222473; L(Test): 0.29532599449157715\n",
            "Epoch 8612/10000: L(Train): 0.32100048661231995; L(Test): 0.29811495542526245\n",
            "Epoch 8613/10000: L(Train): 0.32372093200683594; L(Test): 0.29645219445228577\n",
            "Epoch 8614/10000: L(Train): 0.3176901042461395; L(Test): 0.29505395889282227\n",
            "Epoch 8615/10000: L(Train): 0.3116166591644287; L(Test): 0.2973366677761078\n",
            "Epoch 8616/10000: L(Train): 0.32029610872268677; L(Test): 0.29597437381744385\n",
            "Epoch 8617/10000: L(Train): 0.3215272128582001; L(Test): 0.2943285405635834\n",
            "Epoch 8618/10000: L(Train): 0.31918221712112427; L(Test): 0.2957746684551239\n",
            "Epoch 8619/10000: L(Train): 0.32573267817497253; L(Test): 0.2944907546043396\n",
            "Epoch 8620/10000: L(Train): 0.32329005002975464; L(Test): 0.29345962405204773\n",
            "Epoch 8621/10000: L(Train): 0.3171212375164032; L(Test): 0.29484423995018005\n",
            "Epoch 8622/10000: L(Train): 0.3260400593280792; L(Test): 0.2937007546424866\n",
            "Epoch 8623/10000: L(Train): 0.3105933368206024; L(Test): 0.29341986775398254\n",
            "Epoch 8624/10000: L(Train): 0.3299906253814697; L(Test): 0.29477423429489136\n",
            "Epoch 8625/10000: L(Train): 0.32243621349334717; L(Test): 0.2931494116783142\n",
            "Epoch 8626/10000: L(Train): 0.3143976032733917; L(Test): 0.2922869622707367\n",
            "Epoch 8627/10000: L(Train): 0.3168801963329315; L(Test): 0.2930186986923218\n",
            "Epoch 8628/10000: L(Train): 0.32480108737945557; L(Test): 0.29290562868118286\n",
            "Epoch 8629/10000: L(Train): 0.3159037232398987; L(Test): 0.2939160466194153\n",
            "Epoch 8630/10000: L(Train): 0.3331662118434906; L(Test): 0.294439435005188\n",
            "Epoch 8631/10000: L(Train): 0.32229506969451904; L(Test): 0.2930872440338135\n",
            "Epoch 8632/10000: L(Train): 0.31651565432548523; L(Test): 0.2926565110683441\n",
            "Epoch 8633/10000: L(Train): 0.30681511759757996; L(Test): 0.29293861985206604\n",
            "Epoch 8634/10000: L(Train): 0.31674322485923767; L(Test): 0.2933385670185089\n",
            "Epoch 8635/10000: L(Train): 0.31737980246543884; L(Test): 0.29418495297431946\n",
            "Epoch 8636/10000: L(Train): 0.31449559330940247; L(Test): 0.2928730249404907\n",
            "Epoch 8637/10000: L(Train): 0.3161359131336212; L(Test): 0.2919943332672119\n",
            "Epoch 8638/10000: L(Train): 0.3045318126678467; L(Test): 0.2932455837726593\n",
            "Epoch 8639/10000: L(Train): 0.3155958950519562; L(Test): 0.29310810565948486\n",
            "Epoch 8640/10000: L(Train): 0.32185712456703186; L(Test): 0.2929861843585968\n",
            "Epoch 8641/10000: L(Train): 0.3226717710494995; L(Test): 0.2957179546356201\n",
            "Epoch 8642/10000: L(Train): 0.31572771072387695; L(Test): 0.2955438792705536\n",
            "Epoch 8643/10000: L(Train): 0.3179026246070862; L(Test): 0.2943934202194214\n",
            "Epoch 8644/10000: L(Train): 0.3157312572002411; L(Test): 0.29474595189094543\n",
            "Epoch 8645/10000: L(Train): 0.32951033115386963; L(Test): 0.2951914370059967\n",
            "Epoch 8646/10000: L(Train): 0.32196924090385437; L(Test): 0.2967827320098877\n",
            "Epoch 8647/10000: L(Train): 0.3165649473667145; L(Test): 0.2943360507488251\n",
            "Epoch 8648/10000: L(Train): 0.31500646471977234; L(Test): 0.2931676208972931\n",
            "Epoch 8649/10000: L(Train): 0.3177775740623474; L(Test): 0.29532697796821594\n",
            "Epoch 8650/10000: L(Train): 0.3175591826438904; L(Test): 0.29433298110961914\n",
            "Epoch 8651/10000: L(Train): 0.31784602999687195; L(Test): 0.29535433650016785\n",
            "Epoch 8652/10000: L(Train): 0.3165847957134247; L(Test): 0.2958700358867645\n",
            "Epoch 8653/10000: L(Train): 0.323935329914093; L(Test): 0.2929772734642029\n",
            "Epoch 8654/10000: L(Train): 0.3054223358631134; L(Test): 0.29323628544807434\n",
            "Epoch 8655/10000: L(Train): 0.31900328397750854; L(Test): 0.29291266202926636\n",
            "Epoch 8656/10000: L(Train): 0.3195186257362366; L(Test): 0.2922592759132385\n",
            "Epoch 8657/10000: L(Train): 0.30574268102645874; L(Test): 0.2935737371444702\n",
            "Epoch 8658/10000: L(Train): 0.31823188066482544; L(Test): 0.29453349113464355\n",
            "Epoch 8659/10000: L(Train): 0.3189348578453064; L(Test): 0.2944217622280121\n",
            "Epoch 8660/10000: L(Train): 0.3174654245376587; L(Test): 0.29508650302886963\n",
            "Epoch 8661/10000: L(Train): 0.32619017362594604; L(Test): 0.29683631658554077\n",
            "Epoch 8662/10000: L(Train): 0.3163471519947052; L(Test): 0.2953018844127655\n",
            "Epoch 8663/10000: L(Train): 0.32486093044281006; L(Test): 0.2937341332435608\n",
            "Epoch 8664/10000: L(Train): 0.32051604986190796; L(Test): 0.2988071143627167\n",
            "Epoch 8665/10000: L(Train): 0.33130690455436707; L(Test): 0.29658225178718567\n",
            "Epoch 8666/10000: L(Train): 0.31533390283584595; L(Test): 0.29590749740600586\n",
            "Epoch 8667/10000: L(Train): 0.3136906921863556; L(Test): 0.296602725982666\n",
            "Epoch 8668/10000: L(Train): 0.32037097215652466; L(Test): 0.296232134103775\n",
            "Epoch 8669/10000: L(Train): 0.32082486152648926; L(Test): 0.2948989272117615\n",
            "Epoch 8670/10000: L(Train): 0.3166007697582245; L(Test): 0.2960362434387207\n",
            "Epoch 8671/10000: L(Train): 0.3265376687049866; L(Test): 0.2974189519882202\n",
            "Epoch 8672/10000: L(Train): 0.3200029730796814; L(Test): 0.29703447222709656\n",
            "Epoch 8673/10000: L(Train): 0.32198068499565125; L(Test): 0.29613396525382996\n",
            "Epoch 8674/10000: L(Train): 0.3226868808269501; L(Test): 0.29660937190055847\n",
            "Epoch 8675/10000: L(Train): 0.31411105394363403; L(Test): 0.29579466581344604\n",
            "Epoch 8676/10000: L(Train): 0.32536786794662476; L(Test): 0.29546600580215454\n",
            "Epoch 8677/10000: L(Train): 0.30965203046798706; L(Test): 0.2957533001899719\n",
            "Epoch 8678/10000: L(Train): 0.3145095705986023; L(Test): 0.2955322861671448\n",
            "Epoch 8679/10000: L(Train): 0.3296891748905182; L(Test): 0.2945261597633362\n",
            "Epoch 8680/10000: L(Train): 0.31959623098373413; L(Test): 0.29897257685661316\n",
            "Epoch 8681/10000: L(Train): 0.32210108637809753; L(Test): 0.2969109117984772\n",
            "Epoch 8682/10000: L(Train): 0.3129156529903412; L(Test): 0.30252912640571594\n",
            "Epoch 8683/10000: L(Train): 0.3266291618347168; L(Test): 0.3034411370754242\n",
            "Epoch 8684/10000: L(Train): 0.32567840814590454; L(Test): 0.2975969612598419\n",
            "Epoch 8685/10000: L(Train): 0.32502830028533936; L(Test): 0.29677489399909973\n",
            "Epoch 8686/10000: L(Train): 0.32375413179397583; L(Test): 0.29871609807014465\n",
            "Epoch 8687/10000: L(Train): 0.3212541937828064; L(Test): 0.2989756464958191\n",
            "Epoch 8688/10000: L(Train): 0.3307782709598541; L(Test): 0.29919394850730896\n",
            "Epoch 8689/10000: L(Train): 0.32462966442108154; L(Test): 0.2980441749095917\n",
            "Epoch 8690/10000: L(Train): 0.3301757574081421; L(Test): 0.29709571599960327\n",
            "Epoch 8691/10000: L(Train): 0.3119889497756958; L(Test): 0.29708173871040344\n",
            "Epoch 8692/10000: L(Train): 0.3247816562652588; L(Test): 0.29638224840164185\n",
            "Epoch 8693/10000: L(Train): 0.3190557062625885; L(Test): 0.2951892018318176\n",
            "Epoch 8694/10000: L(Train): 0.31153884530067444; L(Test): 0.2956443428993225\n",
            "Epoch 8695/10000: L(Train): 0.3192519545555115; L(Test): 0.29634037613868713\n",
            "Epoch 8696/10000: L(Train): 0.33062607049942017; L(Test): 0.2959858775138855\n",
            "Epoch 8697/10000: L(Train): 0.3246228098869324; L(Test): 0.2978115975856781\n",
            "Epoch 8698/10000: L(Train): 0.3175472915172577; L(Test): 0.2978164851665497\n",
            "Epoch 8699/10000: L(Train): 0.3283383250236511; L(Test): 0.29739537835121155\n",
            "Epoch 8700/10000: L(Train): 0.3218078911304474; L(Test): 0.2962144911289215\n",
            "Epoch 8701/10000: L(Train): 0.31013625860214233; L(Test): 0.2975064814090729\n",
            "Epoch 8702/10000: L(Train): 0.32194775342941284; L(Test): 0.2975276708602905\n",
            "Epoch 8703/10000: L(Train): 0.3152462840080261; L(Test): 0.2954772114753723\n",
            "Epoch 8704/10000: L(Train): 0.315983384847641; L(Test): 0.2945374548435211\n",
            "Epoch 8705/10000: L(Train): 0.31489425897598267; L(Test): 0.2951682507991791\n",
            "Epoch 8706/10000: L(Train): 0.32815372943878174; L(Test): 0.2955474257469177\n",
            "Epoch 8707/10000: L(Train): 0.32041487097740173; L(Test): 0.2967275083065033\n",
            "Epoch 8708/10000: L(Train): 0.32140883803367615; L(Test): 0.29687076807022095\n",
            "Epoch 8709/10000: L(Train): 0.3215865194797516; L(Test): 0.29527342319488525\n",
            "Epoch 8710/10000: L(Train): 0.32097721099853516; L(Test): 0.29600417613983154\n",
            "Epoch 8711/10000: L(Train): 0.32135647535324097; L(Test): 0.29546692967414856\n",
            "Epoch 8712/10000: L(Train): 0.31097373366355896; L(Test): 0.2959330677986145\n",
            "Epoch 8713/10000: L(Train): 0.31859004497528076; L(Test): 0.2972295880317688\n",
            "Epoch 8714/10000: L(Train): 0.32231202721595764; L(Test): 0.29580068588256836\n",
            "Epoch 8715/10000: L(Train): 0.32269716262817383; L(Test): 0.2938653230667114\n",
            "Epoch 8716/10000: L(Train): 0.31039944291114807; L(Test): 0.29419654607772827\n",
            "Epoch 8717/10000: L(Train): 0.3137716054916382; L(Test): 0.29351893067359924\n",
            "Epoch 8718/10000: L(Train): 0.3174562454223633; L(Test): 0.29368647933006287\n",
            "Epoch 8719/10000: L(Train): 0.3247585892677307; L(Test): 0.29403385519981384\n",
            "Epoch 8720/10000: L(Train): 0.31944239139556885; L(Test): 0.29288554191589355\n",
            "Epoch 8721/10000: L(Train): 0.3181670904159546; L(Test): 0.29350346326828003\n",
            "Epoch 8722/10000: L(Train): 0.3159874379634857; L(Test): 0.294217050075531\n",
            "Epoch 8723/10000: L(Train): 0.3183825612068176; L(Test): 0.29293110966682434\n",
            "Epoch 8724/10000: L(Train): 0.31033995747566223; L(Test): 0.29393067955970764\n",
            "Epoch 8725/10000: L(Train): 0.3150189518928528; L(Test): 0.29416465759277344\n",
            "Epoch 8726/10000: L(Train): 0.31262266635894775; L(Test): 0.29376569390296936\n",
            "Epoch 8727/10000: L(Train): 0.3176169991493225; L(Test): 0.29437491297721863\n",
            "Epoch 8728/10000: L(Train): 0.31934940814971924; L(Test): 0.2949755787849426\n",
            "Epoch 8729/10000: L(Train): 0.3267151713371277; L(Test): 0.29370176792144775\n",
            "Epoch 8730/10000: L(Train): 0.32604286074638367; L(Test): 0.2938399314880371\n",
            "Epoch 8731/10000: L(Train): 0.31247398257255554; L(Test): 0.2936290502548218\n",
            "Epoch 8732/10000: L(Train): 0.31712743639945984; L(Test): 0.29197120666503906\n",
            "Epoch 8733/10000: L(Train): 0.31257057189941406; L(Test): 0.29347357153892517\n",
            "Epoch 8734/10000: L(Train): 0.31031009554862976; L(Test): 0.29398787021636963\n",
            "Epoch 8735/10000: L(Train): 0.32408222556114197; L(Test): 0.29365870356559753\n",
            "Epoch 8736/10000: L(Train): 0.3298228979110718; L(Test): 0.29343482851982117\n",
            "Epoch 8737/10000: L(Train): 0.3165496289730072; L(Test): 0.29258036613464355\n",
            "Epoch 8738/10000: L(Train): 0.31353479623794556; L(Test): 0.2921159565448761\n",
            "Epoch 8739/10000: L(Train): 0.3113349378108978; L(Test): 0.2937758266925812\n",
            "Epoch 8740/10000: L(Train): 0.3143137991428375; L(Test): 0.2929314374923706\n",
            "Epoch 8741/10000: L(Train): 0.32418540120124817; L(Test): 0.29213613271713257\n",
            "Epoch 8742/10000: L(Train): 0.3074699342250824; L(Test): 0.2941288650035858\n",
            "Epoch 8743/10000: L(Train): 0.31340694427490234; L(Test): 0.29293593764305115\n",
            "Epoch 8744/10000: L(Train): 0.3113373816013336; L(Test): 0.29201382398605347\n",
            "Epoch 8745/10000: L(Train): 0.31829598546028137; L(Test): 0.2923719882965088\n",
            "Epoch 8746/10000: L(Train): 0.3099795877933502; L(Test): 0.2926441729068756\n",
            "Epoch 8747/10000: L(Train): 0.3253616690635681; L(Test): 0.29325753450393677\n",
            "Epoch 8748/10000: L(Train): 0.3199235796928406; L(Test): 0.2932543456554413\n",
            "Epoch 8749/10000: L(Train): 0.3188844621181488; L(Test): 0.29293105006217957\n",
            "Epoch 8750/10000: L(Train): 0.32423457503318787; L(Test): 0.2929709553718567\n",
            "Epoch 8751/10000: L(Train): 0.31244194507598877; L(Test): 0.29297903180122375\n",
            "Epoch 8752/10000: L(Train): 0.3245137631893158; L(Test): 0.29240158200263977\n",
            "Epoch 8753/10000: L(Train): 0.31452855467796326; L(Test): 0.2927475571632385\n",
            "Epoch 8754/10000: L(Train): 0.32123857736587524; L(Test): 0.29287463426589966\n",
            "Epoch 8755/10000: L(Train): 0.31475701928138733; L(Test): 0.2939135432243347\n",
            "Epoch 8756/10000: L(Train): 0.3185916543006897; L(Test): 0.29472020268440247\n",
            "Epoch 8757/10000: L(Train): 0.3222731053829193; L(Test): 0.2942523956298828\n",
            "Epoch 8758/10000: L(Train): 0.3124504089355469; L(Test): 0.2956618070602417\n",
            "Epoch 8759/10000: L(Train): 0.32522934675216675; L(Test): 0.29539304971694946\n",
            "Epoch 8760/10000: L(Train): 0.32072943449020386; L(Test): 0.2932591736316681\n",
            "Epoch 8761/10000: L(Train): 0.3153488337993622; L(Test): 0.2927914261817932\n",
            "Epoch 8762/10000: L(Train): 0.33107537031173706; L(Test): 0.29246485233306885\n",
            "Epoch 8763/10000: L(Train): 0.32331743836402893; L(Test): 0.2934597432613373\n",
            "Epoch 8764/10000: L(Train): 0.3246915340423584; L(Test): 0.2940921485424042\n",
            "Epoch 8765/10000: L(Train): 0.3060815632343292; L(Test): 0.2936381697654724\n",
            "Epoch 8766/10000: L(Train): 0.326137512922287; L(Test): 0.293040931224823\n",
            "Epoch 8767/10000: L(Train): 0.31331855058670044; L(Test): 0.2933371365070343\n",
            "Epoch 8768/10000: L(Train): 0.3240971565246582; L(Test): 0.29232078790664673\n",
            "Epoch 8769/10000: L(Train): 0.31132665276527405; L(Test): 0.29491204023361206\n",
            "Epoch 8770/10000: L(Train): 0.31784722208976746; L(Test): 0.2956346869468689\n",
            "Epoch 8771/10000: L(Train): 0.32609039545059204; L(Test): 0.2946193516254425\n",
            "Epoch 8772/10000: L(Train): 0.31526193022727966; L(Test): 0.2948494255542755\n",
            "Epoch 8773/10000: L(Train): 0.3238504230976105; L(Test): 0.2939826250076294\n",
            "Epoch 8774/10000: L(Train): 0.3191678822040558; L(Test): 0.29329589009284973\n",
            "Epoch 8775/10000: L(Train): 0.3153499662876129; L(Test): 0.29455435276031494\n",
            "Epoch 8776/10000: L(Train): 0.3234042227268219; L(Test): 0.2946672737598419\n",
            "Epoch 8777/10000: L(Train): 0.3228553235530853; L(Test): 0.29332318902015686\n",
            "Epoch 8778/10000: L(Train): 0.3191009759902954; L(Test): 0.2941115200519562\n",
            "Epoch 8779/10000: L(Train): 0.31179213523864746; L(Test): 0.2953087389469147\n",
            "Epoch 8780/10000: L(Train): 0.31239038705825806; L(Test): 0.29558613896369934\n",
            "Epoch 8781/10000: L(Train): 0.32416239380836487; L(Test): 0.2955414056777954\n",
            "Epoch 8782/10000: L(Train): 0.3238147497177124; L(Test): 0.29583144187927246\n",
            "Epoch 8783/10000: L(Train): 0.32691144943237305; L(Test): 0.29649245738983154\n",
            "Epoch 8784/10000: L(Train): 0.32487452030181885; L(Test): 0.29760369658470154\n",
            "Epoch 8785/10000: L(Train): 0.32066866755485535; L(Test): 0.2960541248321533\n",
            "Epoch 8786/10000: L(Train): 0.325332373380661; L(Test): 0.29777154326438904\n",
            "Epoch 8787/10000: L(Train): 0.313610702753067; L(Test): 0.30472344160079956\n",
            "Epoch 8788/10000: L(Train): 0.3297882676124573; L(Test): 0.2978476881980896\n",
            "Epoch 8789/10000: L(Train): 0.31848353147506714; L(Test): 0.29723742604255676\n",
            "Epoch 8790/10000: L(Train): 0.3214104473590851; L(Test): 0.2991042733192444\n",
            "Epoch 8791/10000: L(Train): 0.3202752470970154; L(Test): 0.29648467898368835\n",
            "Epoch 8792/10000: L(Train): 0.31723299622535706; L(Test): 0.296501100063324\n",
            "Epoch 8793/10000: L(Train): 0.31610947847366333; L(Test): 0.296914279460907\n",
            "Epoch 8794/10000: L(Train): 0.318289577960968; L(Test): 0.2977924942970276\n",
            "Epoch 8795/10000: L(Train): 0.31765058636665344; L(Test): 0.2992694079875946\n",
            "Epoch 8796/10000: L(Train): 0.32102230191230774; L(Test): 0.2992022931575775\n",
            "Epoch 8797/10000: L(Train): 0.31972017884254456; L(Test): 0.29679346084594727\n",
            "Epoch 8798/10000: L(Train): 0.3181793689727783; L(Test): 0.2961832880973816\n",
            "Epoch 8799/10000: L(Train): 0.31554001569747925; L(Test): 0.29601943492889404\n",
            "Epoch 8800/10000: L(Train): 0.3186929225921631; L(Test): 0.2958305776119232\n",
            "Epoch 8801/10000: L(Train): 0.31625962257385254; L(Test): 0.29626327753067017\n",
            "Epoch 8802/10000: L(Train): 0.3251066207885742; L(Test): 0.29577407240867615\n",
            "Epoch 8803/10000: L(Train): 0.31567513942718506; L(Test): 0.29471883177757263\n",
            "Epoch 8804/10000: L(Train): 0.3171459138393402; L(Test): 0.29410320520401\n",
            "Epoch 8805/10000: L(Train): 0.30558666586875916; L(Test): 0.29447945952415466\n",
            "Epoch 8806/10000: L(Train): 0.3200981616973877; L(Test): 0.29428544640541077\n",
            "Epoch 8807/10000: L(Train): 0.3097274899482727; L(Test): 0.293641597032547\n",
            "Epoch 8808/10000: L(Train): 0.32415106892585754; L(Test): 0.29278460144996643\n",
            "Epoch 8809/10000: L(Train): 0.3111031949520111; L(Test): 0.2925570607185364\n",
            "Epoch 8810/10000: L(Train): 0.3112943470478058; L(Test): 0.292502224445343\n",
            "Epoch 8811/10000: L(Train): 0.32239019870758057; L(Test): 0.29196247458457947\n",
            "Epoch 8812/10000: L(Train): 0.3246043026447296; L(Test): 0.29172033071517944\n",
            "Epoch 8813/10000: L(Train): 0.31493788957595825; L(Test): 0.2923377454280853\n",
            "Epoch 8814/10000: L(Train): 0.31387853622436523; L(Test): 0.2922932505607605\n",
            "Epoch 8815/10000: L(Train): 0.3188037574291229; L(Test): 0.2914503514766693\n",
            "Epoch 8816/10000: L(Train): 0.3220389187335968; L(Test): 0.2915651202201843\n",
            "Epoch 8817/10000: L(Train): 0.3095542788505554; L(Test): 0.29180625081062317\n",
            "Epoch 8818/10000: L(Train): 0.3230358958244324; L(Test): 0.2921297550201416\n",
            "Epoch 8819/10000: L(Train): 0.3220212459564209; L(Test): 0.2922361493110657\n",
            "Epoch 8820/10000: L(Train): 0.31609299778938293; L(Test): 0.29082170128822327\n",
            "Epoch 8821/10000: L(Train): 0.31892552971839905; L(Test): 0.29141294956207275\n",
            "Epoch 8822/10000: L(Train): 0.31572866439819336; L(Test): 0.29234930872917175\n",
            "Epoch 8823/10000: L(Train): 0.3097645342350006; L(Test): 0.2915997803211212\n",
            "Epoch 8824/10000: L(Train): 0.31100666522979736; L(Test): 0.29439833760261536\n",
            "Epoch 8825/10000: L(Train): 0.3227102756500244; L(Test): 0.29303574562072754\n",
            "Epoch 8826/10000: L(Train): 0.314282089471817; L(Test): 0.29347050189971924\n",
            "Epoch 8827/10000: L(Train): 0.31300005316734314; L(Test): 0.29259344935417175\n",
            "Epoch 8828/10000: L(Train): 0.31259000301361084; L(Test): 0.2912348508834839\n",
            "Epoch 8829/10000: L(Train): 0.31924089789390564; L(Test): 0.29339972138404846\n",
            "Epoch 8830/10000: L(Train): 0.3161384165287018; L(Test): 0.2940669655799866\n",
            "Epoch 8831/10000: L(Train): 0.32280856370925903; L(Test): 0.2920946478843689\n",
            "Epoch 8832/10000: L(Train): 0.3163816034793854; L(Test): 0.2934586703777313\n",
            "Epoch 8833/10000: L(Train): 0.3182852268218994; L(Test): 0.29293563961982727\n",
            "Epoch 8834/10000: L(Train): 0.317854106426239; L(Test): 0.294005423784256\n",
            "Epoch 8835/10000: L(Train): 0.33032500743865967; L(Test): 0.2945113480091095\n",
            "Epoch 8836/10000: L(Train): 0.3226434886455536; L(Test): 0.29255619645118713\n",
            "Epoch 8837/10000: L(Train): 0.31963902711868286; L(Test): 0.2924754023551941\n",
            "Epoch 8838/10000: L(Train): 0.3228171169757843; L(Test): 0.293469101190567\n",
            "Epoch 8839/10000: L(Train): 0.31923314929008484; L(Test): 0.29339081048965454\n",
            "Epoch 8840/10000: L(Train): 0.31838124990463257; L(Test): 0.29372286796569824\n",
            "Epoch 8841/10000: L(Train): 0.3197875916957855; L(Test): 0.29461491107940674\n",
            "Epoch 8842/10000: L(Train): 0.3227885663509369; L(Test): 0.2942110002040863\n",
            "Epoch 8843/10000: L(Train): 0.31948167085647583; L(Test): 0.29327136278152466\n",
            "Epoch 8844/10000: L(Train): 0.31047892570495605; L(Test): 0.2944273352622986\n",
            "Epoch 8845/10000: L(Train): 0.31339097023010254; L(Test): 0.2945447862148285\n",
            "Epoch 8846/10000: L(Train): 0.32708656787872314; L(Test): 0.29473641514778137\n",
            "Epoch 8847/10000: L(Train): 0.3242296874523163; L(Test): 0.2959832549095154\n",
            "Epoch 8848/10000: L(Train): 0.3249257504940033; L(Test): 0.29486507177352905\n",
            "Epoch 8849/10000: L(Train): 0.31739047169685364; L(Test): 0.29309019446372986\n",
            "Epoch 8850/10000: L(Train): 0.30899137258529663; L(Test): 0.2937373220920563\n",
            "Epoch 8851/10000: L(Train): 0.3132481575012207; L(Test): 0.29284143447875977\n",
            "Epoch 8852/10000: L(Train): 0.32448408007621765; L(Test): 0.2922971844673157\n",
            "Epoch 8853/10000: L(Train): 0.3102932572364807; L(Test): 0.292717307806015\n",
            "Epoch 8854/10000: L(Train): 0.3257473409175873; L(Test): 0.2929109036922455\n",
            "Epoch 8855/10000: L(Train): 0.32750728726387024; L(Test): 0.29350030422210693\n",
            "Epoch 8856/10000: L(Train): 0.3152024745941162; L(Test): 0.29328033328056335\n",
            "Epoch 8857/10000: L(Train): 0.3195848762989044; L(Test): 0.2930678427219391\n",
            "Epoch 8858/10000: L(Train): 0.3251299262046814; L(Test): 0.2933817505836487\n",
            "Epoch 8859/10000: L(Train): 0.32487186789512634; L(Test): 0.292696088552475\n",
            "Epoch 8860/10000: L(Train): 0.3181830644607544; L(Test): 0.2926924526691437\n",
            "Epoch 8861/10000: L(Train): 0.31836459040641785; L(Test): 0.29356497526168823\n",
            "Epoch 8862/10000: L(Train): 0.31888100504875183; L(Test): 0.293241947889328\n",
            "Epoch 8863/10000: L(Train): 0.32709962129592896; L(Test): 0.29204535484313965\n",
            "Epoch 8864/10000: L(Train): 0.3157825469970703; L(Test): 0.2923761010169983\n",
            "Epoch 8865/10000: L(Train): 0.315456360578537; L(Test): 0.2926173508167267\n",
            "Epoch 8866/10000: L(Train): 0.31706303358078003; L(Test): 0.2922532856464386\n",
            "Epoch 8867/10000: L(Train): 0.322384774684906; L(Test): 0.29230138659477234\n",
            "Epoch 8868/10000: L(Train): 0.3185313642024994; L(Test): 0.29285928606987\n",
            "Epoch 8869/10000: L(Train): 0.32050326466560364; L(Test): 0.2933753728866577\n",
            "Epoch 8870/10000: L(Train): 0.32282593846321106; L(Test): 0.2927245497703552\n",
            "Epoch 8871/10000: L(Train): 0.32448309659957886; L(Test): 0.29240694642066956\n",
            "Epoch 8872/10000: L(Train): 0.32369309663772583; L(Test): 0.29392147064208984\n",
            "Epoch 8873/10000: L(Train): 0.32581639289855957; L(Test): 0.2934463322162628\n",
            "Epoch 8874/10000: L(Train): 0.31174325942993164; L(Test): 0.2922709286212921\n",
            "Epoch 8875/10000: L(Train): 0.3150244951248169; L(Test): 0.29191502928733826\n",
            "Epoch 8876/10000: L(Train): 0.3143620491027832; L(Test): 0.2935754060745239\n",
            "Epoch 8877/10000: L(Train): 0.32638782262802124; L(Test): 0.29445719718933105\n",
            "Epoch 8878/10000: L(Train): 0.3238679766654968; L(Test): 0.29330676794052124\n",
            "Epoch 8879/10000: L(Train): 0.3176165819168091; L(Test): 0.2931935787200928\n",
            "Epoch 8880/10000: L(Train): 0.32278206944465637; L(Test): 0.29400748014450073\n",
            "Epoch 8881/10000: L(Train): 0.3152722120285034; L(Test): 0.293601393699646\n",
            "Epoch 8882/10000: L(Train): 0.31536778807640076; L(Test): 0.2934151887893677\n",
            "Epoch 8883/10000: L(Train): 0.3124040961265564; L(Test): 0.29318830370903015\n",
            "Epoch 8884/10000: L(Train): 0.3115527927875519; L(Test): 0.2948492765426636\n",
            "Epoch 8885/10000: L(Train): 0.3166988790035248; L(Test): 0.2931462228298187\n",
            "Epoch 8886/10000: L(Train): 0.3153722584247589; L(Test): 0.2936461269855499\n",
            "Epoch 8887/10000: L(Train): 0.3144441545009613; L(Test): 0.2957473397254944\n",
            "Epoch 8888/10000: L(Train): 0.3236369788646698; L(Test): 0.29419174790382385\n",
            "Epoch 8889/10000: L(Train): 0.31804323196411133; L(Test): 0.2958667576313019\n",
            "Epoch 8890/10000: L(Train): 0.3104694187641144; L(Test): 0.29755112528800964\n",
            "Epoch 8891/10000: L(Train): 0.32677191495895386; L(Test): 0.2958950400352478\n",
            "Epoch 8892/10000: L(Train): 0.32865098118782043; L(Test): 0.2961033284664154\n",
            "Epoch 8893/10000: L(Train): 0.32271042466163635; L(Test): 0.29397082328796387\n",
            "Epoch 8894/10000: L(Train): 0.3081425726413727; L(Test): 0.2946375012397766\n",
            "Epoch 8895/10000: L(Train): 0.3115832507610321; L(Test): 0.29551249742507935\n",
            "Epoch 8896/10000: L(Train): 0.317374050617218; L(Test): 0.29437392950057983\n",
            "Epoch 8897/10000: L(Train): 0.3171572983264923; L(Test): 0.2945599853992462\n",
            "Epoch 8898/10000: L(Train): 0.32259494066238403; L(Test): 0.29564720392227173\n",
            "Epoch 8899/10000: L(Train): 0.32414576411247253; L(Test): 0.29537829756736755\n",
            "Epoch 8900/10000: L(Train): 0.3266782760620117; L(Test): 0.29559963941574097\n",
            "Epoch 8901/10000: L(Train): 0.31672194600105286; L(Test): 0.29664304852485657\n",
            "Epoch 8902/10000: L(Train): 0.3166409730911255; L(Test): 0.2948102653026581\n",
            "Epoch 8903/10000: L(Train): 0.32099583745002747; L(Test): 0.29659727215766907\n",
            "Epoch 8904/10000: L(Train): 0.3241390287876129; L(Test): 0.2963244616985321\n",
            "Epoch 8905/10000: L(Train): 0.3146555721759796; L(Test): 0.2969277799129486\n",
            "Epoch 8906/10000: L(Train): 0.3186381757259369; L(Test): 0.29810479283332825\n",
            "Epoch 8907/10000: L(Train): 0.3283749520778656; L(Test): 0.29576247930526733\n",
            "Epoch 8908/10000: L(Train): 0.3219985067844391; L(Test): 0.2962213158607483\n",
            "Epoch 8909/10000: L(Train): 0.3325420618057251; L(Test): 0.298618346452713\n",
            "Epoch 8910/10000: L(Train): 0.3197210431098938; L(Test): 0.29718106985092163\n",
            "Epoch 8911/10000: L(Train): 0.31982535123825073; L(Test): 0.2961268424987793\n",
            "Epoch 8912/10000: L(Train): 0.3226735293865204; L(Test): 0.2963249087333679\n",
            "Epoch 8913/10000: L(Train): 0.31875869631767273; L(Test): 0.2950909435749054\n",
            "Epoch 8914/10000: L(Train): 0.3186345100402832; L(Test): 0.2941015064716339\n",
            "Epoch 8915/10000: L(Train): 0.3198486268520355; L(Test): 0.294581800699234\n",
            "Epoch 8916/10000: L(Train): 0.3136768639087677; L(Test): 0.2954230308532715\n",
            "Epoch 8917/10000: L(Train): 0.32251623272895813; L(Test): 0.2953469753265381\n",
            "Epoch 8918/10000: L(Train): 0.3161207139492035; L(Test): 0.2938457429409027\n",
            "Epoch 8919/10000: L(Train): 0.3205845057964325; L(Test): 0.292447030544281\n",
            "Epoch 8920/10000: L(Train): 0.3132897913455963; L(Test): 0.29234400391578674\n",
            "Epoch 8921/10000: L(Train): 0.3161005675792694; L(Test): 0.2933526635169983\n",
            "Epoch 8922/10000: L(Train): 0.32029667496681213; L(Test): 0.2933230698108673\n",
            "Epoch 8923/10000: L(Train): 0.31563320755958557; L(Test): 0.2929399609565735\n",
            "Epoch 8924/10000: L(Train): 0.3164280354976654; L(Test): 0.29254940152168274\n",
            "Epoch 8925/10000: L(Train): 0.3098548352718353; L(Test): 0.2925679087638855\n",
            "Epoch 8926/10000: L(Train): 0.32247263193130493; L(Test): 0.29271364212036133\n",
            "Epoch 8927/10000: L(Train): 0.31593942642211914; L(Test): 0.29296988248825073\n",
            "Epoch 8928/10000: L(Train): 0.32196709513664246; L(Test): 0.29324591159820557\n",
            "Epoch 8929/10000: L(Train): 0.32412189245224; L(Test): 0.29274532198905945\n",
            "Epoch 8930/10000: L(Train): 0.31287652254104614; L(Test): 0.29154685139656067\n",
            "Epoch 8931/10000: L(Train): 0.3189590275287628; L(Test): 0.29205426573753357\n",
            "Epoch 8932/10000: L(Train): 0.3242230713367462; L(Test): 0.29375159740448\n",
            "Epoch 8933/10000: L(Train): 0.3109700679779053; L(Test): 0.29300254583358765\n",
            "Epoch 8934/10000: L(Train): 0.3230836093425751; L(Test): 0.2933722138404846\n",
            "Epoch 8935/10000: L(Train): 0.3212960660457611; L(Test): 0.295175164937973\n",
            "Epoch 8936/10000: L(Train): 0.3160214126110077; L(Test): 0.2957111597061157\n",
            "Epoch 8937/10000: L(Train): 0.31687334179878235; L(Test): 0.294621080160141\n",
            "Epoch 8938/10000: L(Train): 0.322639524936676; L(Test): 0.29371917247772217\n",
            "Epoch 8939/10000: L(Train): 0.31714463233947754; L(Test): 0.29322364926338196\n",
            "Epoch 8940/10000: L(Train): 0.3180273473262787; L(Test): 0.29315057396888733\n",
            "Epoch 8941/10000: L(Train): 0.3231656849384308; L(Test): 0.29324859380722046\n",
            "Epoch 8942/10000: L(Train): 0.3190934360027313; L(Test): 0.29386064410209656\n",
            "Epoch 8943/10000: L(Train): 0.3214074373245239; L(Test): 0.29417937994003296\n",
            "Epoch 8944/10000: L(Train): 0.3212733268737793; L(Test): 0.29409468173980713\n",
            "Epoch 8945/10000: L(Train): 0.31688565015792847; L(Test): 0.29306021332740784\n",
            "Epoch 8946/10000: L(Train): 0.3079996407032013; L(Test): 0.2930430471897125\n",
            "Epoch 8947/10000: L(Train): 0.32106927037239075; L(Test): 0.29350724816322327\n",
            "Epoch 8948/10000: L(Train): 0.32423844933509827; L(Test): 0.2944391965866089\n",
            "Epoch 8949/10000: L(Train): 0.32145610451698303; L(Test): 0.2951495349407196\n",
            "Epoch 8950/10000: L(Train): 0.3088012635707855; L(Test): 0.294925332069397\n",
            "Epoch 8951/10000: L(Train): 0.32128462195396423; L(Test): 0.29448795318603516\n",
            "Epoch 8952/10000: L(Train): 0.3293478190898895; L(Test): 0.29428598284721375\n",
            "Epoch 8953/10000: L(Train): 0.31675317883491516; L(Test): 0.293037086725235\n",
            "Epoch 8954/10000: L(Train): 0.3146979808807373; L(Test): 0.2928404211997986\n",
            "Epoch 8955/10000: L(Train): 0.3207438886165619; L(Test): 0.29449576139450073\n",
            "Epoch 8956/10000: L(Train): 0.32108309864997864; L(Test): 0.2947389781475067\n",
            "Epoch 8957/10000: L(Train): 0.3286410868167877; L(Test): 0.29436197876930237\n",
            "Epoch 8958/10000: L(Train): 0.30981171131134033; L(Test): 0.2960164546966553\n",
            "Epoch 8959/10000: L(Train): 0.31912899017333984; L(Test): 0.2971022129058838\n",
            "Epoch 8960/10000: L(Train): 0.31714928150177; L(Test): 0.2958831787109375\n",
            "Epoch 8961/10000: L(Train): 0.3253070116043091; L(Test): 0.2969590425491333\n",
            "Epoch 8962/10000: L(Train): 0.3241592049598694; L(Test): 0.2970787286758423\n",
            "Epoch 8963/10000: L(Train): 0.32064321637153625; L(Test): 0.29576781392097473\n",
            "Epoch 8964/10000: L(Train): 0.32495394349098206; L(Test): 0.2969703674316406\n",
            "Epoch 8965/10000: L(Train): 0.31840041279792786; L(Test): 0.29559043049812317\n",
            "Epoch 8966/10000: L(Train): 0.31855279207229614; L(Test): 0.29390600323677063\n",
            "Epoch 8967/10000: L(Train): 0.31451451778411865; L(Test): 0.2939995527267456\n",
            "Epoch 8968/10000: L(Train): 0.3151605725288391; L(Test): 0.29289349913597107\n",
            "Epoch 8969/10000: L(Train): 0.3100193440914154; L(Test): 0.29413121938705444\n",
            "Epoch 8970/10000: L(Train): 0.3179785907268524; L(Test): 0.29357385635375977\n",
            "Epoch 8971/10000: L(Train): 0.31636515259742737; L(Test): 0.2929445505142212\n",
            "Epoch 8972/10000: L(Train): 0.3151938021183014; L(Test): 0.2935468256473541\n",
            "Epoch 8973/10000: L(Train): 0.31674835085868835; L(Test): 0.29294338822364807\n",
            "Epoch 8974/10000: L(Train): 0.3141935169696808; L(Test): 0.29285168647766113\n",
            "Epoch 8975/10000: L(Train): 0.3204987049102783; L(Test): 0.29278263449668884\n",
            "Epoch 8976/10000: L(Train): 0.31539472937583923; L(Test): 0.2924381494522095\n",
            "Epoch 8977/10000: L(Train): 0.3167022466659546; L(Test): 0.29153335094451904\n",
            "Epoch 8978/10000: L(Train): 0.3254473805427551; L(Test): 0.2914483845233917\n",
            "Epoch 8979/10000: L(Train): 0.315164715051651; L(Test): 0.29080745577812195\n",
            "Epoch 8980/10000: L(Train): 0.30856791138648987; L(Test): 0.29158687591552734\n",
            "Epoch 8981/10000: L(Train): 0.3113310933113098; L(Test): 0.29126742482185364\n",
            "Epoch 8982/10000: L(Train): 0.30151453614234924; L(Test): 0.29103365540504456\n",
            "Epoch 8983/10000: L(Train): 0.32045772671699524; L(Test): 0.29085448384284973\n",
            "Epoch 8984/10000: L(Train): 0.3161183297634125; L(Test): 0.2909280061721802\n",
            "Epoch 8985/10000: L(Train): 0.3158731758594513; L(Test): 0.29118287563323975\n",
            "Epoch 8986/10000: L(Train): 0.30918288230895996; L(Test): 0.2910526692867279\n",
            "Epoch 8987/10000: L(Train): 0.3135722875595093; L(Test): 0.2920396625995636\n",
            "Epoch 8988/10000: L(Train): 0.32048940658569336; L(Test): 0.29110968112945557\n",
            "Epoch 8989/10000: L(Train): 0.3133595883846283; L(Test): 0.2910284399986267\n",
            "Epoch 8990/10000: L(Train): 0.3220725953578949; L(Test): 0.2907513380050659\n",
            "Epoch 8991/10000: L(Train): 0.32559844851493835; L(Test): 0.29096877574920654\n",
            "Epoch 8992/10000: L(Train): 0.32409483194351196; L(Test): 0.2930924892425537\n",
            "Epoch 8993/10000: L(Train): 0.3129671812057495; L(Test): 0.2932080924510956\n",
            "Epoch 8994/10000: L(Train): 0.3106956481933594; L(Test): 0.2932700216770172\n",
            "Epoch 8995/10000: L(Train): 0.3211842179298401; L(Test): 0.2934713065624237\n",
            "Epoch 8996/10000: L(Train): 0.31670570373535156; L(Test): 0.2927725315093994\n",
            "Epoch 8997/10000: L(Train): 0.32852134108543396; L(Test): 0.2918848991394043\n",
            "Epoch 8998/10000: L(Train): 0.31632915139198303; L(Test): 0.29490765929222107\n",
            "Epoch 8999/10000: L(Train): 0.3145413100719452; L(Test): 0.2942117750644684\n",
            "Epoch 9000/10000: L(Train): 0.3217756152153015; L(Test): 0.2924652397632599\n",
            "Epoch 9001/10000: L(Train): 0.3227424919605255; L(Test): 0.293916255235672\n",
            "Epoch 9002/10000: L(Train): 0.3166925609111786; L(Test): 0.29372820258140564\n",
            "Epoch 9003/10000: L(Train): 0.31971871852874756; L(Test): 0.29467618465423584\n",
            "Epoch 9004/10000: L(Train): 0.31272831559181213; L(Test): 0.29837682843208313\n",
            "Epoch 9005/10000: L(Train): 0.3284575045108795; L(Test): 0.29460635781288147\n",
            "Epoch 9006/10000: L(Train): 0.32694512605667114; L(Test): 0.29555654525756836\n",
            "Epoch 9007/10000: L(Train): 0.32083597779273987; L(Test): 0.29445502161979675\n",
            "Epoch 9008/10000: L(Train): 0.3266925513744354; L(Test): 0.29356229305267334\n",
            "Epoch 9009/10000: L(Train): 0.3160078823566437; L(Test): 0.2948683202266693\n",
            "Epoch 9010/10000: L(Train): 0.3209819495677948; L(Test): 0.2933835983276367\n",
            "Epoch 9011/10000: L(Train): 0.3143133819103241; L(Test): 0.2930590808391571\n",
            "Epoch 9012/10000: L(Train): 0.3184182047843933; L(Test): 0.2938425540924072\n",
            "Epoch 9013/10000: L(Train): 0.3232489228248596; L(Test): 0.2936842143535614\n",
            "Epoch 9014/10000: L(Train): 0.31386610865592957; L(Test): 0.2942318618297577\n",
            "Epoch 9015/10000: L(Train): 0.32190003991127014; L(Test): 0.29528266191482544\n",
            "Epoch 9016/10000: L(Train): 0.31471386551856995; L(Test): 0.2955871820449829\n",
            "Epoch 9017/10000: L(Train): 0.31771594285964966; L(Test): 0.294718861579895\n",
            "Epoch 9018/10000: L(Train): 0.3245604932308197; L(Test): 0.293418288230896\n",
            "Epoch 9019/10000: L(Train): 0.31016212701797485; L(Test): 0.29472216963768005\n",
            "Epoch 9020/10000: L(Train): 0.31729942560195923; L(Test): 0.29361069202423096\n",
            "Epoch 9021/10000: L(Train): 0.32069677114486694; L(Test): 0.2949635982513428\n",
            "Epoch 9022/10000: L(Train): 0.32089751958847046; L(Test): 0.2965940237045288\n",
            "Epoch 9023/10000: L(Train): 0.322658509016037; L(Test): 0.29321980476379395\n",
            "Epoch 9024/10000: L(Train): 0.3184630274772644; L(Test): 0.2942054271697998\n",
            "Epoch 9025/10000: L(Train): 0.31180110573768616; L(Test): 0.29688921570777893\n",
            "Epoch 9026/10000: L(Train): 0.3160950839519501; L(Test): 0.2939932644367218\n",
            "Epoch 9027/10000: L(Train): 0.31400367617607117; L(Test): 0.29719117283821106\n",
            "Epoch 9028/10000: L(Train): 0.31491291522979736; L(Test): 0.29747217893600464\n",
            "Epoch 9029/10000: L(Train): 0.3216138780117035; L(Test): 0.2931331992149353\n",
            "Epoch 9030/10000: L(Train): 0.32109540700912476; L(Test): 0.2953556180000305\n",
            "Epoch 9031/10000: L(Train): 0.322867751121521; L(Test): 0.2937301993370056\n",
            "Epoch 9032/10000: L(Train): 0.316153347492218; L(Test): 0.2941429018974304\n",
            "Epoch 9033/10000: L(Train): 0.3252677917480469; L(Test): 0.2956545054912567\n",
            "Epoch 9034/10000: L(Train): 0.3112160563468933; L(Test): 0.29533645510673523\n",
            "Epoch 9035/10000: L(Train): 0.3296346962451935; L(Test): 0.2945994436740875\n",
            "Epoch 9036/10000: L(Train): 0.3210240602493286; L(Test): 0.2947752773761749\n",
            "Epoch 9037/10000: L(Train): 0.3284846842288971; L(Test): 0.2932543456554413\n",
            "Epoch 9038/10000: L(Train): 0.3178609311580658; L(Test): 0.29326412081718445\n",
            "Epoch 9039/10000: L(Train): 0.3248422145843506; L(Test): 0.29419371485710144\n",
            "Epoch 9040/10000: L(Train): 0.32094520330429077; L(Test): 0.2980157732963562\n",
            "Epoch 9041/10000: L(Train): 0.31850704550743103; L(Test): 0.29963386058807373\n",
            "Epoch 9042/10000: L(Train): 0.3264560103416443; L(Test): 0.299468070268631\n",
            "Epoch 9043/10000: L(Train): 0.3275797963142395; L(Test): 0.30087921023368835\n",
            "Epoch 9044/10000: L(Train): 0.32117772102355957; L(Test): 0.30213791131973267\n",
            "Epoch 9045/10000: L(Train): 0.312511682510376; L(Test): 0.3028154671192169\n",
            "Epoch 9046/10000: L(Train): 0.31853508949279785; L(Test): 0.3045448362827301\n",
            "Epoch 9047/10000: L(Train): 0.33159568905830383; L(Test): 0.30001187324523926\n",
            "Epoch 9048/10000: L(Train): 0.3243483901023865; L(Test): 0.30072855949401855\n",
            "Epoch 9049/10000: L(Train): 0.31871312856674194; L(Test): 0.30328190326690674\n",
            "Epoch 9050/10000: L(Train): 0.3214949369430542; L(Test): 0.30180108547210693\n",
            "Epoch 9051/10000: L(Train): 0.3240625560283661; L(Test): 0.3032800853252411\n",
            "Epoch 9052/10000: L(Train): 0.3251844346523285; L(Test): 0.30211520195007324\n",
            "Epoch 9053/10000: L(Train): 0.3271791338920593; L(Test): 0.2998192608356476\n",
            "Epoch 9054/10000: L(Train): 0.3168761134147644; L(Test): 0.299824595451355\n",
            "Epoch 9055/10000: L(Train): 0.31787705421447754; L(Test): 0.3001745641231537\n",
            "Epoch 9056/10000: L(Train): 0.3274189531803131; L(Test): 0.2980794608592987\n",
            "Epoch 9057/10000: L(Train): 0.3166891038417816; L(Test): 0.2971831262111664\n",
            "Epoch 9058/10000: L(Train): 0.31307485699653625; L(Test): 0.29687654972076416\n",
            "Epoch 9059/10000: L(Train): 0.32803234457969666; L(Test): 0.29730087518692017\n",
            "Epoch 9060/10000: L(Train): 0.32614654302597046; L(Test): 0.29640957713127136\n",
            "Epoch 9061/10000: L(Train): 0.32705390453338623; L(Test): 0.29532745480537415\n",
            "Epoch 9062/10000: L(Train): 0.32607486844062805; L(Test): 0.2959260940551758\n",
            "Epoch 9063/10000: L(Train): 0.3222532868385315; L(Test): 0.2974023222923279\n",
            "Epoch 9064/10000: L(Train): 0.3172248899936676; L(Test): 0.29637202620506287\n",
            "Epoch 9065/10000: L(Train): 0.3160926401615143; L(Test): 0.29467421770095825\n",
            "Epoch 9066/10000: L(Train): 0.31728777289390564; L(Test): 0.29506638646125793\n",
            "Epoch 9067/10000: L(Train): 0.31790807843208313; L(Test): 0.2950108051300049\n",
            "Epoch 9068/10000: L(Train): 0.3198642134666443; L(Test): 0.2940676510334015\n",
            "Epoch 9069/10000: L(Train): 0.3181081712245941; L(Test): 0.29341310262680054\n",
            "Epoch 9070/10000: L(Train): 0.310039758682251; L(Test): 0.2930886745452881\n",
            "Epoch 9071/10000: L(Train): 0.32610464096069336; L(Test): 0.29247042536735535\n",
            "Epoch 9072/10000: L(Train): 0.31189602613449097; L(Test): 0.2919350266456604\n",
            "Epoch 9073/10000: L(Train): 0.3062484860420227; L(Test): 0.2912849187850952\n",
            "Epoch 9074/10000: L(Train): 0.31985726952552795; L(Test): 0.2914067506790161\n",
            "Epoch 9075/10000: L(Train): 0.31864866614341736; L(Test): 0.2928299009799957\n",
            "Epoch 9076/10000: L(Train): 0.311064749956131; L(Test): 0.29265034198760986\n",
            "Epoch 9077/10000: L(Train): 0.3225833475589752; L(Test): 0.29319489002227783\n",
            "Epoch 9078/10000: L(Train): 0.32622772455215454; L(Test): 0.2915479838848114\n",
            "Epoch 9079/10000: L(Train): 0.30695003271102905; L(Test): 0.29121190309524536\n",
            "Epoch 9080/10000: L(Train): 0.32431840896606445; L(Test): 0.29089298844337463\n",
            "Epoch 9081/10000: L(Train): 0.32205531001091003; L(Test): 0.2905255854129791\n",
            "Epoch 9082/10000: L(Train): 0.3103944957256317; L(Test): 0.29058149456977844\n",
            "Epoch 9083/10000: L(Train): 0.3180335462093353; L(Test): 0.2911017835140228\n",
            "Epoch 9084/10000: L(Train): 0.31178802251815796; L(Test): 0.29210713505744934\n",
            "Epoch 9085/10000: L(Train): 0.3210587501525879; L(Test): 0.2922687530517578\n",
            "Epoch 9086/10000: L(Train): 0.3152507543563843; L(Test): 0.2917751371860504\n",
            "Epoch 9087/10000: L(Train): 0.3113929331302643; L(Test): 0.29221680760383606\n",
            "Epoch 9088/10000: L(Train): 0.3228623569011688; L(Test): 0.29390233755111694\n",
            "Epoch 9089/10000: L(Train): 0.3203983008861542; L(Test): 0.29523491859436035\n",
            "Epoch 9090/10000: L(Train): 0.32033872604370117; L(Test): 0.2942661643028259\n",
            "Epoch 9091/10000: L(Train): 0.32412058115005493; L(Test): 0.2939170002937317\n",
            "Epoch 9092/10000: L(Train): 0.32286909222602844; L(Test): 0.29423847794532776\n",
            "Epoch 9093/10000: L(Train): 0.32231244444847107; L(Test): 0.29646170139312744\n",
            "Epoch 9094/10000: L(Train): 0.3224693536758423; L(Test): 0.2953583300113678\n",
            "Epoch 9095/10000: L(Train): 0.31942808628082275; L(Test): 0.29540228843688965\n",
            "Epoch 9096/10000: L(Train): 0.3234156668186188; L(Test): 0.2967621088027954\n",
            "Epoch 9097/10000: L(Train): 0.31800326704978943; L(Test): 0.2951386570930481\n",
            "Epoch 9098/10000: L(Train): 0.3124680519104004; L(Test): 0.2950776219367981\n",
            "Epoch 9099/10000: L(Train): 0.30980220437049866; L(Test): 0.2980213761329651\n",
            "Epoch 9100/10000: L(Train): 0.32892704010009766; L(Test): 0.298061341047287\n",
            "Epoch 9101/10000: L(Train): 0.3236916959285736; L(Test): 0.2955794632434845\n",
            "Epoch 9102/10000: L(Train): 0.32374414801597595; L(Test): 0.29535019397735596\n",
            "Epoch 9103/10000: L(Train): 0.32725533843040466; L(Test): 0.2962593138217926\n",
            "Epoch 9104/10000: L(Train): 0.3268589675426483; L(Test): 0.2988671064376831\n",
            "Epoch 9105/10000: L(Train): 0.33157312870025635; L(Test): 0.2981184422969818\n",
            "Epoch 9106/10000: L(Train): 0.3236677348613739; L(Test): 0.29630160331726074\n",
            "Epoch 9107/10000: L(Train): 0.31338217854499817; L(Test): 0.2986871302127838\n",
            "Epoch 9108/10000: L(Train): 0.3269443213939667; L(Test): 0.2980307638645172\n",
            "Epoch 9109/10000: L(Train): 0.31911906599998474; L(Test): 0.2971506714820862\n",
            "Epoch 9110/10000: L(Train): 0.32468903064727783; L(Test): 0.2980870306491852\n",
            "Epoch 9111/10000: L(Train): 0.316175252199173; L(Test): 0.29747316241264343\n",
            "Epoch 9112/10000: L(Train): 0.3183390498161316; L(Test): 0.2964020073413849\n",
            "Epoch 9113/10000: L(Train): 0.3222372829914093; L(Test): 0.29627561569213867\n",
            "Epoch 9114/10000: L(Train): 0.3204062283039093; L(Test): 0.2961215674877167\n",
            "Epoch 9115/10000: L(Train): 0.3188812732696533; L(Test): 0.2970722019672394\n",
            "Epoch 9116/10000: L(Train): 0.32254281640052795; L(Test): 0.29693302512168884\n",
            "Epoch 9117/10000: L(Train): 0.3255853056907654; L(Test): 0.2950281798839569\n",
            "Epoch 9118/10000: L(Train): 0.3165564239025116; L(Test): 0.2960282564163208\n",
            "Epoch 9119/10000: L(Train): 0.3186891973018646; L(Test): 0.29645463824272156\n",
            "Epoch 9120/10000: L(Train): 0.3207969665527344; L(Test): 0.2954587936401367\n",
            "Epoch 9121/10000: L(Train): 0.3188384473323822; L(Test): 0.29548758268356323\n",
            "Epoch 9122/10000: L(Train): 0.3136823773384094; L(Test): 0.29581916332244873\n",
            "Epoch 9123/10000: L(Train): 0.3216455578804016; L(Test): 0.2963666617870331\n",
            "Epoch 9124/10000: L(Train): 0.3277760148048401; L(Test): 0.29574719071388245\n",
            "Epoch 9125/10000: L(Train): 0.32277119159698486; L(Test): 0.2946029603481293\n",
            "Epoch 9126/10000: L(Train): 0.32272616028785706; L(Test): 0.2953193187713623\n",
            "Epoch 9127/10000: L(Train): 0.3248257040977478; L(Test): 0.2940504252910614\n",
            "Epoch 9128/10000: L(Train): 0.31627610325813293; L(Test): 0.2929210662841797\n",
            "Epoch 9129/10000: L(Train): 0.310192734003067; L(Test): 0.2955842614173889\n",
            "Epoch 9130/10000: L(Train): 0.3174700140953064; L(Test): 0.29300743341445923\n",
            "Epoch 9131/10000: L(Train): 0.3216027617454529; L(Test): 0.29341843724250793\n",
            "Epoch 9132/10000: L(Train): 0.30740106105804443; L(Test): 0.2958109974861145\n",
            "Epoch 9133/10000: L(Train): 0.3179720640182495; L(Test): 0.29524320363998413\n",
            "Epoch 9134/10000: L(Train): 0.3167218863964081; L(Test): 0.29449334740638733\n",
            "Epoch 9135/10000: L(Train): 0.31719088554382324; L(Test): 0.2944430410861969\n",
            "Epoch 9136/10000: L(Train): 0.31860604882240295; L(Test): 0.29494208097457886\n",
            "Epoch 9137/10000: L(Train): 0.3199419677257538; L(Test): 0.2940749228000641\n",
            "Epoch 9138/10000: L(Train): 0.3278302252292633; L(Test): 0.29516950249671936\n",
            "Epoch 9139/10000: L(Train): 0.31263574957847595; L(Test): 0.295454204082489\n",
            "Epoch 9140/10000: L(Train): 0.32350048422813416; L(Test): 0.29376882314682007\n",
            "Epoch 9141/10000: L(Train): 0.31828007102012634; L(Test): 0.29497233033180237\n",
            "Epoch 9142/10000: L(Train): 0.3221963346004486; L(Test): 0.2939368784427643\n",
            "Epoch 9143/10000: L(Train): 0.3184196949005127; L(Test): 0.29308295249938965\n",
            "Epoch 9144/10000: L(Train): 0.3131730258464813; L(Test): 0.2938551902770996\n",
            "Epoch 9145/10000: L(Train): 0.32035982608795166; L(Test): 0.29396936297416687\n",
            "Epoch 9146/10000: L(Train): 0.3150187134742737; L(Test): 0.2933470606803894\n",
            "Epoch 9147/10000: L(Train): 0.32102328538894653; L(Test): 0.2937324047088623\n",
            "Epoch 9148/10000: L(Train): 0.3130277693271637; L(Test): 0.2949814200401306\n",
            "Epoch 9149/10000: L(Train): 0.3228999078273773; L(Test): 0.295007586479187\n",
            "Epoch 9150/10000: L(Train): 0.32600492238998413; L(Test): 0.2930455207824707\n",
            "Epoch 9151/10000: L(Train): 0.3167916238307953; L(Test): 0.29434293508529663\n",
            "Epoch 9152/10000: L(Train): 0.3120163083076477; L(Test): 0.2934068739414215\n",
            "Epoch 9153/10000: L(Train): 0.3162745237350464; L(Test): 0.2931574583053589\n",
            "Epoch 9154/10000: L(Train): 0.31572118401527405; L(Test): 0.2936345636844635\n",
            "Epoch 9155/10000: L(Train): 0.31526315212249756; L(Test): 0.2938309907913208\n",
            "Epoch 9156/10000: L(Train): 0.3238294720649719; L(Test): 0.29323694109916687\n",
            "Epoch 9157/10000: L(Train): 0.321235716342926; L(Test): 0.29245516657829285\n",
            "Epoch 9158/10000: L(Train): 0.32035529613494873; L(Test): 0.2934451401233673\n",
            "Epoch 9159/10000: L(Train): 0.3163873553276062; L(Test): 0.29419225454330444\n",
            "Epoch 9160/10000: L(Train): 0.32046443223953247; L(Test): 0.29331886768341064\n",
            "Epoch 9161/10000: L(Train): 0.31756317615509033; L(Test): 0.2924618124961853\n",
            "Epoch 9162/10000: L(Train): 0.3213101625442505; L(Test): 0.2942914664745331\n",
            "Epoch 9163/10000: L(Train): 0.3229599893093109; L(Test): 0.29228612780570984\n",
            "Epoch 9164/10000: L(Train): 0.31390124559402466; L(Test): 0.29298779368400574\n",
            "Epoch 9165/10000: L(Train): 0.3168579638004303; L(Test): 0.2938072979450226\n",
            "Epoch 9166/10000: L(Train): 0.3185361623764038; L(Test): 0.29273730516433716\n",
            "Epoch 9167/10000: L(Train): 0.3145201504230499; L(Test): 0.29255110025405884\n",
            "Epoch 9168/10000: L(Train): 0.32393163442611694; L(Test): 0.2934942841529846\n",
            "Epoch 9169/10000: L(Train): 0.32680273056030273; L(Test): 0.29314741492271423\n",
            "Epoch 9170/10000: L(Train): 0.3136765658855438; L(Test): 0.29339399933815\n",
            "Epoch 9171/10000: L(Train): 0.319604754447937; L(Test): 0.29413527250289917\n",
            "Epoch 9172/10000: L(Train): 0.32071176171302795; L(Test): 0.2929830849170685\n",
            "Epoch 9173/10000: L(Train): 0.3188581168651581; L(Test): 0.29305320978164673\n",
            "Epoch 9174/10000: L(Train): 0.31884482502937317; L(Test): 0.29250088334083557\n",
            "Epoch 9175/10000: L(Train): 0.31382226943969727; L(Test): 0.2928505837917328\n",
            "Epoch 9176/10000: L(Train): 0.31892335414886475; L(Test): 0.2922045886516571\n",
            "Epoch 9177/10000: L(Train): 0.31643250584602356; L(Test): 0.2917860150337219\n",
            "Epoch 9178/10000: L(Train): 0.3156384527683258; L(Test): 0.29273706674575806\n",
            "Epoch 9179/10000: L(Train): 0.31460848450660706; L(Test): 0.29335761070251465\n",
            "Epoch 9180/10000: L(Train): 0.3146713376045227; L(Test): 0.2931411862373352\n",
            "Epoch 9181/10000: L(Train): 0.30678725242614746; L(Test): 0.29297152161598206\n",
            "Epoch 9182/10000: L(Train): 0.3213488459587097; L(Test): 0.29219770431518555\n",
            "Epoch 9183/10000: L(Train): 0.3209553062915802; L(Test): 0.2920241951942444\n",
            "Epoch 9184/10000: L(Train): 0.31223824620246887; L(Test): 0.2926337718963623\n",
            "Epoch 9185/10000: L(Train): 0.3170713484287262; L(Test): 0.2930460274219513\n",
            "Epoch 9186/10000: L(Train): 0.32146769762039185; L(Test): 0.29316189885139465\n",
            "Epoch 9187/10000: L(Train): 0.32230013608932495; L(Test): 0.2942400872707367\n",
            "Epoch 9188/10000: L(Train): 0.3198421001434326; L(Test): 0.2949962317943573\n",
            "Epoch 9189/10000: L(Train): 0.3219657838344574; L(Test): 0.29677027463912964\n",
            "Epoch 9190/10000: L(Train): 0.32110798358917236; L(Test): 0.2976146638393402\n",
            "Epoch 9191/10000: L(Train): 0.31967127323150635; L(Test): 0.296430766582489\n",
            "Epoch 9192/10000: L(Train): 0.33098477125167847; L(Test): 0.2966400384902954\n",
            "Epoch 9193/10000: L(Train): 0.32905104756355286; L(Test): 0.29684191942214966\n",
            "Epoch 9194/10000: L(Train): 0.32009264826774597; L(Test): 0.29624298214912415\n",
            "Epoch 9195/10000: L(Train): 0.3202563524246216; L(Test): 0.2985832691192627\n",
            "Epoch 9196/10000: L(Train): 0.3272450566291809; L(Test): 0.2995131015777588\n",
            "Epoch 9197/10000: L(Train): 0.33415645360946655; L(Test): 0.29791519045829773\n",
            "Epoch 9198/10000: L(Train): 0.31875544786453247; L(Test): 0.29818418622016907\n",
            "Epoch 9199/10000: L(Train): 0.319854736328125; L(Test): 0.2994280755519867\n",
            "Epoch 9200/10000: L(Train): 0.32452449202537537; L(Test): 0.29954832792282104\n",
            "Epoch 9201/10000: L(Train): 0.32871294021606445; L(Test): 0.2970593273639679\n",
            "Epoch 9202/10000: L(Train): 0.3210887014865875; L(Test): 0.29604285955429077\n",
            "Epoch 9203/10000: L(Train): 0.3263300955295563; L(Test): 0.30081769824028015\n",
            "Epoch 9204/10000: L(Train): 0.33129769563674927; L(Test): 0.30054542422294617\n",
            "Epoch 9205/10000: L(Train): 0.32522115111351013; L(Test): 0.2969125211238861\n",
            "Epoch 9206/10000: L(Train): 0.32204169034957886; L(Test): 0.29667967557907104\n",
            "Epoch 9207/10000: L(Train): 0.3156147003173828; L(Test): 0.29757001996040344\n",
            "Epoch 9208/10000: L(Train): 0.32061508297920227; L(Test): 0.2973299026489258\n",
            "Epoch 9209/10000: L(Train): 0.3263988196849823; L(Test): 0.2971482276916504\n",
            "Epoch 9210/10000: L(Train): 0.3192688822746277; L(Test): 0.2975604236125946\n",
            "Epoch 9211/10000: L(Train): 0.31817904114723206; L(Test): 0.29699334502220154\n",
            "Epoch 9212/10000: L(Train): 0.3060908615589142; L(Test): 0.2967977225780487\n",
            "Epoch 9213/10000: L(Train): 0.3308126926422119; L(Test): 0.2965099513530731\n",
            "Epoch 9214/10000: L(Train): 0.31915074586868286; L(Test): 0.2961297631263733\n",
            "Epoch 9215/10000: L(Train): 0.3075602352619171; L(Test): 0.29668760299682617\n",
            "Epoch 9216/10000: L(Train): 0.32286280393600464; L(Test): 0.29601120948791504\n",
            "Epoch 9217/10000: L(Train): 0.33217698335647583; L(Test): 0.2949275076389313\n",
            "Epoch 9218/10000: L(Train): 0.31788182258605957; L(Test): 0.29485204815864563\n",
            "Epoch 9219/10000: L(Train): 0.319582998752594; L(Test): 0.29453110694885254\n",
            "Epoch 9220/10000: L(Train): 0.328481525182724; L(Test): 0.2940610945224762\n",
            "Epoch 9221/10000: L(Train): 0.3303638696670532; L(Test): 0.2936873435974121\n",
            "Epoch 9222/10000: L(Train): 0.330415278673172; L(Test): 0.2938740849494934\n",
            "Epoch 9223/10000: L(Train): 0.32214611768722534; L(Test): 0.2948261499404907\n",
            "Epoch 9224/10000: L(Train): 0.320671409368515; L(Test): 0.29509952664375305\n",
            "Epoch 9225/10000: L(Train): 0.3110765516757965; L(Test): 0.2959248721599579\n",
            "Epoch 9226/10000: L(Train): 0.3243235647678375; L(Test): 0.2964281439781189\n",
            "Epoch 9227/10000: L(Train): 0.3217221796512604; L(Test): 0.29467305541038513\n",
            "Epoch 9228/10000: L(Train): 0.32127633690834045; L(Test): 0.29409292340278625\n",
            "Epoch 9229/10000: L(Train): 0.3202683925628662; L(Test): 0.2946833074092865\n",
            "Epoch 9230/10000: L(Train): 0.3225765526294708; L(Test): 0.29343274235725403\n",
            "Epoch 9231/10000: L(Train): 0.3228154480457306; L(Test): 0.29333171248435974\n",
            "Epoch 9232/10000: L(Train): 0.3250078856945038; L(Test): 0.2937792241573334\n",
            "Epoch 9233/10000: L(Train): 0.32784032821655273; L(Test): 0.29381176829338074\n",
            "Epoch 9234/10000: L(Train): 0.32446977496147156; L(Test): 0.2939720153808594\n",
            "Epoch 9235/10000: L(Train): 0.3204934298992157; L(Test): 0.2934619188308716\n",
            "Epoch 9236/10000: L(Train): 0.31623005867004395; L(Test): 0.294992059469223\n",
            "Epoch 9237/10000: L(Train): 0.3264414668083191; L(Test): 0.29468581080436707\n",
            "Epoch 9238/10000: L(Train): 0.31691670417785645; L(Test): 0.29405638575553894\n",
            "Epoch 9239/10000: L(Train): 0.3140968382358551; L(Test): 0.29402753710746765\n",
            "Epoch 9240/10000: L(Train): 0.315121591091156; L(Test): 0.2963537573814392\n",
            "Epoch 9241/10000: L(Train): 0.3250530958175659; L(Test): 0.29556095600128174\n",
            "Epoch 9242/10000: L(Train): 0.31106480956077576; L(Test): 0.29563379287719727\n",
            "Epoch 9243/10000: L(Train): 0.3239346742630005; L(Test): 0.29565882682800293\n",
            "Epoch 9244/10000: L(Train): 0.3221017122268677; L(Test): 0.296322762966156\n",
            "Epoch 9245/10000: L(Train): 0.32881951332092285; L(Test): 0.2961407005786896\n",
            "Epoch 9246/10000: L(Train): 0.3181726932525635; L(Test): 0.29534053802490234\n",
            "Epoch 9247/10000: L(Train): 0.323318749666214; L(Test): 0.29547247290611267\n",
            "Epoch 9248/10000: L(Train): 0.3254339098930359; L(Test): 0.2956218421459198\n",
            "Epoch 9249/10000: L(Train): 0.3200373351573944; L(Test): 0.29597195982933044\n",
            "Epoch 9250/10000: L(Train): 0.31655779480934143; L(Test): 0.2958601415157318\n",
            "Epoch 9251/10000: L(Train): 0.31474772095680237; L(Test): 0.29609501361846924\n",
            "Epoch 9252/10000: L(Train): 0.3100146949291229; L(Test): 0.2963687479496002\n",
            "Epoch 9253/10000: L(Train): 0.336087703704834; L(Test): 0.29582884907722473\n",
            "Epoch 9254/10000: L(Train): 0.31701600551605225; L(Test): 0.29649481177330017\n",
            "Epoch 9255/10000: L(Train): 0.3235183656215668; L(Test): 0.2965536415576935\n",
            "Epoch 9256/10000: L(Train): 0.3248199224472046; L(Test): 0.29575008153915405\n",
            "Epoch 9257/10000: L(Train): 0.32064497470855713; L(Test): 0.2950957119464874\n",
            "Epoch 9258/10000: L(Train): 0.322287917137146; L(Test): 0.29491928219795227\n",
            "Epoch 9259/10000: L(Train): 0.31854262948036194; L(Test): 0.295278400182724\n",
            "Epoch 9260/10000: L(Train): 0.33045488595962524; L(Test): 0.2944139540195465\n",
            "Epoch 9261/10000: L(Train): 0.31191474199295044; L(Test): 0.29556477069854736\n",
            "Epoch 9262/10000: L(Train): 0.31831440329551697; L(Test): 0.29613739252090454\n",
            "Epoch 9263/10000: L(Train): 0.3221006989479065; L(Test): 0.2948680818080902\n",
            "Epoch 9264/10000: L(Train): 0.32032760977745056; L(Test): 0.2946445643901825\n",
            "Epoch 9265/10000: L(Train): 0.3214620351791382; L(Test): 0.2957082986831665\n",
            "Epoch 9266/10000: L(Train): 0.3183739185333252; L(Test): 0.2938466966152191\n",
            "Epoch 9267/10000: L(Train): 0.3232978284358978; L(Test): 0.29418230056762695\n",
            "Epoch 9268/10000: L(Train): 0.31692367792129517; L(Test): 0.29721707105636597\n",
            "Epoch 9269/10000: L(Train): 0.3277324140071869; L(Test): 0.29659754037857056\n",
            "Epoch 9270/10000: L(Train): 0.3258385956287384; L(Test): 0.296556681394577\n",
            "Epoch 9271/10000: L(Train): 0.3268478214740753; L(Test): 0.2977646589279175\n",
            "Epoch 9272/10000: L(Train): 0.3175792694091797; L(Test): 0.29431653022766113\n",
            "Epoch 9273/10000: L(Train): 0.3180326521396637; L(Test): 0.2961054742336273\n",
            "Epoch 9274/10000: L(Train): 0.3213586211204529; L(Test): 0.2971351146697998\n",
            "Epoch 9275/10000: L(Train): 0.3107253909111023; L(Test): 0.2952001690864563\n",
            "Epoch 9276/10000: L(Train): 0.32313641905784607; L(Test): 0.2955295145511627\n",
            "Epoch 9277/10000: L(Train): 0.3202582895755768; L(Test): 0.29731038212776184\n",
            "Epoch 9278/10000: L(Train): 0.3256789445877075; L(Test): 0.2952134311199188\n",
            "Epoch 9279/10000: L(Train): 0.31881237030029297; L(Test): 0.29658272862434387\n",
            "Epoch 9280/10000: L(Train): 0.32267439365386963; L(Test): 0.2974534034729004\n",
            "Epoch 9281/10000: L(Train): 0.3160255551338196; L(Test): 0.29678839445114136\n",
            "Epoch 9282/10000: L(Train): 0.33005836606025696; L(Test): 0.2986587584018707\n",
            "Epoch 9283/10000: L(Train): 0.3307802677154541; L(Test): 0.2952924370765686\n",
            "Epoch 9284/10000: L(Train): 0.323349267244339; L(Test): 0.2963787615299225\n",
            "Epoch 9285/10000: L(Train): 0.3119940161705017; L(Test): 0.2967391610145569\n",
            "Epoch 9286/10000: L(Train): 0.3228488564491272; L(Test): 0.2965203523635864\n",
            "Epoch 9287/10000: L(Train): 0.3240649402141571; L(Test): 0.29868292808532715\n",
            "Epoch 9288/10000: L(Train): 0.3293406069278717; L(Test): 0.2986820340156555\n",
            "Epoch 9289/10000: L(Train): 0.3218526244163513; L(Test): 0.2971329391002655\n",
            "Epoch 9290/10000: L(Train): 0.3103160560131073; L(Test): 0.30060112476348877\n",
            "Epoch 9291/10000: L(Train): 0.31864404678344727; L(Test): 0.30048462748527527\n",
            "Epoch 9292/10000: L(Train): 0.3246382176876068; L(Test): 0.29545384645462036\n",
            "Epoch 9293/10000: L(Train): 0.3209488093852997; L(Test): 0.2965600788593292\n",
            "Epoch 9294/10000: L(Train): 0.3324269652366638; L(Test): 0.29779052734375\n",
            "Epoch 9295/10000: L(Train): 0.3181450664997101; L(Test): 0.2978105843067169\n",
            "Epoch 9296/10000: L(Train): 0.3243008553981781; L(Test): 0.29704749584198\n",
            "Epoch 9297/10000: L(Train): 0.32190561294555664; L(Test): 0.29580968618392944\n",
            "Epoch 9298/10000: L(Train): 0.3145436644554138; L(Test): 0.29620516300201416\n",
            "Epoch 9299/10000: L(Train): 0.32965633273124695; L(Test): 0.2973397672176361\n",
            "Epoch 9300/10000: L(Train): 0.324211061000824; L(Test): 0.2961810529232025\n",
            "Epoch 9301/10000: L(Train): 0.321908175945282; L(Test): 0.2952250838279724\n",
            "Epoch 9302/10000: L(Train): 0.32679617404937744; L(Test): 0.29508352279663086\n",
            "Epoch 9303/10000: L(Train): 0.3172634541988373; L(Test): 0.295545756816864\n",
            "Epoch 9304/10000: L(Train): 0.31709718704223633; L(Test): 0.2956054210662842\n",
            "Epoch 9305/10000: L(Train): 0.32584086060523987; L(Test): 0.29784446954727173\n",
            "Epoch 9306/10000: L(Train): 0.32420748472213745; L(Test): 0.295612633228302\n",
            "Epoch 9307/10000: L(Train): 0.31690239906311035; L(Test): 0.29790860414505005\n",
            "Epoch 9308/10000: L(Train): 0.3284732401371002; L(Test): 0.29799777269363403\n",
            "Epoch 9309/10000: L(Train): 0.31360018253326416; L(Test): 0.29687243700027466\n",
            "Epoch 9310/10000: L(Train): 0.3270546793937683; L(Test): 0.29671424627304077\n",
            "Epoch 9311/10000: L(Train): 0.3291146159172058; L(Test): 0.2977931499481201\n",
            "Epoch 9312/10000: L(Train): 0.32329061627388; L(Test): 0.2965777516365051\n",
            "Epoch 9313/10000: L(Train): 0.3190382421016693; L(Test): 0.29687902331352234\n",
            "Epoch 9314/10000: L(Train): 0.3236173391342163; L(Test): 0.29875972867012024\n",
            "Epoch 9315/10000: L(Train): 0.3270910382270813; L(Test): 0.29858455061912537\n",
            "Epoch 9316/10000: L(Train): 0.31704211235046387; L(Test): 0.29731491208076477\n",
            "Epoch 9317/10000: L(Train): 0.3221834897994995; L(Test): 0.2969740927219391\n",
            "Epoch 9318/10000: L(Train): 0.31372398138046265; L(Test): 0.29693329334259033\n",
            "Epoch 9319/10000: L(Train): 0.32937386631965637; L(Test): 0.29619911313056946\n",
            "Epoch 9320/10000: L(Train): 0.32440492510795593; L(Test): 0.2958375811576843\n",
            "Epoch 9321/10000: L(Train): 0.3259464502334595; L(Test): 0.29663047194480896\n",
            "Epoch 9322/10000: L(Train): 0.3207155168056488; L(Test): 0.29612112045288086\n",
            "Epoch 9323/10000: L(Train): 0.3134034276008606; L(Test): 0.29577183723449707\n",
            "Epoch 9324/10000: L(Train): 0.3269508481025696; L(Test): 0.29575276374816895\n",
            "Epoch 9325/10000: L(Train): 0.31430742144584656; L(Test): 0.29475536942481995\n",
            "Epoch 9326/10000: L(Train): 0.3209616243839264; L(Test): 0.29429370164871216\n",
            "Epoch 9327/10000: L(Train): 0.3217146694660187; L(Test): 0.29511263966560364\n",
            "Epoch 9328/10000: L(Train): 0.32452183961868286; L(Test): 0.29473090171813965\n",
            "Epoch 9329/10000: L(Train): 0.327987939119339; L(Test): 0.2933487296104431\n",
            "Epoch 9330/10000: L(Train): 0.31761378049850464; L(Test): 0.2926485538482666\n",
            "Epoch 9331/10000: L(Train): 0.31550735235214233; L(Test): 0.29278838634490967\n",
            "Epoch 9332/10000: L(Train): 0.31949806213378906; L(Test): 0.29379624128341675\n",
            "Epoch 9333/10000: L(Train): 0.32407087087631226; L(Test): 0.29397591948509216\n",
            "Epoch 9334/10000: L(Train): 0.30993756651878357; L(Test): 0.29376181960105896\n",
            "Epoch 9335/10000: L(Train): 0.3266318440437317; L(Test): 0.29311010241508484\n",
            "Epoch 9336/10000: L(Train): 0.3189324140548706; L(Test): 0.29308924078941345\n",
            "Epoch 9337/10000: L(Train): 0.30739596486091614; L(Test): 0.29326897859573364\n",
            "Epoch 9338/10000: L(Train): 0.31963402032852173; L(Test): 0.2935943007469177\n",
            "Epoch 9339/10000: L(Train): 0.3118840754032135; L(Test): 0.2940136790275574\n",
            "Epoch 9340/10000: L(Train): 0.3195704519748688; L(Test): 0.29325562715530396\n",
            "Epoch 9341/10000: L(Train): 0.31973421573638916; L(Test): 0.29402047395706177\n",
            "Epoch 9342/10000: L(Train): 0.3210034668445587; L(Test): 0.29442185163497925\n",
            "Epoch 9343/10000: L(Train): 0.32786911725997925; L(Test): 0.293002188205719\n",
            "Epoch 9344/10000: L(Train): 0.3125828206539154; L(Test): 0.2941872179508209\n",
            "Epoch 9345/10000: L(Train): 0.3195706307888031; L(Test): 0.29500123858451843\n",
            "Epoch 9346/10000: L(Train): 0.31665998697280884; L(Test): 0.2940259575843811\n",
            "Epoch 9347/10000: L(Train): 0.3233747184276581; L(Test): 0.2931048274040222\n",
            "Epoch 9348/10000: L(Train): 0.3212631344795227; L(Test): 0.2924973666667938\n",
            "Epoch 9349/10000: L(Train): 0.3183882534503937; L(Test): 0.2931038439273834\n",
            "Epoch 9350/10000: L(Train): 0.31274884939193726; L(Test): 0.29561665654182434\n",
            "Epoch 9351/10000: L(Train): 0.31354668736457825; L(Test): 0.2950342893600464\n",
            "Epoch 9352/10000: L(Train): 0.32289251685142517; L(Test): 0.2917349636554718\n",
            "Epoch 9353/10000: L(Train): 0.32388734817504883; L(Test): 0.29188665747642517\n",
            "Epoch 9354/10000: L(Train): 0.323599636554718; L(Test): 0.2937084436416626\n",
            "Epoch 9355/10000: L(Train): 0.3203737139701843; L(Test): 0.29280683398246765\n",
            "Epoch 9356/10000: L(Train): 0.31111347675323486; L(Test): 0.2927078604698181\n",
            "Epoch 9357/10000: L(Train): 0.31829527020454407; L(Test): 0.29357025027275085\n",
            "Epoch 9358/10000: L(Train): 0.31664109230041504; L(Test): 0.2930735945701599\n",
            "Epoch 9359/10000: L(Train): 0.324081152677536; L(Test): 0.2935292422771454\n",
            "Epoch 9360/10000: L(Train): 0.31590205430984497; L(Test): 0.29347336292266846\n",
            "Epoch 9361/10000: L(Train): 0.31125736236572266; L(Test): 0.2937760055065155\n",
            "Epoch 9362/10000: L(Train): 0.32679617404937744; L(Test): 0.29356175661087036\n",
            "Epoch 9363/10000: L(Train): 0.31261226534843445; L(Test): 0.292772114276886\n",
            "Epoch 9364/10000: L(Train): 0.3184751868247986; L(Test): 0.29284536838531494\n",
            "Epoch 9365/10000: L(Train): 0.3157212436199188; L(Test): 0.2918964922428131\n",
            "Epoch 9366/10000: L(Train): 0.31664928793907166; L(Test): 0.29165181517601013\n",
            "Epoch 9367/10000: L(Train): 0.3165610730648041; L(Test): 0.2923129200935364\n",
            "Epoch 9368/10000: L(Train): 0.3227290213108063; L(Test): 0.29331445693969727\n",
            "Epoch 9369/10000: L(Train): 0.3217376470565796; L(Test): 0.29317033290863037\n",
            "Epoch 9370/10000: L(Train): 0.3213347792625427; L(Test): 0.2922894060611725\n",
            "Epoch 9371/10000: L(Train): 0.31819307804107666; L(Test): 0.2920275628566742\n",
            "Epoch 9372/10000: L(Train): 0.3263404071331024; L(Test): 0.2915138006210327\n",
            "Epoch 9373/10000: L(Train): 0.31353700160980225; L(Test): 0.29059597849845886\n",
            "Epoch 9374/10000: L(Train): 0.3152536153793335; L(Test): 0.2898849844932556\n",
            "Epoch 9375/10000: L(Train): 0.3247414529323578; L(Test): 0.29089435935020447\n",
            "Epoch 9376/10000: L(Train): 0.31924721598625183; L(Test): 0.291207492351532\n",
            "Epoch 9377/10000: L(Train): 0.3256557285785675; L(Test): 0.29098397493362427\n",
            "Epoch 9378/10000: L(Train): 0.3100435733795166; L(Test): 0.29150092601776123\n",
            "Epoch 9379/10000: L(Train): 0.3217499852180481; L(Test): 0.29151487350463867\n",
            "Epoch 9380/10000: L(Train): 0.31969133019447327; L(Test): 0.29063090682029724\n",
            "Epoch 9381/10000: L(Train): 0.31872496008872986; L(Test): 0.2901860177516937\n",
            "Epoch 9382/10000: L(Train): 0.3148549795150757; L(Test): 0.29156771302223206\n",
            "Epoch 9383/10000: L(Train): 0.3117654323577881; L(Test): 0.29147207736968994\n",
            "Epoch 9384/10000: L(Train): 0.3199708163738251; L(Test): 0.2911541759967804\n",
            "Epoch 9385/10000: L(Train): 0.3258746564388275; L(Test): 0.29151591658592224\n",
            "Epoch 9386/10000: L(Train): 0.32123252749443054; L(Test): 0.2930561602115631\n",
            "Epoch 9387/10000: L(Train): 0.31988683342933655; L(Test): 0.2937770187854767\n",
            "Epoch 9388/10000: L(Train): 0.32126641273498535; L(Test): 0.2929013967514038\n",
            "Epoch 9389/10000: L(Train): 0.31826484203338623; L(Test): 0.29301223158836365\n",
            "Epoch 9390/10000: L(Train): 0.32470107078552246; L(Test): 0.2931002974510193\n",
            "Epoch 9391/10000: L(Train): 0.3184029161930084; L(Test): 0.2923279106616974\n",
            "Epoch 9392/10000: L(Train): 0.3174892067909241; L(Test): 0.2927158772945404\n",
            "Epoch 9393/10000: L(Train): 0.31997981667518616; L(Test): 0.29306894540786743\n",
            "Epoch 9394/10000: L(Train): 0.3256204426288605; L(Test): 0.29295578598976135\n",
            "Epoch 9395/10000: L(Train): 0.3199487030506134; L(Test): 0.29311403632164\n",
            "Epoch 9396/10000: L(Train): 0.33559107780456543; L(Test): 0.2939651608467102\n",
            "Epoch 9397/10000: L(Train): 0.3273809850215912; L(Test): 0.29398277401924133\n",
            "Epoch 9398/10000: L(Train): 0.3113030791282654; L(Test): 0.2940312623977661\n",
            "Epoch 9399/10000: L(Train): 0.31860241293907166; L(Test): 0.29431772232055664\n",
            "Epoch 9400/10000: L(Train): 0.32238978147506714; L(Test): 0.29366248846054077\n",
            "Epoch 9401/10000: L(Train): 0.3152886927127838; L(Test): 0.293908953666687\n",
            "Epoch 9402/10000: L(Train): 0.3247619569301605; L(Test): 0.2930479049682617\n",
            "Epoch 9403/10000: L(Train): 0.3141951858997345; L(Test): 0.2938995361328125\n",
            "Epoch 9404/10000: L(Train): 0.31405916810035706; L(Test): 0.29517030715942383\n",
            "Epoch 9405/10000: L(Train): 0.32285037636756897; L(Test): 0.2949148416519165\n",
            "Epoch 9406/10000: L(Train): 0.31999722123146057; L(Test): 0.29548153281211853\n",
            "Epoch 9407/10000: L(Train): 0.32380056381225586; L(Test): 0.2948456406593323\n",
            "Epoch 9408/10000: L(Train): 0.3180676996707916; L(Test): 0.2944256365299225\n",
            "Epoch 9409/10000: L(Train): 0.3089856207370758; L(Test): 0.29377418756484985\n",
            "Epoch 9410/10000: L(Train): 0.31962111592292786; L(Test): 0.29410603642463684\n",
            "Epoch 9411/10000: L(Train): 0.32015731930732727; L(Test): 0.29354721307754517\n",
            "Epoch 9412/10000: L(Train): 0.3112756311893463; L(Test): 0.293064147233963\n",
            "Epoch 9413/10000: L(Train): 0.3069263994693756; L(Test): 0.2940150201320648\n",
            "Epoch 9414/10000: L(Train): 0.3208469748497009; L(Test): 0.2937051057815552\n",
            "Epoch 9415/10000: L(Train): 0.33019518852233887; L(Test): 0.2937699556350708\n",
            "Epoch 9416/10000: L(Train): 0.32817742228507996; L(Test): 0.2922782003879547\n",
            "Epoch 9417/10000: L(Train): 0.3168811798095703; L(Test): 0.29535409808158875\n",
            "Epoch 9418/10000: L(Train): 0.3271402418613434; L(Test): 0.2960835099220276\n",
            "Epoch 9419/10000: L(Train): 0.31823599338531494; L(Test): 0.29475459456443787\n",
            "Epoch 9420/10000: L(Train): 0.3200762867927551; L(Test): 0.29710209369659424\n",
            "Epoch 9421/10000: L(Train): 0.32124048471450806; L(Test): 0.2967291474342346\n",
            "Epoch 9422/10000: L(Train): 0.3261083960533142; L(Test): 0.2956826388835907\n",
            "Epoch 9423/10000: L(Train): 0.32588130235671997; L(Test): 0.2966179847717285\n",
            "Epoch 9424/10000: L(Train): 0.31747087836265564; L(Test): 0.29607468843460083\n",
            "Epoch 9425/10000: L(Train): 0.30881690979003906; L(Test): 0.29446446895599365\n",
            "Epoch 9426/10000: L(Train): 0.31049442291259766; L(Test): 0.2957647740840912\n",
            "Epoch 9427/10000: L(Train): 0.3205971121788025; L(Test): 0.2952616214752197\n",
            "Epoch 9428/10000: L(Train): 0.32208120822906494; L(Test): 0.2945123314857483\n",
            "Epoch 9429/10000: L(Train): 0.32173800468444824; L(Test): 0.2952587306499481\n",
            "Epoch 9430/10000: L(Train): 0.32756540179252625; L(Test): 0.2961990535259247\n",
            "Epoch 9431/10000: L(Train): 0.3173361122608185; L(Test): 0.2957598865032196\n",
            "Epoch 9432/10000: L(Train): 0.32233357429504395; L(Test): 0.2951738238334656\n",
            "Epoch 9433/10000: L(Train): 0.3109447658061981; L(Test): 0.2943725883960724\n",
            "Epoch 9434/10000: L(Train): 0.3241649568080902; L(Test): 0.29470962285995483\n",
            "Epoch 9435/10000: L(Train): 0.3249867856502533; L(Test): 0.29489046335220337\n",
            "Epoch 9436/10000: L(Train): 0.32775455713272095; L(Test): 0.2947880029678345\n",
            "Epoch 9437/10000: L(Train): 0.32914650440216064; L(Test): 0.2941198945045471\n",
            "Epoch 9438/10000: L(Train): 0.3144407272338867; L(Test): 0.29391804337501526\n",
            "Epoch 9439/10000: L(Train): 0.31164708733558655; L(Test): 0.29412171244621277\n",
            "Epoch 9440/10000: L(Train): 0.31274330615997314; L(Test): 0.29349592328071594\n",
            "Epoch 9441/10000: L(Train): 0.3091106414794922; L(Test): 0.29322850704193115\n",
            "Epoch 9442/10000: L(Train): 0.3195095360279083; L(Test): 0.2941698729991913\n",
            "Epoch 9443/10000: L(Train): 0.3261123299598694; L(Test): 0.2939092814922333\n",
            "Epoch 9444/10000: L(Train): 0.3202270269393921; L(Test): 0.2939855754375458\n",
            "Epoch 9445/10000: L(Train): 0.31483596563339233; L(Test): 0.29520776867866516\n",
            "Epoch 9446/10000: L(Train): 0.3181249499320984; L(Test): 0.2947291135787964\n",
            "Epoch 9447/10000: L(Train): 0.32451656460762024; L(Test): 0.29373541474342346\n",
            "Epoch 9448/10000: L(Train): 0.32999980449676514; L(Test): 0.2932794988155365\n",
            "Epoch 9449/10000: L(Train): 0.3221849501132965; L(Test): 0.29284530878067017\n",
            "Epoch 9450/10000: L(Train): 0.3308253884315491; L(Test): 0.2938574552536011\n",
            "Epoch 9451/10000: L(Train): 0.31342512369155884; L(Test): 0.2946244776248932\n",
            "Epoch 9452/10000: L(Train): 0.32445549964904785; L(Test): 0.2929123640060425\n",
            "Epoch 9453/10000: L(Train): 0.3221575617790222; L(Test): 0.2931860685348511\n",
            "Epoch 9454/10000: L(Train): 0.3252973258495331; L(Test): 0.2933339774608612\n",
            "Epoch 9455/10000: L(Train): 0.3247855603694916; L(Test): 0.2938060462474823\n",
            "Epoch 9456/10000: L(Train): 0.30711501836776733; L(Test): 0.2965371310710907\n",
            "Epoch 9457/10000: L(Train): 0.3146840035915375; L(Test): 0.29641029238700867\n",
            "Epoch 9458/10000: L(Train): 0.32269787788391113; L(Test): 0.29370740056037903\n",
            "Epoch 9459/10000: L(Train): 0.31301048398017883; L(Test): 0.29472631216049194\n",
            "Epoch 9460/10000: L(Train): 0.31918415427207947; L(Test): 0.29283708333969116\n",
            "Epoch 9461/10000: L(Train): 0.3311494290828705; L(Test): 0.2920282483100891\n",
            "Epoch 9462/10000: L(Train): 0.31888583302497864; L(Test): 0.2934311032295227\n",
            "Epoch 9463/10000: L(Train): 0.32104983925819397; L(Test): 0.2933008372783661\n",
            "Epoch 9464/10000: L(Train): 0.3207378387451172; L(Test): 0.2935473620891571\n",
            "Epoch 9465/10000: L(Train): 0.33059635758399963; L(Test): 0.29476141929626465\n",
            "Epoch 9466/10000: L(Train): 0.3221321105957031; L(Test): 0.2922179698944092\n",
            "Epoch 9467/10000: L(Train): 0.3104557394981384; L(Test): 0.2930105924606323\n",
            "Epoch 9468/10000: L(Train): 0.3155926764011383; L(Test): 0.2950628697872162\n",
            "Epoch 9469/10000: L(Train): 0.31055009365081787; L(Test): 0.292288213968277\n",
            "Epoch 9470/10000: L(Train): 0.316118061542511; L(Test): 0.29241183400154114\n",
            "Epoch 9471/10000: L(Train): 0.32225313782691956; L(Test): 0.2924562692642212\n",
            "Epoch 9472/10000: L(Train): 0.3213713467121124; L(Test): 0.2911780774593353\n",
            "Epoch 9473/10000: L(Train): 0.3155139088630676; L(Test): 0.291828989982605\n",
            "Epoch 9474/10000: L(Train): 0.3319411873817444; L(Test): 0.2914772927761078\n",
            "Epoch 9475/10000: L(Train): 0.3192810118198395; L(Test): 0.29013538360595703\n",
            "Epoch 9476/10000: L(Train): 0.3127675950527191; L(Test): 0.2910742163658142\n",
            "Epoch 9477/10000: L(Train): 0.3078286051750183; L(Test): 0.2912449836730957\n",
            "Epoch 9478/10000: L(Train): 0.3225034475326538; L(Test): 0.2897148132324219\n",
            "Epoch 9479/10000: L(Train): 0.31360214948654175; L(Test): 0.2912443280220032\n",
            "Epoch 9480/10000: L(Train): 0.3153640925884247; L(Test): 0.2921450436115265\n",
            "Epoch 9481/10000: L(Train): 0.31662094593048096; L(Test): 0.29049021005630493\n",
            "Epoch 9482/10000: L(Train): 0.3201102018356323; L(Test): 0.291500061750412\n",
            "Epoch 9483/10000: L(Train): 0.3141571581363678; L(Test): 0.29390451312065125\n",
            "Epoch 9484/10000: L(Train): 0.31459149718284607; L(Test): 0.2931140661239624\n",
            "Epoch 9485/10000: L(Train): 0.3190113306045532; L(Test): 0.2960089445114136\n",
            "Epoch 9486/10000: L(Train): 0.31849709153175354; L(Test): 0.3023926615715027\n",
            "Epoch 9487/10000: L(Train): 0.328737735748291; L(Test): 0.29656466841697693\n",
            "Epoch 9488/10000: L(Train): 0.32253220677375793; L(Test): 0.2957513928413391\n",
            "Epoch 9489/10000: L(Train): 0.32298940420150757; L(Test): 0.2998908460140228\n",
            "Epoch 9490/10000: L(Train): 0.3181387484073639; L(Test): 0.29866355657577515\n",
            "Epoch 9491/10000: L(Train): 0.3287051320075989; L(Test): 0.29684722423553467\n",
            "Epoch 9492/10000: L(Train): 0.32353684306144714; L(Test): 0.29942283034324646\n",
            "Epoch 9493/10000: L(Train): 0.32537147402763367; L(Test): 0.2985379695892334\n",
            "Epoch 9494/10000: L(Train): 0.3279553949832916; L(Test): 0.2959858775138855\n",
            "Epoch 9495/10000: L(Train): 0.3186604678630829; L(Test): 0.2973305583000183\n",
            "Epoch 9496/10000: L(Train): 0.33009764552116394; L(Test): 0.297838419675827\n",
            "Epoch 9497/10000: L(Train): 0.32357093691825867; L(Test): 0.297981858253479\n",
            "Epoch 9498/10000: L(Train): 0.3183801770210266; L(Test): 0.2991265654563904\n",
            "Epoch 9499/10000: L(Train): 0.327579140663147; L(Test): 0.2985229790210724\n",
            "Epoch 9500/10000: L(Train): 0.32576635479927063; L(Test): 0.2961316406726837\n",
            "Epoch 9501/10000: L(Train): 0.32607293128967285; L(Test): 0.2963527739048004\n",
            "Epoch 9502/10000: L(Train): 0.3136937916278839; L(Test): 0.29725903272628784\n",
            "Epoch 9503/10000: L(Train): 0.31336772441864014; L(Test): 0.29540520906448364\n",
            "Epoch 9504/10000: L(Train): 0.3200913071632385; L(Test): 0.29478919506073\n",
            "Epoch 9505/10000: L(Train): 0.3184494972229004; L(Test): 0.29694435000419617\n",
            "Epoch 9506/10000: L(Train): 0.31817060708999634; L(Test): 0.2965909242630005\n",
            "Epoch 9507/10000: L(Train): 0.32267892360687256; L(Test): 0.29531627893447876\n",
            "Epoch 9508/10000: L(Train): 0.3137361705303192; L(Test): 0.29398030042648315\n",
            "Epoch 9509/10000: L(Train): 0.3286805748939514; L(Test): 0.2939208447933197\n",
            "Epoch 9510/10000: L(Train): 0.32022419571876526; L(Test): 0.2940880358219147\n",
            "Epoch 9511/10000: L(Train): 0.3165493905544281; L(Test): 0.29304707050323486\n",
            "Epoch 9512/10000: L(Train): 0.3188842833042145; L(Test): 0.29210221767425537\n",
            "Epoch 9513/10000: L(Train): 0.3176455795764923; L(Test): 0.2926427125930786\n",
            "Epoch 9514/10000: L(Train): 0.312878280878067; L(Test): 0.2925178110599518\n",
            "Epoch 9515/10000: L(Train): 0.3277739882469177; L(Test): 0.29321905970573425\n",
            "Epoch 9516/10000: L(Train): 0.32143738865852356; L(Test): 0.293660968542099\n",
            "Epoch 9517/10000: L(Train): 0.32455816864967346; L(Test): 0.29325366020202637\n",
            "Epoch 9518/10000: L(Train): 0.3255488872528076; L(Test): 0.29252511262893677\n",
            "Epoch 9519/10000: L(Train): 0.31760266423225403; L(Test): 0.29308441281318665\n",
            "Epoch 9520/10000: L(Train): 0.3155028820037842; L(Test): 0.2938691973686218\n",
            "Epoch 9521/10000: L(Train): 0.3114764988422394; L(Test): 0.29318737983703613\n",
            "Epoch 9522/10000: L(Train): 0.32226771116256714; L(Test): 0.29224929213523865\n",
            "Epoch 9523/10000: L(Train): 0.31497853994369507; L(Test): 0.2929283380508423\n",
            "Epoch 9524/10000: L(Train): 0.3218920826911926; L(Test): 0.29359179735183716\n",
            "Epoch 9525/10000: L(Train): 0.3140610456466675; L(Test): 0.2922835648059845\n",
            "Epoch 9526/10000: L(Train): 0.3210027813911438; L(Test): 0.2921719253063202\n",
            "Epoch 9527/10000: L(Train): 0.30891337990760803; L(Test): 0.29327651858329773\n",
            "Epoch 9528/10000: L(Train): 0.31578710675239563; L(Test): 0.2929777503013611\n",
            "Epoch 9529/10000: L(Train): 0.31095772981643677; L(Test): 0.29198986291885376\n",
            "Epoch 9530/10000: L(Train): 0.31183362007141113; L(Test): 0.2934233248233795\n",
            "Epoch 9531/10000: L(Train): 0.3283599615097046; L(Test): 0.29243209958076477\n",
            "Epoch 9532/10000: L(Train): 0.31586727499961853; L(Test): 0.2906292974948883\n",
            "Epoch 9533/10000: L(Train): 0.31669163703918457; L(Test): 0.2918702960014343\n",
            "Epoch 9534/10000: L(Train): 0.3179442286491394; L(Test): 0.29308438301086426\n",
            "Epoch 9535/10000: L(Train): 0.3124404549598694; L(Test): 0.2919017970561981\n",
            "Epoch 9536/10000: L(Train): 0.3128697872161865; L(Test): 0.2928559482097626\n",
            "Epoch 9537/10000: L(Train): 0.31718289852142334; L(Test): 0.2931663990020752\n",
            "Epoch 9538/10000: L(Train): 0.3286207616329193; L(Test): 0.29277825355529785\n",
            "Epoch 9539/10000: L(Train): 0.31847691535949707; L(Test): 0.29304781556129456\n",
            "Epoch 9540/10000: L(Train): 0.31723552942276; L(Test): 0.29411420226097107\n",
            "Epoch 9541/10000: L(Train): 0.32751256227493286; L(Test): 0.29473090171813965\n",
            "Epoch 9542/10000: L(Train): 0.3316313624382019; L(Test): 0.2940048277378082\n",
            "Epoch 9543/10000: L(Train): 0.3278047740459442; L(Test): 0.2932465970516205\n",
            "Epoch 9544/10000: L(Train): 0.32079455256462097; L(Test): 0.2932263910770416\n",
            "Epoch 9545/10000: L(Train): 0.32586222887039185; L(Test): 0.2950112521648407\n",
            "Epoch 9546/10000: L(Train): 0.30882003903388977; L(Test): 0.29746294021606445\n",
            "Epoch 9547/10000: L(Train): 0.32352501153945923; L(Test): 0.2978562116622925\n",
            "Epoch 9548/10000: L(Train): 0.3284461498260498; L(Test): 0.2968240976333618\n",
            "Epoch 9549/10000: L(Train): 0.3281048536300659; L(Test): 0.2937563955783844\n",
            "Epoch 9550/10000: L(Train): 0.3159579634666443; L(Test): 0.2936539649963379\n",
            "Epoch 9551/10000: L(Train): 0.32526662945747375; L(Test): 0.2952267527580261\n",
            "Epoch 9552/10000: L(Train): 0.3291614353656769; L(Test): 0.2953270375728607\n",
            "Epoch 9553/10000: L(Train): 0.3206862807273865; L(Test): 0.29413536190986633\n",
            "Epoch 9554/10000: L(Train): 0.3156740963459015; L(Test): 0.29499271512031555\n",
            "Epoch 9555/10000: L(Train): 0.3158351480960846; L(Test): 0.296072781085968\n",
            "Epoch 9556/10000: L(Train): 0.32180145382881165; L(Test): 0.2955092489719391\n",
            "Epoch 9557/10000: L(Train): 0.3217630684375763; L(Test): 0.2939806878566742\n",
            "Epoch 9558/10000: L(Train): 0.32138580083847046; L(Test): 0.29361584782600403\n",
            "Epoch 9559/10000: L(Train): 0.3185727000236511; L(Test): 0.2938459813594818\n",
            "Epoch 9560/10000: L(Train): 0.31490558385849; L(Test): 0.29350513219833374\n",
            "Epoch 9561/10000: L(Train): 0.3271771967411041; L(Test): 0.29307234287261963\n",
            "Epoch 9562/10000: L(Train): 0.3223017156124115; L(Test): 0.2931327521800995\n",
            "Epoch 9563/10000: L(Train): 0.3088011145591736; L(Test): 0.29298341274261475\n",
            "Epoch 9564/10000: L(Train): 0.32338276505470276; L(Test): 0.296071857213974\n",
            "Epoch 9565/10000: L(Train): 0.31464314460754395; L(Test): 0.2960169315338135\n",
            "Epoch 9566/10000: L(Train): 0.31553807854652405; L(Test): 0.29537197947502136\n",
            "Epoch 9567/10000: L(Train): 0.31536051630973816; L(Test): 0.29517608880996704\n",
            "Epoch 9568/10000: L(Train): 0.3154250979423523; L(Test): 0.29545697569847107\n",
            "Epoch 9569/10000: L(Train): 0.32687708735466003; L(Test): 0.2938002943992615\n",
            "Epoch 9570/10000: L(Train): 0.32148003578186035; L(Test): 0.29356321692466736\n",
            "Epoch 9571/10000: L(Train): 0.3179907500743866; L(Test): 0.29430267214775085\n",
            "Epoch 9572/10000: L(Train): 0.32558152079582214; L(Test): 0.2946723699569702\n",
            "Epoch 9573/10000: L(Train): 0.3168027698993683; L(Test): 0.29569655656814575\n",
            "Epoch 9574/10000: L(Train): 0.32558131217956543; L(Test): 0.29503199458122253\n",
            "Epoch 9575/10000: L(Train): 0.31601452827453613; L(Test): 0.29360127449035645\n",
            "Epoch 9576/10000: L(Train): 0.32500168681144714; L(Test): 0.29362624883651733\n",
            "Epoch 9577/10000: L(Train): 0.3145263195037842; L(Test): 0.2945808470249176\n",
            "Epoch 9578/10000: L(Train): 0.31416213512420654; L(Test): 0.29439792037010193\n",
            "Epoch 9579/10000: L(Train): 0.32091546058654785; L(Test): 0.29438185691833496\n",
            "Epoch 9580/10000: L(Train): 0.318859726190567; L(Test): 0.29383373260498047\n",
            "Epoch 9581/10000: L(Train): 0.3147273659706116; L(Test): 0.29333990812301636\n",
            "Epoch 9582/10000: L(Train): 0.32395920157432556; L(Test): 0.2927737832069397\n",
            "Epoch 9583/10000: L(Train): 0.3242396116256714; L(Test): 0.29233279824256897\n",
            "Epoch 9584/10000: L(Train): 0.3116139769554138; L(Test): 0.29225850105285645\n",
            "Epoch 9585/10000: L(Train): 0.31843164563179016; L(Test): 0.29253673553466797\n",
            "Epoch 9586/10000: L(Train): 0.31011539697647095; L(Test): 0.2934538722038269\n",
            "Epoch 9587/10000: L(Train): 0.31373104453086853; L(Test): 0.29405713081359863\n",
            "Epoch 9588/10000: L(Train): 0.3140328824520111; L(Test): 0.2932502329349518\n",
            "Epoch 9589/10000: L(Train): 0.30548161268234253; L(Test): 0.2933332026004791\n",
            "Epoch 9590/10000: L(Train): 0.3205540180206299; L(Test): 0.29268500208854675\n",
            "Epoch 9591/10000: L(Train): 0.3161526918411255; L(Test): 0.2919440269470215\n",
            "Epoch 9592/10000: L(Train): 0.31204286217689514; L(Test): 0.2919064164161682\n",
            "Epoch 9593/10000: L(Train): 0.31595152616500854; L(Test): 0.293174684047699\n",
            "Epoch 9594/10000: L(Train): 0.3117322623729706; L(Test): 0.2928389310836792\n",
            "Epoch 9595/10000: L(Train): 0.3221380114555359; L(Test): 0.29217374324798584\n",
            "Epoch 9596/10000: L(Train): 0.3137645721435547; L(Test): 0.2921918034553528\n",
            "Epoch 9597/10000: L(Train): 0.3158668577671051; L(Test): 0.2915176749229431\n",
            "Epoch 9598/10000: L(Train): 0.32475703954696655; L(Test): 0.2914423644542694\n",
            "Epoch 9599/10000: L(Train): 0.31993988156318665; L(Test): 0.29213953018188477\n",
            "Epoch 9600/10000: L(Train): 0.3155618906021118; L(Test): 0.29089686274528503\n",
            "Epoch 9601/10000: L(Train): 0.30979645252227783; L(Test): 0.29178744554519653\n",
            "Epoch 9602/10000: L(Train): 0.3210467994213104; L(Test): 0.29085609316825867\n",
            "Epoch 9603/10000: L(Train): 0.316482275724411; L(Test): 0.2913751006126404\n",
            "Epoch 9604/10000: L(Train): 0.3269411325454712; L(Test): 0.29338201880455017\n",
            "Epoch 9605/10000: L(Train): 0.3211686313152313; L(Test): 0.29020270705223083\n",
            "Epoch 9606/10000: L(Train): 0.32085373997688293; L(Test): 0.29230186343193054\n",
            "Epoch 9607/10000: L(Train): 0.31707820296287537; L(Test): 0.2907191216945648\n",
            "Epoch 9608/10000: L(Train): 0.3162536025047302; L(Test): 0.2921382486820221\n",
            "Epoch 9609/10000: L(Train): 0.319793701171875; L(Test): 0.2946499288082123\n",
            "Epoch 9610/10000: L(Train): 0.32185935974121094; L(Test): 0.29235029220581055\n",
            "Epoch 9611/10000: L(Train): 0.32709941267967224; L(Test): 0.2911670506000519\n",
            "Epoch 9612/10000: L(Train): 0.31510114669799805; L(Test): 0.29175540804862976\n",
            "Epoch 9613/10000: L(Train): 0.3172222077846527; L(Test): 0.2908787429332733\n",
            "Epoch 9614/10000: L(Train): 0.3174203932285309; L(Test): 0.2911769449710846\n",
            "Epoch 9615/10000: L(Train): 0.3153725266456604; L(Test): 0.2929706871509552\n",
            "Epoch 9616/10000: L(Train): 0.323864221572876; L(Test): 0.29447513818740845\n",
            "Epoch 9617/10000: L(Train): 0.31559044122695923; L(Test): 0.2933189868927002\n",
            "Epoch 9618/10000: L(Train): 0.3276480436325073; L(Test): 0.29442083835601807\n",
            "Epoch 9619/10000: L(Train): 0.31620049476623535; L(Test): 0.2951486110687256\n",
            "Epoch 9620/10000: L(Train): 0.32333046197891235; L(Test): 0.29383745789527893\n",
            "Epoch 9621/10000: L(Train): 0.32846686244010925; L(Test): 0.29391372203826904\n",
            "Epoch 9622/10000: L(Train): 0.31974294781684875; L(Test): 0.2944285273551941\n",
            "Epoch 9623/10000: L(Train): 0.32615408301353455; L(Test): 0.2944456934928894\n",
            "Epoch 9624/10000: L(Train): 0.31261879205703735; L(Test): 0.29472842812538147\n",
            "Epoch 9625/10000: L(Train): 0.32601580023765564; L(Test): 0.29399555921554565\n",
            "Epoch 9626/10000: L(Train): 0.30761969089508057; L(Test): 0.29384371638298035\n",
            "Epoch 9627/10000: L(Train): 0.31709885597229004; L(Test): 0.2944176495075226\n",
            "Epoch 9628/10000: L(Train): 0.31216201186180115; L(Test): 0.2946814298629761\n",
            "Epoch 9629/10000: L(Train): 0.3202550411224365; L(Test): 0.29456961154937744\n",
            "Epoch 9630/10000: L(Train): 0.3212772607803345; L(Test): 0.2941925525665283\n",
            "Epoch 9631/10000: L(Train): 0.3159570097923279; L(Test): 0.29457131028175354\n",
            "Epoch 9632/10000: L(Train): 0.31533104181289673; L(Test): 0.29398640990257263\n",
            "Epoch 9633/10000: L(Train): 0.31807941198349; L(Test): 0.29384845495224\n",
            "Epoch 9634/10000: L(Train): 0.31545233726501465; L(Test): 0.2946031391620636\n",
            "Epoch 9635/10000: L(Train): 0.3200606107711792; L(Test): 0.2950746715068817\n",
            "Epoch 9636/10000: L(Train): 0.3167140781879425; L(Test): 0.29398244619369507\n",
            "Epoch 9637/10000: L(Train): 0.31995561718940735; L(Test): 0.2924797534942627\n",
            "Epoch 9638/10000: L(Train): 0.31227347254753113; L(Test): 0.2944325804710388\n",
            "Epoch 9639/10000: L(Train): 0.3103790879249573; L(Test): 0.2942112982273102\n",
            "Epoch 9640/10000: L(Train): 0.3215007483959198; L(Test): 0.29310131072998047\n",
            "Epoch 9641/10000: L(Train): 0.31086838245391846; L(Test): 0.2929549217224121\n",
            "Epoch 9642/10000: L(Train): 0.32318398356437683; L(Test): 0.2932611107826233\n",
            "Epoch 9643/10000: L(Train): 0.32180413603782654; L(Test): 0.29261520504951477\n",
            "Epoch 9644/10000: L(Train): 0.31595513224601746; L(Test): 0.29207944869995117\n",
            "Epoch 9645/10000: L(Train): 0.3132857382297516; L(Test): 0.2940829396247864\n",
            "Epoch 9646/10000: L(Train): 0.3199211657047272; L(Test): 0.2930806577205658\n",
            "Epoch 9647/10000: L(Train): 0.3084096312522888; L(Test): 0.29261261224746704\n",
            "Epoch 9648/10000: L(Train): 0.3186877369880676; L(Test): 0.29223474860191345\n",
            "Epoch 9649/10000: L(Train): 0.31877782940864563; L(Test): 0.29298651218414307\n",
            "Epoch 9650/10000: L(Train): 0.3266102969646454; L(Test): 0.29488304257392883\n",
            "Epoch 9651/10000: L(Train): 0.32520240545272827; L(Test): 0.2937723398208618\n",
            "Epoch 9652/10000: L(Train): 0.3191413879394531; L(Test): 0.29296791553497314\n",
            "Epoch 9653/10000: L(Train): 0.3065139055252075; L(Test): 0.2929452657699585\n",
            "Epoch 9654/10000: L(Train): 0.32840657234191895; L(Test): 0.29533421993255615\n",
            "Epoch 9655/10000: L(Train): 0.3271501660346985; L(Test): 0.29363611340522766\n",
            "Epoch 9656/10000: L(Train): 0.32339775562286377; L(Test): 0.29418644309043884\n",
            "Epoch 9657/10000: L(Train): 0.3218957483768463; L(Test): 0.29459479451179504\n",
            "Epoch 9658/10000: L(Train): 0.32046350836753845; L(Test): 0.29362064599990845\n",
            "Epoch 9659/10000: L(Train): 0.32201504707336426; L(Test): 0.29307836294174194\n",
            "Epoch 9660/10000: L(Train): 0.3158855736255646; L(Test): 0.29402804374694824\n",
            "Epoch 9661/10000: L(Train): 0.3227841854095459; L(Test): 0.2948906421661377\n",
            "Epoch 9662/10000: L(Train): 0.3209289312362671; L(Test): 0.29354923963546753\n",
            "Epoch 9663/10000: L(Train): 0.32135817408561707; L(Test): 0.2927054762840271\n",
            "Epoch 9664/10000: L(Train): 0.3120696544647217; L(Test): 0.2942805886268616\n",
            "Epoch 9665/10000: L(Train): 0.3159216046333313; L(Test): 0.2966024577617645\n",
            "Epoch 9666/10000: L(Train): 0.32644689083099365; L(Test): 0.2973954975605011\n",
            "Epoch 9667/10000: L(Train): 0.32856494188308716; L(Test): 0.2975524663925171\n",
            "Epoch 9668/10000: L(Train): 0.32305872440338135; L(Test): 0.29637593030929565\n",
            "Epoch 9669/10000: L(Train): 0.32238876819610596; L(Test): 0.2944890558719635\n",
            "Epoch 9670/10000: L(Train): 0.3167458474636078; L(Test): 0.29434311389923096\n",
            "Epoch 9671/10000: L(Train): 0.31308722496032715; L(Test): 0.29451242089271545\n",
            "Epoch 9672/10000: L(Train): 0.3172666132450104; L(Test): 0.2936633825302124\n",
            "Epoch 9673/10000: L(Train): 0.32027667760849; L(Test): 0.29326876997947693\n",
            "Epoch 9674/10000: L(Train): 0.31174221634864807; L(Test): 0.2929549813270569\n",
            "Epoch 9675/10000: L(Train): 0.31996649503707886; L(Test): 0.29369810223579407\n",
            "Epoch 9676/10000: L(Train): 0.31521010398864746; L(Test): 0.29424798488616943\n",
            "Epoch 9677/10000: L(Train): 0.31021547317504883; L(Test): 0.29391762614250183\n",
            "Epoch 9678/10000: L(Train): 0.3152045011520386; L(Test): 0.29390862584114075\n",
            "Epoch 9679/10000: L(Train): 0.31366315484046936; L(Test): 0.2930973470211029\n",
            "Epoch 9680/10000: L(Train): 0.32312828302383423; L(Test): 0.29249417781829834\n",
            "Epoch 9681/10000: L(Train): 0.31661391258239746; L(Test): 0.2925654947757721\n",
            "Epoch 9682/10000: L(Train): 0.3135359287261963; L(Test): 0.29126065969467163\n",
            "Epoch 9683/10000: L(Train): 0.313450425863266; L(Test): 0.2904542088508606\n",
            "Epoch 9684/10000: L(Train): 0.3169606328010559; L(Test): 0.2927265167236328\n",
            "Epoch 9685/10000: L(Train): 0.31516313552856445; L(Test): 0.29376116394996643\n",
            "Epoch 9686/10000: L(Train): 0.3201696574687958; L(Test): 0.2920510172843933\n",
            "Epoch 9687/10000: L(Train): 0.32028642296791077; L(Test): 0.29167863726615906\n",
            "Epoch 9688/10000: L(Train): 0.3215760588645935; L(Test): 0.2920704782009125\n",
            "Epoch 9689/10000: L(Train): 0.31202927231788635; L(Test): 0.29187867045402527\n",
            "Epoch 9690/10000: L(Train): 0.3239400386810303; L(Test): 0.29201966524124146\n",
            "Epoch 9691/10000: L(Train): 0.31753990054130554; L(Test): 0.29226550459861755\n",
            "Epoch 9692/10000: L(Train): 0.3281118869781494; L(Test): 0.29181841015815735\n",
            "Epoch 9693/10000: L(Train): 0.31965118646621704; L(Test): 0.2916892468929291\n",
            "Epoch 9694/10000: L(Train): 0.31617966294288635; L(Test): 0.29136213660240173\n",
            "Epoch 9695/10000: L(Train): 0.31691545248031616; L(Test): 0.2919802963733673\n",
            "Epoch 9696/10000: L(Train): 0.32645848393440247; L(Test): 0.29299870133399963\n",
            "Epoch 9697/10000: L(Train): 0.31748202443122864; L(Test): 0.2924574613571167\n",
            "Epoch 9698/10000: L(Train): 0.3257594406604767; L(Test): 0.2911547124385834\n",
            "Epoch 9699/10000: L(Train): 0.31586527824401855; L(Test): 0.29188522696495056\n",
            "Epoch 9700/10000: L(Train): 0.3111228048801422; L(Test): 0.2923716902732849\n",
            "Epoch 9701/10000: L(Train): 0.3264201879501343; L(Test): 0.29184746742248535\n",
            "Epoch 9702/10000: L(Train): 0.32307305932044983; L(Test): 0.29388120770454407\n",
            "Epoch 9703/10000: L(Train): 0.3068733215332031; L(Test): 0.2953513264656067\n",
            "Epoch 9704/10000: L(Train): 0.31632888317108154; L(Test): 0.29478543996810913\n",
            "Epoch 9705/10000: L(Train): 0.31195268034935; L(Test): 0.2940593659877777\n",
            "Epoch 9706/10000: L(Train): 0.3114083707332611; L(Test): 0.29519420862197876\n",
            "Epoch 9707/10000: L(Train): 0.3155573010444641; L(Test): 0.2967056334018707\n",
            "Epoch 9708/10000: L(Train): 0.323826402425766; L(Test): 0.29784825444221497\n",
            "Epoch 9709/10000: L(Train): 0.33315157890319824; L(Test): 0.29507631063461304\n",
            "Epoch 9710/10000: L(Train): 0.31697535514831543; L(Test): 0.29654940962791443\n",
            "Epoch 9711/10000: L(Train): 0.32971349358558655; L(Test): 0.2994724214076996\n",
            "Epoch 9712/10000: L(Train): 0.3253997266292572; L(Test): 0.2986961305141449\n",
            "Epoch 9713/10000: L(Train): 0.33072012662887573; L(Test): 0.2968384027481079\n",
            "Epoch 9714/10000: L(Train): 0.3239402174949646; L(Test): 0.2975730001926422\n",
            "Epoch 9715/10000: L(Train): 0.3135710656642914; L(Test): 0.29805800318717957\n",
            "Epoch 9716/10000: L(Train): 0.3246530294418335; L(Test): 0.2971292734146118\n",
            "Epoch 9717/10000: L(Train): 0.32968518137931824; L(Test): 0.29817724227905273\n",
            "Epoch 9718/10000: L(Train): 0.3248887062072754; L(Test): 0.2987568974494934\n",
            "Epoch 9719/10000: L(Train): 0.3135513663291931; L(Test): 0.29829680919647217\n",
            "Epoch 9720/10000: L(Train): 0.31283658742904663; L(Test): 0.29803964495658875\n",
            "Epoch 9721/10000: L(Train): 0.31928345561027527; L(Test): 0.2973659038543701\n",
            "Epoch 9722/10000: L(Train): 0.3206216096878052; L(Test): 0.2975793480873108\n",
            "Epoch 9723/10000: L(Train): 0.3123064339160919; L(Test): 0.297600120306015\n",
            "Epoch 9724/10000: L(Train): 0.31602761149406433; L(Test): 0.2972446382045746\n",
            "Epoch 9725/10000: L(Train): 0.31619200110435486; L(Test): 0.2963569164276123\n",
            "Epoch 9726/10000: L(Train): 0.3231460452079773; L(Test): 0.29587608575820923\n",
            "Epoch 9727/10000: L(Train): 0.3222138583660126; L(Test): 0.2955891191959381\n",
            "Epoch 9728/10000: L(Train): 0.33142176270484924; L(Test): 0.2984481453895569\n",
            "Epoch 9729/10000: L(Train): 0.324535071849823; L(Test): 0.2986997961997986\n",
            "Epoch 9730/10000: L(Train): 0.32109034061431885; L(Test): 0.29726642370224\n",
            "Epoch 9731/10000: L(Train): 0.3144281804561615; L(Test): 0.2970775365829468\n",
            "Epoch 9732/10000: L(Train): 0.31411805748939514; L(Test): 0.29663050174713135\n",
            "Epoch 9733/10000: L(Train): 0.3189547657966614; L(Test): 0.29500535130500793\n",
            "Epoch 9734/10000: L(Train): 0.3181266784667969; L(Test): 0.295320600271225\n",
            "Epoch 9735/10000: L(Train): 0.3211723566055298; L(Test): 0.2967381179332733\n",
            "Epoch 9736/10000: L(Train): 0.3248901665210724; L(Test): 0.29599401354789734\n",
            "Epoch 9737/10000: L(Train): 0.3286705017089844; L(Test): 0.2948973774909973\n",
            "Epoch 9738/10000: L(Train): 0.32166561484336853; L(Test): 0.29547423124313354\n",
            "Epoch 9739/10000: L(Train): 0.3167889714241028; L(Test): 0.29531925916671753\n",
            "Epoch 9740/10000: L(Train): 0.32470986247062683; L(Test): 0.2947019934654236\n",
            "Epoch 9741/10000: L(Train): 0.32147902250289917; L(Test): 0.296143501996994\n",
            "Epoch 9742/10000: L(Train): 0.33493077754974365; L(Test): 0.29617932438850403\n",
            "Epoch 9743/10000: L(Train): 0.31944453716278076; L(Test): 0.29511338472366333\n",
            "Epoch 9744/10000: L(Train): 0.3167615830898285; L(Test): 0.2963050603866577\n",
            "Epoch 9745/10000: L(Train): 0.32416030764579773; L(Test): 0.2958930730819702\n",
            "Epoch 9746/10000: L(Train): 0.3277488648891449; L(Test): 0.2947203516960144\n",
            "Epoch 9747/10000: L(Train): 0.33295103907585144; L(Test): 0.2954307496547699\n",
            "Epoch 9748/10000: L(Train): 0.32536911964416504; L(Test): 0.29519733786582947\n",
            "Epoch 9749/10000: L(Train): 0.320860892534256; L(Test): 0.2949378490447998\n",
            "Epoch 9750/10000: L(Train): 0.32355600595474243; L(Test): 0.2953851521015167\n",
            "Epoch 9751/10000: L(Train): 0.3223232328891754; L(Test): 0.2950967252254486\n",
            "Epoch 9752/10000: L(Train): 0.31718242168426514; L(Test): 0.2941133975982666\n",
            "Epoch 9753/10000: L(Train): 0.3140585124492645; L(Test): 0.2943628430366516\n",
            "Epoch 9754/10000: L(Train): 0.3173713684082031; L(Test): 0.2952997386455536\n",
            "Epoch 9755/10000: L(Train): 0.309982568025589; L(Test): 0.29413264989852905\n",
            "Epoch 9756/10000: L(Train): 0.30831673741340637; L(Test): 0.2943931221961975\n",
            "Epoch 9757/10000: L(Train): 0.3283023536205292; L(Test): 0.2952349781990051\n",
            "Epoch 9758/10000: L(Train): 0.31428447365760803; L(Test): 0.2939966917037964\n",
            "Epoch 9759/10000: L(Train): 0.31709325313568115; L(Test): 0.2933559715747833\n",
            "Epoch 9760/10000: L(Train): 0.3251630663871765; L(Test): 0.2943415939807892\n",
            "Epoch 9761/10000: L(Train): 0.32184523344039917; L(Test): 0.29361817240715027\n",
            "Epoch 9762/10000: L(Train): 0.3244612216949463; L(Test): 0.29425540566444397\n",
            "Epoch 9763/10000: L(Train): 0.31987252831459045; L(Test): 0.2935558259487152\n",
            "Epoch 9764/10000: L(Train): 0.3230659067630768; L(Test): 0.29563888907432556\n",
            "Epoch 9765/10000: L(Train): 0.31937283277511597; L(Test): 0.29761648178100586\n",
            "Epoch 9766/10000: L(Train): 0.32277408242225647; L(Test): 0.29667699337005615\n",
            "Epoch 9767/10000: L(Train): 0.3230287432670593; L(Test): 0.2943193316459656\n",
            "Epoch 9768/10000: L(Train): 0.3286829888820648; L(Test): 0.2942074239253998\n",
            "Epoch 9769/10000: L(Train): 0.3106439411640167; L(Test): 0.293643057346344\n",
            "Epoch 9770/10000: L(Train): 0.31683486700057983; L(Test): 0.29342788457870483\n",
            "Epoch 9771/10000: L(Train): 0.31672200560569763; L(Test): 0.29436245560646057\n",
            "Epoch 9772/10000: L(Train): 0.32588446140289307; L(Test): 0.2940692901611328\n",
            "Epoch 9773/10000: L(Train): 0.3118615746498108; L(Test): 0.2954837381839752\n",
            "Epoch 9774/10000: L(Train): 0.31827017664909363; L(Test): 0.29536351561546326\n",
            "Epoch 9775/10000: L(Train): 0.32256558537483215; L(Test): 0.2934555113315582\n",
            "Epoch 9776/10000: L(Train): 0.32098388671875; L(Test): 0.2936241924762726\n",
            "Epoch 9777/10000: L(Train): 0.31831976771354675; L(Test): 0.2943313717842102\n",
            "Epoch 9778/10000: L(Train): 0.3133755326271057; L(Test): 0.2939467430114746\n",
            "Epoch 9779/10000: L(Train): 0.3098483681678772; L(Test): 0.2926582098007202\n",
            "Epoch 9780/10000: L(Train): 0.3099619448184967; L(Test): 0.29317528009414673\n",
            "Epoch 9781/10000: L(Train): 0.31694382429122925; L(Test): 0.29480284452438354\n",
            "Epoch 9782/10000: L(Train): 0.3186914324760437; L(Test): 0.2938675880432129\n",
            "Epoch 9783/10000: L(Train): 0.3105621039867401; L(Test): 0.2932073175907135\n",
            "Epoch 9784/10000: L(Train): 0.3200412094593048; L(Test): 0.29468801617622375\n",
            "Epoch 9785/10000: L(Train): 0.32178762555122375; L(Test): 0.294086754322052\n",
            "Epoch 9786/10000: L(Train): 0.32511863112449646; L(Test): 0.29340845346450806\n",
            "Epoch 9787/10000: L(Train): 0.32264238595962524; L(Test): 0.293753445148468\n",
            "Epoch 9788/10000: L(Train): 0.31953680515289307; L(Test): 0.29361966252326965\n",
            "Epoch 9789/10000: L(Train): 0.3173975944519043; L(Test): 0.29347851872444153\n",
            "Epoch 9790/10000: L(Train): 0.31914201378822327; L(Test): 0.2926008403301239\n",
            "Epoch 9791/10000: L(Train): 0.31665122509002686; L(Test): 0.2922874987125397\n",
            "Epoch 9792/10000: L(Train): 0.3244284391403198; L(Test): 0.293073445558548\n",
            "Epoch 9793/10000: L(Train): 0.3219963014125824; L(Test): 0.2931486666202545\n",
            "Epoch 9794/10000: L(Train): 0.3101138472557068; L(Test): 0.29168248176574707\n",
            "Epoch 9795/10000: L(Train): 0.31374478340148926; L(Test): 0.2914333939552307\n",
            "Epoch 9796/10000: L(Train): 0.3230816125869751; L(Test): 0.2921121418476105\n",
            "Epoch 9797/10000: L(Train): 0.31887346506118774; L(Test): 0.2917375862598419\n",
            "Epoch 9798/10000: L(Train): 0.3188830614089966; L(Test): 0.29233846068382263\n",
            "Epoch 9799/10000: L(Train): 0.31048932671546936; L(Test): 0.29210224747657776\n",
            "Epoch 9800/10000: L(Train): 0.309557169675827; L(Test): 0.2913913130760193\n",
            "Epoch 9801/10000: L(Train): 0.30491676926612854; L(Test): 0.2910170555114746\n",
            "Epoch 9802/10000: L(Train): 0.32057517766952515; L(Test): 0.2907463610172272\n",
            "Epoch 9803/10000: L(Train): 0.31115567684173584; L(Test): 0.2912239134311676\n",
            "Epoch 9804/10000: L(Train): 0.31608447432518005; L(Test): 0.2904522120952606\n",
            "Epoch 9805/10000: L(Train): 0.31530144810676575; L(Test): 0.28986480832099915\n",
            "Epoch 9806/10000: L(Train): 0.3138791620731354; L(Test): 0.2899452745914459\n",
            "Epoch 9807/10000: L(Train): 0.31437137722969055; L(Test): 0.2901040315628052\n",
            "Epoch 9808/10000: L(Train): 0.3136240541934967; L(Test): 0.289897620677948\n",
            "Epoch 9809/10000: L(Train): 0.3129737973213196; L(Test): 0.28995606303215027\n",
            "Epoch 9810/10000: L(Train): 0.31613218784332275; L(Test): 0.2899923324584961\n",
            "Epoch 9811/10000: L(Train): 0.3162246644496918; L(Test): 0.29051458835601807\n",
            "Epoch 9812/10000: L(Train): 0.3158910870552063; L(Test): 0.29047107696533203\n",
            "Epoch 9813/10000: L(Train): 0.32034263014793396; L(Test): 0.29039299488067627\n",
            "Epoch 9814/10000: L(Train): 0.31023523211479187; L(Test): 0.29016512632369995\n",
            "Epoch 9815/10000: L(Train): 0.31543126702308655; L(Test): 0.28991371393203735\n",
            "Epoch 9816/10000: L(Train): 0.3151315748691559; L(Test): 0.29004138708114624\n",
            "Epoch 9817/10000: L(Train): 0.3141723871231079; L(Test): 0.2908574342727661\n",
            "Epoch 9818/10000: L(Train): 0.3168815076351166; L(Test): 0.2916797399520874\n",
            "Epoch 9819/10000: L(Train): 0.32439905405044556; L(Test): 0.29117053747177124\n",
            "Epoch 9820/10000: L(Train): 0.3189360499382019; L(Test): 0.2901018559932709\n",
            "Epoch 9821/10000: L(Train): 0.3181588351726532; L(Test): 0.28973957896232605\n",
            "Epoch 9822/10000: L(Train): 0.3139878809452057; L(Test): 0.29006144404411316\n",
            "Epoch 9823/10000: L(Train): 0.3078576326370239; L(Test): 0.2902643084526062\n",
            "Epoch 9824/10000: L(Train): 0.3089710474014282; L(Test): 0.29047396779060364\n",
            "Epoch 9825/10000: L(Train): 0.3188146948814392; L(Test): 0.2911899983882904\n",
            "Epoch 9826/10000: L(Train): 0.3148030936717987; L(Test): 0.2923453450202942\n",
            "Epoch 9827/10000: L(Train): 0.31221553683280945; L(Test): 0.2932452857494354\n",
            "Epoch 9828/10000: L(Train): 0.32244160771369934; L(Test): 0.2930206060409546\n",
            "Epoch 9829/10000: L(Train): 0.3169795870780945; L(Test): 0.29286810755729675\n",
            "Epoch 9830/10000: L(Train): 0.31207793951034546; L(Test): 0.29261279106140137\n",
            "Epoch 9831/10000: L(Train): 0.3127586841583252; L(Test): 0.29232850670814514\n",
            "Epoch 9832/10000: L(Train): 0.31488898396492004; L(Test): 0.2915613651275635\n",
            "Epoch 9833/10000: L(Train): 0.305655300617218; L(Test): 0.29270750284194946\n",
            "Epoch 9834/10000: L(Train): 0.3172122538089752; L(Test): 0.2924591600894928\n",
            "Epoch 9835/10000: L(Train): 0.3238299787044525; L(Test): 0.2927195429801941\n",
            "Epoch 9836/10000: L(Train): 0.32107409834861755; L(Test): 0.2920229136943817\n",
            "Epoch 9837/10000: L(Train): 0.32505515217781067; L(Test): 0.2914593517780304\n",
            "Epoch 9838/10000: L(Train): 0.3088132441043854; L(Test): 0.29131969809532166\n",
            "Epoch 9839/10000: L(Train): 0.31080877780914307; L(Test): 0.2930721640586853\n",
            "Epoch 9840/10000: L(Train): 0.31235602498054504; L(Test): 0.2939606010913849\n",
            "Epoch 9841/10000: L(Train): 0.321586549282074; L(Test): 0.2936922311782837\n",
            "Epoch 9842/10000: L(Train): 0.3162086009979248; L(Test): 0.29669690132141113\n",
            "Epoch 9843/10000: L(Train): 0.3257306218147278; L(Test): 0.2948603332042694\n",
            "Epoch 9844/10000: L(Train): 0.31811726093292236; L(Test): 0.292306125164032\n",
            "Epoch 9845/10000: L(Train): 0.31026485562324524; L(Test): 0.29301348328590393\n",
            "Epoch 9846/10000: L(Train): 0.3171654939651489; L(Test): 0.29261356592178345\n",
            "Epoch 9847/10000: L(Train): 0.31645438075065613; L(Test): 0.29294678568840027\n",
            "Epoch 9848/10000: L(Train): 0.32017090916633606; L(Test): 0.2926127016544342\n",
            "Epoch 9849/10000: L(Train): 0.3229256868362427; L(Test): 0.29179123044013977\n",
            "Epoch 9850/10000: L(Train): 0.3222144842147827; L(Test): 0.2937978506088257\n",
            "Epoch 9851/10000: L(Train): 0.3166043758392334; L(Test): 0.2925868332386017\n",
            "Epoch 9852/10000: L(Train): 0.32545268535614014; L(Test): 0.29335126280784607\n",
            "Epoch 9853/10000: L(Train): 0.31281110644340515; L(Test): 0.2955001890659332\n",
            "Epoch 9854/10000: L(Train): 0.3249724507331848; L(Test): 0.2937275469303131\n",
            "Epoch 9855/10000: L(Train): 0.3224738538265228; L(Test): 0.29250583052635193\n",
            "Epoch 9856/10000: L(Train): 0.3163473308086395; L(Test): 0.29183292388916016\n",
            "Epoch 9857/10000: L(Train): 0.3183993101119995; L(Test): 0.2908822000026703\n",
            "Epoch 9858/10000: L(Train): 0.3223780393600464; L(Test): 0.2937752902507782\n",
            "Epoch 9859/10000: L(Train): 0.31404227018356323; L(Test): 0.2921314835548401\n",
            "Epoch 9860/10000: L(Train): 0.3151429295539856; L(Test): 0.2929252088069916\n",
            "Epoch 9861/10000: L(Train): 0.3176664113998413; L(Test): 0.29366445541381836\n",
            "Epoch 9862/10000: L(Train): 0.3198968172073364; L(Test): 0.293058842420578\n",
            "Epoch 9863/10000: L(Train): 0.3266952335834503; L(Test): 0.2931820750236511\n",
            "Epoch 9864/10000: L(Train): 0.32321593165397644; L(Test): 0.293652206659317\n",
            "Epoch 9865/10000: L(Train): 0.32394224405288696; L(Test): 0.2931249737739563\n",
            "Epoch 9866/10000: L(Train): 0.31029996275901794; L(Test): 0.2926114797592163\n",
            "Epoch 9867/10000: L(Train): 0.3207133114337921; L(Test): 0.29192042350769043\n",
            "Epoch 9868/10000: L(Train): 0.31788837909698486; L(Test): 0.2914091944694519\n",
            "Epoch 9869/10000: L(Train): 0.3154021203517914; L(Test): 0.29148298501968384\n",
            "Epoch 9870/10000: L(Train): 0.31666505336761475; L(Test): 0.2917759120464325\n",
            "Epoch 9871/10000: L(Train): 0.3185087740421295; L(Test): 0.29160958528518677\n",
            "Epoch 9872/10000: L(Train): 0.32264402508735657; L(Test): 0.2917501628398895\n",
            "Epoch 9873/10000: L(Train): 0.323810338973999; L(Test): 0.29154789447784424\n",
            "Epoch 9874/10000: L(Train): 0.3117682635784149; L(Test): 0.2922373414039612\n",
            "Epoch 9875/10000: L(Train): 0.31711745262145996; L(Test): 0.29171988368034363\n",
            "Epoch 9876/10000: L(Train): 0.3139891028404236; L(Test): 0.2911736071109772\n",
            "Epoch 9877/10000: L(Train): 0.3148736357688904; L(Test): 0.2916525900363922\n",
            "Epoch 9878/10000: L(Train): 0.31581270694732666; L(Test): 0.29031243920326233\n",
            "Epoch 9879/10000: L(Train): 0.32973411679267883; L(Test): 0.2900436818599701\n",
            "Epoch 9880/10000: L(Train): 0.3112829923629761; L(Test): 0.2909904420375824\n",
            "Epoch 9881/10000: L(Train): 0.31739678978919983; L(Test): 0.2899644374847412\n",
            "Epoch 9882/10000: L(Train): 0.3112808167934418; L(Test): 0.28985974192619324\n",
            "Epoch 9883/10000: L(Train): 0.32120421528816223; L(Test): 0.2910822033882141\n",
            "Epoch 9884/10000: L(Train): 0.32152488827705383; L(Test): 0.2916760742664337\n",
            "Epoch 9885/10000: L(Train): 0.31113579869270325; L(Test): 0.2912549078464508\n",
            "Epoch 9886/10000: L(Train): 0.31628918647766113; L(Test): 0.2922017276287079\n",
            "Epoch 9887/10000: L(Train): 0.32474273443222046; L(Test): 0.2920413911342621\n",
            "Epoch 9888/10000: L(Train): 0.31086134910583496; L(Test): 0.29244181513786316\n",
            "Epoch 9889/10000: L(Train): 0.32009008526802063; L(Test): 0.2905574440956116\n",
            "Epoch 9890/10000: L(Train): 0.3183804452419281; L(Test): 0.29220902919769287\n",
            "Epoch 9891/10000: L(Train): 0.32272589206695557; L(Test): 0.2924591898918152\n",
            "Epoch 9892/10000: L(Train): 0.30794164538383484; L(Test): 0.29304444789886475\n",
            "Epoch 9893/10000: L(Train): 0.3167022168636322; L(Test): 0.2936396896839142\n",
            "Epoch 9894/10000: L(Train): 0.32209911942481995; L(Test): 0.29273656010627747\n",
            "Epoch 9895/10000: L(Train): 0.3179161846637726; L(Test): 0.2928057014942169\n",
            "Epoch 9896/10000: L(Train): 0.3167302906513214; L(Test): 0.2940591871738434\n",
            "Epoch 9897/10000: L(Train): 0.32009896636009216; L(Test): 0.2934260070323944\n",
            "Epoch 9898/10000: L(Train): 0.3219687044620514; L(Test): 0.2933708429336548\n",
            "Epoch 9899/10000: L(Train): 0.31387659907341003; L(Test): 0.293413907289505\n",
            "Epoch 9900/10000: L(Train): 0.31511154770851135; L(Test): 0.29312556982040405\n",
            "Epoch 9901/10000: L(Train): 0.31505367159843445; L(Test): 0.29400378465652466\n",
            "Epoch 9902/10000: L(Train): 0.32665181159973145; L(Test): 0.29345840215682983\n",
            "Epoch 9903/10000: L(Train): 0.31199929118156433; L(Test): 0.29202499985694885\n",
            "Epoch 9904/10000: L(Train): 0.31009140610694885; L(Test): 0.2913515269756317\n",
            "Epoch 9905/10000: L(Train): 0.32527652382850647; L(Test): 0.29205432534217834\n",
            "Epoch 9906/10000: L(Train): 0.32304683327674866; L(Test): 0.2925354242324829\n",
            "Epoch 9907/10000: L(Train): 0.3078788220882416; L(Test): 0.2915214002132416\n",
            "Epoch 9908/10000: L(Train): 0.3160034120082855; L(Test): 0.29086294770240784\n",
            "Epoch 9909/10000: L(Train): 0.3152197599411011; L(Test): 0.29173511266708374\n",
            "Epoch 9910/10000: L(Train): 0.3170510530471802; L(Test): 0.2921038568019867\n",
            "Epoch 9911/10000: L(Train): 0.32083436846733093; L(Test): 0.2926647365093231\n",
            "Epoch 9912/10000: L(Train): 0.32026153802871704; L(Test): 0.29217877984046936\n",
            "Epoch 9913/10000: L(Train): 0.32662153244018555; L(Test): 0.2936936616897583\n",
            "Epoch 9914/10000: L(Train): 0.3224126100540161; L(Test): 0.2942589223384857\n",
            "Epoch 9915/10000: L(Train): 0.3238140046596527; L(Test): 0.29397210478782654\n",
            "Epoch 9916/10000: L(Train): 0.3136439025402069; L(Test): 0.29478031396865845\n",
            "Epoch 9917/10000: L(Train): 0.3160223364830017; L(Test): 0.29446011781692505\n",
            "Epoch 9918/10000: L(Train): 0.323978990316391; L(Test): 0.2940564453601837\n",
            "Epoch 9919/10000: L(Train): 0.3227350115776062; L(Test): 0.2936699688434601\n",
            "Epoch 9920/10000: L(Train): 0.32083186507225037; L(Test): 0.29417338967323303\n",
            "Epoch 9921/10000: L(Train): 0.3287523090839386; L(Test): 0.2973930239677429\n",
            "Epoch 9922/10000: L(Train): 0.3193666636943817; L(Test): 0.2956978678703308\n",
            "Epoch 9923/10000: L(Train): 0.3246423602104187; L(Test): 0.2930959463119507\n",
            "Epoch 9924/10000: L(Train): 0.31245529651641846; L(Test): 0.296610027551651\n",
            "Epoch 9925/10000: L(Train): 0.3187475800514221; L(Test): 0.2961696982383728\n",
            "Epoch 9926/10000: L(Train): 0.3367806375026703; L(Test): 0.2960887849330902\n",
            "Epoch 9927/10000: L(Train): 0.32578563690185547; L(Test): 0.2977495491504669\n",
            "Epoch 9928/10000: L(Train): 0.3156677186489105; L(Test): 0.2936619222164154\n",
            "Epoch 9929/10000: L(Train): 0.3207539916038513; L(Test): 0.29538238048553467\n",
            "Epoch 9930/10000: L(Train): 0.3259146511554718; L(Test): 0.29810038208961487\n",
            "Epoch 9931/10000: L(Train): 0.31998732686042786; L(Test): 0.29680120944976807\n",
            "Epoch 9932/10000: L(Train): 0.31802666187286377; L(Test): 0.29718250036239624\n",
            "Epoch 9933/10000: L(Train): 0.3238448202610016; L(Test): 0.2969398498535156\n",
            "Epoch 9934/10000: L(Train): 0.32065245509147644; L(Test): 0.2962779998779297\n",
            "Epoch 9935/10000: L(Train): 0.3206998407840729; L(Test): 0.2959670424461365\n",
            "Epoch 9936/10000: L(Train): 0.318593829870224; L(Test): 0.29505419731140137\n",
            "Epoch 9937/10000: L(Train): 0.316325843334198; L(Test): 0.29456549882888794\n",
            "Epoch 9938/10000: L(Train): 0.32108038663864136; L(Test): 0.295566588640213\n",
            "Epoch 9939/10000: L(Train): 0.3214678168296814; L(Test): 0.295209139585495\n",
            "Epoch 9940/10000: L(Train): 0.31488698720932007; L(Test): 0.29430124163627625\n",
            "Epoch 9941/10000: L(Train): 0.3167702555656433; L(Test): 0.2945554256439209\n",
            "Epoch 9942/10000: L(Train): 0.3164386451244354; L(Test): 0.2944968044757843\n",
            "Epoch 9943/10000: L(Train): 0.3207792341709137; L(Test): 0.2941698729991913\n",
            "Epoch 9944/10000: L(Train): 0.3158174455165863; L(Test): 0.2945106029510498\n",
            "Epoch 9945/10000: L(Train): 0.32609036564826965; L(Test): 0.29419082403182983\n",
            "Epoch 9946/10000: L(Train): 0.318202942609787; L(Test): 0.2940560281276703\n",
            "Epoch 9947/10000: L(Train): 0.32078543305397034; L(Test): 0.2942092716693878\n",
            "Epoch 9948/10000: L(Train): 0.3170730471611023; L(Test): 0.29297709465026855\n",
            "Epoch 9949/10000: L(Train): 0.3247668445110321; L(Test): 0.29361867904663086\n",
            "Epoch 9950/10000: L(Train): 0.3091489374637604; L(Test): 0.29462993144989014\n",
            "Epoch 9951/10000: L(Train): 0.31455564498901367; L(Test): 0.29351475834846497\n",
            "Epoch 9952/10000: L(Train): 0.33145374059677124; L(Test): 0.2926866114139557\n",
            "Epoch 9953/10000: L(Train): 0.32140985131263733; L(Test): 0.29251956939697266\n",
            "Epoch 9954/10000: L(Train): 0.3168638348579407; L(Test): 0.29280006885528564\n",
            "Epoch 9955/10000: L(Train): 0.32027295231819153; L(Test): 0.2935301661491394\n",
            "Epoch 9956/10000: L(Train): 0.30567777156829834; L(Test): 0.2923424541950226\n",
            "Epoch 9957/10000: L(Train): 0.319747656583786; L(Test): 0.2917444705963135\n",
            "Epoch 9958/10000: L(Train): 0.32630276679992676; L(Test): 0.2926607131958008\n",
            "Epoch 9959/10000: L(Train): 0.31879255175590515; L(Test): 0.2914678454399109\n",
            "Epoch 9960/10000: L(Train): 0.31747615337371826; L(Test): 0.29166942834854126\n",
            "Epoch 9961/10000: L(Train): 0.3104094862937927; L(Test): 0.2934260368347168\n",
            "Epoch 9962/10000: L(Train): 0.3217296600341797; L(Test): 0.29182347655296326\n",
            "Epoch 9963/10000: L(Train): 0.3135252892971039; L(Test): 0.29312485456466675\n",
            "Epoch 9964/10000: L(Train): 0.31556272506713867; L(Test): 0.2954244911670685\n",
            "Epoch 9965/10000: L(Train): 0.31673020124435425; L(Test): 0.2921406328678131\n",
            "Epoch 9966/10000: L(Train): 0.31491848826408386; L(Test): 0.2951008081436157\n",
            "Epoch 9967/10000: L(Train): 0.3157424032688141; L(Test): 0.29347851872444153\n",
            "Epoch 9968/10000: L(Train): 0.3229581415653229; L(Test): 0.2931937575340271\n",
            "Epoch 9969/10000: L(Train): 0.32037609815597534; L(Test): 0.29453811049461365\n",
            "Epoch 9970/10000: L(Train): 0.31738045811653137; L(Test): 0.29417818784713745\n",
            "Epoch 9971/10000: L(Train): 0.3139231204986572; L(Test): 0.29383182525634766\n",
            "Epoch 9972/10000: L(Train): 0.31911155581474304; L(Test): 0.2942928075790405\n",
            "Epoch 9973/10000: L(Train): 0.31265461444854736; L(Test): 0.29343774914741516\n",
            "Epoch 9974/10000: L(Train): 0.3151649236679077; L(Test): 0.29303252696990967\n",
            "Epoch 9975/10000: L(Train): 0.32090166211128235; L(Test): 0.2924742102622986\n",
            "Epoch 9976/10000: L(Train): 0.31747502088546753; L(Test): 0.29279765486717224\n",
            "Epoch 9977/10000: L(Train): 0.32827988266944885; L(Test): 0.2934960722923279\n",
            "Epoch 9978/10000: L(Train): 0.3114814758300781; L(Test): 0.2947233021259308\n",
            "Epoch 9979/10000: L(Train): 0.3248973786830902; L(Test): 0.2933870553970337\n",
            "Epoch 9980/10000: L(Train): 0.3307386040687561; L(Test): 0.29195114970207214\n",
            "Epoch 9981/10000: L(Train): 0.32691866159439087; L(Test): 0.29190003871917725\n",
            "Epoch 9982/10000: L(Train): 0.31554991006851196; L(Test): 0.29176443815231323\n",
            "Epoch 9983/10000: L(Train): 0.3131188750267029; L(Test): 0.29384303092956543\n",
            "Epoch 9984/10000: L(Train): 0.3242301046848297; L(Test): 0.2955012023448944\n",
            "Epoch 9985/10000: L(Train): 0.3161519169807434; L(Test): 0.2927101254463196\n",
            "Epoch 9986/10000: L(Train): 0.31581932306289673; L(Test): 0.29313769936561584\n",
            "Epoch 9987/10000: L(Train): 0.325015127658844; L(Test): 0.29500284790992737\n",
            "Epoch 9988/10000: L(Train): 0.31544995307922363; L(Test): 0.29319658875465393\n",
            "Epoch 9989/10000: L(Train): 0.32301023602485657; L(Test): 0.29346510767936707\n",
            "Epoch 9990/10000: L(Train): 0.3181442618370056; L(Test): 0.293636679649353\n",
            "Epoch 9991/10000: L(Train): 0.3198549151420593; L(Test): 0.2921207547187805\n",
            "Epoch 9992/10000: L(Train): 0.31784480810165405; L(Test): 0.2918117940425873\n",
            "Epoch 9993/10000: L(Train): 0.3297138214111328; L(Test): 0.29176485538482666\n",
            "Epoch 9994/10000: L(Train): 0.3245200514793396; L(Test): 0.29254329204559326\n",
            "Epoch 9995/10000: L(Train): 0.30929240584373474; L(Test): 0.2939469516277313\n",
            "Epoch 9996/10000: L(Train): 0.3129520118236542; L(Test): 0.2933378219604492\n",
            "Epoch 9997/10000: L(Train): 0.31599190831184387; L(Test): 0.2932340204715729\n",
            "Epoch 9998/10000: L(Train): 0.31746429204940796; L(Test): 0.2913803458213806\n",
            "Epoch 9999/10000: L(Train): 0.3268718123435974; L(Test): 0.2911669909954071\n",
            "Epoch 10000/10000: L(Train): 0.3179633617401123; L(Test): 0.29166099429130554\n",
            "Trained GRU parameters saved to ../../weinhardt2025/params/ganesh2024a/gru_ganesh2024a.pkl\n"
          ]
        }
      ],
      "source": [
        "epochs = 10000\n",
        "\n",
        "gru = GRU(n_actions=n_actions, additional_inputs=1).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gru.parameters(), lr=0.01)\n",
        "\n",
        "gru = training(\n",
        "    gru=gru,\n",
        "    optimizer=optimizer,\n",
        "    dataset_train=dataset,\n",
        "    dataset_test=dataset,\n",
        "    epochs=epochs,\n",
        "    )\n",
        "\n",
        "torch.save(gru.state_dict(), path_gru)\n",
        "print(\"Trained GRU parameters saved to \" + path_gru)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "gru_agent = setup_agent_gru(path_gru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot SPICE against benchmark models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "value_reward_chosen[t+1] = -0.933 1 + 0.008 value_reward_chosen[t] + 0.07 contr_diff + 1.015 reward + 0.0 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.234 1 + 1.0 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plotting\n",
        "participant_id = 7\n",
        "\n",
        "estimator.print_spice_model(participant_id)\n",
        "\n",
        "agents = {\n",
        "    # add baseline agent here\n",
        "    'rnn': estimator.rnn_agent,\n",
        "    'spice': estimator.spice_agent,\n",
        "    'gru': gru_agent,\n",
        "}\n",
        "\n",
        "fig, axs = plot_session(agents, dataset.xs[participant_id])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "spice",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
