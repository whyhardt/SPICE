{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf7jlYw4NA0v",
        "outputId": "0969ca34-675d-422e-cbfb-7387d9bcd8ad"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/whyhardt/SPICE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oXIbg826NS5i",
        "outputId": "3825864a-cb2d-4ad5-f2e5-79a4e81dfc3e"
      },
      "outputs": [],
      "source": [
        "# !pip install -e SPICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f0uVlABYznR5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from spice.estimator import SpiceEstimator\n",
        "from spice.resources.spice_utils import SpiceConfig\n",
        "from spice.utils.convert_dataset import convert_dataset\n",
        "from spice.resources.rnn import BaseRNN\n",
        "from spice.utils.plotting import plot_session\n",
        "\n",
        "# For custom RNN\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load the data first with the `convert_dataset` method. This method returns a `SpiceDataset` object which we can use right away "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of dataset: torch.Size([1176, 24, 8])\n",
            "Number of participants: 98\n",
            "Number of actions in dataset: 2\n",
            "Number of additional inputs: 1\n"
          ]
        }
      ],
      "source": [
        "# Load your data\n",
        "dataset = convert_dataset(\n",
        "    file = '../data/ganesh2024a/ganesh2024a.csv',\n",
        "    df_participant_id='subjID',\n",
        "    df_choice='chose_high',\n",
        "    df_reward='reward',\n",
        "    df_block='blocks',\n",
        "    additional_inputs=['contrast_difference'],\n",
        "    timeshift_additional_inputs=True,\n",
        "    )\n",
        "\n",
        "# structure of dataset:\n",
        "# dataset has two main attributes: xs -> inputs; ys -> targets (next action)\n",
        "# shape: (n_participants*n_blocks*n_experiments, n_timesteps, features)\n",
        "# features are (n_actions * action, n_actions * reward, n_additional_inputs * additional_input, block_number, experiment_id, participant_id)\n",
        "\n",
        "# in order to set up the participant embedding we have to compute the number of unique participants in our data \n",
        "# to get the number of participants n_participants we do:\n",
        "n_participants = len(dataset.xs[..., -1].unique())\n",
        "\n",
        "print(f\"Shape of dataset: {dataset.xs.shape}\")\n",
        "print(f\"Number of participants: {n_participants}\")\n",
        "n_actions = dataset.ys.shape[-1]\n",
        "print(f\"Number of actions in dataset: {n_actions}\")\n",
        "print(f\"Number of additional inputs: {dataset.xs.shape[-1]-2*n_actions-3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to define the configuration for SPICE with a `SpiceConfig` object.\n",
        "\n",
        "The `SpiceConfig` takes as arguments \n",
        "1. `library_setup (dict)`: Defining the variable names of each module.\n",
        "2. `memory_state (dict)`: Defining the memory state variables and their initial values.\n",
        "3. `states_in_logit (list)`: Defining which of the memory state variables are used later for the logit computation. This is necessary for some background processes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "spice_config = SpiceConfig(\n",
        "    library_setup={\n",
        "        'value_reward_chosen': ['contr_diff', 'reward'],\n",
        "        'value_reward_not_chosen': ['contr_diff'],\n",
        "        'value_choice': ['contr_diff', 'choice'],\n",
        "    },\n",
        "    \n",
        "    memory_state={\n",
        "            'value_reward': 0.,\n",
        "            'value_choice': 0.,\n",
        "        }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we are going to define the SPICE model which is a child of the `BaseRNN` and `torch.nn.Module` class and takes as required arguments:\n",
        "1. `spice_config (SpiceConfig)`: previously defined SpiceConfig object\n",
        "2. `n_actions (int)`: number of possible actions in your dataset (including non-displayed ones if applicable).\n",
        "3. `n_participants (int)`: number of participants in your dataset.\n",
        "\n",
        "As usual for a `torch.nn.Module` we have to define at least the `__init__` method and the `forward` method.\n",
        "The `forward` method gets called when computing a forward pass through the model and takes as inputs `(inputs (SpiceDataset.xs), prev_state (dict, default: None), batch_first (bool, default: False))` and returns `(logits (torch.Tensor, shape: (n_participants*n_blocks*n_experiments, timesteps, n_actions)), updated_state (dict))`. Two necessary method calls inside the forward pass are:\n",
        "1. `self.init_forward_pass(inputs, prev_state, batch_first) -> SpiceSignals`: returns a `SpiceSignals` object which carries all relevant information already processed.\n",
        "2. `self.post_forward_pass(SpiceSignals, batch_first) -> SpiceSignals`: does some re-arranging of the logits to adhere to `batch_first`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z0kOR2Qgz0FZ"
      },
      "outputs": [],
      "source": [
        "class SPICERNN(BaseRNN):\n",
        "    \n",
        "    def __init__(self, spice_config, **kwargs):\n",
        "        super().__init__(spice_config=spice_config, **kwargs)\n",
        "        \n",
        "        # participant embedding\n",
        "        self.participant_embedding = self.setup_embedding(num_embeddings=self.n_participants, embedding_size=self.embedding_size, dropout=0.)\n",
        "        \n",
        "        # set up the submodules\n",
        "        self.setup_module(key_module='value_reward_chosen', input_size=2+self.embedding_size)\n",
        "        self.setup_module(key_module='value_reward_not_chosen', input_size=1+self.embedding_size)\n",
        "        self.setup_module(key_module='value_choice', input_size=2+self.embedding_size)\n",
        "        \n",
        "    def forward(self, inputs, prev_state, batch_first=False):\n",
        "        \n",
        "        spice_signals = self.init_forward_pass(inputs, prev_state, batch_first)\n",
        "        \n",
        "        contr_diffs = spice_signals.additional_inputs.repeat(1, 1, self.n_actions)\n",
        "        rewards_chosen = (spice_signals.actions * spice_signals.rewards).sum(dim=-1, keepdim=True).repeat(1, 1, self.n_actions)\n",
        "        \n",
        "        # time-invariant participant features\n",
        "        participant_embeddings = self.participant_embedding(spice_signals.participant_ids)\n",
        "        \n",
        "        for timestep in spice_signals.timesteps:\n",
        "            \n",
        "            # update chosen value\n",
        "            self.call_module(\n",
        "                key_module='value_reward_chosen',\n",
        "                key_state='value_reward',\n",
        "                action_mask=spice_signals.actions[timestep],\n",
        "                inputs=(contr_diffs[timestep], rewards_chosen[timestep]),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "            )\n",
        "            \n",
        "            # update not chosen value\n",
        "            self.call_module(\n",
        "                key_module='value_reward_not_chosen',\n",
        "                key_state='value_reward',\n",
        "                action_mask=1-spice_signals.actions[timestep],\n",
        "                inputs=(contr_diffs[timestep]),  # add input rewards_chosen[timestep] for counterfactual updating (adjust in config as well)\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "            )\n",
        "            \n",
        "            # same for choice values\n",
        "            self.call_module(\n",
        "                key_module='value_choice',\n",
        "                key_state='value_choice',\n",
        "                action_mask=spice_signals.actions[timestep],\n",
        "                inputs=(contr_diffs[timestep], spice_signals.actions[timestep]),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "            )\n",
        "            \n",
        "            spice_signals.logits[timestep] = self.state['value_reward'] + self.state['value_choice']\n",
        "            \n",
        "        spice_signals = self.post_forward_pass(spice_signals, batch_first)\n",
        "        \n",
        "        return spice_signals.logits, self.get_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's setup now the `SpiceEstimator` object and fit it to the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "3EnmDiUMWq6e",
        "outputId": "e53b1bbd-4173-4d2c-bcdc-15832bc31bd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training on cpu...\n",
            "================================================================================\n",
            "\n",
            "Training the RNN...\n",
            "================================================================================\n",
            "Epoch 1/1000 --- L(Train): 0.9838015 --- L(Val, RNN): 0.8348428 --- L(Val, SINDy): 0.7118579 --- Time: 0.38s; --- Convergence: 5.83e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.011 1 + 0.991 value_reward_chosen[t] + -0.008 contr_diff + 0.009 reward + 0.009 value_reward_chosen^2 + 0.007 value_reward_chosen*contr_diff + -0.009 value_reward_chosen*reward + 0.004 contr_diff^2 + -0.006 contr_diff*reward + 0.011 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.008 1 + 0.988 value_reward_not_chosen[t] + -0.001 contr_diff + 0.007 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.004 1 + 1.0 value_choice[t] + -0.001 contr_diff + 0.003 choice + 0.001 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 2/1000 --- L(Train): 0.8220631 --- L(Val, RNN): 0.7406831 --- L(Val, SINDy): 0.6996619 --- Time: 0.27s; --- Convergence: 3.38e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.019 1 + 0.982 value_reward_chosen[t] + -0.016 contr_diff + 0.019 reward + 0.003 value_reward_chosen^2 + 0.008 value_reward_chosen*contr_diff + -0.018 value_reward_chosen*reward + 0.01 contr_diff^2 + -0.014 contr_diff*reward + 0.02 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.017 1 + 0.979 value_reward_not_chosen[t] + -0.006 contr_diff + -0.0 value_reward_not_chosen^2 + -0.009 value_reward_not_chosen*contr_diff + -0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 0.998 value_choice[t] + -0.002 contr_diff + 0.001 choice + 0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 3/1000 --- L(Train): 0.7439281 --- L(Val, RNN): 0.6981753 --- L(Val, SINDy): 0.6771830 --- Time: 0.26s; --- Convergence: 1.90e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.028 1 + 0.972 value_reward_chosen[t] + -0.024 contr_diff + 0.027 reward + -0.004 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + -0.028 value_reward_chosen*reward + 0.017 contr_diff^2 + -0.022 contr_diff*reward + 0.028 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.026 1 + 0.97 value_reward_not_chosen[t] + -0.004 contr_diff + -0.008 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + -0.018 contr_diff^2 \n",
            "value_choice[t+1] = 0.007 1 + 0.995 value_choice[t] + -0.004 contr_diff + 0.006 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + -0.005 value_choice*choice + 0.001 contr_diff^2 + -0.004 contr_diff*choice + 0.006 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 4/1000 --- L(Train): 0.6965417 --- L(Val, RNN): 0.6738877 --- L(Val, SINDy): 0.6629604 --- Time: 0.28s; --- Convergence: 1.07e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.037 1 + 0.963 value_reward_chosen[t] + -0.028 contr_diff + 0.036 reward + -0.013 value_reward_chosen^2 + 0.008 value_reward_chosen*contr_diff + -0.037 value_reward_chosen*reward + 0.024 contr_diff^2 + -0.023 contr_diff*reward + 0.037 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.035 1 + 0.961 value_reward_not_chosen[t] + -0.007 contr_diff + -0.017 value_reward_not_chosen^2 + -0.009 value_reward_not_chosen*contr_diff + -0.024 contr_diff^2 \n",
            "value_choice[t+1] = 0.013 1 + 0.99 value_choice[t] + -0.008 contr_diff + 0.012 choice + -0.003 value_choice^2 + -0.001 value_choice*contr_diff + -0.01 value_choice*choice + 0.002 contr_diff^2 + -0.008 contr_diff*choice + 0.012 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 5/1000 --- L(Train): 0.6799182 --- L(Val, RNN): 0.6477502 --- L(Val, SINDy): 0.6540226 --- Time: 0.35s; --- Convergence: 6.68e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.046 1 + 0.954 value_reward_chosen[t] + -0.034 contr_diff + 0.045 reward + -0.022 value_reward_chosen^2 + 0.011 value_reward_chosen*contr_diff + -0.046 value_reward_chosen*reward + 0.03 contr_diff^2 + -0.026 contr_diff*reward + 0.046 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.044 1 + 0.951 value_reward_not_chosen[t] + -0.011 contr_diff + -0.026 value_reward_not_chosen^2 + -0.015 value_reward_not_chosen*contr_diff + -0.031 contr_diff^2 \n",
            "value_choice[t+1] = 0.014 1 + 0.984 value_choice[t] + -0.013 contr_diff + 0.014 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.016 value_choice*choice + 0.003 contr_diff^2 + -0.013 contr_diff*choice + 0.014 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 6/1000 --- L(Train): 0.6499532 --- L(Val, RNN): 0.6244000 --- L(Val, SINDy): 0.6433337 --- Time: 0.30s; --- Convergence: 4.51e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.055 1 + 0.945 value_reward_chosen[t] + -0.031 contr_diff + 0.053 reward + -0.031 value_reward_chosen^2 + 0.016 value_reward_chosen*contr_diff + -0.054 value_reward_chosen*reward + 0.038 contr_diff^2 + -0.024 contr_diff*reward + 0.055 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.053 1 + 0.942 value_reward_not_chosen[t] + -0.013 contr_diff + -0.035 value_reward_not_chosen^2 + -0.02 value_reward_not_chosen*contr_diff + -0.039 contr_diff^2 \n",
            "value_choice[t+1] = 0.017 1 + 0.977 value_choice[t] + -0.02 contr_diff + 0.016 choice + -0.014 value_choice^2 + 0.002 value_choice*contr_diff + -0.024 value_choice*choice + 0.004 contr_diff^2 + -0.019 contr_diff*choice + 0.016 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 7/1000 --- L(Train): 0.6243415 --- L(Val, RNN): 0.6057788 --- L(Val, SINDy): 0.6273864 --- Time: 0.32s; --- Convergence: 3.18e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.064 1 + 0.936 value_reward_chosen[t] + -0.026 contr_diff + 0.062 reward + -0.04 value_reward_chosen^2 + 0.023 value_reward_chosen*contr_diff + -0.061 value_reward_chosen*reward + 0.045 contr_diff^2 + -0.019 contr_diff*reward + 0.063 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.063 1 + 0.932 value_reward_not_chosen[t] + -0.017 contr_diff + -0.045 value_reward_not_chosen^2 + -0.027 value_reward_not_chosen*contr_diff + -0.047 contr_diff^2 \n",
            "value_choice[t+1] = 0.021 1 + 0.969 value_choice[t] + -0.027 contr_diff + 0.021 choice + -0.021 value_choice^2 + 0.005 value_choice*contr_diff + -0.032 value_choice*choice + 0.006 contr_diff^2 + -0.026 contr_diff*choice + 0.02 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 8/1000 --- L(Train): 0.6144574 --- L(Val, RNN): 0.5886898 --- L(Val, SINDy): 0.6064101 --- Time: 0.35s; --- Convergence: 2.45e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.073 1 + 0.927 value_reward_chosen[t] + -0.02 contr_diff + 0.071 reward + -0.05 value_reward_chosen^2 + 0.031 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.054 contr_diff^2 + -0.014 contr_diff*reward + 0.072 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.073 1 + 0.922 value_reward_not_chosen[t] + -0.018 contr_diff + -0.054 value_reward_not_chosen^2 + -0.034 value_reward_not_chosen*contr_diff + -0.056 contr_diff^2 \n",
            "value_choice[t+1] = 0.027 1 + 0.961 value_choice[t] + -0.034 contr_diff + 0.026 choice + -0.028 value_choice^2 + 0.01 value_choice*contr_diff + -0.04 value_choice*choice + 0.009 contr_diff^2 + -0.034 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 9/1000 --- L(Train): 0.5913885 --- L(Val, RNN): 0.5776061 --- L(Val, SINDy): 0.5858441 --- Time: 0.36s; --- Convergence: 1.78e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.082 1 + 0.918 value_reward_chosen[t] + -0.013 contr_diff + 0.08 reward + -0.06 value_reward_chosen^2 + 0.039 value_reward_chosen*contr_diff + -0.074 value_reward_chosen*reward + 0.063 contr_diff^2 + -0.007 contr_diff*reward + 0.082 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.082 1 + 0.913 value_reward_not_chosen[t] + -0.015 contr_diff + -0.064 value_reward_not_chosen^2 + -0.04 value_reward_not_chosen*contr_diff + -0.064 contr_diff^2 \n",
            "value_choice[t+1] = 0.032 1 + 0.952 value_choice[t] + -0.04 contr_diff + 0.032 choice + -0.036 value_choice^2 + 0.015 value_choice*contr_diff + -0.048 value_choice*choice + 0.011 contr_diff^2 + -0.04 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 10/1000 --- L(Train): 0.5733888 --- L(Val, RNN): 0.5702618 --- L(Val, SINDy): 0.5664667 --- Time: 0.40s; --- Convergence: 1.26e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.092 1 + 0.909 value_reward_chosen[t] + -0.01 contr_diff + 0.09 reward + -0.07 value_reward_chosen^2 + 0.045 value_reward_chosen*contr_diff + -0.081 value_reward_chosen*reward + 0.071 contr_diff^2 + -0.002 contr_diff*reward + 0.091 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.092 1 + 0.903 value_reward_not_chosen[t] + -0.011 contr_diff + -0.073 value_reward_not_chosen^2 + -0.043 value_reward_not_chosen*contr_diff + -0.073 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.943 value_choice[t] + -0.045 contr_diff + 0.035 choice + -0.044 value_choice^2 + 0.021 value_choice*contr_diff + -0.057 value_choice*choice + 0.013 contr_diff^2 + -0.044 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 11/1000 --- L(Train): 0.5684907 --- L(Val, RNN): 0.5640618 --- L(Val, SINDy): 0.5485656 --- Time: 0.30s; --- Convergence: 9.38e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.101 1 + 0.9 value_reward_chosen[t] + -0.005 contr_diff + 0.1 reward + -0.08 value_reward_chosen^2 + 0.051 value_reward_chosen*contr_diff + -0.086 value_reward_chosen*reward + 0.079 contr_diff^2 + 0.003 contr_diff*reward + 0.101 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.101 1 + 0.893 value_reward_not_chosen[t] + -0.005 contr_diff + -0.083 value_reward_not_chosen^2 + -0.044 value_reward_not_chosen*contr_diff + -0.081 contr_diff^2 \n",
            "value_choice[t+1] = 0.039 1 + 0.934 value_choice[t] + -0.049 contr_diff + 0.039 choice + -0.053 value_choice^2 + 0.027 value_choice*contr_diff + -0.066 value_choice*choice + 0.016 contr_diff^2 + -0.049 contr_diff*choice + 0.038 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 12/1000 --- L(Train): 0.5729112 --- L(Val, RNN): 0.5576388 --- L(Val, SINDy): 0.5337279 --- Time: 0.38s; --- Convergence: 7.90e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.111 1 + 0.891 value_reward_chosen[t] + 0.0 contr_diff + 0.11 reward + -0.09 value_reward_chosen^2 + 0.058 value_reward_chosen*contr_diff + -0.092 value_reward_chosen*reward + 0.088 contr_diff^2 + 0.009 contr_diff*reward + 0.111 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.111 1 + 0.883 value_reward_not_chosen[t] + -0.001 contr_diff + -0.093 value_reward_not_chosen^2 + -0.047 value_reward_not_chosen*contr_diff + -0.089 contr_diff^2 \n",
            "value_choice[t+1] = 0.045 1 + 0.924 value_choice[t] + -0.052 contr_diff + 0.044 choice + -0.062 value_choice^2 + 0.034 value_choice*contr_diff + -0.076 value_choice*choice + 0.019 contr_diff^2 + -0.052 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 13/1000 --- L(Train): 0.5520129 --- L(Val, RNN): 0.5530080 --- L(Val, SINDy): 0.5208219 --- Time: 0.38s; --- Convergence: 6.27e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.12 1 + 0.882 value_reward_chosen[t] + 0.004 contr_diff + 0.119 reward + -0.1 value_reward_chosen^2 + 0.066 value_reward_chosen*contr_diff + -0.097 value_reward_chosen*reward + 0.097 contr_diff^2 + 0.015 contr_diff*reward + 0.121 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.121 1 + 0.873 value_reward_not_chosen[t] + 0.005 contr_diff + -0.102 value_reward_not_chosen^2 + -0.046 value_reward_not_chosen*contr_diff + -0.098 contr_diff^2 \n",
            "value_choice[t+1] = 0.051 1 + 0.915 value_choice[t] + -0.053 contr_diff + 0.05 choice + -0.071 value_choice^2 + 0.041 value_choice*contr_diff + -0.086 value_choice*choice + 0.023 contr_diff^2 + -0.053 contr_diff*choice + 0.05 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 14/1000 --- L(Train): 0.5553598 --- L(Val, RNN): 0.5492018 --- L(Val, SINDy): 0.5098132 --- Time: 0.29s; --- Convergence: 5.04e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.129 1 + 0.873 value_reward_chosen[t] + 0.008 contr_diff + 0.13 reward + -0.11 value_reward_chosen^2 + 0.073 value_reward_chosen*contr_diff + -0.103 value_reward_chosen*reward + 0.107 contr_diff^2 + 0.02 contr_diff*reward + 0.131 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.131 1 + 0.863 value_reward_not_chosen[t] + 0.01 contr_diff + -0.112 value_reward_not_chosen^2 + -0.046 value_reward_not_chosen*contr_diff + -0.107 contr_diff^2 \n",
            "value_choice[t+1] = 0.057 1 + 0.905 value_choice[t] + -0.054 contr_diff + 0.056 choice + -0.08 value_choice^2 + 0.049 value_choice*contr_diff + -0.095 value_choice*choice + 0.027 contr_diff^2 + -0.054 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 15/1000 --- L(Train): 0.5585126 --- L(Val, RNN): 0.5453779 --- L(Val, SINDy): 0.5021569 --- Time: 0.32s; --- Convergence: 4.43e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.139 1 + 0.863 value_reward_chosen[t] + 0.007 contr_diff + 0.14 reward + -0.12 value_reward_chosen^2 + 0.076 value_reward_chosen*contr_diff + -0.107 value_reward_chosen*reward + 0.117 contr_diff^2 + 0.022 contr_diff*reward + 0.141 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.14 1 + 0.853 value_reward_not_chosen[t] + 0.017 contr_diff + -0.122 value_reward_not_chosen^2 + -0.044 value_reward_not_chosen*contr_diff + -0.116 contr_diff^2 \n",
            "value_choice[t+1] = 0.063 1 + 0.895 value_choice[t] + -0.051 contr_diff + 0.063 choice + -0.09 value_choice^2 + 0.058 value_choice*contr_diff + -0.105 value_choice*choice + 0.031 contr_diff^2 + -0.051 contr_diff*choice + 0.063 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 16/1000 --- L(Train): 0.5519485 --- L(Val, RNN): 0.5420110 --- L(Val, SINDy): 0.4964458 --- Time: 0.32s; --- Convergence: 3.90e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.149 1 + 0.854 value_reward_chosen[t] + 0.004 contr_diff + 0.15 reward + -0.131 value_reward_chosen^2 + 0.075 value_reward_chosen*contr_diff + -0.112 value_reward_chosen*reward + 0.127 contr_diff^2 + 0.021 contr_diff*reward + 0.151 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.149 1 + 0.843 value_reward_not_chosen[t] + 0.025 contr_diff + -0.132 value_reward_not_chosen^2 + -0.04 value_reward_not_chosen*contr_diff + -0.126 contr_diff^2 \n",
            "value_choice[t+1] = 0.068 1 + 0.885 value_choice[t] + -0.047 contr_diff + 0.068 choice + -0.1 value_choice^2 + 0.067 value_choice*contr_diff + -0.115 value_choice*choice + 0.035 contr_diff^2 + -0.047 contr_diff*choice + 0.067 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 17/1000 --- L(Train): 0.5533192 --- L(Val, RNN): 0.5388978 --- L(Val, SINDy): 0.4921098 --- Time: 0.31s; --- Convergence: 3.51e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.159 1 + 0.844 value_reward_chosen[t] + 0.005 contr_diff + 0.161 reward + -0.141 value_reward_chosen^2 + 0.078 value_reward_chosen*contr_diff + -0.115 value_reward_chosen*reward + 0.137 contr_diff^2 + 0.024 contr_diff*reward + 0.162 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.158 1 + 0.833 value_reward_not_chosen[t] + 0.033 contr_diff + -0.142 value_reward_not_chosen^2 + -0.038 value_reward_not_chosen*contr_diff + -0.135 contr_diff^2 \n",
            "value_choice[t+1] = 0.074 1 + 0.875 value_choice[t] + -0.047 contr_diff + 0.073 choice + -0.11 value_choice^2 + 0.076 value_choice*contr_diff + -0.126 value_choice*choice + 0.038 contr_diff^2 + -0.047 contr_diff*choice + 0.073 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 18/1000 --- L(Train): 0.5434101 --- L(Val, RNN): 0.5360082 --- L(Val, SINDy): 0.4881325 --- Time: 0.29s; --- Convergence: 3.20e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.17 1 + 0.834 value_reward_chosen[t] + 0.005 contr_diff + 0.171 reward + -0.152 value_reward_chosen^2 + 0.08 value_reward_chosen*contr_diff + -0.118 value_reward_chosen*reward + 0.147 contr_diff^2 + 0.026 contr_diff*reward + 0.172 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.166 1 + 0.823 value_reward_not_chosen[t] + 0.04 contr_diff + -0.152 value_reward_not_chosen^2 + -0.036 value_reward_not_chosen*contr_diff + -0.144 contr_diff^2 \n",
            "value_choice[t+1] = 0.08 1 + 0.864 value_choice[t] + -0.047 contr_diff + 0.08 choice + -0.12 value_choice^2 + 0.085 value_choice*contr_diff + -0.136 value_choice*choice + 0.042 contr_diff^2 + -0.046 contr_diff*choice + 0.08 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 19/1000 --- L(Train): 0.5486796 --- L(Val, RNN): 0.5332186 --- L(Val, SINDy): 0.4853588 --- Time: 0.44s; --- Convergence: 2.99e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.181 1 + 0.825 value_reward_chosen[t] + 0.006 contr_diff + 0.182 reward + -0.163 value_reward_chosen^2 + 0.083 value_reward_chosen*contr_diff + -0.121 value_reward_chosen*reward + 0.157 contr_diff^2 + 0.029 contr_diff*reward + 0.183 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.174 1 + 0.812 value_reward_not_chosen[t] + 0.047 contr_diff + -0.162 value_reward_not_chosen^2 + -0.035 value_reward_not_chosen*contr_diff + -0.153 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.853 value_choice[t] + -0.048 contr_diff + 0.087 choice + -0.131 value_choice^2 + 0.094 value_choice*contr_diff + -0.147 value_choice*choice + 0.046 contr_diff^2 + -0.047 contr_diff*choice + 0.087 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 20/1000 --- L(Train): 0.5386416 --- L(Val, RNN): 0.5305455 --- L(Val, SINDy): 0.4833470 --- Time: 0.29s; --- Convergence: 2.83e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.191 1 + 0.815 value_reward_chosen[t] + 0.008 contr_diff + 0.193 reward + -0.173 value_reward_chosen^2 + 0.087 value_reward_chosen*contr_diff + -0.122 value_reward_chosen*reward + 0.168 contr_diff^2 + 0.034 contr_diff*reward + 0.194 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.183 1 + 0.801 value_reward_not_chosen[t] + 0.053 contr_diff + -0.172 value_reward_not_chosen^2 + -0.036 value_reward_not_chosen*contr_diff + -0.162 contr_diff^2 \n",
            "value_choice[t+1] = 0.095 1 + 0.842 value_choice[t] + -0.051 contr_diff + 0.094 choice + -0.142 value_choice^2 + 0.103 value_choice*contr_diff + -0.158 value_choice*choice + 0.048 contr_diff^2 + -0.05 contr_diff*choice + 0.094 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 21/1000 --- L(Train): 0.5292169 --- L(Val, RNN): 0.5277675 --- L(Val, SINDy): 0.4818604 --- Time: 0.31s; --- Convergence: 2.81e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.203 1 + 0.805 value_reward_chosen[t] + 0.007 contr_diff + 0.205 reward + -0.184 value_reward_chosen^2 + 0.087 value_reward_chosen*contr_diff + -0.123 value_reward_chosen*reward + 0.179 contr_diff^2 + 0.036 contr_diff*reward + 0.206 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.189 1 + 0.791 value_reward_not_chosen[t] + 0.059 contr_diff + -0.182 value_reward_not_chosen^2 + -0.035 value_reward_not_chosen*contr_diff + -0.17 contr_diff^2 \n",
            "value_choice[t+1] = 0.1 1 + 0.831 value_choice[t] + -0.054 contr_diff + 0.1 choice + -0.153 value_choice^2 + 0.113 value_choice*contr_diff + -0.169 value_choice*choice + 0.048 contr_diff^2 + -0.054 contr_diff*choice + 0.099 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 22/1000 --- L(Train): 0.5245899 --- L(Val, RNN): 0.5249273 --- L(Val, SINDy): 0.4800245 --- Time: 0.31s; --- Convergence: 2.82e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.214 1 + 0.795 value_reward_chosen[t] + 0.007 contr_diff + 0.216 reward + -0.194 value_reward_chosen^2 + 0.089 value_reward_chosen*contr_diff + -0.123 value_reward_chosen*reward + 0.189 contr_diff^2 + 0.041 contr_diff*reward + 0.217 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.196 1 + 0.78 value_reward_not_chosen[t] + 0.065 contr_diff + -0.191 value_reward_not_chosen^2 + -0.034 value_reward_not_chosen*contr_diff + -0.178 contr_diff^2 \n",
            "value_choice[t+1] = 0.107 1 + 0.82 value_choice[t] + -0.058 contr_diff + 0.106 choice + -0.164 value_choice^2 + 0.122 value_choice*contr_diff + -0.181 value_choice*choice + 0.047 contr_diff^2 + -0.058 contr_diff*choice + 0.106 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 23/1000 --- L(Train): 0.5285456 --- L(Val, RNN): 0.5218384 --- L(Val, SINDy): 0.4787402 --- Time: 0.33s; --- Convergence: 2.96e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.225 1 + 0.785 value_reward_chosen[t] + 0.007 contr_diff + 0.227 reward + -0.204 value_reward_chosen^2 + 0.091 value_reward_chosen*contr_diff + -0.122 value_reward_chosen*reward + 0.199 contr_diff^2 + 0.044 contr_diff*reward + 0.228 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.201 1 + 0.77 value_reward_not_chosen[t] + 0.071 contr_diff + -0.2 value_reward_not_chosen^2 + -0.034 value_reward_not_chosen*contr_diff + -0.185 contr_diff^2 \n",
            "value_choice[t+1] = 0.113 1 + 0.809 value_choice[t] + -0.062 contr_diff + 0.113 choice + -0.175 value_choice^2 + 0.13 value_choice*contr_diff + -0.192 value_choice*choice + 0.047 contr_diff^2 + -0.061 contr_diff*choice + 0.112 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 24/1000 --- L(Train): 0.5183483 --- L(Val, RNN): 0.5185483 --- L(Val, SINDy): 0.4780224 --- Time: 0.30s; --- Convergence: 3.12e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.236 1 + 0.776 value_reward_chosen[t] + 0.007 contr_diff + 0.238 reward + -0.213 value_reward_chosen^2 + 0.093 value_reward_chosen*contr_diff + -0.121 value_reward_chosen*reward + 0.209 contr_diff^2 + 0.049 contr_diff*reward + 0.239 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.204 1 + 0.76 value_reward_not_chosen[t] + 0.076 contr_diff + -0.208 value_reward_not_chosen^2 + -0.033 value_reward_not_chosen*contr_diff + -0.191 contr_diff^2 \n",
            "value_choice[t+1] = 0.12 1 + 0.798 value_choice[t] + -0.066 contr_diff + 0.119 choice + -0.186 value_choice^2 + 0.138 value_choice*contr_diff + -0.203 value_choice*choice + 0.045 contr_diff^2 + -0.065 contr_diff*choice + 0.119 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 25/1000 --- L(Train): 0.5297650 --- L(Val, RNN): 0.5153323 --- L(Val, SINDy): 0.4779282 --- Time: 0.37s; --- Convergence: 3.17e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.248 1 + 0.767 value_reward_chosen[t] + 0.007 contr_diff + 0.249 reward + -0.222 value_reward_chosen^2 + 0.098 value_reward_chosen*contr_diff + -0.119 value_reward_chosen*reward + 0.22 contr_diff^2 + 0.051 contr_diff*reward + 0.25 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.207 1 + 0.75 value_reward_not_chosen[t] + 0.081 contr_diff + -0.215 value_reward_not_chosen^2 + -0.033 value_reward_not_chosen*contr_diff + -0.196 contr_diff^2 \n",
            "value_choice[t+1] = 0.125 1 + 0.787 value_choice[t] + -0.07 contr_diff + 0.125 choice + -0.197 value_choice^2 + 0.147 value_choice*contr_diff + -0.214 value_choice*choice + 0.043 contr_diff^2 + -0.069 contr_diff*choice + 0.124 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 26/1000 --- L(Train): 0.5205449 --- L(Val, RNN): 0.5125025 --- L(Val, SINDy): 0.4770327 --- Time: 0.39s; --- Convergence: 3.00e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.26 1 + 0.758 value_reward_chosen[t] + 0.005 contr_diff + 0.261 reward + -0.23 value_reward_chosen^2 + 0.102 value_reward_chosen*contr_diff + -0.117 value_reward_chosen*reward + 0.231 contr_diff^2 + 0.053 contr_diff*reward + 0.262 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.207 1 + 0.741 value_reward_not_chosen[t] + 0.086 contr_diff + -0.221 value_reward_not_chosen^2 + -0.032 value_reward_not_chosen*contr_diff + -0.2 contr_diff^2 \n",
            "value_choice[t+1] = 0.132 1 + 0.776 value_choice[t] + -0.073 contr_diff + 0.131 choice + -0.208 value_choice^2 + 0.156 value_choice*contr_diff + -0.225 value_choice*choice + 0.04 contr_diff^2 + -0.073 contr_diff*choice + 0.131 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 27/1000 --- L(Train): 0.5214927 --- L(Val, RNN): 0.5099062 --- L(Val, SINDy): 0.4756770 --- Time: 0.31s; --- Convergence: 2.80e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.272 1 + 0.748 value_reward_chosen[t] + 0.003 contr_diff + 0.272 reward + -0.239 value_reward_chosen^2 + 0.106 value_reward_chosen*contr_diff + -0.114 value_reward_chosen*reward + 0.243 contr_diff^2 + 0.052 contr_diff*reward + 0.273 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.204 1 + 0.731 value_reward_not_chosen[t] + 0.089 contr_diff + -0.225 value_reward_not_chosen^2 + -0.032 value_reward_not_chosen*contr_diff + -0.201 contr_diff^2 \n",
            "value_choice[t+1] = 0.139 1 + 0.764 value_choice[t] + -0.075 contr_diff + 0.138 choice + -0.219 value_choice^2 + 0.166 value_choice*contr_diff + -0.236 value_choice*choice + 0.037 contr_diff^2 + -0.075 contr_diff*choice + 0.138 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 28/1000 --- L(Train): 0.5103477 --- L(Val, RNN): 0.5073660 --- L(Val, SINDy): 0.4747190 --- Time: 0.33s; --- Convergence: 2.67e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.283 1 + 0.738 value_reward_chosen[t] + -0.001 contr_diff + 0.284 reward + -0.248 value_reward_chosen^2 + 0.11 value_reward_chosen*contr_diff + -0.11 value_reward_chosen*reward + 0.254 contr_diff^2 + 0.051 contr_diff*reward + 0.285 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.201 1 + 0.723 value_reward_not_chosen[t] + 0.091 contr_diff + -0.228 value_reward_not_chosen^2 + -0.033 value_reward_not_chosen*contr_diff + -0.202 contr_diff^2 \n",
            "value_choice[t+1] = 0.146 1 + 0.753 value_choice[t] + -0.077 contr_diff + 0.145 choice + -0.231 value_choice^2 + 0.176 value_choice*contr_diff + -0.247 value_choice*choice + 0.033 contr_diff^2 + -0.076 contr_diff*choice + 0.145 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 29/1000 --- L(Train): 0.5086989 --- L(Val, RNN): 0.5051051 --- L(Val, SINDy): 0.4742864 --- Time: 0.28s; --- Convergence: 2.46e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.296 1 + 0.729 value_reward_chosen[t] + -0.005 contr_diff + 0.295 reward + -0.256 value_reward_chosen^2 + 0.113 value_reward_chosen*contr_diff + -0.106 value_reward_chosen*reward + 0.265 contr_diff^2 + 0.047 contr_diff*reward + 0.296 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.197 1 + 0.714 value_reward_not_chosen[t] + 0.091 contr_diff + -0.229 value_reward_not_chosen^2 + -0.033 value_reward_not_chosen*contr_diff + -0.202 contr_diff^2 \n",
            "value_choice[t+1] = 0.151 1 + 0.742 value_choice[t] + -0.077 contr_diff + 0.15 choice + -0.242 value_choice^2 + 0.186 value_choice*contr_diff + -0.259 value_choice*choice + 0.028 contr_diff^2 + -0.076 contr_diff*choice + 0.15 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 30/1000 --- L(Train): 0.5095662 --- L(Val, RNN): 0.5031824 --- L(Val, SINDy): 0.4727677 --- Time: 0.32s; --- Convergence: 2.19e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.308 1 + 0.719 value_reward_chosen[t] + -0.01 contr_diff + 0.307 reward + -0.263 value_reward_chosen^2 + 0.115 value_reward_chosen*contr_diff + -0.102 value_reward_chosen*reward + 0.276 contr_diff^2 + 0.044 contr_diff*reward + 0.308 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.192 1 + 0.707 value_reward_not_chosen[t] + 0.091 contr_diff + -0.229 value_reward_not_chosen^2 + -0.034 value_reward_not_chosen*contr_diff + -0.202 contr_diff^2 \n",
            "value_choice[t+1] = 0.157 1 + 0.73 value_choice[t] + -0.076 contr_diff + 0.156 choice + -0.254 value_choice^2 + 0.195 value_choice*contr_diff + -0.27 value_choice*choice + 0.023 contr_diff^2 + -0.076 contr_diff*choice + 0.156 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 31/1000 --- L(Train): 0.5118660 --- L(Val, RNN): 0.5010020 --- L(Val, SINDy): 0.4714603 --- Time: 0.33s; --- Convergence: 2.19e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.32 1 + 0.709 value_reward_chosen[t] + -0.016 contr_diff + 0.318 reward + -0.27 value_reward_chosen^2 + 0.117 value_reward_chosen*contr_diff + -0.098 value_reward_chosen*reward + 0.287 contr_diff^2 + 0.04 contr_diff*reward + 0.319 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.186 1 + 0.699 value_reward_not_chosen[t] + 0.09 contr_diff + -0.227 value_reward_not_chosen^2 + -0.034 value_reward_not_chosen*contr_diff + -0.2 contr_diff^2 \n",
            "value_choice[t+1] = 0.162 1 + 0.719 value_choice[t] + -0.076 contr_diff + 0.161 choice + -0.265 value_choice^2 + 0.204 value_choice*contr_diff + -0.282 value_choice*choice + 0.017 contr_diff^2 + -0.075 contr_diff*choice + 0.16 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 32/1000 --- L(Train): 0.4965480 --- L(Val, RNN): 0.4986571 --- L(Val, SINDy): 0.4702550 --- Time: 0.29s; --- Convergence: 2.27e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.332 1 + 0.699 value_reward_chosen[t] + -0.02 contr_diff + 0.329 reward + -0.276 value_reward_chosen^2 + 0.12 value_reward_chosen*contr_diff + -0.095 value_reward_chosen*reward + 0.298 contr_diff^2 + 0.036 contr_diff*reward + 0.33 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.18 1 + 0.693 value_reward_not_chosen[t] + 0.088 contr_diff + -0.224 value_reward_not_chosen^2 + -0.033 value_reward_not_chosen*contr_diff + -0.199 contr_diff^2 \n",
            "value_choice[t+1] = 0.168 1 + 0.708 value_choice[t] + -0.075 contr_diff + 0.167 choice + -0.276 value_choice^2 + 0.213 value_choice*contr_diff + -0.293 value_choice*choice + 0.012 contr_diff^2 + -0.074 contr_diff*choice + 0.167 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 33/1000 --- L(Train): 0.4990793 --- L(Val, RNN): 0.4964803 --- L(Val, SINDy): 0.4695584 --- Time: 0.27s; --- Convergence: 2.22e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.345 1 + 0.689 value_reward_chosen[t] + -0.026 contr_diff + 0.34 reward + -0.282 value_reward_chosen^2 + 0.117 value_reward_chosen*contr_diff + -0.092 value_reward_chosen*reward + 0.31 contr_diff^2 + 0.03 contr_diff*reward + 0.341 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.174 1 + 0.686 value_reward_not_chosen[t] + 0.084 contr_diff + -0.219 value_reward_not_chosen^2 + -0.034 value_reward_not_chosen*contr_diff + -0.197 contr_diff^2 \n",
            "value_choice[t+1] = 0.174 1 + 0.697 value_choice[t] + -0.073 contr_diff + 0.173 choice + -0.288 value_choice^2 + 0.222 value_choice*contr_diff + -0.304 value_choice*choice + 0.007 contr_diff^2 + -0.072 contr_diff*choice + 0.173 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 34/1000 --- L(Train): 0.4914984 --- L(Val, RNN): 0.4944007 --- L(Val, SINDy): 0.4678110 --- Time: 0.31s; --- Convergence: 2.15e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.357 1 + 0.679 value_reward_chosen[t] + -0.03 contr_diff + 0.351 reward + -0.288 value_reward_chosen^2 + 0.113 value_reward_chosen*contr_diff + -0.09 value_reward_chosen*reward + 0.321 contr_diff^2 + 0.024 contr_diff*reward + 0.352 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.169 1 + 0.681 value_reward_not_chosen[t] + 0.079 contr_diff + -0.214 value_reward_not_chosen^2 + -0.034 value_reward_not_chosen*contr_diff + -0.196 contr_diff^2 \n",
            "value_choice[t+1] = 0.178 1 + 0.686 value_choice[t] + -0.07 contr_diff + 0.178 choice + -0.298 value_choice^2 + 0.23 value_choice*contr_diff + -0.315 value_choice*choice + 0.001 contr_diff^2 + -0.069 contr_diff*choice + 0.177 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 35/1000 --- L(Train): 0.4973799 --- L(Val, RNN): 0.4924633 --- L(Val, SINDy): 0.4667472 --- Time: 0.32s; --- Convergence: 2.04e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.369 1 + 0.669 value_reward_chosen[t] + -0.033 contr_diff + 0.36 reward + -0.293 value_reward_chosen^2 + 0.11 value_reward_chosen*contr_diff + -0.089 value_reward_chosen*reward + 0.333 contr_diff^2 + 0.02 contr_diff*reward + 0.362 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.166 1 + 0.677 value_reward_not_chosen[t] + 0.075 contr_diff + -0.209 value_reward_not_chosen^2 + -0.033 value_reward_not_chosen*contr_diff + -0.196 contr_diff^2 \n",
            "value_choice[t+1] = 0.182 1 + 0.675 value_choice[t] + -0.069 contr_diff + 0.182 choice + -0.309 value_choice^2 + 0.237 value_choice*contr_diff + -0.325 value_choice*choice + -0.004 contr_diff^2 + -0.068 contr_diff*choice + 0.181 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 36/1000 --- L(Train): 0.4902535 --- L(Val, RNN): 0.4906292 --- L(Val, SINDy): 0.4648498 --- Time: 0.36s; --- Convergence: 1.94e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.382 1 + 0.658 value_reward_chosen[t] + -0.035 contr_diff + 0.37 reward + -0.299 value_reward_chosen^2 + 0.105 value_reward_chosen*contr_diff + -0.089 value_reward_chosen*reward + 0.345 contr_diff^2 + 0.015 contr_diff*reward + 0.371 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.165 1 + 0.673 value_reward_not_chosen[t] + 0.071 contr_diff + -0.203 value_reward_not_chosen^2 + -0.03 value_reward_not_chosen*contr_diff + -0.198 contr_diff^2 \n",
            "value_choice[t+1] = 0.186 1 + 0.665 value_choice[t] + -0.069 contr_diff + 0.186 choice + -0.319 value_choice^2 + 0.243 value_choice*contr_diff + -0.335 value_choice*choice + -0.009 contr_diff^2 + -0.068 contr_diff*choice + 0.185 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 37/1000 --- L(Train): 0.4987963 --- L(Val, RNN): 0.4889094 --- L(Val, SINDy): 0.4633130 --- Time: 0.28s; --- Convergence: 1.83e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.394 1 + 0.647 value_reward_chosen[t] + -0.035 contr_diff + 0.379 reward + -0.304 value_reward_chosen^2 + 0.098 value_reward_chosen*contr_diff + -0.09 value_reward_chosen*reward + 0.357 contr_diff^2 + 0.01 contr_diff*reward + 0.38 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.166 1 + 0.671 value_reward_not_chosen[t] + 0.067 contr_diff + -0.197 value_reward_not_chosen^2 + -0.027 value_reward_not_chosen*contr_diff + -0.201 contr_diff^2 \n",
            "value_choice[t+1] = 0.191 1 + 0.656 value_choice[t] + -0.071 contr_diff + 0.191 choice + -0.328 value_choice^2 + 0.246 value_choice*contr_diff + -0.344 value_choice*choice + -0.012 contr_diff^2 + -0.07 contr_diff*choice + 0.19 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 38/1000 --- L(Train): 0.4833019 --- L(Val, RNN): 0.4872921 --- L(Val, SINDy): 0.4626240 --- Time: 0.30s; --- Convergence: 1.72e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.406 1 + 0.635 value_reward_chosen[t] + -0.034 contr_diff + 0.387 reward + -0.309 value_reward_chosen^2 + 0.093 value_reward_chosen*contr_diff + -0.094 value_reward_chosen*reward + 0.369 contr_diff^2 + 0.007 contr_diff*reward + 0.388 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.17 1 + 0.67 value_reward_not_chosen[t] + 0.066 contr_diff + -0.19 value_reward_not_chosen^2 + -0.022 value_reward_not_chosen*contr_diff + -0.207 contr_diff^2 \n",
            "value_choice[t+1] = 0.194 1 + 0.647 value_choice[t] + -0.074 contr_diff + 0.193 choice + -0.338 value_choice^2 + 0.249 value_choice*contr_diff + -0.354 value_choice*choice + -0.017 contr_diff^2 + -0.073 contr_diff*choice + 0.193 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 39/1000 --- L(Train): 0.4850499 --- L(Val, RNN): 0.4857315 --- L(Val, SINDy): 0.4658108 --- Time: 0.33s; --- Convergence: 1.64e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.419 1 + 0.623 value_reward_chosen[t] + -0.032 contr_diff + 0.396 reward + -0.314 value_reward_chosen^2 + 0.087 value_reward_chosen*contr_diff + -0.1 value_reward_chosen*reward + 0.381 contr_diff^2 + 0.006 contr_diff*reward + 0.397 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.176 1 + 0.67 value_reward_not_chosen[t] + 0.066 contr_diff + -0.183 value_reward_not_chosen^2 + -0.016 value_reward_not_chosen*contr_diff + -0.213 contr_diff^2 \n",
            "value_choice[t+1] = 0.198 1 + 0.638 value_choice[t] + -0.077 contr_diff + 0.197 choice + -0.346 value_choice^2 + 0.251 value_choice*contr_diff + -0.362 value_choice*choice + -0.02 contr_diff^2 + -0.076 contr_diff*choice + 0.197 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 40/1000 --- L(Train): 0.4918551 --- L(Val, RNN): 0.4842443 --- L(Val, SINDy): 0.4701858 --- Time: 0.38s; --- Convergence: 1.56e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.43 1 + 0.611 value_reward_chosen[t] + -0.03 contr_diff + 0.402 reward + -0.32 value_reward_chosen^2 + 0.081 value_reward_chosen*contr_diff + -0.107 value_reward_chosen*reward + 0.393 contr_diff^2 + 0.004 contr_diff*reward + 0.404 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.182 1 + 0.669 value_reward_not_chosen[t] + 0.065 contr_diff + -0.176 value_reward_not_chosen^2 + -0.012 value_reward_not_chosen*contr_diff + -0.218 contr_diff^2 \n",
            "value_choice[t+1] = 0.202 1 + 0.631 value_choice[t] + -0.08 contr_diff + 0.201 choice + -0.354 value_choice^2 + 0.253 value_choice*contr_diff + -0.37 value_choice*choice + -0.023 contr_diff^2 + -0.079 contr_diff*choice + 0.201 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 41/1000 --- L(Train): 0.4917004 --- L(Val, RNN): 0.4827722 --- L(Val, SINDy): 0.4713457 --- Time: 0.30s; --- Convergence: 1.52e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.441 1 + 0.598 value_reward_chosen[t] + -0.029 contr_diff + 0.409 reward + -0.326 value_reward_chosen^2 + 0.074 value_reward_chosen*contr_diff + -0.115 value_reward_chosen*reward + 0.404 contr_diff^2 + 0.001 contr_diff*reward + 0.41 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.188 1 + 0.67 value_reward_not_chosen[t] + 0.063 contr_diff + -0.169 value_reward_not_chosen^2 + -0.01 value_reward_not_chosen*contr_diff + -0.224 contr_diff^2 \n",
            "value_choice[t+1] = 0.206 1 + 0.623 value_choice[t] + -0.081 contr_diff + 0.205 choice + -0.361 value_choice^2 + 0.254 value_choice*contr_diff + -0.377 value_choice*choice + -0.025 contr_diff^2 + -0.08 contr_diff*choice + 0.205 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 42/1000 --- L(Train): 0.4879300 --- L(Val, RNN): 0.4813506 --- L(Val, SINDy): 0.4690514 --- Time: 0.32s; --- Convergence: 1.47e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.453 1 + 0.586 value_reward_chosen[t] + -0.03 contr_diff + 0.415 reward + -0.331 value_reward_chosen^2 + 0.069 value_reward_chosen*contr_diff + -0.123 value_reward_chosen*reward + 0.415 contr_diff^2 + -0.002 contr_diff*reward + 0.416 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.196 1 + 0.673 value_reward_not_chosen[t] + 0.06 contr_diff + -0.161 value_reward_not_chosen^2 + -0.008 value_reward_not_chosen*contr_diff + -0.23 contr_diff^2 \n",
            "value_choice[t+1] = 0.207 1 + 0.617 value_choice[t] + -0.081 contr_diff + 0.206 choice + -0.368 value_choice^2 + 0.255 value_choice*contr_diff + -0.384 value_choice*choice + -0.028 contr_diff^2 + -0.08 contr_diff*choice + 0.206 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 43/1000 --- L(Train): 0.4884174 --- L(Val, RNN): 0.4800314 --- L(Val, SINDy): 0.4686182 --- Time: 0.27s; --- Convergence: 1.39e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.464 1 + 0.573 value_reward_chosen[t] + -0.028 contr_diff + 0.42 reward + -0.336 value_reward_chosen^2 + 0.063 value_reward_chosen*contr_diff + -0.132 value_reward_chosen*reward + 0.427 contr_diff^2 + -0.003 contr_diff*reward + 0.421 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.202 1 + 0.676 value_reward_not_chosen[t] + 0.059 contr_diff + -0.152 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.235 contr_diff^2 \n",
            "value_choice[t+1] = 0.209 1 + 0.611 value_choice[t] + -0.079 contr_diff + 0.209 choice + -0.374 value_choice^2 + 0.257 value_choice*contr_diff + -0.39 value_choice*choice + -0.03 contr_diff^2 + -0.078 contr_diff*choice + 0.208 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 44/1000 --- L(Train): 0.4797561 --- L(Val, RNN): 0.4788605 --- L(Val, SINDy): 0.4661199 --- Time: 0.32s; --- Convergence: 1.28e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.475 1 + 0.561 value_reward_chosen[t] + -0.025 contr_diff + 0.425 reward + -0.341 value_reward_chosen^2 + 0.058 value_reward_chosen*contr_diff + -0.142 value_reward_chosen*reward + 0.438 contr_diff^2 + -0.004 contr_diff*reward + 0.426 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.209 1 + 0.68 value_reward_not_chosen[t] + 0.061 contr_diff + -0.144 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.239 contr_diff^2 \n",
            "value_choice[t+1] = 0.212 1 + 0.606 value_choice[t] + -0.077 contr_diff + 0.211 choice + -0.379 value_choice^2 + 0.258 value_choice*contr_diff + -0.395 value_choice*choice + -0.031 contr_diff^2 + -0.076 contr_diff*choice + 0.211 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 45/1000 --- L(Train): 0.4853532 --- L(Val, RNN): 0.4774660 --- L(Val, SINDy): 0.4781245 --- Time: 0.29s; --- Convergence: 1.34e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.485 1 + 0.549 value_reward_chosen[t] + -0.023 contr_diff + 0.429 reward + -0.346 value_reward_chosen^2 + 0.053 value_reward_chosen*contr_diff + -0.15 value_reward_chosen*reward + 0.448 contr_diff^2 + -0.004 contr_diff*reward + 0.43 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.214 1 + 0.684 value_reward_not_chosen[t] + 0.061 contr_diff + -0.136 value_reward_not_chosen^2 + 0.005 value_reward_not_chosen*contr_diff + -0.242 contr_diff^2 \n",
            "value_choice[t+1] = 0.214 1 + 0.601 value_choice[t] + -0.074 contr_diff + 0.213 choice + -0.384 value_choice^2 + 0.259 value_choice*contr_diff + -0.399 value_choice*choice + -0.032 contr_diff^2 + -0.073 contr_diff*choice + 0.213 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 46/1000 --- L(Train): 0.4818761 --- L(Val, RNN): 0.4761879 --- L(Val, SINDy): 0.4861611 --- Time: 0.31s; --- Convergence: 1.31e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.495 1 + 0.537 value_reward_chosen[t] + -0.021 contr_diff + 0.432 reward + -0.35 value_reward_chosen^2 + 0.047 value_reward_chosen*contr_diff + -0.159 value_reward_chosen*reward + 0.457 contr_diff^2 + -0.005 contr_diff*reward + 0.433 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.217 1 + 0.69 value_reward_not_chosen[t] + 0.061 contr_diff + -0.126 value_reward_not_chosen^2 + 0.008 value_reward_not_chosen*contr_diff + -0.241 contr_diff^2 \n",
            "value_choice[t+1] = 0.213 1 + 0.597 value_choice[t] + -0.072 contr_diff + 0.213 choice + -0.387 value_choice^2 + 0.259 value_choice*contr_diff + -0.403 value_choice*choice + -0.034 contr_diff^2 + -0.071 contr_diff*choice + 0.212 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 47/1000 --- L(Train): 0.4834990 --- L(Val, RNN): 0.4748915 --- L(Val, SINDy): 0.4930833 --- Time: 0.45s; --- Convergence: 1.30e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.506 1 + 0.525 value_reward_chosen[t] + -0.02 contr_diff + 0.435 reward + -0.353 value_reward_chosen^2 + 0.042 value_reward_chosen*contr_diff + -0.167 value_reward_chosen*reward + 0.467 contr_diff^2 + -0.007 contr_diff*reward + 0.437 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.215 1 + 0.697 value_reward_not_chosen[t] + 0.058 contr_diff + -0.116 value_reward_not_chosen^2 + 0.008 value_reward_not_chosen*contr_diff + -0.236 contr_diff^2 \n",
            "value_choice[t+1] = 0.216 1 + 0.595 value_choice[t] + -0.07 contr_diff + 0.215 choice + -0.39 value_choice^2 + 0.257 value_choice*contr_diff + -0.406 value_choice*choice + -0.034 contr_diff^2 + -0.068 contr_diff*choice + 0.215 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 48/1000 --- L(Train): 0.4928288 --- L(Val, RNN): 0.4736466 --- L(Val, SINDy): 0.5039542 --- Time: 0.32s; --- Convergence: 1.27e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.516 1 + 0.513 value_reward_chosen[t] + -0.02 contr_diff + 0.438 reward + -0.358 value_reward_chosen^2 + 0.036 value_reward_chosen*contr_diff + -0.176 value_reward_chosen*reward + 0.476 contr_diff^2 + -0.01 contr_diff*reward + 0.439 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.211 1 + 0.706 value_reward_not_chosen[t] + 0.057 contr_diff + -0.106 value_reward_not_chosen^2 + 0.01 value_reward_not_chosen*contr_diff + -0.229 contr_diff^2 \n",
            "value_choice[t+1] = 0.215 1 + 0.593 value_choice[t] + -0.068 contr_diff + 0.214 choice + -0.392 value_choice^2 + 0.254 value_choice*contr_diff + -0.408 value_choice*choice + -0.034 contr_diff^2 + -0.067 contr_diff*choice + 0.214 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 49/1000 --- L(Train): 0.4929634 --- L(Val, RNN): 0.4724648 --- L(Val, SINDy): 0.5104154 --- Time: 0.27s; --- Convergence: 1.23e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.526 1 + 0.501 value_reward_chosen[t] + -0.021 contr_diff + 0.441 reward + -0.361 value_reward_chosen^2 + 0.03 value_reward_chosen*contr_diff + -0.184 value_reward_chosen*reward + 0.484 contr_diff^2 + -0.013 contr_diff*reward + 0.442 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.205 1 + 0.715 value_reward_not_chosen[t] + 0.055 contr_diff + -0.095 value_reward_not_chosen^2 + 0.009 value_reward_not_chosen*contr_diff + -0.22 contr_diff^2 \n",
            "value_choice[t+1] = 0.215 1 + 0.592 value_choice[t] + -0.067 contr_diff + 0.215 choice + -0.393 value_choice^2 + 0.25 value_choice*contr_diff + -0.408 value_choice*choice + -0.033 contr_diff^2 + -0.066 contr_diff*choice + 0.214 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 50/1000 --- L(Train): 0.4710655 --- L(Val, RNN): 0.4713215 --- L(Val, SINDy): 0.5234668 --- Time: 0.28s; --- Convergence: 1.19e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.536 1 + 0.488 value_reward_chosen[t] + -0.022 contr_diff + 0.442 reward + -0.366 value_reward_chosen^2 + 0.024 value_reward_chosen*contr_diff + -0.192 value_reward_chosen*reward + 0.491 contr_diff^2 + -0.016 contr_diff*reward + 0.444 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.197 1 + 0.726 value_reward_not_chosen[t] + 0.053 contr_diff + -0.085 value_reward_not_chosen^2 + 0.008 value_reward_not_chosen*contr_diff + -0.21 contr_diff^2 \n",
            "value_choice[t+1] = 0.215 1 + 0.592 value_choice[t] + -0.066 contr_diff + 0.214 choice + -0.393 value_choice^2 + 0.244 value_choice*contr_diff + -0.409 value_choice*choice + -0.032 contr_diff^2 + -0.065 contr_diff*choice + 0.214 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 51/1000 --- L(Train): 0.4815032 --- L(Val, RNN): 0.4701861 --- L(Val, SINDy): 0.5281665 --- Time: 0.27s; --- Convergence: 1.16e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.545 1 + 0.475 value_reward_chosen[t] + -0.022 contr_diff + 0.444 reward + -0.371 value_reward_chosen^2 + 0.017 value_reward_chosen*contr_diff + -0.199 value_reward_chosen*reward + 0.498 contr_diff^2 + -0.019 contr_diff*reward + 0.445 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.19 1 + 0.736 value_reward_not_chosen[t] + 0.05 contr_diff + -0.075 value_reward_not_chosen^2 + 0.006 value_reward_not_chosen*contr_diff + -0.199 contr_diff^2 \n",
            "value_choice[t+1] = 0.213 1 + 0.592 value_choice[t] + -0.066 contr_diff + 0.212 choice + -0.393 value_choice^2 + 0.238 value_choice*contr_diff + -0.408 value_choice*choice + -0.032 contr_diff^2 + -0.064 contr_diff*choice + 0.211 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 52/1000 --- L(Train): 0.4720682 --- L(Val, RNN): 0.4691828 --- L(Val, SINDy): 0.5373446 --- Time: 0.30s; --- Convergence: 1.08e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.555 1 + 0.462 value_reward_chosen[t] + -0.023 contr_diff + 0.446 reward + -0.374 value_reward_chosen^2 + 0.012 value_reward_chosen*contr_diff + -0.204 value_reward_chosen*reward + 0.504 contr_diff^2 + -0.023 contr_diff*reward + 0.447 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.186 1 + 0.748 value_reward_not_chosen[t] + 0.049 contr_diff + -0.068 value_reward_not_chosen^2 + 0.004 value_reward_not_chosen*contr_diff + -0.189 contr_diff^2 \n",
            "value_choice[t+1] = 0.214 1 + 0.595 value_choice[t] + -0.064 contr_diff + 0.213 choice + -0.391 value_choice^2 + 0.23 value_choice*contr_diff + -0.406 value_choice*choice + -0.031 contr_diff^2 + -0.063 contr_diff*choice + 0.212 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 53/1000 --- L(Train): 0.4696669 --- L(Val, RNN): 0.4681725 --- L(Val, SINDy): 0.5535114 --- Time: 0.34s; --- Convergence: 1.05e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.566 1 + 0.449 value_reward_chosen[t] + -0.021 contr_diff + 0.448 reward + -0.375 value_reward_chosen^2 + 0.007 value_reward_chosen*contr_diff + -0.208 value_reward_chosen*reward + 0.51 contr_diff^2 + -0.024 contr_diff*reward + 0.449 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.185 1 + 0.758 value_reward_not_chosen[t] + 0.048 contr_diff + -0.063 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.182 contr_diff^2 \n",
            "value_choice[t+1] = 0.21 1 + 0.596 value_choice[t] + -0.062 contr_diff + 0.21 choice + -0.39 value_choice^2 + 0.223 value_choice*contr_diff + -0.404 value_choice*choice + -0.032 contr_diff^2 + -0.061 contr_diff*choice + 0.209 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 54/1000 --- L(Train): 0.4701646 --- L(Val, RNN): 0.4671485 --- L(Val, SINDy): 0.5564929 --- Time: 0.34s; --- Convergence: 1.04e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.575 1 + 0.435 value_reward_chosen[t] + -0.017 contr_diff + 0.451 reward + -0.375 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.211 value_reward_chosen*reward + 0.515 contr_diff^2 + -0.025 contr_diff*reward + 0.452 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.187 1 + 0.768 value_reward_not_chosen[t] + 0.048 contr_diff + -0.06 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.176 contr_diff^2 \n",
            "value_choice[t+1] = 0.207 1 + 0.598 value_choice[t] + -0.06 contr_diff + 0.206 choice + -0.388 value_choice^2 + 0.216 value_choice*contr_diff + -0.403 value_choice*choice + -0.033 contr_diff^2 + -0.059 contr_diff*choice + 0.206 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 55/1000 --- L(Train): 0.4693756 --- L(Val, RNN): 0.4660882 --- L(Val, SINDy): 0.5500165 --- Time: 0.30s; --- Convergence: 1.05e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.585 1 + 0.422 value_reward_chosen[t] + -0.01 contr_diff + 0.453 reward + -0.374 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.213 value_reward_chosen*reward + 0.519 contr_diff^2 + -0.027 contr_diff*reward + 0.454 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.192 1 + 0.776 value_reward_not_chosen[t] + 0.051 contr_diff + -0.06 value_reward_not_chosen^2 + -0.01 value_reward_not_chosen*contr_diff + -0.175 contr_diff^2 \n",
            "value_choice[t+1] = 0.209 1 + 0.602 value_choice[t] + -0.057 contr_diff + 0.208 choice + -0.384 value_choice^2 + 0.208 value_choice*contr_diff + -0.399 value_choice*choice + -0.029 contr_diff^2 + -0.055 contr_diff*choice + 0.208 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 56/1000 --- L(Train): 0.4608616 --- L(Val, RNN): 0.4650478 --- L(Val, SINDy): 0.5371456 --- Time: 0.30s; --- Convergence: 1.04e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.595 1 + 0.408 value_reward_chosen[t] + -0.005 contr_diff + 0.455 reward + -0.373 value_reward_chosen^2 + -0.007 value_reward_chosen*contr_diff + -0.214 value_reward_chosen*reward + 0.523 contr_diff^2 + -0.03 contr_diff*reward + 0.456 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.198 1 + 0.784 value_reward_not_chosen[t] + 0.055 contr_diff + -0.061 value_reward_not_chosen^2 + -0.015 value_reward_not_chosen*contr_diff + -0.175 contr_diff^2 \n",
            "value_choice[t+1] = 0.207 1 + 0.604 value_choice[t] + -0.053 contr_diff + 0.206 choice + -0.382 value_choice^2 + 0.2 value_choice*contr_diff + -0.396 value_choice*choice + -0.026 contr_diff^2 + -0.052 contr_diff*choice + 0.205 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 57/1000 --- L(Train): 0.4656790 --- L(Val, RNN): 0.4640540 --- L(Val, SINDy): 0.5355442 --- Time: 0.27s; --- Convergence: 1.02e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.603 1 + 0.393 value_reward_chosen[t] + 0.002 contr_diff + 0.457 reward + -0.373 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + -0.214 value_reward_chosen*reward + 0.525 contr_diff^2 + -0.032 contr_diff*reward + 0.458 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.206 1 + 0.791 value_reward_not_chosen[t] + 0.059 contr_diff + -0.063 value_reward_not_chosen^2 + -0.021 value_reward_not_chosen*contr_diff + -0.177 contr_diff^2 \n",
            "value_choice[t+1] = 0.201 1 + 0.605 value_choice[t] + -0.05 contr_diff + 0.2 choice + -0.381 value_choice^2 + 0.192 value_choice*contr_diff + -0.395 value_choice*choice + -0.026 contr_diff^2 + -0.049 contr_diff*choice + 0.2 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 58/1000 --- L(Train): 0.4676756 --- L(Val, RNN): 0.4631739 --- L(Val, SINDy): 0.5349403 --- Time: 0.27s; --- Convergence: 9.50e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.611 1 + 0.379 value_reward_chosen[t] + 0.011 contr_diff + 0.459 reward + -0.372 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + -0.213 value_reward_chosen*reward + 0.526 contr_diff^2 + -0.038 contr_diff*reward + 0.461 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.215 1 + 0.795 value_reward_not_chosen[t] + 0.064 contr_diff + -0.064 value_reward_not_chosen^2 + -0.026 value_reward_not_chosen*contr_diff + -0.181 contr_diff^2 \n",
            "value_choice[t+1] = 0.2 1 + 0.608 value_choice[t] + -0.048 contr_diff + 0.199 choice + -0.378 value_choice^2 + 0.184 value_choice*contr_diff + -0.392 value_choice*choice + -0.024 contr_diff^2 + -0.047 contr_diff*choice + 0.199 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 59/1000 --- L(Train): 0.4671302 --- L(Val, RNN): 0.4621473 --- L(Val, SINDy): 0.5264189 --- Time: 0.27s; --- Convergence: 9.88e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.618 1 + 0.364 value_reward_chosen[t] + 0.019 contr_diff + 0.461 reward + -0.372 value_reward_chosen^2 + -0.015 value_reward_chosen*contr_diff + -0.211 value_reward_chosen*reward + 0.526 contr_diff^2 + -0.043 contr_diff*reward + 0.462 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.224 1 + 0.798 value_reward_not_chosen[t] + 0.067 contr_diff + -0.064 value_reward_not_chosen^2 + -0.029 value_reward_not_chosen*contr_diff + -0.185 contr_diff^2 \n",
            "value_choice[t+1] = 0.201 1 + 0.612 value_choice[t] + -0.048 contr_diff + 0.201 choice + -0.374 value_choice^2 + 0.175 value_choice*contr_diff + -0.388 value_choice*choice + -0.02 contr_diff^2 + -0.046 contr_diff*choice + 0.2 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 60/1000 --- L(Train): 0.4597705 --- L(Val, RNN): 0.4611220 --- L(Val, SINDy): 0.5258080 --- Time: 0.35s; --- Convergence: 1.01e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.624 1 + 0.349 value_reward_chosen[t] + 0.029 contr_diff + 0.462 reward + -0.372 value_reward_chosen^2 + -0.018 value_reward_chosen*contr_diff + -0.208 value_reward_chosen*reward + 0.524 contr_diff^2 + -0.049 contr_diff*reward + 0.463 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.233 1 + 0.799 value_reward_not_chosen[t] + 0.071 contr_diff + -0.064 value_reward_not_chosen^2 + -0.031 value_reward_not_chosen*contr_diff + -0.189 contr_diff^2 \n",
            "value_choice[t+1] = 0.2 1 + 0.615 value_choice[t] + -0.047 contr_diff + 0.2 choice + -0.372 value_choice^2 + 0.166 value_choice*contr_diff + -0.386 value_choice*choice + -0.018 contr_diff^2 + -0.046 contr_diff*choice + 0.199 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 61/1000 --- L(Train): 0.4717714 --- L(Val, RNN): 0.4601540 --- L(Val, SINDy): 0.5236909 --- Time: 0.30s; --- Convergence: 9.87e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.627 1 + 0.334 value_reward_chosen[t] + 0.04 contr_diff + 0.462 reward + -0.374 value_reward_chosen^2 + -0.015 value_reward_chosen*contr_diff + -0.205 value_reward_chosen*reward + 0.52 contr_diff^2 + -0.054 contr_diff*reward + 0.463 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.242 1 + 0.799 value_reward_not_chosen[t] + 0.073 contr_diff + -0.062 value_reward_not_chosen^2 + -0.034 value_reward_not_chosen*contr_diff + -0.194 contr_diff^2 \n",
            "value_choice[t+1] = 0.196 1 + 0.615 value_choice[t] + -0.045 contr_diff + 0.195 choice + -0.371 value_choice^2 + 0.158 value_choice*contr_diff + -0.385 value_choice*choice + -0.02 contr_diff^2 + -0.044 contr_diff*choice + 0.195 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 62/1000 --- L(Train): 0.4661135 --- L(Val, RNN): 0.4592000 --- L(Val, SINDy): 0.5189180 --- Time: 0.33s; --- Convergence: 9.71e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.63 1 + 0.319 value_reward_chosen[t] + 0.05 contr_diff + 0.462 reward + -0.377 value_reward_chosen^2 + -0.012 value_reward_chosen*contr_diff + -0.201 value_reward_chosen*reward + 0.516 contr_diff^2 + -0.059 contr_diff*reward + 0.463 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.251 1 + 0.798 value_reward_not_chosen[t] + 0.074 contr_diff + -0.059 value_reward_not_chosen^2 + -0.037 value_reward_not_chosen*contr_diff + -0.199 contr_diff^2 \n",
            "value_choice[t+1] = 0.193 1 + 0.617 value_choice[t] + -0.044 contr_diff + 0.192 choice + -0.37 value_choice^2 + 0.15 value_choice*contr_diff + -0.384 value_choice*choice + -0.021 contr_diff^2 + -0.043 contr_diff*choice + 0.192 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 63/1000 --- L(Train): 0.4802847 --- L(Val, RNN): 0.4582685 --- L(Val, SINDy): 0.5226319 --- Time: 0.31s; --- Convergence: 9.51e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.63 1 + 0.303 value_reward_chosen[t] + 0.063 contr_diff + 0.46 reward + -0.381 value_reward_chosen^2 + -0.008 value_reward_chosen*contr_diff + -0.196 value_reward_chosen*reward + 0.509 contr_diff^2 + -0.065 contr_diff*reward + 0.461 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.258 1 + 0.794 value_reward_not_chosen[t] + 0.073 contr_diff + -0.054 value_reward_not_chosen^2 + -0.037 value_reward_not_chosen*contr_diff + -0.204 contr_diff^2 \n",
            "value_choice[t+1] = 0.196 1 + 0.621 value_choice[t] + -0.042 contr_diff + 0.195 choice + -0.366 value_choice^2 + 0.143 value_choice*contr_diff + -0.379 value_choice*choice + -0.018 contr_diff^2 + -0.041 contr_diff*choice + 0.195 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 64/1000 --- L(Train): 0.4534543 --- L(Val, RNN): 0.4573793 --- L(Val, SINDy): 0.5188376 --- Time: 0.30s; --- Convergence: 9.20e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.629 1 + 0.287 value_reward_chosen[t] + 0.075 contr_diff + 0.458 reward + -0.385 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.19 value_reward_chosen*reward + 0.5 contr_diff^2 + -0.073 contr_diff*reward + 0.459 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.266 1 + 0.79 value_reward_not_chosen[t] + 0.068 contr_diff + -0.048 value_reward_not_chosen^2 + -0.037 value_reward_not_chosen*contr_diff + -0.208 contr_diff^2 \n",
            "value_choice[t+1] = 0.196 1 + 0.623 value_choice[t] + -0.039 contr_diff + 0.196 choice + -0.363 value_choice^2 + 0.136 value_choice*contr_diff + -0.377 value_choice*choice + -0.015 contr_diff^2 + -0.038 contr_diff*choice + 0.195 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 65/1000 --- L(Train): 0.4597874 --- L(Val, RNN): 0.4566369 --- L(Val, SINDy): 0.5126375 --- Time: 0.27s; --- Convergence: 8.31e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.626 1 + 0.271 value_reward_chosen[t] + 0.087 contr_diff + 0.456 reward + -0.389 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.181 value_reward_chosen*reward + 0.49 contr_diff^2 + -0.082 contr_diff*reward + 0.457 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.272 1 + 0.784 value_reward_not_chosen[t] + 0.06 contr_diff + -0.04 value_reward_not_chosen^2 + -0.035 value_reward_not_chosen*contr_diff + -0.212 contr_diff^2 \n",
            "value_choice[t+1] = 0.193 1 + 0.622 value_choice[t] + -0.036 contr_diff + 0.192 choice + -0.364 value_choice^2 + 0.129 value_choice*contr_diff + -0.378 value_choice*choice + -0.017 contr_diff^2 + -0.034 contr_diff*choice + 0.192 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 66/1000 --- L(Train): 0.4660771 --- L(Val, RNN): 0.4559537 --- L(Val, SINDy): 0.4976474 --- Time: 0.27s; --- Convergence: 7.57e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.623 1 + 0.256 value_reward_chosen[t] + 0.101 contr_diff + 0.453 reward + -0.392 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.172 value_reward_chosen*reward + 0.479 contr_diff^2 + -0.088 contr_diff*reward + 0.454 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.277 1 + 0.778 value_reward_not_chosen[t] + 0.054 contr_diff + -0.031 value_reward_not_chosen^2 + -0.031 value_reward_not_chosen*contr_diff + -0.216 contr_diff^2 \n",
            "value_choice[t+1] = 0.19 1 + 0.622 value_choice[t] + -0.033 contr_diff + 0.189 choice + -0.365 value_choice^2 + 0.123 value_choice*contr_diff + -0.379 value_choice*choice + -0.017 contr_diff^2 + -0.032 contr_diff*choice + 0.189 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 67/1000 --- L(Train): 0.4599099 --- L(Val, RNN): 0.4550987 --- L(Val, SINDy): 0.4859398 --- Time: 0.27s; --- Convergence: 8.06e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.618 1 + 0.241 value_reward_chosen[t] + 0.114 contr_diff + 0.451 reward + -0.393 value_reward_chosen^2 + 0.006 value_reward_chosen*contr_diff + -0.16 value_reward_chosen*reward + 0.468 contr_diff^2 + -0.095 contr_diff*reward + 0.452 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.283 1 + 0.773 value_reward_not_chosen[t] + 0.049 contr_diff + -0.022 value_reward_not_chosen^2 + -0.025 value_reward_not_chosen*contr_diff + -0.219 contr_diff^2 \n",
            "value_choice[t+1] = 0.19 1 + 0.623 value_choice[t] + -0.033 contr_diff + 0.19 choice + -0.364 value_choice^2 + 0.116 value_choice*contr_diff + -0.377 value_choice*choice + -0.016 contr_diff^2 + -0.031 contr_diff*choice + 0.189 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 68/1000 --- L(Train): 0.4626399 --- L(Val, RNN): 0.4542775 --- L(Val, SINDy): 0.4772253 --- Time: 0.29s; --- Convergence: 8.14e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.612 1 + 0.225 value_reward_chosen[t] + 0.128 contr_diff + 0.448 reward + -0.394 value_reward_chosen^2 + 0.011 value_reward_chosen*contr_diff + -0.148 value_reward_chosen*reward + 0.455 contr_diff^2 + -0.105 contr_diff*reward + 0.45 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.289 1 + 0.77 value_reward_not_chosen[t] + 0.043 contr_diff + -0.014 value_reward_not_chosen^2 + -0.018 value_reward_not_chosen*contr_diff + -0.223 contr_diff^2 \n",
            "value_choice[t+1] = 0.193 1 + 0.625 value_choice[t] + -0.033 contr_diff + 0.192 choice + -0.361 value_choice^2 + 0.11 value_choice*contr_diff + -0.375 value_choice*choice + -0.013 contr_diff^2 + -0.032 contr_diff*choice + 0.191 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 69/1000 --- L(Train): 0.4641555 --- L(Val, RNN): 0.4533950 --- L(Val, SINDy): 0.4733501 --- Time: 0.30s; --- Convergence: 8.48e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.604 1 + 0.21 value_reward_chosen[t] + 0.143 contr_diff + 0.445 reward + -0.395 value_reward_chosen^2 + 0.013 value_reward_chosen*contr_diff + -0.135 value_reward_chosen*reward + 0.44 contr_diff^2 + -0.116 contr_diff*reward + 0.446 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.295 1 + 0.767 value_reward_not_chosen[t] + 0.036 contr_diff + -0.006 value_reward_not_chosen^2 + -0.01 value_reward_not_chosen*contr_diff + -0.228 contr_diff^2 \n",
            "value_choice[t+1] = 0.193 1 + 0.626 value_choice[t] + -0.032 contr_diff + 0.192 choice + -0.36 value_choice^2 + 0.104 value_choice*contr_diff + -0.374 value_choice*choice + -0.012 contr_diff^2 + -0.031 contr_diff*choice + 0.192 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 70/1000 --- L(Train): 0.4528925 --- L(Val, RNN): 0.4525581 --- L(Val, SINDy): 0.4854586 --- Time: 0.31s; --- Convergence: 8.42e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.596 1 + 0.195 value_reward_chosen[t] + 0.156 contr_diff + 0.444 reward + -0.393 value_reward_chosen^2 + 0.012 value_reward_chosen*contr_diff + -0.121 value_reward_chosen*reward + 0.426 contr_diff^2 + -0.127 contr_diff*reward + 0.445 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.303 1 + 0.766 value_reward_not_chosen[t] + 0.029 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.234 contr_diff^2 \n",
            "value_choice[t+1] = 0.191 1 + 0.625 value_choice[t] + -0.03 contr_diff + 0.19 choice + -0.362 value_choice^2 + 0.098 value_choice*contr_diff + -0.375 value_choice*choice + -0.013 contr_diff^2 + -0.029 contr_diff*choice + 0.19 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 71/1000 --- L(Train): 0.4510346 --- L(Val, RNN): 0.4518172 --- L(Val, SINDy): 0.4969518 --- Time: 0.30s; --- Convergence: 7.92e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.59 1 + 0.181 value_reward_chosen[t] + 0.168 contr_diff + 0.445 reward + -0.387 value_reward_chosen^2 + 0.011 value_reward_chosen*contr_diff + -0.106 value_reward_chosen*reward + 0.413 contr_diff^2 + -0.137 contr_diff*reward + 0.446 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.312 1 + 0.766 value_reward_not_chosen[t] + 0.021 contr_diff + 0.007 value_reward_not_chosen^2 + 0.008 value_reward_not_chosen*contr_diff + -0.239 contr_diff^2 \n",
            "value_choice[t+1] = 0.189 1 + 0.623 value_choice[t] + -0.028 contr_diff + 0.188 choice + -0.363 value_choice^2 + 0.094 value_choice*contr_diff + -0.377 value_choice*choice + -0.013 contr_diff^2 + -0.026 contr_diff*choice + 0.188 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 72/1000 --- L(Train): 0.4474217 --- L(Val, RNN): 0.4510014 --- L(Val, SINDy): 0.4999566 --- Time: 0.32s; --- Convergence: 8.04e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.583 1 + 0.168 value_reward_chosen[t] + 0.181 contr_diff + 0.447 reward + -0.379 value_reward_chosen^2 + 0.008 value_reward_chosen*contr_diff + -0.091 value_reward_chosen*reward + 0.399 contr_diff^2 + -0.146 contr_diff*reward + 0.448 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.321 1 + 0.767 value_reward_not_chosen[t] + 0.016 contr_diff + 0.013 value_reward_not_chosen^2 + 0.016 value_reward_not_chosen*contr_diff + -0.245 contr_diff^2 \n",
            "value_choice[t+1] = 0.19 1 + 0.623 value_choice[t] + -0.026 contr_diff + 0.189 choice + -0.363 value_choice^2 + 0.09 value_choice*contr_diff + -0.377 value_choice*choice + -0.012 contr_diff^2 + -0.024 contr_diff*choice + 0.188 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 73/1000 --- L(Train): 0.4560229 --- L(Val, RNN): 0.4501405 --- L(Val, SINDy): 0.4989503 --- Time: 0.26s; --- Convergence: 8.32e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.576 1 + 0.157 value_reward_chosen[t] + 0.193 contr_diff + 0.45 reward + -0.37 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + -0.074 value_reward_chosen*reward + 0.385 contr_diff^2 + -0.155 contr_diff*reward + 0.451 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.33 1 + 0.768 value_reward_not_chosen[t] + 0.011 contr_diff + 0.019 value_reward_not_chosen^2 + 0.023 value_reward_not_chosen*contr_diff + -0.251 contr_diff^2 \n",
            "value_choice[t+1] = 0.191 1 + 0.624 value_choice[t] + -0.024 contr_diff + 0.19 choice + -0.363 value_choice^2 + 0.085 value_choice*contr_diff + -0.377 value_choice*choice + -0.009 contr_diff^2 + -0.022 contr_diff*choice + 0.19 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 74/1000 --- L(Train): 0.4650185 --- L(Val, RNN): 0.4493070 --- L(Val, SINDy): 0.4944140 --- Time: 0.32s; --- Convergence: 8.33e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.568 1 + 0.146 value_reward_chosen[t] + 0.205 contr_diff + 0.453 reward + -0.362 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.058 value_reward_chosen*reward + 0.37 contr_diff^2 + -0.161 contr_diff*reward + 0.454 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.339 1 + 0.769 value_reward_not_chosen[t] + 0.01 contr_diff + 0.025 value_reward_not_chosen^2 + 0.027 value_reward_not_chosen*contr_diff + -0.257 contr_diff^2 \n",
            "value_choice[t+1] = 0.192 1 + 0.624 value_choice[t] + -0.024 contr_diff + 0.191 choice + -0.363 value_choice^2 + 0.08 value_choice*contr_diff + -0.377 value_choice*choice + -0.008 contr_diff^2 + -0.022 contr_diff*choice + 0.19 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 75/1000 --- L(Train): 0.4453644 --- L(Val, RNN): 0.4486074 --- L(Val, SINDy): 0.4874902 --- Time: 0.28s; --- Convergence: 7.66e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.557 1 + 0.133 value_reward_chosen[t] + 0.22 contr_diff + 0.452 reward + -0.358 value_reward_chosen^2 + -0.008 value_reward_chosen*contr_diff + -0.042 value_reward_chosen*reward + 0.354 contr_diff^2 + -0.164 contr_diff*reward + 0.453 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.347 1 + 0.771 value_reward_not_chosen[t] + 0.015 contr_diff + 0.031 value_reward_not_chosen^2 + 0.029 value_reward_not_chosen*contr_diff + -0.261 contr_diff^2 \n",
            "value_choice[t+1] = 0.19 1 + 0.621 value_choice[t] + -0.023 contr_diff + 0.189 choice + -0.365 value_choice^2 + 0.076 value_choice*contr_diff + -0.379 value_choice*choice + -0.011 contr_diff^2 + -0.022 contr_diff*choice + 0.188 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 76/1000 --- L(Train): 0.4503939 --- L(Val, RNN): 0.4479550 --- L(Val, SINDy): 0.4764342 --- Time: 0.43s; --- Convergence: 7.09e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.544 1 + 0.121 value_reward_chosen[t] + 0.233 contr_diff + 0.448 reward + -0.356 value_reward_chosen^2 + -0.014 value_reward_chosen*contr_diff + -0.027 value_reward_chosen*reward + 0.339 contr_diff^2 + -0.168 contr_diff*reward + 0.449 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.354 1 + 0.774 value_reward_not_chosen[t] + 0.019 contr_diff + 0.037 value_reward_not_chosen^2 + 0.03 value_reward_not_chosen*contr_diff + -0.265 contr_diff^2 \n",
            "value_choice[t+1] = 0.189 1 + 0.62 value_choice[t] + -0.023 contr_diff + 0.189 choice + -0.367 value_choice^2 + 0.071 value_choice*contr_diff + -0.38 value_choice*choice + -0.013 contr_diff^2 + -0.021 contr_diff*choice + 0.188 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 77/1000 --- L(Train): 0.4556122 --- L(Val, RNN): 0.4473152 --- L(Val, SINDy): 0.4591166 --- Time: 0.30s; --- Convergence: 6.75e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.53 1 + 0.108 value_reward_chosen[t] + 0.245 contr_diff + 0.442 reward + -0.356 value_reward_chosen^2 + -0.021 value_reward_chosen*contr_diff + -0.014 value_reward_chosen*reward + 0.324 contr_diff^2 + -0.172 contr_diff*reward + 0.443 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.36 1 + 0.777 value_reward_not_chosen[t] + 0.023 contr_diff + 0.043 value_reward_not_chosen^2 + 0.03 value_reward_not_chosen*contr_diff + -0.267 contr_diff^2 \n",
            "value_choice[t+1] = 0.191 1 + 0.62 value_choice[t] + -0.021 contr_diff + 0.19 choice + -0.366 value_choice^2 + 0.067 value_choice*contr_diff + -0.38 value_choice*choice + -0.013 contr_diff^2 + -0.02 contr_diff*choice + 0.19 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 78/1000 --- L(Train): 0.4553187 --- L(Val, RNN): 0.4465551 --- L(Val, SINDy): 0.4509085 --- Time: 0.34s; --- Convergence: 7.17e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.516 1 + 0.094 value_reward_chosen[t] + 0.254 contr_diff + 0.434 reward + -0.36 value_reward_chosen^2 + -0.029 value_reward_chosen*contr_diff + -0.001 value_reward_chosen*reward + 0.308 contr_diff^2 + -0.175 contr_diff*reward + 0.435 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.364 1 + 0.779 value_reward_not_chosen[t] + 0.026 contr_diff + 0.049 value_reward_not_chosen^2 + 0.03 value_reward_not_chosen*contr_diff + -0.266 contr_diff^2 \n",
            "value_choice[t+1] = 0.192 1 + 0.62 value_choice[t] + -0.019 contr_diff + 0.192 choice + -0.367 value_choice^2 + 0.064 value_choice*contr_diff + -0.381 value_choice*choice + -0.013 contr_diff^2 + -0.017 contr_diff*choice + 0.191 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 79/1000 --- L(Train): 0.4444123 --- L(Val, RNN): 0.4459319 --- L(Val, SINDy): 0.4646755 --- Time: 0.31s; --- Convergence: 6.70e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.5 1 + 0.081 value_reward_chosen[t] + 0.263 contr_diff + 0.426 reward + -0.365 value_reward_chosen^2 + -0.036 value_reward_chosen*contr_diff + 0.012 value_reward_chosen*reward + 0.292 contr_diff^2 + -0.18 contr_diff*reward + 0.427 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.365 1 + 0.782 value_reward_not_chosen[t] + 0.028 contr_diff + 0.056 value_reward_not_chosen^2 + 0.031 value_reward_not_chosen*contr_diff + -0.263 contr_diff^2 \n",
            "value_choice[t+1] = 0.191 1 + 0.618 value_choice[t] + -0.016 contr_diff + 0.191 choice + -0.369 value_choice^2 + 0.061 value_choice*contr_diff + -0.383 value_choice*choice + -0.015 contr_diff^2 + -0.015 contr_diff*choice + 0.19 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 80/1000 --- L(Train): 0.4421531 --- L(Val, RNN): 0.4453093 --- L(Val, SINDy): 0.4851328 --- Time: 0.38s; --- Convergence: 6.46e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.485 1 + 0.068 value_reward_chosen[t] + 0.271 contr_diff + 0.417 reward + -0.371 value_reward_chosen^2 + -0.043 value_reward_chosen*contr_diff + 0.025 value_reward_chosen*reward + 0.276 contr_diff^2 + -0.185 contr_diff*reward + 0.419 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.365 1 + 0.785 value_reward_not_chosen[t] + 0.028 contr_diff + 0.062 value_reward_not_chosen^2 + 0.031 value_reward_not_chosen*contr_diff + -0.258 contr_diff^2 \n",
            "value_choice[t+1] = 0.191 1 + 0.616 value_choice[t] + -0.015 contr_diff + 0.19 choice + -0.37 value_choice^2 + 0.057 value_choice*contr_diff + -0.384 value_choice*choice + -0.015 contr_diff^2 + -0.014 contr_diff*choice + 0.189 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 81/1000 --- L(Train): 0.4569927 --- L(Val, RNN): 0.4445345 --- L(Val, SINDy): 0.4940403 --- Time: 0.41s; --- Convergence: 7.11e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.468 1 + 0.055 value_reward_chosen[t] + 0.278 contr_diff + 0.409 reward + -0.378 value_reward_chosen^2 + -0.049 value_reward_chosen*contr_diff + 0.037 value_reward_chosen*reward + 0.259 contr_diff^2 + -0.192 contr_diff*reward + 0.41 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.363 1 + 0.788 value_reward_not_chosen[t] + 0.027 contr_diff + 0.067 value_reward_not_chosen^2 + 0.033 value_reward_not_chosen*contr_diff + -0.25 contr_diff^2 \n",
            "value_choice[t+1] = 0.193 1 + 0.616 value_choice[t] + -0.017 contr_diff + 0.192 choice + -0.37 value_choice^2 + 0.053 value_choice*contr_diff + -0.384 value_choice*choice + -0.013 contr_diff^2 + -0.015 contr_diff*choice + 0.191 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 82/1000 --- L(Train): 0.4429391 --- L(Val, RNN): 0.4439043 --- L(Val, SINDy): 0.5201632 --- Time: 0.30s; --- Convergence: 6.70e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.453 1 + 0.044 value_reward_chosen[t] + 0.284 contr_diff + 0.403 reward + -0.384 value_reward_chosen^2 + -0.055 value_reward_chosen*contr_diff + 0.05 value_reward_chosen*reward + 0.244 contr_diff^2 + -0.2 contr_diff*reward + 0.404 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.361 1 + 0.791 value_reward_not_chosen[t] + 0.026 contr_diff + 0.071 value_reward_not_chosen^2 + 0.034 value_reward_not_chosen*contr_diff + -0.243 contr_diff^2 \n",
            "value_choice[t+1] = 0.194 1 + 0.616 value_choice[t] + -0.018 contr_diff + 0.193 choice + -0.37 value_choice^2 + 0.048 value_choice*contr_diff + -0.384 value_choice*choice + -0.011 contr_diff^2 + -0.016 contr_diff*choice + 0.192 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 83/1000 --- L(Train): 0.4479432 --- L(Val, RNN): 0.4431575 --- L(Val, SINDy): 0.5480376 --- Time: 0.36s; --- Convergence: 7.09e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.438 1 + 0.034 value_reward_chosen[t] + 0.289 contr_diff + 0.4 reward + -0.386 value_reward_chosen^2 + -0.061 value_reward_chosen*contr_diff + 0.065 value_reward_chosen*reward + 0.229 contr_diff^2 + -0.209 contr_diff*reward + 0.401 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.358 1 + 0.796 value_reward_not_chosen[t] + 0.025 contr_diff + 0.074 value_reward_not_chosen^2 + 0.035 value_reward_not_chosen*contr_diff + -0.235 contr_diff^2 \n",
            "value_choice[t+1] = 0.192 1 + 0.614 value_choice[t] + -0.017 contr_diff + 0.192 choice + -0.372 value_choice^2 + 0.045 value_choice*contr_diff + -0.386 value_choice*choice + -0.011 contr_diff^2 + -0.016 contr_diff*choice + 0.191 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 84/1000 --- L(Train): 0.4582210 --- L(Val, RNN): 0.4425463 --- L(Val, SINDy): 0.5663981 --- Time: 0.32s; --- Convergence: 6.60e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.426 1 + 0.027 value_reward_chosen[t] + 0.293 contr_diff + 0.401 reward + -0.386 value_reward_chosen^2 + -0.067 value_reward_chosen*contr_diff + 0.08 value_reward_chosen*reward + 0.215 contr_diff^2 + -0.218 contr_diff*reward + 0.402 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.356 1 + 0.801 value_reward_not_chosen[t] + 0.024 contr_diff + 0.075 value_reward_not_chosen^2 + 0.037 value_reward_not_chosen*contr_diff + -0.228 contr_diff^2 \n",
            "value_choice[t+1] = 0.191 1 + 0.612 value_choice[t] + -0.016 contr_diff + 0.19 choice + -0.374 value_choice^2 + 0.042 value_choice*contr_diff + -0.388 value_choice*choice + -0.01 contr_diff^2 + -0.015 contr_diff*choice + 0.189 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 85/1000 --- L(Train): 0.4576381 --- L(Val, RNN): 0.4418695 --- L(Val, SINDy): 0.5749636 --- Time: 0.34s; --- Convergence: 6.68e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.413 1 + 0.02 value_reward_chosen[t] + 0.295 contr_diff + 0.403 reward + -0.384 value_reward_chosen^2 + -0.072 value_reward_chosen*contr_diff + 0.095 value_reward_chosen*reward + 0.202 contr_diff^2 + -0.227 contr_diff*reward + 0.404 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.353 1 + 0.807 value_reward_not_chosen[t] + 0.022 contr_diff + 0.074 value_reward_not_chosen^2 + 0.038 value_reward_not_chosen*contr_diff + -0.22 contr_diff^2 \n",
            "value_choice[t+1] = 0.19 1 + 0.611 value_choice[t] + -0.013 contr_diff + 0.189 choice + -0.375 value_choice^2 + 0.041 value_choice*contr_diff + -0.39 value_choice*choice + -0.009 contr_diff^2 + -0.011 contr_diff*choice + 0.188 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 86/1000 --- L(Train): 0.4538013 --- L(Val, RNN): 0.4412059 --- L(Val, SINDy): 0.5729835 --- Time: 0.28s; --- Convergence: 6.66e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.4 1 + 0.014 value_reward_chosen[t] + 0.294 contr_diff + 0.407 reward + -0.384 value_reward_chosen^2 + -0.081 value_reward_chosen*contr_diff + 0.112 value_reward_chosen*reward + 0.188 contr_diff^2 + -0.238 contr_diff*reward + 0.408 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.351 1 + 0.815 value_reward_not_chosen[t] + 0.018 contr_diff + 0.071 value_reward_not_chosen^2 + 0.037 value_reward_not_chosen*contr_diff + -0.211 contr_diff^2 \n",
            "value_choice[t+1] = 0.191 1 + 0.612 value_choice[t] + -0.009 contr_diff + 0.19 choice + -0.375 value_choice^2 + 0.039 value_choice*contr_diff + -0.389 value_choice*choice + -0.006 contr_diff^2 + -0.008 contr_diff*choice + 0.189 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 87/1000 --- L(Train): 0.4425117 --- L(Val, RNN): 0.4406227 --- L(Val, SINDy): 0.5626475 --- Time: 0.32s; --- Convergence: 6.25e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.386 1 + 0.008 value_reward_chosen[t] + 0.291 contr_diff + 0.412 reward + -0.384 value_reward_chosen^2 + -0.088 value_reward_chosen*contr_diff + 0.127 value_reward_chosen*reward + 0.174 contr_diff^2 + -0.25 contr_diff*reward + 0.413 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.351 1 + 0.824 value_reward_not_chosen[t] + 0.014 contr_diff + 0.065 value_reward_not_chosen^2 + 0.038 value_reward_not_chosen*contr_diff + -0.203 contr_diff^2 \n",
            "value_choice[t+1] = 0.189 1 + 0.611 value_choice[t] + -0.007 contr_diff + 0.188 choice + -0.376 value_choice^2 + 0.036 value_choice*contr_diff + -0.39 value_choice*choice + -0.005 contr_diff^2 + -0.005 contr_diff*choice + 0.187 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 88/1000 --- L(Train): 0.4312031 --- L(Val, RNN): 0.4401194 --- L(Val, SINDy): 0.5466264 --- Time: 0.31s; --- Convergence: 5.64e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.371 1 + 0.002 value_reward_chosen[t] + 0.286 contr_diff + 0.414 reward + -0.389 value_reward_chosen^2 + -0.092 value_reward_chosen*contr_diff + 0.141 value_reward_chosen*reward + 0.158 contr_diff^2 + -0.263 contr_diff*reward + 0.416 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.351 1 + 0.835 value_reward_not_chosen[t] + 0.01 contr_diff + 0.059 value_reward_not_chosen^2 + 0.037 value_reward_not_chosen*contr_diff + -0.194 contr_diff^2 \n",
            "value_choice[t+1] = 0.186 1 + 0.61 value_choice[t] + -0.01 contr_diff + 0.186 choice + -0.376 value_choice^2 + 0.03 value_choice*contr_diff + -0.391 value_choice*choice + -0.004 contr_diff^2 + -0.008 contr_diff*choice + 0.185 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 89/1000 --- L(Train): 0.4386407 --- L(Val, RNN): 0.4397748 --- L(Val, SINDy): 0.5218393 --- Time: 0.29s; --- Convergence: 4.54e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.354 1 + -0.007 value_reward_chosen[t] + 0.28 contr_diff + 0.415 reward + -0.397 value_reward_chosen^2 + -0.098 value_reward_chosen*contr_diff + 0.15 value_reward_chosen*reward + 0.142 contr_diff^2 + -0.27 contr_diff*reward + 0.416 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.352 1 + 0.843 value_reward_not_chosen[t] + 0.005 contr_diff + 0.054 value_reward_not_chosen^2 + 0.034 value_reward_not_chosen*contr_diff + -0.185 contr_diff^2 \n",
            "value_choice[t+1] = 0.185 1 + 0.611 value_choice[t] + -0.011 contr_diff + 0.185 choice + -0.375 value_choice^2 + 0.024 value_choice*contr_diff + -0.39 value_choice*choice + -0.002 contr_diff^2 + -0.009 contr_diff*choice + 0.184 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 90/1000 --- L(Train): 0.4435754 --- L(Val, RNN): 0.4393274 --- L(Val, SINDy): 0.4876657 --- Time: 0.30s; --- Convergence: 4.51e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.338 1 + -0.015 value_reward_chosen[t] + 0.275 contr_diff + 0.415 reward + -0.405 value_reward_chosen^2 + -0.101 value_reward_chosen*contr_diff + 0.158 value_reward_chosen*reward + 0.126 contr_diff^2 + -0.277 contr_diff*reward + 0.416 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.352 1 + 0.849 value_reward_not_chosen[t] + 0.002 contr_diff + 0.052 value_reward_not_chosen^2 + 0.031 value_reward_not_chosen*contr_diff + -0.177 contr_diff^2 \n",
            "value_choice[t+1] = 0.182 1 + 0.611 value_choice[t] + -0.01 contr_diff + 0.182 choice + -0.376 value_choice^2 + 0.019 value_choice*contr_diff + -0.39 value_choice*choice + -0.001 contr_diff^2 + -0.009 contr_diff*choice + 0.181 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 91/1000 --- L(Train): 0.4481532 --- L(Val, RNN): 0.4388371 --- L(Val, SINDy): 0.4644855 --- Time: 0.34s; --- Convergence: 4.71e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.321 1 + -0.024 value_reward_chosen[t] + 0.267 contr_diff + 0.415 reward + -0.414 value_reward_chosen^2 + -0.101 value_reward_chosen*contr_diff + 0.164 value_reward_chosen*reward + 0.111 contr_diff^2 + -0.286 contr_diff*reward + 0.416 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.352 1 + 0.853 value_reward_not_chosen[t] + 0.001 contr_diff + 0.052 value_reward_not_chosen^2 + 0.027 value_reward_not_chosen*contr_diff + -0.168 contr_diff^2 \n",
            "value_choice[t+1] = 0.177 1 + 0.609 value_choice[t] + -0.006 contr_diff + 0.176 choice + -0.377 value_choice^2 + 0.016 value_choice*contr_diff + -0.391 value_choice*choice + -0.003 contr_diff^2 + -0.004 contr_diff*choice + 0.176 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 92/1000 --- L(Train): 0.4434887 --- L(Val, RNN): 0.4383529 --- L(Val, SINDy): 0.4519696 --- Time: 0.26s; --- Convergence: 4.77e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.304 1 + -0.034 value_reward_chosen[t] + 0.261 contr_diff + 0.417 reward + -0.423 value_reward_chosen^2 + -0.103 value_reward_chosen*contr_diff + 0.169 value_reward_chosen*reward + 0.094 contr_diff^2 + -0.29 contr_diff*reward + 0.418 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.353 1 + 0.854 value_reward_not_chosen[t] + -0.001 contr_diff + 0.055 value_reward_not_chosen^2 + 0.021 value_reward_not_chosen*contr_diff + -0.161 contr_diff^2 \n",
            "value_choice[t+1] = 0.173 1 + 0.609 value_choice[t] + -0.001 contr_diff + 0.172 choice + -0.377 value_choice^2 + 0.011 value_choice*contr_diff + -0.391 value_choice*choice + -0.003 contr_diff^2 + 0.001 contr_diff*choice + 0.172 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 93/1000 --- L(Train): 0.4449249 --- L(Val, RNN): 0.4377396 --- L(Val, SINDy): 0.4441273 --- Time: 0.33s; --- Convergence: 5.45e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.287 1 + -0.042 value_reward_chosen[t] + 0.254 contr_diff + 0.422 reward + -0.43 value_reward_chosen^2 + -0.104 value_reward_chosen*contr_diff + 0.174 value_reward_chosen*reward + 0.078 contr_diff^2 + -0.293 contr_diff*reward + 0.423 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.354 1 + 0.853 value_reward_not_chosen[t] + -0.004 contr_diff + 0.059 value_reward_not_chosen^2 + 0.014 value_reward_not_chosen*contr_diff + -0.154 contr_diff^2 \n",
            "value_choice[t+1] = 0.172 1 + 0.613 value_choice[t] + -0.001 contr_diff + 0.172 choice + -0.374 value_choice^2 + 0.005 value_choice*contr_diff + -0.388 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.171 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 94/1000 --- L(Train): 0.4370372 --- L(Val, RNN): 0.4374948 --- L(Val, SINDy): 0.4415874 --- Time: 0.33s; --- Convergence: 3.95e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.272 1 + -0.049 value_reward_chosen[t] + 0.247 contr_diff + 0.429 reward + -0.435 value_reward_chosen^2 + -0.103 value_reward_chosen*contr_diff + 0.182 value_reward_chosen*reward + 0.063 contr_diff^2 + -0.296 contr_diff*reward + 0.43 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.357 1 + 0.852 value_reward_not_chosen[t] + -0.006 contr_diff + 0.063 value_reward_not_chosen^2 + 0.007 value_reward_not_chosen*contr_diff + -0.148 contr_diff^2 \n",
            "value_choice[t+1] = 0.169 1 + 0.615 value_choice[t] + -0.003 contr_diff + 0.168 choice + -0.372 value_choice^2 + -0.003 value_choice*contr_diff + -0.386 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.168 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 95/1000 --- L(Train): 0.4378573 --- L(Val, RNN): 0.4371090 --- L(Val, SINDy): 0.4401305 --- Time: 0.40s; --- Convergence: 3.90e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.257 1 + -0.055 value_reward_chosen[t] + 0.24 contr_diff + 0.439 reward + -0.438 value_reward_chosen^2 + -0.1 value_reward_chosen*contr_diff + 0.189 value_reward_chosen*reward + 0.047 contr_diff^2 + -0.299 contr_diff*reward + 0.44 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.36 1 + 0.851 value_reward_not_chosen[t] + -0.009 contr_diff + 0.065 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.142 contr_diff^2 \n",
            "value_choice[t+1] = 0.162 1 + 0.613 value_choice[t] + -0.003 contr_diff + 0.161 choice + -0.373 value_choice^2 + -0.01 value_choice*contr_diff + -0.387 value_choice*choice + -0.005 contr_diff^2 + -0.001 contr_diff*choice + 0.16 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 96/1000 --- L(Train): 0.4439186 --- L(Val, RNN): 0.4368362 --- L(Val, SINDy): 0.4451838 --- Time: 0.38s; --- Convergence: 3.32e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.242 1 + -0.061 value_reward_chosen[t] + 0.231 contr_diff + 0.449 reward + -0.438 value_reward_chosen^2 + -0.097 value_reward_chosen*contr_diff + 0.197 value_reward_chosen*reward + 0.033 contr_diff^2 + -0.304 contr_diff*reward + 0.45 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.365 1 + 0.851 value_reward_not_chosen[t] + -0.012 contr_diff + 0.067 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + -0.137 contr_diff^2 \n",
            "value_choice[t+1] = 0.155 1 + 0.614 value_choice[t] + -0.004 contr_diff + 0.154 choice + -0.372 value_choice^2 + -0.017 value_choice*contr_diff + -0.387 value_choice*choice + -0.009 contr_diff^2 + -0.002 contr_diff*choice + 0.154 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 97/1000 --- L(Train): 0.4372693 --- L(Val, RNN): 0.4363948 --- L(Val, SINDy): 0.4519212 --- Time: 0.40s; --- Convergence: 3.87e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.228 1 + -0.065 value_reward_chosen[t] + 0.222 contr_diff + 0.461 reward + -0.438 value_reward_chosen^2 + -0.095 value_reward_chosen*contr_diff + 0.204 value_reward_chosen*reward + 0.018 contr_diff^2 + -0.307 contr_diff*reward + 0.462 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.37 1 + 0.852 value_reward_not_chosen[t] + -0.017 contr_diff + 0.068 value_reward_not_chosen^2 + -0.009 value_reward_not_chosen*contr_diff + -0.132 contr_diff^2 \n",
            "value_choice[t+1] = 0.147 1 + 0.614 value_choice[t] + -0.002 contr_diff + 0.146 choice + -0.372 value_choice^2 + -0.023 value_choice*contr_diff + -0.387 value_choice*choice + -0.014 contr_diff^2 + -0.0 contr_diff*choice + 0.145 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 98/1000 --- L(Train): 0.4328293 --- L(Val, RNN): 0.4357787 --- L(Val, SINDy): 0.4539100 --- Time: 0.26s; --- Convergence: 5.01e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.214 1 + -0.069 value_reward_chosen[t] + 0.211 contr_diff + 0.474 reward + -0.435 value_reward_chosen^2 + -0.091 value_reward_chosen*contr_diff + 0.212 value_reward_chosen*reward + 0.004 contr_diff^2 + -0.312 contr_diff*reward + 0.475 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.375 1 + 0.852 value_reward_not_chosen[t] + -0.022 contr_diff + 0.069 value_reward_not_chosen^2 + -0.011 value_reward_not_chosen*contr_diff + -0.127 contr_diff^2 \n",
            "value_choice[t+1] = 0.136 1 + 0.614 value_choice[t] + 0.0 contr_diff + 0.136 choice + -0.372 value_choice^2 + -0.031 value_choice*contr_diff + -0.387 value_choice*choice + -0.022 contr_diff^2 + 0.002 contr_diff*choice + 0.135 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 99/1000 --- L(Train): 0.4350249 --- L(Val, RNN): 0.4352398 --- L(Val, SINDy): 0.4546262 --- Time: 0.27s; --- Convergence: 5.20e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.202 1 + -0.072 value_reward_chosen[t] + 0.203 contr_diff + 0.488 reward + -0.43 value_reward_chosen^2 + -0.089 value_reward_chosen*contr_diff + 0.22 value_reward_chosen*reward + -0.009 contr_diff^2 + -0.312 contr_diff*reward + 0.489 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.379 1 + 0.852 value_reward_not_chosen[t] + -0.028 contr_diff + 0.069 value_reward_not_chosen^2 + -0.012 value_reward_not_chosen*contr_diff + -0.121 contr_diff^2 \n",
            "value_choice[t+1] = 0.133 1 + 0.619 value_choice[t] + 0.003 contr_diff + 0.132 choice + -0.367 value_choice^2 + -0.039 value_choice*contr_diff + -0.381 value_choice*choice + -0.023 contr_diff^2 + 0.005 contr_diff*choice + 0.131 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 100/1000 --- L(Train): 0.4346382 --- L(Val, RNN): 0.4346573 --- L(Val, SINDy): 0.4542471 --- Time: 0.29s; --- Convergence: 5.51e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.19 1 + -0.074 value_reward_chosen[t] + 0.197 contr_diff + 0.502 reward + -0.423 value_reward_chosen^2 + -0.089 value_reward_chosen*contr_diff + 0.228 value_reward_chosen*reward + -0.022 contr_diff^2 + -0.307 contr_diff*reward + 0.503 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.382 1 + 0.85 value_reward_not_chosen[t] + -0.036 contr_diff + 0.07 value_reward_not_chosen^2 + -0.013 value_reward_not_chosen*contr_diff + -0.114 contr_diff^2 \n",
            "value_choice[t+1] = 0.128 1 + 0.625 value_choice[t] + 0.001 contr_diff + 0.127 choice + -0.362 value_choice^2 + -0.049 value_choice*contr_diff + -0.375 value_choice*choice + -0.024 contr_diff^2 + 0.003 contr_diff*choice + 0.127 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 101/1000 --- L(Train): 0.4307017 --- L(Val, RNN): 0.4342442 --- L(Val, SINDy): 0.4523962 --- Time: 0.26s; --- Convergence: 4.82e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.176 1 + -0.077 value_reward_chosen[t] + 0.19 contr_diff + 0.514 reward + -0.415 value_reward_chosen^2 + -0.087 value_reward_chosen*contr_diff + 0.233 value_reward_chosen*reward + -0.037 contr_diff^2 + -0.3 contr_diff*reward + 0.515 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.384 1 + 0.847 value_reward_not_chosen[t] + -0.043 contr_diff + 0.071 value_reward_not_chosen^2 + -0.015 value_reward_not_chosen*contr_diff + -0.104 contr_diff^2 \n",
            "value_choice[t+1] = 0.118 1 + 0.626 value_choice[t] + -0.001 contr_diff + 0.117 choice + -0.36 value_choice^2 + -0.062 value_choice*contr_diff + -0.374 value_choice*choice + -0.033 contr_diff^2 + 0.001 contr_diff*choice + 0.117 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 102/1000 --- L(Train): 0.4394529 --- L(Val, RNN): 0.4339401 --- L(Val, SINDy): 0.4529564 --- Time: 0.34s; --- Convergence: 3.93e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.162 1 + -0.081 value_reward_chosen[t] + 0.182 contr_diff + 0.523 reward + -0.409 value_reward_chosen^2 + -0.082 value_reward_chosen*contr_diff + 0.234 value_reward_chosen*reward + -0.051 contr_diff^2 + -0.294 contr_diff*reward + 0.525 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.384 1 + 0.844 value_reward_not_chosen[t] + -0.047 contr_diff + 0.073 value_reward_not_chosen^2 + -0.018 value_reward_not_chosen*contr_diff + -0.094 contr_diff^2 \n",
            "value_choice[t+1] = 0.105 1 + 0.623 value_choice[t] + -0.003 contr_diff + 0.104 choice + -0.361 value_choice^2 + -0.076 value_choice*contr_diff + -0.377 value_choice*choice + -0.044 contr_diff^2 + -0.001 contr_diff*choice + 0.104 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 103/1000 --- L(Train): 0.4509105 --- L(Val, RNN): 0.4334889 --- L(Val, SINDy): 0.4531675 --- Time: 0.32s; --- Convergence: 4.22e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.148 1 + -0.085 value_reward_chosen[t] + 0.176 contr_diff + 0.531 reward + -0.404 value_reward_chosen^2 + -0.08 value_reward_chosen*contr_diff + 0.235 value_reward_chosen*reward + -0.065 contr_diff^2 + -0.284 contr_diff*reward + 0.532 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.384 1 + 0.842 value_reward_not_chosen[t] + -0.05 contr_diff + 0.073 value_reward_not_chosen^2 + -0.021 value_reward_not_chosen*contr_diff + -0.085 contr_diff^2 \n",
            "value_choice[t+1] = 0.089 1 + 0.617 value_choice[t] + -0.009 contr_diff + 0.089 choice + -0.365 value_choice^2 + -0.092 value_choice*contr_diff + -0.384 value_choice*choice + -0.059 contr_diff^2 + -0.007 contr_diff*choice + 0.088 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 104/1000 --- L(Train): 0.4335071 --- L(Val, RNN): 0.4330659 --- L(Val, SINDy): 0.4593209 --- Time: 0.29s; --- Convergence: 4.23e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.134 1 + -0.089 value_reward_chosen[t] + 0.169 contr_diff + 0.536 reward + -0.399 value_reward_chosen^2 + -0.076 value_reward_chosen*contr_diff + 0.234 value_reward_chosen*reward + -0.079 contr_diff^2 + -0.273 contr_diff*reward + 0.537 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.385 1 + 0.841 value_reward_not_chosen[t] + -0.052 contr_diff + 0.071 value_reward_not_chosen^2 + -0.026 value_reward_not_chosen*contr_diff + -0.075 contr_diff^2 \n",
            "value_choice[t+1] = 0.075 1 + 0.605 value_choice[t] + -0.02 contr_diff + 0.074 choice + -0.371 value_choice^2 + -0.108 value_choice*contr_diff + -0.395 value_choice*choice + -0.072 contr_diff^2 + -0.018 contr_diff*choice + 0.073 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 105/1000 --- L(Train): 0.4493624 --- L(Val, RNN): 0.4326563 --- L(Val, SINDy): 0.4648770 --- Time: 0.43s; --- Convergence: 4.16e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.121 1 + -0.092 value_reward_chosen[t] + 0.163 contr_diff + 0.54 reward + -0.394 value_reward_chosen^2 + -0.071 value_reward_chosen*contr_diff + 0.233 value_reward_chosen*reward + -0.093 contr_diff^2 + -0.262 contr_diff*reward + 0.541 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.386 1 + 0.842 value_reward_not_chosen[t] + -0.053 contr_diff + 0.067 value_reward_not_chosen^2 + -0.031 value_reward_not_chosen*contr_diff + -0.065 contr_diff^2 \n",
            "value_choice[t+1] = 0.058 1 + 0.592 value_choice[t] + -0.031 contr_diff + 0.057 choice + -0.379 value_choice^2 + -0.124 value_choice*contr_diff + -0.409 value_choice*choice + -0.087 contr_diff^2 + -0.029 contr_diff*choice + 0.057 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 106/1000 --- L(Train): 0.4409307 --- L(Val, RNN): 0.4322682 --- L(Val, SINDy): 0.4681438 --- Time: 0.39s; --- Convergence: 4.02e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.108 1 + -0.095 value_reward_chosen[t] + 0.157 contr_diff + 0.543 reward + -0.387 value_reward_chosen^2 + -0.066 value_reward_chosen*contr_diff + 0.231 value_reward_chosen*reward + -0.106 contr_diff^2 + -0.251 contr_diff*reward + 0.544 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.387 1 + 0.843 value_reward_not_chosen[t] + -0.054 contr_diff + 0.062 value_reward_not_chosen^2 + -0.036 value_reward_not_chosen*contr_diff + -0.056 contr_diff^2 \n",
            "value_choice[t+1] = 0.047 1 + 0.587 value_choice[t] + -0.042 contr_diff + 0.047 choice + -0.381 value_choice^2 + -0.138 value_choice*contr_diff + -0.413 value_choice*choice + -0.099 contr_diff^2 + -0.04 contr_diff*choice + 0.046 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 107/1000 --- L(Train): 0.4322470 --- L(Val, RNN): 0.4319495 --- L(Val, SINDy): 0.4651862 --- Time: 0.27s; --- Convergence: 3.60e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.096 1 + -0.096 value_reward_chosen[t] + 0.152 contr_diff + 0.545 reward + -0.379 value_reward_chosen^2 + -0.061 value_reward_chosen*contr_diff + 0.23 value_reward_chosen*reward + -0.117 contr_diff^2 + -0.24 contr_diff*reward + 0.546 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.386 1 + 0.842 value_reward_not_chosen[t] + -0.056 contr_diff + 0.058 value_reward_not_chosen^2 + -0.039 value_reward_not_chosen*contr_diff + -0.046 contr_diff^2 \n",
            "value_choice[t+1] = 0.046 1 + 0.592 value_choice[t] + -0.045 contr_diff + 0.045 choice + -0.374 value_choice^2 + -0.144 value_choice*contr_diff + -0.408 value_choice*choice + -0.1 contr_diff^2 + -0.043 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 108/1000 --- L(Train): 0.4345914 --- L(Val, RNN): 0.4316612 --- L(Val, SINDy): 0.4553468 --- Time: 0.27s; --- Convergence: 3.24e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.086 1 + -0.096 value_reward_chosen[t] + 0.149 contr_diff + 0.549 reward + -0.368 value_reward_chosen^2 + -0.06 value_reward_chosen*contr_diff + 0.231 value_reward_chosen*reward + -0.128 contr_diff^2 + -0.226 contr_diff*reward + 0.55 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.383 1 + 0.84 value_reward_not_chosen[t] + -0.058 contr_diff + 0.056 value_reward_not_chosen^2 + -0.04 value_reward_not_chosen*contr_diff + -0.035 contr_diff^2 \n",
            "value_choice[t+1] = 0.052 1 + 0.603 value_choice[t] + -0.04 contr_diff + 0.051 choice + -0.362 value_choice^2 + -0.142 value_choice*contr_diff + -0.397 value_choice*choice + -0.095 contr_diff^2 + -0.038 contr_diff*choice + 0.051 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 109/1000 --- L(Train): 0.4267519 --- L(Val, RNN): 0.4314463 --- L(Val, SINDy): 0.4443780 --- Time: 0.39s; --- Convergence: 2.70e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.077 1 + -0.093 value_reward_chosen[t] + 0.143 contr_diff + 0.556 reward + -0.354 value_reward_chosen^2 + -0.058 value_reward_chosen*contr_diff + 0.234 value_reward_chosen*reward + -0.137 contr_diff^2 + -0.217 contr_diff*reward + 0.557 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.378 1 + 0.836 value_reward_not_chosen[t] + -0.059 contr_diff + 0.053 value_reward_not_chosen^2 + -0.038 value_reward_not_chosen*contr_diff + -0.023 contr_diff^2 \n",
            "value_choice[t+1] = 0.062 1 + 0.617 value_choice[t] + -0.031 contr_diff + 0.061 choice + -0.347 value_choice^2 + -0.133 value_choice*contr_diff + -0.383 value_choice*choice + -0.086 contr_diff^2 + -0.029 contr_diff*choice + 0.061 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 110/1000 --- L(Train): 0.4381388 --- L(Val, RNN): 0.4311614 --- L(Val, SINDy): 0.4379698 --- Time: 0.30s; --- Convergence: 2.77e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.07 1 + -0.089 value_reward_chosen[t] + 0.137 contr_diff + 0.563 reward + -0.34 value_reward_chosen^2 + -0.058 value_reward_chosen*contr_diff + 0.237 value_reward_chosen*reward + -0.145 contr_diff^2 + -0.207 contr_diff*reward + 0.564 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.373 1 + 0.832 value_reward_not_chosen[t] + -0.06 contr_diff + 0.05 value_reward_not_chosen^2 + -0.037 value_reward_not_chosen*contr_diff + -0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.072 1 + 0.631 value_choice[t] + -0.022 contr_diff + 0.072 choice + -0.333 value_choice^2 + -0.125 value_choice*contr_diff + -0.369 value_choice*choice + -0.076 contr_diff^2 + -0.02 contr_diff*choice + 0.071 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 111/1000 --- L(Train): 0.4378349 --- L(Val, RNN): 0.4307224 --- L(Val, SINDy): 0.4333133 --- Time: 0.27s; --- Convergence: 3.58e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.064 1 + -0.085 value_reward_chosen[t] + 0.133 contr_diff + 0.569 reward + -0.323 value_reward_chosen^2 + -0.061 value_reward_chosen*contr_diff + 0.24 value_reward_chosen*reward + -0.152 contr_diff^2 + -0.196 contr_diff*reward + 0.571 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.368 1 + 0.83 value_reward_not_chosen[t] + -0.061 contr_diff + 0.045 value_reward_not_chosen^2 + -0.036 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.079 1 + 0.642 value_choice[t] + -0.018 contr_diff + 0.078 choice + -0.321 value_choice^2 + -0.12 value_choice*contr_diff + -0.358 value_choice*choice + -0.071 contr_diff^2 + -0.016 contr_diff*choice + 0.077 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 112/1000 --- L(Train): 0.4425742 --- L(Val, RNN): 0.4303711 --- L(Val, SINDy): 0.4323485 --- Time: 0.33s; --- Convergence: 3.55e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.057 1 + -0.082 value_reward_chosen[t] + 0.131 contr_diff + 0.574 reward + -0.308 value_reward_chosen^2 + -0.065 value_reward_chosen*contr_diff + 0.24 value_reward_chosen*reward + -0.159 contr_diff^2 + -0.183 contr_diff*reward + 0.575 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.364 1 + 0.831 value_reward_not_chosen[t] + -0.062 contr_diff + 0.037 value_reward_not_chosen^2 + -0.036 value_reward_not_chosen*contr_diff + 0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.079 1 + 0.652 value_choice[t] + -0.018 contr_diff + 0.079 choice + -0.31 value_choice^2 + -0.117 value_choice*contr_diff + -0.349 value_choice*choice + -0.071 contr_diff^2 + -0.016 contr_diff*choice + 0.078 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 113/1000 --- L(Train): 0.4372530 --- L(Val, RNN): 0.4301387 --- L(Val, SINDy): 0.4319152 --- Time: 0.32s; --- Convergence: 2.94e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.049 1 + -0.08 value_reward_chosen[t] + 0.129 contr_diff + 0.574 reward + -0.294 value_reward_chosen^2 + -0.069 value_reward_chosen*contr_diff + 0.236 value_reward_chosen*reward + -0.165 contr_diff^2 + -0.171 contr_diff*reward + 0.575 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.361 1 + 0.832 value_reward_not_chosen[t] + -0.061 contr_diff + 0.03 value_reward_not_chosen^2 + -0.036 value_reward_not_chosen*contr_diff + 0.021 contr_diff^2 \n",
            "value_choice[t+1] = 0.078 1 + 0.659 value_choice[t] + -0.021 contr_diff + 0.077 choice + -0.301 value_choice^2 + -0.117 value_choice*contr_diff + -0.341 value_choice*choice + -0.073 contr_diff^2 + -0.019 contr_diff*choice + 0.077 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 114/1000 --- L(Train): 0.4242670 --- L(Val, RNN): 0.4299813 --- L(Val, SINDy): 0.4315016 --- Time: 0.40s; --- Convergence: 2.25e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.04 1 + -0.082 value_reward_chosen[t] + 0.127 contr_diff + 0.569 reward + -0.282 value_reward_chosen^2 + -0.068 value_reward_chosen*contr_diff + 0.228 value_reward_chosen*reward + -0.173 contr_diff^2 + -0.159 contr_diff*reward + 0.57 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.359 1 + 0.832 value_reward_not_chosen[t] + -0.058 contr_diff + 0.023 value_reward_not_chosen^2 + -0.035 value_reward_not_chosen*contr_diff + 0.029 contr_diff^2 \n",
            "value_choice[t+1] = 0.078 1 + 0.667 value_choice[t] + -0.025 contr_diff + 0.078 choice + -0.291 value_choice^2 + -0.117 value_choice*contr_diff + -0.333 value_choice*choice + -0.074 contr_diff^2 + -0.023 contr_diff*choice + 0.077 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 115/1000 --- L(Train): 0.4313454 --- L(Val, RNN): 0.4298492 --- L(Val, SINDy): 0.4312450 --- Time: 0.29s; --- Convergence: 1.79e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.031 1 + -0.085 value_reward_chosen[t] + 0.127 contr_diff + 0.562 reward + -0.27 value_reward_chosen^2 + -0.07 value_reward_chosen*contr_diff + 0.218 value_reward_chosen*reward + -0.179 contr_diff^2 + -0.146 contr_diff*reward + 0.563 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.358 1 + 0.828 value_reward_not_chosen[t] + -0.054 contr_diff + 0.02 value_reward_not_chosen^2 + -0.034 value_reward_not_chosen*contr_diff + 0.035 contr_diff^2 \n",
            "value_choice[t+1] = 0.081 1 + 0.678 value_choice[t] + -0.027 contr_diff + 0.081 choice + -0.278 value_choice^2 + -0.114 value_choice*contr_diff + -0.322 value_choice*choice + -0.072 contr_diff^2 + -0.025 contr_diff*choice + 0.08 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 116/1000 --- L(Train): 0.4346165 --- L(Val, RNN): 0.4294195 --- L(Val, SINDy): 0.4310642 --- Time: 0.28s; --- Convergence: 3.04e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.022 1 + -0.09 value_reward_chosen[t] + 0.126 contr_diff + 0.555 reward + -0.257 value_reward_chosen^2 + -0.072 value_reward_chosen*contr_diff + 0.207 value_reward_chosen*reward + -0.185 contr_diff^2 + -0.134 contr_diff*reward + 0.557 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.36 1 + 0.822 value_reward_not_chosen[t] + -0.047 contr_diff + 0.018 value_reward_not_chosen^2 + -0.034 value_reward_not_chosen*contr_diff + 0.038 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.691 value_choice[t] + -0.027 contr_diff + 0.086 choice + -0.263 value_choice^2 + -0.109 value_choice*contr_diff + -0.309 value_choice*choice + -0.068 contr_diff^2 + -0.025 contr_diff*choice + 0.085 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 117/1000 --- L(Train): 0.4353333 --- L(Val, RNN): 0.4289296 --- L(Val, SINDy): 0.4311359 --- Time: 0.34s; --- Convergence: 3.97e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.013 1 + -0.093 value_reward_chosen[t] + 0.127 contr_diff + 0.548 reward + -0.243 value_reward_chosen^2 + -0.073 value_reward_chosen*contr_diff + 0.196 value_reward_chosen*reward + -0.191 contr_diff^2 + -0.121 contr_diff*reward + 0.549 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.363 1 + 0.813 value_reward_not_chosen[t] + -0.042 contr_diff + 0.019 value_reward_not_chosen^2 + -0.031 value_reward_not_chosen*contr_diff + 0.04 contr_diff^2 \n",
            "value_choice[t+1] = 0.093 1 + 0.705 value_choice[t] + -0.027 contr_diff + 0.092 choice + -0.248 value_choice^2 + -0.104 value_choice*contr_diff + -0.296 value_choice*choice + -0.064 contr_diff^2 + -0.025 contr_diff*choice + 0.091 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 118/1000 --- L(Train): 0.4322488 --- L(Val, RNN): 0.4285614 --- L(Val, SINDy): 0.4352957 --- Time: 0.28s; --- Convergence: 3.83e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.003 1 + -0.096 value_reward_chosen[t] + 0.128 contr_diff + 0.541 reward + -0.229 value_reward_chosen^2 + -0.07 value_reward_chosen*contr_diff + 0.185 value_reward_chosen*reward + -0.197 contr_diff^2 + -0.11 contr_diff*reward + 0.542 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.37 1 + 0.808 value_reward_not_chosen[t] + -0.037 contr_diff + 0.02 value_reward_not_chosen^2 + -0.024 value_reward_not_chosen*contr_diff + 0.036 contr_diff^2 \n",
            "value_choice[t+1] = 0.1 1 + 0.718 value_choice[t] + -0.023 contr_diff + 0.099 choice + -0.232 value_choice^2 + -0.094 value_choice*contr_diff + -0.282 value_choice*choice + -0.058 contr_diff^2 + -0.021 contr_diff*choice + 0.098 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 119/1000 --- L(Train): 0.4302878 --- L(Val, RNN): 0.4282877 --- L(Val, SINDy): 0.4434967 --- Time: 0.36s; --- Convergence: 3.28e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.007 1 + -0.098 value_reward_chosen[t] + 0.127 contr_diff + 0.535 reward + -0.215 value_reward_chosen^2 + -0.066 value_reward_chosen*contr_diff + 0.173 value_reward_chosen*reward + -0.203 contr_diff^2 + -0.101 contr_diff*reward + 0.536 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.38 1 + 0.803 value_reward_not_chosen[t] + -0.031 contr_diff + 0.019 value_reward_not_chosen^2 + -0.017 value_reward_not_chosen*contr_diff + 0.03 contr_diff^2 \n",
            "value_choice[t+1] = 0.105 1 + 0.729 value_choice[t] + -0.02 contr_diff + 0.104 choice + -0.22 value_choice^2 + -0.084 value_choice*contr_diff + -0.271 value_choice*choice + -0.053 contr_diff^2 + -0.018 contr_diff*choice + 0.103 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 120/1000 --- L(Train): 0.4281724 --- L(Val, RNN): 0.4281348 --- L(Val, SINDy): 0.4478947 --- Time: 0.33s; --- Convergence: 2.41e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.018 1 + -0.098 value_reward_chosen[t] + 0.128 contr_diff + 0.529 reward + -0.202 value_reward_chosen^2 + -0.063 value_reward_chosen*contr_diff + 0.161 value_reward_chosen*reward + -0.209 contr_diff^2 + -0.091 contr_diff*reward + 0.53 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.39 1 + 0.8 value_reward_not_chosen[t] + -0.027 contr_diff + 0.02 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + 0.024 contr_diff^2 \n",
            "value_choice[t+1] = 0.105 1 + 0.734 value_choice[t] + -0.022 contr_diff + 0.105 choice + -0.214 value_choice^2 + -0.08 value_choice*contr_diff + -0.267 value_choice*choice + -0.052 contr_diff^2 + -0.02 contr_diff*choice + 0.104 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 121/1000 --- L(Train): 0.4243205 --- L(Val, RNN): 0.4278749 --- L(Val, SINDy): 0.4499896 --- Time: 0.29s; --- Convergence: 2.50e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.028 1 + -0.098 value_reward_chosen[t] + 0.129 contr_diff + 0.525 reward + -0.19 value_reward_chosen^2 + -0.061 value_reward_chosen*contr_diff + 0.152 value_reward_chosen*reward + -0.215 contr_diff^2 + -0.082 contr_diff*reward + 0.526 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.398 1 + 0.796 value_reward_not_chosen[t] + -0.024 contr_diff + 0.021 value_reward_not_chosen^2 + 0.004 value_reward_not_chosen*contr_diff + 0.018 contr_diff^2 \n",
            "value_choice[t+1] = 0.104 1 + 0.736 value_choice[t] + -0.025 contr_diff + 0.103 choice + -0.211 value_choice^2 + -0.078 value_choice*contr_diff + -0.264 value_choice*choice + -0.052 contr_diff^2 + -0.023 contr_diff*choice + 0.102 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 122/1000 --- L(Train): 0.4295344 --- L(Val, RNN): 0.4275958 --- L(Val, SINDy): 0.4506048 --- Time: 0.33s; --- Convergence: 2.65e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.039 1 + -0.097 value_reward_chosen[t] + 0.128 contr_diff + 0.523 reward + -0.179 value_reward_chosen^2 + -0.057 value_reward_chosen*contr_diff + 0.143 value_reward_chosen*reward + -0.222 contr_diff^2 + -0.075 contr_diff*reward + 0.524 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.404 1 + 0.791 value_reward_not_chosen[t] + -0.024 contr_diff + 0.024 value_reward_not_chosen^2 + 0.013 value_reward_not_chosen*contr_diff + 0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.099 1 + 0.735 value_choice[t] + -0.034 contr_diff + 0.098 choice + -0.212 value_choice^2 + -0.082 value_choice*contr_diff + -0.266 value_choice*choice + -0.057 contr_diff^2 + -0.032 contr_diff*choice + 0.098 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 123/1000 --- L(Train): 0.4317959 --- L(Val, RNN): 0.4274233 --- L(Val, SINDy): 0.4497562 --- Time: 0.29s; --- Convergence: 2.19e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.05 1 + -0.095 value_reward_chosen[t] + 0.125 contr_diff + 0.52 reward + -0.17 value_reward_chosen^2 + -0.051 value_reward_chosen*contr_diff + 0.134 value_reward_chosen*reward + -0.229 contr_diff^2 + -0.071 contr_diff*reward + 0.521 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.406 1 + 0.787 value_reward_not_chosen[t] + -0.022 contr_diff + 0.027 value_reward_not_chosen^2 + 0.023 value_reward_not_chosen*contr_diff + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.092 1 + 0.731 value_choice[t] + -0.045 contr_diff + 0.092 choice + -0.214 value_choice^2 + -0.089 value_choice*contr_diff + -0.27 value_choice*choice + -0.064 contr_diff^2 + -0.043 contr_diff*choice + 0.091 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 124/1000 --- L(Train): 0.4286711 --- L(Val, RNN): 0.4273368 --- L(Val, SINDy): 0.4479733 --- Time: 0.31s; --- Convergence: 1.53e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.063 1 + -0.092 value_reward_chosen[t] + 0.12 contr_diff + 0.52 reward + -0.162 value_reward_chosen^2 + -0.044 value_reward_chosen*contr_diff + 0.127 value_reward_chosen*reward + -0.237 contr_diff^2 + -0.069 contr_diff*reward + 0.521 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.405 1 + 0.787 value_reward_not_chosen[t] + -0.017 contr_diff + 0.026 value_reward_not_chosen^2 + 0.033 value_reward_not_chosen*contr_diff + 0.016 contr_diff^2 \n",
            "value_choice[t+1] = 0.083 1 + 0.724 value_choice[t] + -0.056 contr_diff + 0.083 choice + -0.22 value_choice^2 + -0.096 value_choice*contr_diff + -0.276 value_choice*choice + -0.073 contr_diff^2 + -0.054 contr_diff*choice + 0.082 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 125/1000 --- L(Train): 0.4282466 --- L(Val, RNN): 0.4270607 --- L(Val, SINDy): 0.4459067 --- Time: 0.30s; --- Convergence: 2.14e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.073 1 + -0.087 value_reward_chosen[t] + 0.113 contr_diff + 0.524 reward + -0.152 value_reward_chosen^2 + -0.037 value_reward_chosen*contr_diff + 0.121 value_reward_chosen*reward + -0.245 contr_diff^2 + -0.069 contr_diff*reward + 0.525 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.401 1 + 0.788 value_reward_not_chosen[t] + -0.013 contr_diff + 0.025 value_reward_not_chosen^2 + 0.04 value_reward_not_chosen*contr_diff + 0.02 contr_diff^2 \n",
            "value_choice[t+1] = 0.075 1 + 0.718 value_choice[t] + -0.065 contr_diff + 0.074 choice + -0.225 value_choice^2 + -0.1 value_choice*contr_diff + -0.282 value_choice*choice + -0.082 contr_diff^2 + -0.063 contr_diff*choice + 0.074 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 126/1000 --- L(Train): 0.4322739 --- L(Val, RNN): 0.4266230 --- L(Val, SINDy): 0.4401176 --- Time: 0.29s; --- Convergence: 3.26e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.08 1 + -0.081 value_reward_chosen[t] + 0.107 contr_diff + 0.53 reward + -0.141 value_reward_chosen^2 + -0.031 value_reward_chosen*contr_diff + 0.118 value_reward_chosen*reward + -0.251 contr_diff^2 + -0.07 contr_diff*reward + 0.531 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.396 1 + 0.791 value_reward_not_chosen[t] + -0.01 contr_diff + 0.022 value_reward_not_chosen^2 + 0.048 value_reward_not_chosen*contr_diff + 0.024 contr_diff^2 \n",
            "value_choice[t+1] = 0.068 1 + 0.713 value_choice[t] + -0.072 contr_diff + 0.067 choice + -0.229 value_choice^2 + -0.104 value_choice*contr_diff + -0.287 value_choice*choice + -0.089 contr_diff^2 + -0.07 contr_diff*choice + 0.066 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 127/1000 --- L(Train): 0.4362848 --- L(Val, RNN): 0.4264973 --- L(Val, SINDy): 0.4306632 --- Time: 0.30s; --- Convergence: 2.26e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.087 1 + -0.075 value_reward_chosen[t] + 0.099 contr_diff + 0.537 reward + -0.13 value_reward_chosen^2 + -0.024 value_reward_chosen*contr_diff + 0.117 value_reward_chosen*reward + -0.257 contr_diff^2 + -0.072 contr_diff*reward + 0.538 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.391 1 + 0.795 value_reward_not_chosen[t] + -0.006 contr_diff + 0.018 value_reward_not_chosen^2 + 0.049 value_reward_not_chosen*contr_diff + 0.029 contr_diff^2 \n",
            "value_choice[t+1] = 0.062 1 + 0.71 value_choice[t] + -0.077 contr_diff + 0.062 choice + -0.231 value_choice^2 + -0.105 value_choice*contr_diff + -0.29 value_choice*choice + -0.095 contr_diff^2 + -0.075 contr_diff*choice + 0.061 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 128/1000 --- L(Train): 0.4282090 --- L(Val, RNN): 0.4267028 --- L(Val, SINDy): 0.4285772 --- Time: 0.27s; --- Convergence: 2.16e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.094 1 + -0.068 value_reward_chosen[t] + 0.093 contr_diff + 0.545 reward + -0.12 value_reward_chosen^2 + -0.019 value_reward_chosen*contr_diff + 0.116 value_reward_chosen*reward + -0.263 contr_diff^2 + -0.074 contr_diff*reward + 0.546 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.386 1 + 0.8 value_reward_not_chosen[t] + -0.0 contr_diff + 0.013 value_reward_not_chosen^2 + 0.044 value_reward_not_chosen*contr_diff + 0.033 contr_diff^2 \n",
            "value_choice[t+1] = 0.058 1 + 0.708 value_choice[t] + -0.077 contr_diff + 0.057 choice + -0.232 value_choice^2 + -0.102 value_choice*contr_diff + -0.292 value_choice*choice + -0.098 contr_diff^2 + -0.075 contr_diff*choice + 0.057 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 129/1000 --- L(Train): 0.4274055 --- L(Val, RNN): 0.4267165 --- L(Val, SINDy): 0.4283766 --- Time: 0.27s; --- Convergence: 1.15e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.101 1 + -0.062 value_reward_chosen[t] + 0.087 contr_diff + 0.552 reward + -0.112 value_reward_chosen^2 + -0.015 value_reward_chosen*contr_diff + 0.116 value_reward_chosen*reward + -0.268 contr_diff^2 + -0.075 contr_diff*reward + 0.553 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.38 1 + 0.803 value_reward_not_chosen[t] + 0.007 contr_diff + 0.011 value_reward_not_chosen^2 + 0.038 value_reward_not_chosen*contr_diff + 0.037 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.707 value_choice[t] + -0.077 contr_diff + 0.055 choice + -0.233 value_choice^2 + -0.099 value_choice*contr_diff + -0.293 value_choice*choice + -0.102 contr_diff^2 + -0.075 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 130/1000 --- L(Train): 0.4253938 --- L(Val, RNN): 0.4263670 --- L(Val, SINDy): 0.4282960 --- Time: 0.30s; --- Convergence: 2.32e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.108 1 + -0.056 value_reward_chosen[t] + 0.081 contr_diff + 0.56 reward + -0.103 value_reward_chosen^2 + -0.013 value_reward_chosen*contr_diff + 0.114 value_reward_chosen*reward + -0.273 contr_diff^2 + -0.074 contr_diff*reward + 0.561 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.373 1 + 0.806 value_reward_not_chosen[t] + 0.011 contr_diff + 0.011 value_reward_not_chosen^2 + 0.029 value_reward_not_chosen*contr_diff + 0.041 contr_diff^2 \n",
            "value_choice[t+1] = 0.053 1 + 0.707 value_choice[t] + -0.076 contr_diff + 0.053 choice + -0.232 value_choice^2 + -0.094 value_choice*contr_diff + -0.293 value_choice*choice + -0.103 contr_diff^2 + -0.074 contr_diff*choice + 0.052 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 131/1000 --- L(Train): 0.4171816 --- L(Val, RNN): 0.4260103 --- L(Val, SINDy): 0.4280583 --- Time: 0.25s; --- Convergence: 2.94e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.115 1 + -0.051 value_reward_chosen[t] + 0.077 contr_diff + 0.567 reward + -0.096 value_reward_chosen^2 + -0.013 value_reward_chosen*contr_diff + 0.113 value_reward_chosen*reward + -0.277 contr_diff^2 + -0.072 contr_diff*reward + 0.569 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.366 1 + 0.808 value_reward_not_chosen[t] + 0.013 contr_diff + 0.012 value_reward_not_chosen^2 + 0.022 value_reward_not_chosen*contr_diff + 0.044 contr_diff^2 \n",
            "value_choice[t+1] = 0.053 1 + 0.708 value_choice[t] + -0.073 contr_diff + 0.053 choice + -0.231 value_choice^2 + -0.089 value_choice*contr_diff + -0.292 value_choice*choice + -0.104 contr_diff^2 + -0.071 contr_diff*choice + 0.052 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 132/1000 --- L(Train): 0.4260331 --- L(Val, RNN): 0.4257545 --- L(Val, SINDy): 0.4279267 --- Time: 0.27s; --- Convergence: 2.75e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.122 1 + -0.046 value_reward_chosen[t] + 0.073 contr_diff + 0.574 reward + -0.089 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + 0.11 value_reward_chosen*reward + -0.282 contr_diff^2 + -0.071 contr_diff*reward + 0.576 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.361 1 + 0.812 value_reward_not_chosen[t] + 0.014 contr_diff + 0.012 value_reward_not_chosen^2 + 0.017 value_reward_not_chosen*contr_diff + 0.047 contr_diff^2 \n",
            "value_choice[t+1] = 0.054 1 + 0.711 value_choice[t] + -0.07 contr_diff + 0.053 choice + -0.228 value_choice^2 + -0.083 value_choice*contr_diff + -0.29 value_choice*choice + -0.104 contr_diff^2 + -0.068 contr_diff*choice + 0.053 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 133/1000 --- L(Train): 0.4242075 --- L(Val, RNN): 0.4257019 --- L(Val, SINDy): 0.4278682 --- Time: 0.32s; --- Convergence: 1.64e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.128 1 + -0.042 value_reward_chosen[t] + 0.068 contr_diff + 0.582 reward + -0.082 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + 0.106 value_reward_chosen*reward + -0.285 contr_diff^2 + -0.069 contr_diff*reward + 0.583 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.358 1 + 0.817 value_reward_not_chosen[t] + 0.01 contr_diff + 0.01 value_reward_not_chosen^2 + 0.014 value_reward_not_chosen*contr_diff + 0.046 contr_diff^2 \n",
            "value_choice[t+1] = 0.054 1 + 0.713 value_choice[t] + -0.067 contr_diff + 0.054 choice + -0.226 value_choice^2 + -0.077 value_choice*contr_diff + -0.288 value_choice*choice + -0.104 contr_diff^2 + -0.065 contr_diff*choice + 0.053 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 134/1000 --- L(Train): 0.4422088 --- L(Val, RNN): 0.4256090 --- L(Val, SINDy): 0.4278813 --- Time: 0.39s; --- Convergence: 1.28e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.135 1 + -0.039 value_reward_chosen[t] + 0.064 contr_diff + 0.589 reward + -0.076 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + 0.102 value_reward_chosen*reward + -0.289 contr_diff^2 + -0.067 contr_diff*reward + 0.59 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.356 1 + 0.822 value_reward_not_chosen[t] + 0.005 contr_diff + 0.008 value_reward_not_chosen^2 + 0.012 value_reward_not_chosen*contr_diff + 0.046 contr_diff^2 \n",
            "value_choice[t+1] = 0.053 1 + 0.714 value_choice[t] + -0.065 contr_diff + 0.053 choice + -0.225 value_choice^2 + -0.072 value_choice*contr_diff + -0.287 value_choice*choice + -0.105 contr_diff^2 + -0.063 contr_diff*choice + 0.052 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 135/1000 --- L(Train): 0.4199951 --- L(Val, RNN): 0.4256228 --- L(Val, SINDy): 0.4274709 --- Time: 0.35s; --- Convergence: 7.11e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.141 1 + -0.038 value_reward_chosen[t] + 0.06 contr_diff + 0.594 reward + -0.071 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + 0.095 value_reward_chosen*reward + -0.291 contr_diff^2 + -0.065 contr_diff*reward + 0.595 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.355 1 + 0.823 value_reward_not_chosen[t] + 0.001 contr_diff + 0.011 value_reward_not_chosen^2 + 0.011 value_reward_not_chosen*contr_diff + 0.044 contr_diff^2 \n",
            "value_choice[t+1] = 0.048 1 + 0.713 value_choice[t] + -0.064 contr_diff + 0.047 choice + -0.224 value_choice^2 + -0.067 value_choice*contr_diff + -0.287 value_choice*choice + -0.107 contr_diff^2 + -0.062 contr_diff*choice + 0.047 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 136/1000 --- L(Train): 0.4389881 --- L(Val, RNN): 0.4254986 --- L(Val, SINDy): 0.4271480 --- Time: 0.31s; --- Convergence: 9.76e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.148 1 + -0.038 value_reward_chosen[t] + 0.057 contr_diff + 0.598 reward + -0.066 value_reward_chosen^2 + -0.008 value_reward_chosen*contr_diff + 0.087 value_reward_chosen*reward + -0.294 contr_diff^2 + -0.062 contr_diff*reward + 0.599 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.358 1 + 0.824 value_reward_not_chosen[t] + -0.002 contr_diff + 0.015 value_reward_not_chosen^2 + 0.011 value_reward_not_chosen*contr_diff + 0.038 contr_diff^2 \n",
            "value_choice[t+1] = 0.045 1 + 0.714 value_choice[t] + -0.062 contr_diff + 0.044 choice + -0.223 value_choice^2 + -0.062 value_choice*contr_diff + -0.287 value_choice*choice + -0.106 contr_diff^2 + -0.06 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 137/1000 --- L(Train): 0.4266598 --- L(Val, RNN): 0.4259706 --- L(Val, SINDy): 0.4269003 --- Time: 0.30s; --- Convergence: 2.85e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + -0.039 value_reward_chosen[t] + 0.055 contr_diff + 0.598 reward + -0.064 value_reward_chosen^2 + -0.006 value_reward_chosen*contr_diff + 0.077 value_reward_chosen*reward + -0.297 contr_diff^2 + -0.059 contr_diff*reward + 0.599 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.364 1 + 0.825 value_reward_not_chosen[t] + -0.002 contr_diff + 0.018 value_reward_not_chosen^2 + 0.009 value_reward_not_chosen*contr_diff + 0.029 contr_diff^2 \n",
            "value_choice[t+1] = 0.045 1 + 0.718 value_choice[t] + -0.057 contr_diff + 0.044 choice + -0.218 value_choice^2 + -0.054 value_choice*contr_diff + -0.282 value_choice*choice + -0.101 contr_diff^2 + -0.055 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 138/1000 --- L(Train): 0.4318669 --- L(Val, RNN): 0.4251741 --- L(Val, SINDy): 0.4268750 --- Time: 0.35s; --- Convergence: 5.41e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + -0.042 value_reward_chosen[t] + 0.054 contr_diff + 0.596 reward + -0.066 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + 0.066 value_reward_chosen*reward + -0.302 contr_diff^2 + -0.054 contr_diff*reward + 0.597 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.372 1 + 0.826 value_reward_not_chosen[t] + -0.002 contr_diff + 0.023 value_reward_not_chosen^2 + 0.01 value_reward_not_chosen*contr_diff + 0.018 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.726 value_choice[t] + -0.049 contr_diff + 0.049 choice + -0.208 value_choice^2 + -0.041 value_choice*contr_diff + -0.274 value_choice*choice + -0.092 contr_diff^2 + -0.047 contr_diff*choice + 0.048 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 139/1000 --- L(Train): 0.4259146 --- L(Val, RNN): 0.4250957 --- L(Val, SINDy): 0.4270321 --- Time: 0.37s; --- Convergence: 3.10e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.179 1 + -0.045 value_reward_chosen[t] + 0.053 contr_diff + 0.591 reward + -0.07 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + 0.054 value_reward_chosen*reward + -0.308 contr_diff^2 + -0.049 contr_diff*reward + 0.592 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.381 1 + 0.825 value_reward_not_chosen[t] + 0.001 contr_diff + 0.03 value_reward_not_chosen^2 + 0.01 value_reward_not_chosen*contr_diff + 0.006 contr_diff^2 \n",
            "value_choice[t+1] = 0.057 1 + 0.737 value_choice[t] + -0.039 contr_diff + 0.056 choice + -0.196 value_choice^2 + -0.027 value_choice*contr_diff + -0.263 value_choice*choice + -0.081 contr_diff^2 + -0.037 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 140/1000 --- L(Train): 0.4317952 --- L(Val, RNN): 0.4250991 --- L(Val, SINDy): 0.4272116 --- Time: 0.34s; --- Convergence: 1.56e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.189 1 + -0.046 value_reward_chosen[t] + 0.053 contr_diff + 0.59 reward + -0.071 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.045 value_reward_chosen*reward + -0.312 contr_diff^2 + -0.044 contr_diff*reward + 0.591 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.39 1 + 0.823 value_reward_not_chosen[t] + 0.004 contr_diff + 0.038 value_reward_not_chosen^2 + 0.012 value_reward_not_chosen*contr_diff + -0.006 contr_diff^2 \n",
            "value_choice[t+1] = 0.067 1 + 0.75 value_choice[t] + -0.028 contr_diff + 0.066 choice + -0.183 value_choice^2 + -0.011 value_choice*contr_diff + -0.25 value_choice*choice + -0.068 contr_diff^2 + -0.026 contr_diff*choice + 0.065 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 141/1000 --- L(Train): 0.4171645 --- L(Val, RNN): 0.4249962 --- L(Val, SINDy): 0.4270561 --- Time: 0.32s; --- Convergence: 1.30e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.196 1 + -0.044 value_reward_chosen[t] + 0.052 contr_diff + 0.594 reward + -0.069 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + 0.04 value_reward_chosen*reward + -0.313 contr_diff^2 + -0.038 contr_diff*reward + 0.595 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.397 1 + 0.821 value_reward_not_chosen[t] + 0.005 contr_diff + 0.044 value_reward_not_chosen^2 + 0.013 value_reward_not_chosen*contr_diff + -0.015 contr_diff^2 \n",
            "value_choice[t+1] = 0.076 1 + 0.763 value_choice[t] + -0.019 contr_diff + 0.075 choice + -0.169 value_choice^2 + 0.004 value_choice*contr_diff + -0.238 value_choice*choice + -0.055 contr_diff^2 + -0.017 contr_diff*choice + 0.075 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 142/1000 --- L(Train): 0.4288554 --- L(Val, RNN): 0.4248386 --- L(Val, SINDy): 0.4269867 --- Time: 0.29s; --- Convergence: 1.44e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.201 1 + -0.041 value_reward_chosen[t] + 0.052 contr_diff + 0.599 reward + -0.065 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + 0.036 value_reward_chosen*reward + -0.314 contr_diff^2 + -0.032 contr_diff*reward + 0.6 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.402 1 + 0.819 value_reward_not_chosen[t] + 0.002 contr_diff + 0.05 value_reward_not_chosen^2 + 0.015 value_reward_not_chosen*contr_diff + -0.022 contr_diff^2 \n",
            "value_choice[t+1] = 0.084 1 + 0.773 value_choice[t] + -0.013 contr_diff + 0.083 choice + -0.158 value_choice^2 + 0.017 value_choice*contr_diff + -0.227 value_choice*choice + -0.043 contr_diff^2 + -0.011 contr_diff*choice + 0.083 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 143/1000 --- L(Train): 0.4391021 --- L(Val, RNN): 0.4246628 --- L(Val, SINDy): 0.4269206 --- Time: 0.33s; --- Convergence: 1.60e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.205 1 + -0.037 value_reward_chosen[t] + 0.053 contr_diff + 0.605 reward + -0.061 value_reward_chosen^2 + -0.006 value_reward_chosen*contr_diff + 0.034 value_reward_chosen*reward + -0.314 contr_diff^2 + -0.025 contr_diff*reward + 0.606 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.405 1 + 0.818 value_reward_not_chosen[t] + -0.002 contr_diff + 0.052 value_reward_not_chosen^2 + 0.018 value_reward_not_chosen*contr_diff + -0.027 contr_diff^2 \n",
            "value_choice[t+1] = 0.09 1 + 0.781 value_choice[t] + -0.01 contr_diff + 0.089 choice + -0.15 value_choice^2 + 0.028 value_choice*contr_diff + -0.22 value_choice*choice + -0.033 contr_diff^2 + -0.008 contr_diff*choice + 0.089 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 144/1000 --- L(Train): 0.4153901 --- L(Val, RNN): 0.4245745 --- L(Val, SINDy): 0.4268695 --- Time: 0.36s; --- Convergence: 1.24e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.209 1 + -0.032 value_reward_chosen[t] + 0.054 contr_diff + 0.61 reward + -0.058 value_reward_chosen^2 + -0.008 value_reward_chosen*contr_diff + 0.033 value_reward_chosen*reward + -0.314 contr_diff^2 + -0.02 contr_diff*reward + 0.612 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.405 1 + 0.82 value_reward_not_chosen[t] + -0.007 contr_diff + 0.052 value_reward_not_chosen^2 + 0.02 value_reward_not_chosen*contr_diff + -0.03 contr_diff^2 \n",
            "value_choice[t+1] = 0.093 1 + 0.784 value_choice[t] + -0.008 contr_diff + 0.092 choice + -0.146 value_choice^2 + 0.037 value_choice*contr_diff + -0.216 value_choice*choice + -0.025 contr_diff^2 + -0.006 contr_diff*choice + 0.091 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 145/1000 --- L(Train): 0.4264938 --- L(Val, RNN): 0.4244175 --- L(Val, SINDy): 0.4267601 --- Time: 0.29s; --- Convergence: 1.41e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.214 1 + -0.028 value_reward_chosen[t] + 0.056 contr_diff + 0.616 reward + -0.054 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + 0.031 value_reward_chosen*reward + -0.314 contr_diff^2 + -0.013 contr_diff*reward + 0.617 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.4 1 + 0.821 value_reward_not_chosen[t] + -0.014 contr_diff + 0.053 value_reward_not_chosen^2 + 0.022 value_reward_not_chosen*contr_diff + -0.026 contr_diff^2 \n",
            "value_choice[t+1] = 0.093 1 + 0.784 value_choice[t] + -0.013 contr_diff + 0.092 choice + -0.146 value_choice^2 + 0.039 value_choice*contr_diff + -0.216 value_choice*choice + -0.024 contr_diff^2 + -0.011 contr_diff*choice + 0.091 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 146/1000 --- L(Train): 0.4288618 --- L(Val, RNN): 0.4242767 --- L(Val, SINDy): 0.4263284 --- Time: 0.27s; --- Convergence: 1.41e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.222 1 + -0.025 value_reward_chosen[t] + 0.06 contr_diff + 0.618 reward + -0.054 value_reward_chosen^2 + -0.012 value_reward_chosen*contr_diff + 0.029 value_reward_chosen*reward + -0.316 contr_diff^2 + -0.007 contr_diff*reward + 0.619 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.394 1 + 0.822 value_reward_not_chosen[t] + -0.019 contr_diff + 0.052 value_reward_not_chosen^2 + 0.019 value_reward_not_chosen*contr_diff + -0.021 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.78 value_choice[t] + -0.021 contr_diff + 0.088 choice + -0.149 value_choice^2 + 0.036 value_choice*contr_diff + -0.22 value_choice*choice + -0.027 contr_diff^2 + -0.019 contr_diff*choice + 0.087 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 147/1000 --- L(Train): 0.4329032 --- L(Val, RNN): 0.4241627 --- L(Val, SINDy): 0.4261231 --- Time: 0.29s; --- Convergence: 1.27e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.232 1 + -0.025 value_reward_chosen[t] + 0.064 contr_diff + 0.614 reward + -0.059 value_reward_chosen^2 + -0.012 value_reward_chosen*contr_diff + 0.023 value_reward_chosen*reward + -0.32 contr_diff^2 + -0.001 contr_diff*reward + 0.615 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.391 1 + 0.832 value_reward_not_chosen[t] + -0.019 contr_diff + 0.043 value_reward_not_chosen^2 + 0.013 value_reward_not_chosen*contr_diff + -0.016 contr_diff^2 \n",
            "value_choice[t+1] = 0.08 1 + 0.771 value_choice[t] + -0.032 contr_diff + 0.079 choice + -0.158 value_choice^2 + 0.031 value_choice*contr_diff + -0.229 value_choice*choice + -0.032 contr_diff^2 + -0.03 contr_diff*choice + 0.078 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 148/1000 --- L(Train): 0.4270363 --- L(Val, RNN): 0.4239640 --- L(Val, SINDy): 0.4259952 --- Time: 0.29s; --- Convergence: 1.63e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.242 1 + -0.024 value_reward_chosen[t] + 0.069 contr_diff + 0.613 reward + -0.062 value_reward_chosen^2 + -0.012 value_reward_chosen*contr_diff + 0.019 value_reward_chosen*reward + -0.322 contr_diff^2 + 0.003 contr_diff*reward + 0.614 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.386 1 + 0.837 value_reward_not_chosen[t] + -0.015 contr_diff + 0.037 value_reward_not_chosen^2 + 0.005 value_reward_not_chosen*contr_diff + -0.009 contr_diff^2 \n",
            "value_choice[t+1] = 0.069 1 + 0.761 value_choice[t] + -0.045 contr_diff + 0.069 choice + -0.167 value_choice^2 + 0.025 value_choice*contr_diff + -0.24 value_choice*choice + -0.039 contr_diff^2 + -0.043 contr_diff*choice + 0.068 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 149/1000 --- L(Train): 0.4234207 --- L(Val, RNN): 0.4238535 --- L(Val, SINDy): 0.4257830 --- Time: 0.36s; --- Convergence: 1.37e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.248 1 + -0.022 value_reward_chosen[t] + 0.073 contr_diff + 0.615 reward + -0.061 value_reward_chosen^2 + -0.015 value_reward_chosen*contr_diff + 0.017 value_reward_chosen*reward + -0.321 contr_diff^2 + 0.006 contr_diff*reward + 0.616 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.379 1 + 0.839 value_reward_not_chosen[t] + -0.011 contr_diff + 0.035 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.059 1 + 0.751 value_choice[t] + -0.057 contr_diff + 0.059 choice + -0.177 value_choice^2 + 0.017 value_choice*contr_diff + -0.25 value_choice*choice + -0.047 contr_diff^2 + -0.055 contr_diff*choice + 0.058 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 150/1000 --- L(Train): 0.4197873 --- L(Val, RNN): 0.4237430 --- L(Val, SINDy): 0.4255485 --- Time: 0.34s; --- Convergence: 1.24e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.252 1 + -0.017 value_reward_chosen[t] + 0.077 contr_diff + 0.62 reward + -0.058 value_reward_chosen^2 + -0.019 value_reward_chosen*contr_diff + 0.018 value_reward_chosen*reward + -0.319 contr_diff^2 + 0.008 contr_diff*reward + 0.621 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.373 1 + 0.836 value_reward_not_chosen[t] + -0.01 contr_diff + 0.036 value_reward_not_chosen^2 + -0.008 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.741 value_choice[t] + -0.069 contr_diff + 0.05 choice + -0.185 value_choice^2 + 0.012 value_choice*contr_diff + -0.259 value_choice*choice + -0.054 contr_diff^2 + -0.067 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 151/1000 --- L(Train): 0.4311903 --- L(Val, RNN): 0.4236260 --- L(Val, SINDy): 0.4253558 --- Time: 0.33s; --- Convergence: 1.20e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.255 1 + -0.011 value_reward_chosen[t] + 0.08 contr_diff + 0.626 reward + -0.054 value_reward_chosen^2 + -0.022 value_reward_chosen*contr_diff + 0.02 value_reward_chosen*reward + -0.316 contr_diff^2 + 0.005 contr_diff*reward + 0.628 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.371 1 + 0.833 value_reward_not_chosen[t] + -0.01 contr_diff + 0.039 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.045 1 + 0.735 value_choice[t] + -0.076 contr_diff + 0.044 choice + -0.191 value_choice^2 + 0.01 value_choice*contr_diff + -0.265 value_choice*choice + -0.058 contr_diff^2 + -0.074 contr_diff*choice + 0.043 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 152/1000 --- L(Train): 0.4217816 --- L(Val, RNN): 0.4234618 --- L(Val, SINDy): 0.4252649 --- Time: 0.31s; --- Convergence: 1.42e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.256 1 + -0.005 value_reward_chosen[t] + 0.082 contr_diff + 0.634 reward + -0.048 value_reward_chosen^2 + -0.023 value_reward_chosen*contr_diff + 0.024 value_reward_chosen*reward + -0.311 contr_diff^2 + 0.001 contr_diff*reward + 0.635 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.375 1 + 0.834 value_reward_not_chosen[t] + -0.01 contr_diff + 0.038 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.042 1 + 0.732 value_choice[t] + -0.079 contr_diff + 0.041 choice + -0.193 value_choice^2 + 0.011 value_choice*contr_diff + -0.268 value_choice*choice + -0.059 contr_diff^2 + -0.077 contr_diff*choice + 0.041 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 153/1000 --- L(Train): 0.4198630 --- L(Val, RNN): 0.4233152 --- L(Val, SINDy): 0.4252117 --- Time: 0.28s; --- Convergence: 1.44e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.257 1 + 0.0 value_reward_chosen[t] + 0.084 contr_diff + 0.642 reward + -0.042 value_reward_chosen^2 + -0.024 value_reward_chosen*contr_diff + 0.027 value_reward_chosen*reward + -0.307 contr_diff^2 + -0.004 contr_diff*reward + 0.643 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.379 1 + 0.835 value_reward_not_chosen[t] + -0.009 contr_diff + 0.036 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.04 1 + 0.73 value_choice[t] + -0.082 contr_diff + 0.039 choice + -0.195 value_choice^2 + 0.013 value_choice*contr_diff + -0.27 value_choice*choice + -0.059 contr_diff^2 + -0.08 contr_diff*choice + 0.039 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 154/1000 --- L(Train): 0.4220170 --- L(Val, RNN): 0.4231625 --- L(Val, SINDy): 0.4251746 --- Time: 0.31s; --- Convergence: 1.49e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.257 1 + 0.005 value_reward_chosen[t] + 0.086 contr_diff + 0.649 reward + -0.036 value_reward_chosen^2 + -0.027 value_reward_chosen*contr_diff + 0.03 value_reward_chosen*reward + -0.301 contr_diff^2 + -0.007 contr_diff*reward + 0.65 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.385 1 + 0.837 value_reward_not_chosen[t] + -0.009 contr_diff + 0.033 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.04 1 + 0.73 value_choice[t] + -0.082 contr_diff + 0.039 choice + -0.196 value_choice^2 + 0.016 value_choice*contr_diff + -0.271 value_choice*choice + -0.058 contr_diff^2 + -0.08 contr_diff*choice + 0.039 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 155/1000 --- L(Train): 0.4411197 --- L(Val, RNN): 0.4230050 --- L(Val, SINDy): 0.4276157 --- Time: 0.28s; --- Convergence: 1.53e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.256 1 + 0.009 value_reward_chosen[t] + 0.087 contr_diff + 0.657 reward + -0.028 value_reward_chosen^2 + -0.028 value_reward_chosen*contr_diff + 0.031 value_reward_chosen*reward + -0.295 contr_diff^2 + -0.011 contr_diff*reward + 0.658 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.391 1 + 0.836 value_reward_not_chosen[t] + -0.007 contr_diff + 0.032 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.007 contr_diff^2 \n",
            "value_choice[t+1] = 0.042 1 + 0.73 value_choice[t] + -0.081 contr_diff + 0.041 choice + -0.195 value_choice^2 + 0.02 value_choice*contr_diff + -0.27 value_choice*choice + -0.056 contr_diff^2 + -0.079 contr_diff*choice + 0.04 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 156/1000 --- L(Train): 0.4159690 --- L(Val, RNN): 0.4229001 --- L(Val, SINDy): 0.4278919 --- Time: 0.36s; --- Convergence: 1.29e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.256 1 + 0.01 value_reward_chosen[t] + 0.089 contr_diff + 0.663 reward + -0.021 value_reward_chosen^2 + -0.025 value_reward_chosen*contr_diff + 0.03 value_reward_chosen*reward + -0.288 contr_diff^2 + -0.017 contr_diff*reward + 0.664 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.398 1 + 0.832 value_reward_not_chosen[t] + -0.004 contr_diff + 0.033 value_reward_not_chosen^2 + -0.008 value_reward_not_chosen*contr_diff + -0.012 contr_diff^2 \n",
            "value_choice[t+1] = 0.043 1 + 0.731 value_choice[t] + -0.082 contr_diff + 0.042 choice + -0.195 value_choice^2 + 0.024 value_choice*contr_diff + -0.269 value_choice*choice + -0.054 contr_diff^2 + -0.08 contr_diff*choice + 0.041 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 157/1000 --- L(Train): 0.4258685 --- L(Val, RNN): 0.4228069 --- L(Val, SINDy): 0.4281484 --- Time: 0.41s; --- Convergence: 1.11e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.257 1 + 0.007 value_reward_chosen[t] + 0.091 contr_diff + 0.665 reward + -0.017 value_reward_chosen^2 + -0.023 value_reward_chosen*contr_diff + 0.025 value_reward_chosen*reward + -0.283 contr_diff^2 + -0.022 contr_diff*reward + 0.666 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.404 1 + 0.824 value_reward_not_chosen[t] + -0.006 contr_diff + 0.035 value_reward_not_chosen^2 + -0.013 value_reward_not_chosen*contr_diff + -0.017 contr_diff^2 \n",
            "value_choice[t+1] = 0.041 1 + 0.731 value_choice[t] + -0.084 contr_diff + 0.041 choice + -0.195 value_choice^2 + 0.027 value_choice*contr_diff + -0.269 value_choice*choice + -0.056 contr_diff^2 + -0.081 contr_diff*choice + 0.04 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 158/1000 --- L(Train): 0.4241930 --- L(Val, RNN): 0.4226407 --- L(Val, SINDy): 0.4282710 --- Time: 0.28s; --- Convergence: 1.39e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.262 1 + -0.0 value_reward_chosen[t] + 0.095 contr_diff + 0.66 reward + -0.018 value_reward_chosen^2 + -0.018 value_reward_chosen*contr_diff + 0.015 value_reward_chosen*reward + -0.279 contr_diff^2 + -0.025 contr_diff*reward + 0.662 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.41 1 + 0.814 value_reward_not_chosen[t] + -0.011 contr_diff + 0.039 value_reward_not_chosen^2 + -0.01 value_reward_not_chosen*contr_diff + -0.021 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.731 value_choice[t] + -0.088 contr_diff + 0.035 choice + -0.195 value_choice^2 + 0.028 value_choice*contr_diff + -0.269 value_choice*choice + -0.062 contr_diff^2 + -0.086 contr_diff*choice + 0.034 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 159/1000 --- L(Train): 0.4193325 --- L(Val, RNN): 0.4224619 --- L(Val, SINDy): 0.4280488 --- Time: 0.30s; --- Convergence: 1.59e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.27 1 + -0.009 value_reward_chosen[t] + 0.101 contr_diff + 0.653 reward + -0.023 value_reward_chosen^2 + -0.012 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.277 contr_diff^2 + -0.027 contr_diff*reward + 0.654 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.417 1 + 0.806 value_reward_not_chosen[t] + -0.018 contr_diff + 0.039 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.025 contr_diff^2 \n",
            "value_choice[t+1] = 0.035 1 + 0.736 value_choice[t] + -0.086 contr_diff + 0.034 choice + -0.191 value_choice^2 + 0.034 value_choice*contr_diff + -0.264 value_choice*choice + -0.063 contr_diff^2 + -0.084 contr_diff*choice + 0.034 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 160/1000 --- L(Train): 0.4298111 --- L(Val, RNN): 0.4223354 --- L(Val, SINDy): 0.4277169 --- Time: 0.30s; --- Convergence: 1.43e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.278 1 + -0.017 value_reward_chosen[t] + 0.106 contr_diff + 0.647 reward + -0.026 value_reward_chosen^2 + -0.008 value_reward_chosen*contr_diff + -0.007 value_reward_chosen*reward + -0.272 contr_diff^2 + -0.027 contr_diff*reward + 0.648 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.426 1 + 0.803 value_reward_not_chosen[t] + -0.026 contr_diff + 0.032 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.027 contr_diff^2 \n",
            "value_choice[t+1] = 0.041 1 + 0.746 value_choice[t] + -0.076 contr_diff + 0.041 choice + -0.18 value_choice^2 + 0.047 value_choice*contr_diff + -0.254 value_choice*choice + -0.056 contr_diff^2 + -0.074 contr_diff*choice + 0.04 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 161/1000 --- L(Train): 0.4187569 --- L(Val, RNN): 0.4223481 --- L(Val, SINDy): 0.4274638 --- Time: 0.35s; --- Convergence: 7.77e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.283 1 + -0.021 value_reward_chosen[t] + 0.11 contr_diff + 0.643 reward + -0.026 value_reward_chosen^2 + -0.006 value_reward_chosen*contr_diff + -0.014 value_reward_chosen*reward + -0.267 contr_diff^2 + -0.029 contr_diff*reward + 0.644 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.429 1 + 0.797 value_reward_not_chosen[t] + -0.03 contr_diff + 0.029 value_reward_not_chosen^2 + 0.005 value_reward_not_chosen*contr_diff + -0.025 contr_diff^2 \n",
            "value_choice[t+1] = 0.051 1 + 0.76 value_choice[t] + -0.064 contr_diff + 0.05 choice + -0.167 value_choice^2 + 0.063 value_choice*contr_diff + -0.241 value_choice*choice + -0.046 contr_diff^2 + -0.062 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 162/1000 --- L(Train): 0.4216117 --- L(Val, RNN): 0.4221834 --- L(Val, SINDy): 0.4273067 --- Time: 0.31s; --- Convergence: 1.21e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.283 1 + -0.02 value_reward_chosen[t] + 0.113 contr_diff + 0.645 reward + -0.022 value_reward_chosen^2 + -0.008 value_reward_chosen*contr_diff + -0.014 value_reward_chosen*reward + -0.258 contr_diff^2 + -0.031 contr_diff*reward + 0.646 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.427 1 + 0.788 value_reward_not_chosen[t] + -0.031 contr_diff + 0.027 value_reward_not_chosen^2 + 0.004 value_reward_not_chosen*contr_diff + -0.018 contr_diff^2 \n",
            "value_choice[t+1] = 0.061 1 + 0.774 value_choice[t] + -0.05 contr_diff + 0.06 choice + -0.153 value_choice^2 + 0.08 value_choice*contr_diff + -0.226 value_choice*choice + -0.034 contr_diff^2 + -0.048 contr_diff*choice + 0.059 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 163/1000 --- L(Train): 0.4234611 --- L(Val, RNN): 0.4220929 --- L(Val, SINDy): 0.4270532 --- Time: 0.45s; --- Convergence: 1.06e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.286 1 + -0.019 value_reward_chosen[t] + 0.114 contr_diff + 0.646 reward + -0.02 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + -0.013 value_reward_chosen*reward + -0.251 contr_diff^2 + -0.033 contr_diff*reward + 0.647 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.423 1 + 0.78 value_reward_not_chosen[t] + -0.029 contr_diff + 0.025 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.071 1 + 0.787 value_choice[t] + -0.038 contr_diff + 0.07 choice + -0.139 value_choice^2 + 0.096 value_choice*contr_diff + -0.213 value_choice*choice + -0.024 contr_diff^2 + -0.036 contr_diff*choice + 0.069 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 164/1000 --- L(Train): 0.4228273 --- L(Val, RNN): 0.4220349 --- L(Val, SINDy): 0.4264107 --- Time: 0.37s; --- Convergence: 8.19e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.288 1 + -0.016 value_reward_chosen[t] + 0.116 contr_diff + 0.647 reward + -0.018 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + -0.012 value_reward_chosen*reward + -0.244 contr_diff^2 + -0.035 contr_diff*reward + 0.648 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.42 1 + 0.776 value_reward_not_chosen[t] + -0.027 contr_diff + 0.02 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.079 1 + 0.798 value_choice[t] + -0.027 contr_diff + 0.078 choice + -0.128 value_choice^2 + 0.11 value_choice*contr_diff + -0.202 value_choice*choice + -0.014 contr_diff^2 + -0.025 contr_diff*choice + 0.078 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 165/1000 --- L(Train): 0.4272941 --- L(Val, RNN): 0.4220135 --- L(Val, SINDy): 0.4239677 --- Time: 0.28s; --- Convergence: 5.17e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.293 1 + -0.013 value_reward_chosen[t] + 0.115 contr_diff + 0.647 reward + -0.018 value_reward_chosen^2 + -0.008 value_reward_chosen*contr_diff + -0.009 value_reward_chosen*reward + -0.24 contr_diff^2 + -0.038 contr_diff*reward + 0.649 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.415 1 + 0.777 value_reward_not_chosen[t] + -0.022 contr_diff + 0.011 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + 0.007 contr_diff^2 \n",
            "value_choice[t+1] = 0.084 1 + 0.805 value_choice[t] + -0.02 contr_diff + 0.083 choice + -0.122 value_choice^2 + 0.12 value_choice*contr_diff + -0.196 value_choice*choice + -0.008 contr_diff^2 + -0.018 contr_diff*choice + 0.083 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 166/1000 --- L(Train): 0.4178527 --- L(Val, RNN): 0.4220495 --- L(Val, SINDy): 0.4232399 --- Time: 0.33s; --- Convergence: 4.38e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.297 1 + -0.009 value_reward_chosen[t] + 0.113 contr_diff + 0.649 reward + -0.018 value_reward_chosen^2 + -0.007 value_reward_chosen*contr_diff + -0.006 value_reward_chosen*reward + -0.237 contr_diff^2 + -0.041 contr_diff*reward + 0.651 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.407 1 + 0.778 value_reward_not_chosen[t] + -0.018 contr_diff + 0.001 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + 0.019 contr_diff^2 \n",
            "value_choice[t+1] = 0.086 1 + 0.807 value_choice[t] + -0.018 contr_diff + 0.085 choice + -0.12 value_choice^2 + 0.126 value_choice*contr_diff + -0.194 value_choice*choice + -0.006 contr_diff^2 + -0.016 contr_diff*choice + 0.084 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 167/1000 --- L(Train): 0.4321066 --- L(Val, RNN): 0.4218294 --- L(Val, SINDy): 0.4229418 --- Time: 0.28s; --- Convergence: 1.32e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.298 1 + -0.002 value_reward_chosen[t] + 0.11 contr_diff + 0.655 reward + -0.015 value_reward_chosen^2 + -0.007 value_reward_chosen*contr_diff + 0.001 value_reward_chosen*reward + -0.232 contr_diff^2 + -0.045 contr_diff*reward + 0.656 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.396 1 + 0.776 value_reward_not_chosen[t] + -0.016 contr_diff + -0.005 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.033 contr_diff^2 \n",
            "value_choice[t+1] = 0.083 1 + 0.804 value_choice[t] + -0.02 contr_diff + 0.083 choice + -0.124 value_choice^2 + 0.129 value_choice*contr_diff + -0.196 value_choice*choice + -0.007 contr_diff^2 + -0.018 contr_diff*choice + 0.082 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 168/1000 --- L(Train): 0.4307505 --- L(Val, RNN): 0.4219529 --- L(Val, SINDy): 0.4228687 --- Time: 0.29s; --- Convergence: 1.28e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.303 1 + 0.002 value_reward_chosen[t] + 0.108 contr_diff + 0.657 reward + -0.016 value_reward_chosen^2 + -0.005 value_reward_chosen*contr_diff + 0.005 value_reward_chosen*reward + -0.229 contr_diff^2 + -0.047 contr_diff*reward + 0.658 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.384 1 + 0.773 value_reward_not_chosen[t] + -0.016 contr_diff + -0.008 value_reward_not_chosen^2 + 0.005 value_reward_not_chosen*contr_diff + 0.047 contr_diff^2 \n",
            "value_choice[t+1] = 0.078 1 + 0.798 value_choice[t] + -0.025 contr_diff + 0.077 choice + -0.131 value_choice^2 + 0.128 value_choice*contr_diff + -0.203 value_choice*choice + -0.011 contr_diff^2 + -0.023 contr_diff*choice + 0.076 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 169/1000 --- L(Train): 0.4230050 --- L(Val, RNN): 0.4219365 --- L(Val, SINDy): 0.4229336 --- Time: 0.30s; --- Convergence: 7.21e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.311 1 + 0.003 value_reward_chosen[t] + 0.107 contr_diff + 0.657 reward + -0.02 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + 0.006 value_reward_chosen*reward + -0.227 contr_diff^2 + -0.048 contr_diff*reward + 0.658 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.373 1 + 0.771 value_reward_not_chosen[t] + -0.016 contr_diff + -0.012 value_reward_not_chosen^2 + 0.01 value_reward_not_chosen*contr_diff + 0.059 contr_diff^2 \n",
            "value_choice[t+1] = 0.069 1 + 0.788 value_choice[t] + -0.031 contr_diff + 0.068 choice + -0.14 value_choice^2 + 0.126 value_choice*contr_diff + -0.212 value_choice*choice + -0.017 contr_diff^2 + -0.029 contr_diff*choice + 0.067 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 170/1000 --- L(Train): 0.4253892 --- L(Val, RNN): 0.4220295 --- L(Val, SINDy): 0.4228937 --- Time: 0.40s; --- Convergence: 8.25e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.317 1 + 0.003 value_reward_chosen[t] + 0.104 contr_diff + 0.657 reward + -0.024 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + 0.007 value_reward_chosen*reward + -0.225 contr_diff^2 + -0.051 contr_diff*reward + 0.659 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.365 1 + 0.773 value_reward_not_chosen[t] + -0.016 contr_diff + -0.018 value_reward_not_chosen^2 + 0.015 value_reward_not_chosen*contr_diff + 0.068 contr_diff^2 \n",
            "value_choice[t+1] = 0.06 1 + 0.779 value_choice[t] + -0.038 contr_diff + 0.059 choice + -0.15 value_choice^2 + 0.124 value_choice*contr_diff + -0.221 value_choice*choice + -0.022 contr_diff^2 + -0.036 contr_diff*choice + 0.059 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 171/1000 --- L(Train): 0.4270903 --- L(Val, RNN): 0.4222628 --- L(Val, SINDy): 0.4228469 --- Time: 0.30s; --- Convergence: 1.58e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.325 1 + 0.002 value_reward_chosen[t] + 0.102 contr_diff + 0.657 reward + -0.028 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.007 value_reward_chosen*reward + -0.223 contr_diff^2 + -0.052 contr_diff*reward + 0.658 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.359 1 + 0.775 value_reward_not_chosen[t] + -0.014 contr_diff + -0.023 value_reward_not_chosen^2 + 0.017 value_reward_not_chosen*contr_diff + 0.075 contr_diff^2 \n",
            "value_choice[t+1] = 0.053 1 + 0.771 value_choice[t] + -0.044 contr_diff + 0.052 choice + -0.158 value_choice^2 + 0.122 value_choice*contr_diff + -0.229 value_choice*choice + -0.026 contr_diff^2 + -0.042 contr_diff*choice + 0.051 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 172/1000 --- L(Train): 0.4262935 --- L(Val, RNN): 0.4219894 --- L(Val, SINDy): 0.4225341 --- Time: 0.41s; --- Convergence: 2.16e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.326 1 + 0.005 value_reward_chosen[t] + 0.1 contr_diff + 0.663 reward + -0.026 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + 0.013 value_reward_chosen*reward + -0.218 contr_diff^2 + -0.054 contr_diff*reward + 0.664 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.352 1 + 0.775 value_reward_not_chosen[t] + -0.013 contr_diff + -0.025 value_reward_not_chosen^2 + 0.019 value_reward_not_chosen*contr_diff + 0.08 contr_diff^2 \n",
            "value_choice[t+1] = 0.049 1 + 0.766 value_choice[t] + -0.049 contr_diff + 0.048 choice + -0.162 value_choice^2 + 0.122 value_choice*contr_diff + -0.234 value_choice*choice + -0.028 contr_diff^2 + -0.047 contr_diff*choice + 0.047 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 173/1000 --- L(Train): 0.4263181 --- L(Val, RNN): 0.4218924 --- L(Val, SINDy): 0.4224181 --- Time: 0.34s; --- Convergence: 1.56e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.329 1 + 0.006 value_reward_chosen[t] + 0.098 contr_diff + 0.666 reward + -0.026 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + 0.016 value_reward_chosen*reward + -0.213 contr_diff^2 + -0.055 contr_diff*reward + 0.667 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.347 1 + 0.773 value_reward_not_chosen[t] + -0.013 contr_diff + -0.024 value_reward_not_chosen^2 + 0.02 value_reward_not_chosen*contr_diff + 0.084 contr_diff^2 \n",
            "value_choice[t+1] = 0.048 1 + 0.765 value_choice[t] + -0.053 contr_diff + 0.047 choice + -0.164 value_choice^2 + 0.122 value_choice*contr_diff + -0.236 value_choice*choice + -0.03 contr_diff^2 + -0.051 contr_diff*choice + 0.046 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 174/1000 --- L(Train): 0.4263804 --- L(Val, RNN): 0.4220472 --- L(Val, SINDy): 0.4224030 --- Time: 0.31s; --- Convergence: 1.56e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.329 1 + 0.007 value_reward_chosen[t] + 0.097 contr_diff + 0.673 reward + -0.022 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.019 value_reward_chosen*reward + -0.204 contr_diff^2 + -0.058 contr_diff*reward + 0.674 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.348 1 + 0.777 value_reward_not_chosen[t] + -0.01 contr_diff + -0.026 value_reward_not_chosen^2 + 0.017 value_reward_not_chosen*contr_diff + 0.079 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.767 value_choice[t] + -0.051 contr_diff + 0.05 choice + -0.161 value_choice^2 + 0.127 value_choice*contr_diff + -0.233 value_choice*choice + -0.025 contr_diff^2 + -0.049 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 175/1000 --- L(Train): 0.4206595 --- L(Val, RNN): 0.4219024 --- L(Val, SINDy): 0.4222858 --- Time: 0.30s; --- Convergence: 1.50e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.325 1 + 0.009 value_reward_chosen[t] + 0.096 contr_diff + 0.681 reward + -0.015 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + 0.022 value_reward_chosen*reward + -0.192 contr_diff^2 + -0.053 contr_diff*reward + 0.682 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.351 1 + 0.781 value_reward_not_chosen[t] + -0.005 contr_diff + -0.028 value_reward_not_chosen^2 + 0.01 value_reward_not_chosen*contr_diff + 0.072 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.771 value_choice[t] + -0.049 contr_diff + 0.054 choice + -0.156 value_choice^2 + 0.135 value_choice*contr_diff + -0.229 value_choice*choice + -0.019 contr_diff^2 + -0.047 contr_diff*choice + 0.053 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 176/1000 --- L(Train): 0.4212805 --- L(Val, RNN): 0.4219221 --- L(Val, SINDy): 0.4223422 --- Time: 0.30s; --- Convergence: 8.49e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.328 1 + 0.003 value_reward_chosen[t] + 0.098 contr_diff + 0.681 reward + -0.016 value_reward_chosen^2 + -0.016 value_reward_chosen*contr_diff + 0.016 value_reward_chosen*reward + -0.182 contr_diff^2 + -0.048 contr_diff*reward + 0.682 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.35 1 + 0.778 value_reward_not_chosen[t] + -0.001 contr_diff + -0.022 value_reward_not_chosen^2 + 0.004 value_reward_not_chosen*contr_diff + 0.065 contr_diff^2 \n",
            "value_choice[t+1] = 0.06 1 + 0.776 value_choice[t] + -0.046 contr_diff + 0.059 choice + -0.15 value_choice^2 + 0.143 value_choice*contr_diff + -0.224 value_choice*choice + -0.012 contr_diff^2 + -0.044 contr_diff*choice + 0.059 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 177/1000 --- L(Train): 0.4134749 --- L(Val, RNN): 0.4217586 --- L(Val, SINDy): 0.4222322 --- Time: 0.31s; --- Convergence: 1.24e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.331 1 + -0.004 value_reward_chosen[t] + 0.1 contr_diff + 0.681 reward + -0.016 value_reward_chosen^2 + -0.021 value_reward_chosen*contr_diff + 0.01 value_reward_chosen*reward + -0.173 contr_diff^2 + -0.043 contr_diff*reward + 0.682 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.352 1 + 0.779 value_reward_not_chosen[t] + 0.001 contr_diff + -0.017 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.056 contr_diff^2 \n",
            "value_choice[t+1] = 0.066 1 + 0.781 value_choice[t] + -0.042 contr_diff + 0.065 choice + -0.144 value_choice^2 + 0.151 value_choice*contr_diff + -0.219 value_choice*choice + -0.006 contr_diff^2 + -0.039 contr_diff*choice + 0.065 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 178/1000 --- L(Train): 0.4195535 --- L(Val, RNN): 0.4217007 --- L(Val, SINDy): 0.4221854 --- Time: 0.38s; --- Convergence: 9.10e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.336 1 + -0.012 value_reward_chosen[t] + 0.099 contr_diff + 0.678 reward + -0.02 value_reward_chosen^2 + -0.023 value_reward_chosen*contr_diff + 0.002 value_reward_chosen*reward + -0.167 contr_diff^2 + -0.038 contr_diff*reward + 0.679 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.358 1 + 0.785 value_reward_not_chosen[t] + -0.005 contr_diff + -0.017 value_reward_not_chosen^2 + 0.006 value_reward_not_chosen*contr_diff + 0.044 contr_diff^2 \n",
            "value_choice[t+1] = 0.07 1 + 0.785 value_choice[t] + -0.035 contr_diff + 0.07 choice + -0.14 value_choice^2 + 0.162 value_choice*contr_diff + -0.215 value_choice*choice + -0.001 contr_diff^2 + -0.033 contr_diff*choice + 0.069 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 179/1000 --- L(Train): 0.4257939 --- L(Val, RNN): 0.4218867 --- L(Val, SINDy): 0.4223376 --- Time: 0.45s; --- Convergence: 1.39e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.345 1 + -0.022 value_reward_chosen[t] + 0.099 contr_diff + 0.672 reward + -0.026 value_reward_chosen^2 + -0.021 value_reward_chosen*contr_diff + -0.007 value_reward_chosen*reward + -0.165 contr_diff^2 + -0.034 contr_diff*reward + 0.673 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.363 1 + 0.793 value_reward_not_chosen[t] + -0.009 contr_diff + -0.017 value_reward_not_chosen^2 + 0.011 value_reward_not_chosen*contr_diff + 0.033 contr_diff^2 \n",
            "value_choice[t+1] = 0.073 1 + 0.787 value_choice[t] + -0.028 contr_diff + 0.072 choice + -0.138 value_choice^2 + 0.171 value_choice*contr_diff + -0.214 value_choice*choice + 0.003 contr_diff^2 + -0.026 contr_diff*choice + 0.072 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 180/1000 --- L(Train): 0.4150407 --- L(Val, RNN): 0.4215549 --- L(Val, SINDy): 0.4224268 --- Time: 0.37s; --- Convergence: 2.35e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.346 1 + -0.021 value_reward_chosen[t] + 0.096 contr_diff + 0.674 reward + -0.024 value_reward_chosen^2 + -0.02 value_reward_chosen*contr_diff + -0.006 value_reward_chosen*reward + -0.159 contr_diff^2 + -0.033 contr_diff*reward + 0.675 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.364 1 + 0.796 value_reward_not_chosen[t] + -0.013 contr_diff + -0.013 value_reward_not_chosen^2 + 0.017 value_reward_not_chosen*contr_diff + 0.024 contr_diff^2 \n",
            "value_choice[t+1] = 0.073 1 + 0.785 value_choice[t] + -0.022 contr_diff + 0.072 choice + -0.139 value_choice^2 + 0.18 value_choice*contr_diff + -0.215 value_choice*choice + 0.004 contr_diff^2 + -0.02 contr_diff*choice + 0.071 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 181/1000 --- L(Train): 0.4324372 --- L(Val, RNN): 0.4214839 --- L(Val, SINDy): 0.4223619 --- Time: 0.32s; --- Convergence: 1.53e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.346 1 + -0.018 value_reward_chosen[t] + 0.092 contr_diff + 0.679 reward + -0.021 value_reward_chosen^2 + -0.019 value_reward_chosen*contr_diff + -0.003 value_reward_chosen*reward + -0.153 contr_diff^2 + -0.034 contr_diff*reward + 0.68 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.361 1 + 0.797 value_reward_not_chosen[t] + -0.011 contr_diff + -0.007 value_reward_not_chosen^2 + 0.021 value_reward_not_chosen*contr_diff + 0.018 contr_diff^2 \n",
            "value_choice[t+1] = 0.07 1 + 0.782 value_choice[t] + -0.017 contr_diff + 0.069 choice + -0.141 value_choice^2 + 0.186 value_choice*contr_diff + -0.218 value_choice*choice + 0.002 contr_diff^2 + -0.015 contr_diff*choice + 0.068 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 182/1000 --- L(Train): 0.4151483 --- L(Val, RNN): 0.4218325 --- L(Val, SINDy): 0.4221497 --- Time: 0.35s; --- Convergence: 2.51e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.345 1 + -0.014 value_reward_chosen[t] + 0.088 contr_diff + 0.683 reward + -0.017 value_reward_chosen^2 + -0.017 value_reward_chosen*contr_diff + 0.001 value_reward_chosen*reward + -0.146 contr_diff^2 + -0.033 contr_diff*reward + 0.684 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.359 1 + 0.799 value_reward_not_chosen[t] + -0.009 contr_diff + -0.002 value_reward_not_chosen^2 + 0.024 value_reward_not_chosen*contr_diff + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.066 1 + 0.778 value_choice[t] + -0.012 contr_diff + 0.065 choice + -0.144 value_choice^2 + 0.191 value_choice*contr_diff + -0.222 value_choice*choice + -0.0 contr_diff^2 + -0.01 contr_diff*choice + 0.064 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 183/1000 --- L(Train): 0.4045965 --- L(Val, RNN): 0.4214992 --- L(Val, SINDy): 0.4218051 --- Time: 0.36s; --- Convergence: 2.92e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.343 1 + -0.008 value_reward_chosen[t] + 0.084 contr_diff + 0.689 reward + -0.012 value_reward_chosen^2 + -0.015 value_reward_chosen*contr_diff + 0.007 value_reward_chosen*reward + -0.14 contr_diff^2 + -0.035 contr_diff*reward + 0.69 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.357 1 + 0.804 value_reward_not_chosen[t] + -0.006 contr_diff + -0.001 value_reward_not_chosen^2 + 0.024 value_reward_not_chosen*contr_diff + 0.008 contr_diff^2 \n",
            "value_choice[t+1] = 0.061 1 + 0.773 value_choice[t] + -0.009 contr_diff + 0.06 choice + -0.148 value_choice^2 + 0.196 value_choice*contr_diff + -0.227 value_choice*choice + -0.003 contr_diff^2 + -0.007 contr_diff*choice + 0.06 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 184/1000 --- L(Train): 0.4352201 --- L(Val, RNN): 0.4215383 --- L(Val, SINDy): 0.4217379 --- Time: 0.44s; --- Convergence: 1.66e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.349 1 + -0.006 value_reward_chosen[t] + 0.084 contr_diff + 0.688 reward + -0.014 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + 0.009 value_reward_chosen*reward + -0.142 contr_diff^2 + -0.03 contr_diff*reward + 0.689 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.357 1 + 0.812 value_reward_not_chosen[t] + 0.0 contr_diff + -0.003 value_reward_not_chosen^2 + 0.018 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.057 1 + 0.768 value_choice[t] + -0.011 contr_diff + 0.057 choice + -0.152 value_choice^2 + 0.196 value_choice*contr_diff + -0.232 value_choice*choice + -0.005 contr_diff^2 + -0.009 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 185/1000 --- L(Train): 0.4219610 --- L(Val, RNN): 0.4217088 --- L(Val, SINDy): 0.4217383 --- Time: 0.38s; --- Convergence: 1.68e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.354 1 + -0.004 value_reward_chosen[t] + 0.085 contr_diff + 0.688 reward + -0.015 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + 0.011 value_reward_chosen*reward + -0.142 contr_diff^2 + -0.026 contr_diff*reward + 0.689 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.355 1 + 0.818 value_reward_not_chosen[t] + 0.005 contr_diff + -0.005 value_reward_not_chosen^2 + 0.013 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.765 value_choice[t] + -0.02 contr_diff + 0.054 choice + -0.154 value_choice^2 + 0.19 value_choice*contr_diff + -0.235 value_choice*choice + -0.004 contr_diff^2 + -0.018 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 186/1000 --- L(Train): 0.4189911 --- L(Val, RNN): 0.4214357 --- L(Val, SINDy): 0.4216083 --- Time: 0.45s; --- Convergence: 2.21e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.354 1 + 0.002 value_reward_chosen[t] + 0.085 contr_diff + 0.692 reward + -0.012 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + 0.015 value_reward_chosen*reward + -0.137 contr_diff^2 + -0.02 contr_diff*reward + 0.693 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.349 1 + 0.82 value_reward_not_chosen[t] + 0.0 contr_diff + -0.004 value_reward_not_chosen^2 + 0.01 value_reward_not_chosen*contr_diff + 0.007 contr_diff^2 \n",
            "value_choice[t+1] = 0.054 1 + 0.763 value_choice[t] + -0.032 contr_diff + 0.053 choice + -0.156 value_choice^2 + 0.183 value_choice*contr_diff + -0.237 value_choice*choice + -0.002 contr_diff^2 + -0.03 contr_diff*choice + 0.053 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 187/1000 --- L(Train): 0.4234333 --- L(Val, RNN): 0.4215056 --- L(Val, SINDy): 0.4217495 --- Time: 0.30s; --- Convergence: 1.45e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.355 1 + 0.005 value_reward_chosen[t] + 0.085 contr_diff + 0.695 reward + -0.01 value_reward_chosen^2 + -0.005 value_reward_chosen*contr_diff + 0.018 value_reward_chosen*reward + -0.132 contr_diff^2 + -0.015 contr_diff*reward + 0.696 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.343 1 + 0.82 value_reward_not_chosen[t] + -0.003 contr_diff + -0.002 value_reward_not_chosen^2 + 0.007 value_reward_not_chosen*contr_diff + 0.01 contr_diff^2 \n",
            "value_choice[t+1] = 0.054 1 + 0.762 value_choice[t] + -0.043 contr_diff + 0.054 choice + -0.157 value_choice^2 + 0.177 value_choice*contr_diff + -0.239 value_choice*choice + -0.0 contr_diff^2 + -0.041 contr_diff*choice + 0.053 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 188/1000 --- L(Train): 0.4155560 --- L(Val, RNN): 0.4213944 --- L(Val, SINDy): 0.4218360 --- Time: 0.31s; --- Convergence: 1.28e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.354 1 + 0.008 value_reward_chosen[t] + 0.085 contr_diff + 0.699 reward + -0.006 value_reward_chosen^2 + -0.008 value_reward_chosen*contr_diff + 0.021 value_reward_chosen*reward + -0.124 contr_diff^2 + -0.011 contr_diff*reward + 0.7 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.339 1 + 0.822 value_reward_not_chosen[t] + -0.008 contr_diff + -0.003 value_reward_not_chosen^2 + 0.005 value_reward_not_chosen*contr_diff + 0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.056 1 + 0.761 value_choice[t] + -0.051 contr_diff + 0.055 choice + -0.156 value_choice^2 + 0.173 value_choice*contr_diff + -0.239 value_choice*choice + 0.002 contr_diff^2 + -0.049 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 189/1000 --- L(Train): 0.4276792 --- L(Val, RNN): 0.4213130 --- L(Val, SINDy): 0.4215632 --- Time: 0.39s; --- Convergence: 1.05e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.353 1 + 0.009 value_reward_chosen[t] + 0.084 contr_diff + 0.702 reward + -0.003 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + 0.022 value_reward_chosen*reward + -0.117 contr_diff^2 + -0.008 contr_diff*reward + 0.703 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.337 1 + 0.823 value_reward_not_chosen[t] + -0.013 contr_diff + -0.006 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.057 1 + 0.761 value_choice[t] + -0.057 contr_diff + 0.057 choice + -0.156 value_choice^2 + 0.171 value_choice*contr_diff + -0.239 value_choice*choice + 0.005 contr_diff^2 + -0.055 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 190/1000 --- L(Train): 0.4301112 --- L(Val, RNN): 0.4214624 --- L(Val, SINDy): 0.4214828 --- Time: 0.42s; --- Convergence: 1.27e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.355 1 + 0.007 value_reward_chosen[t] + 0.086 contr_diff + 0.701 reward + -0.002 value_reward_chosen^2 + -0.008 value_reward_chosen*contr_diff + 0.02 value_reward_chosen*reward + -0.113 contr_diff^2 + -0.006 contr_diff*reward + 0.702 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.335 1 + 0.823 value_reward_not_chosen[t] + -0.017 contr_diff + -0.007 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.059 1 + 0.761 value_choice[t] + -0.06 contr_diff + 0.058 choice + -0.156 value_choice^2 + 0.171 value_choice*contr_diff + -0.239 value_choice*choice + 0.008 contr_diff^2 + -0.058 contr_diff*choice + 0.058 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 191/1000 --- L(Train): 0.4366145 --- L(Val, RNN): 0.4213457 --- L(Val, SINDy): 0.4215540 --- Time: 0.35s; --- Convergence: 1.22e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.356 1 + 0.004 value_reward_chosen[t] + 0.084 contr_diff + 0.702 reward + -0.002 value_reward_chosen^2 + -0.01 value_reward_chosen*contr_diff + 0.016 value_reward_chosen*reward + -0.107 contr_diff^2 + -0.005 contr_diff*reward + 0.703 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.333 1 + 0.822 value_reward_not_chosen[t] + -0.023 contr_diff + -0.009 value_reward_not_chosen^2 + -0.01 value_reward_not_chosen*contr_diff + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.06 1 + 0.76 value_choice[t] + -0.057 contr_diff + 0.059 choice + -0.157 value_choice^2 + 0.176 value_choice*contr_diff + -0.24 value_choice*choice + 0.009 contr_diff^2 + -0.055 contr_diff*choice + 0.058 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 192/1000 --- L(Train): 0.4245897 --- L(Val, RNN): 0.4212583 --- L(Val, SINDy): 0.4216145 --- Time: 0.44s; --- Convergence: 1.05e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.36 1 + -0.003 value_reward_chosen[t] + 0.086 contr_diff + 0.698 reward + -0.005 value_reward_chosen^2 + -0.007 value_reward_chosen*contr_diff + 0.009 value_reward_chosen*reward + -0.105 contr_diff^2 + -0.006 contr_diff*reward + 0.699 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.331 1 + 0.819 value_reward_not_chosen[t] + -0.027 contr_diff + -0.009 value_reward_not_chosen^2 + -0.018 value_reward_not_chosen*contr_diff + 0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.06 1 + 0.758 value_choice[t] + -0.051 contr_diff + 0.059 choice + -0.158 value_choice^2 + 0.184 value_choice*contr_diff + -0.242 value_choice*choice + 0.009 contr_diff^2 + -0.049 contr_diff*choice + 0.058 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 193/1000 --- L(Train): 0.4150004 --- L(Val, RNN): 0.4211896 --- L(Val, SINDy): 0.4215720 --- Time: 0.32s; --- Convergence: 8.67e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.365 1 + -0.009 value_reward_chosen[t] + 0.086 contr_diff + 0.695 reward + -0.007 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + 0.002 value_reward_chosen*reward + -0.103 contr_diff^2 + -0.008 contr_diff*reward + 0.696 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.33 1 + 0.817 value_reward_not_chosen[t] + -0.029 contr_diff + -0.01 value_reward_not_chosen^2 + -0.025 value_reward_not_chosen*contr_diff + 0.012 contr_diff^2 \n",
            "value_choice[t+1] = 0.058 1 + 0.755 value_choice[t] + -0.043 contr_diff + 0.057 choice + -0.16 value_choice^2 + 0.193 value_choice*contr_diff + -0.245 value_choice*choice + 0.007 contr_diff^2 + -0.041 contr_diff*choice + 0.057 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 194/1000 --- L(Train): 0.4075032 --- L(Val, RNN): 0.4211118 --- L(Val, SINDy): 0.4216260 --- Time: 0.32s; --- Convergence: 8.22e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.366 1 + -0.013 value_reward_chosen[t] + 0.084 contr_diff + 0.694 reward + -0.008 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.002 value_reward_chosen*reward + -0.099 contr_diff^2 + -0.012 contr_diff*reward + 0.695 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.331 1 + 0.816 value_reward_not_chosen[t] + -0.03 contr_diff + -0.013 value_reward_not_chosen^2 + -0.029 value_reward_not_chosen*contr_diff + 0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.752 value_choice[t] + -0.034 contr_diff + 0.055 choice + -0.163 value_choice^2 + 0.202 value_choice*contr_diff + -0.249 value_choice*choice + 0.004 contr_diff^2 + -0.032 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 195/1000 --- L(Train): 0.4233262 --- L(Val, RNN): 0.4211050 --- L(Val, SINDy): 0.4217505 --- Time: 0.32s; --- Convergence: 4.45e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.368 1 + -0.015 value_reward_chosen[t] + 0.083 contr_diff + 0.693 reward + -0.008 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.004 value_reward_chosen*reward + -0.096 contr_diff^2 + -0.017 contr_diff*reward + 0.695 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.333 1 + 0.815 value_reward_not_chosen[t] + -0.032 contr_diff + -0.016 value_reward_not_chosen^2 + -0.029 value_reward_not_chosen*contr_diff + 0.008 contr_diff^2 \n",
            "value_choice[t+1] = 0.051 1 + 0.747 value_choice[t] + -0.026 contr_diff + 0.05 choice + -0.167 value_choice^2 + 0.21 value_choice*contr_diff + -0.254 value_choice*choice + -0.0 contr_diff^2 + -0.024 contr_diff*choice + 0.05 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 196/1000 --- L(Train): 0.4211505 --- L(Val, RNN): 0.4209578 --- L(Val, SINDy): 0.4214692 --- Time: 0.44s; --- Convergence: 9.58e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.367 1 + -0.013 value_reward_chosen[t] + 0.081 contr_diff + 0.696 reward + -0.005 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.003 value_reward_chosen*reward + -0.091 contr_diff^2 + -0.023 contr_diff*reward + 0.697 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.334 1 + 0.813 value_reward_not_chosen[t] + -0.033 contr_diff + -0.017 value_reward_not_chosen^2 + -0.024 value_reward_not_chosen*contr_diff + 0.006 contr_diff^2 \n",
            "value_choice[t+1] = 0.046 1 + 0.741 value_choice[t] + -0.021 contr_diff + 0.045 choice + -0.171 value_choice^2 + 0.216 value_choice*contr_diff + -0.259 value_choice*choice + -0.005 contr_diff^2 + -0.019 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 197/1000 --- L(Train): 0.4193294 --- L(Val, RNN): 0.4209860 --- L(Val, SINDy): 0.4213582 --- Time: 0.31s; --- Convergence: 6.20e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.367 1 + -0.011 value_reward_chosen[t] + 0.08 contr_diff + 0.698 reward + -0.003 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.002 value_reward_chosen*reward + -0.087 contr_diff^2 + -0.027 contr_diff*reward + 0.699 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.334 1 + 0.811 value_reward_not_chosen[t] + -0.034 contr_diff + -0.016 value_reward_not_chosen^2 + -0.019 value_reward_not_chosen*contr_diff + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.04 1 + 0.736 value_choice[t] + -0.017 contr_diff + 0.04 choice + -0.174 value_choice^2 + 0.22 value_choice*contr_diff + -0.264 value_choice*choice + -0.01 contr_diff^2 + -0.015 contr_diff*choice + 0.039 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 198/1000 --- L(Train): 0.4229139 --- L(Val, RNN): 0.4209730 --- L(Val, SINDy): 0.4214013 --- Time: 0.33s; --- Convergence: 3.75e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.367 1 + -0.008 value_reward_chosen[t] + 0.086 contr_diff + 0.7 reward + -0.001 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.084 contr_diff^2 + -0.026 contr_diff*reward + 0.701 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.335 1 + 0.809 value_reward_not_chosen[t] + -0.029 contr_diff + -0.016 value_reward_not_chosen^2 + -0.014 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.732 value_choice[t] + -0.02 contr_diff + 0.036 choice + -0.177 value_choice^2 + 0.22 value_choice*contr_diff + -0.268 value_choice*choice + -0.013 contr_diff^2 + -0.018 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 199/1000 --- L(Train): 0.4177192 --- L(Val, RNN): 0.4208561 --- L(Val, SINDy): 0.4221256 --- Time: 0.39s; --- Convergence: 7.72e-05; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.368 1 + -0.006 value_reward_chosen[t] + 0.092 contr_diff + 0.701 reward + -0.001 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + 0.001 value_reward_chosen*reward + -0.081 contr_diff^2 + -0.025 contr_diff*reward + 0.703 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.337 1 + 0.809 value_reward_not_chosen[t] + -0.022 contr_diff + -0.017 value_reward_not_chosen^2 + -0.011 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.033 1 + 0.729 value_choice[t] + -0.024 contr_diff + 0.032 choice + -0.18 value_choice^2 + 0.219 value_choice*contr_diff + -0.272 value_choice*choice + -0.016 contr_diff^2 + -0.022 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\n",
            ">>> Warmup complete (epoch 200). Reset optimizer state for 3 SINDy parameters (fresh start at full regularization strength).\n",
            "\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 200/1000 --- L(Train): 0.4165717 --- L(Val, RNN): 0.4210000 --- L(Val, SINDy): 0.4226851 --- Time: 0.48s; --- Convergence: 1.11e-04; LR: 1.00e-02; Metric: 0.4210000; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.372 1 + -0.007 value_reward_chosen[t] + 0.098 contr_diff + 0.699 reward + -0.004 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.001 value_reward_chosen*reward + -0.081 contr_diff^2 + -0.025 contr_diff*reward + 0.7 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.338 1 + 0.808 value_reward_not_chosen[t] + -0.012 contr_diff + -0.017 value_reward_not_chosen^2 + -0.007 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.03 1 + 0.726 value_choice[t] + -0.029 contr_diff + 0.029 choice + -0.182 value_choice^2 + 0.217 value_choice*contr_diff + -0.275 value_choice*choice + -0.019 contr_diff^2 + -0.027 contr_diff*choice + 0.029 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 1, 1, 1, 0, 1, 0\n",
            "value_reward_not_chosen: 0, 0, 1, 1, 1, 1\n",
            "value_choice: 1, 0, 1, 1, 0, 0, 0, 1, 1, 1\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 201/1000 --- L(Train): 0.4271507 --- L(Val, RNN): 0.4208679 --- L(Val, SINDy): 0.4218893 --- Time: 0.40s; --- Convergence: 1.21e-04; LR: 1.00e-02; Metric: 0.4208679; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.382 1 + 0.003 value_reward_chosen[t] + 0.088 contr_diff + 0.689 reward + -0.014 value_reward_chosen^2 + -0.007 value_reward_chosen*contr_diff + 0.009 value_reward_chosen*reward + -0.091 contr_diff^2 + -0.035 contr_diff*reward + 0.69 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.328 1 + 0.798 value_reward_not_chosen[t] + -0.003 contr_diff + -0.007 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.008 contr_diff^2 \n",
            "value_choice[t+1] = 0.02 1 + 0.716 value_choice[t] + -0.039 contr_diff + 0.019 choice + -0.192 value_choice^2 + 0.207 value_choice*contr_diff + -0.285 value_choice*choice + -0.029 contr_diff^2 + -0.037 contr_diff*choice + 0.019 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 2, 2, 2, 0, 2, 0\n",
            "value_reward_not_chosen: 0, 0, 2, 2, 2, 2\n",
            "value_choice: 2, 0, 2, 2, 0, 0, 0, 2, 2, 2\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 202/1000 --- L(Train): 0.4196479 --- L(Val, RNN): 0.4212854 --- L(Val, SINDy): 0.4212182 --- Time: 0.28s; --- Convergence: 2.69e-04; LR: 1.00e-02; Metric: 0.4208679; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.392 1 + -0.003 value_reward_chosen[t] + 0.094 contr_diff + 0.68 reward + -0.024 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.1 contr_diff^2 + -0.028 contr_diff*reward + 0.681 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.336 1 + 0.806 value_reward_not_chosen[t] + 0.006 contr_diff + -0.014 value_reward_not_chosen^2 + 0.004 value_reward_not_chosen*contr_diff + -0.016 contr_diff^2 \n",
            "value_choice[t+1] = 0.027 1 + 0.723 value_choice[t] + -0.049 contr_diff + 0.027 choice + -0.185 value_choice^2 + 0.198 value_choice*contr_diff + -0.277 value_choice*choice + -0.022 contr_diff^2 + -0.047 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 3, 3, 3, 0, 3, 0\n",
            "value_reward_not_chosen: 0, 0, 3, 3, 3, 3\n",
            "value_choice: 3, 0, 3, 3, 0, 0, 0, 3, 3, 3\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 203/1000 --- L(Train): 0.4278607 --- L(Val, RNN): 0.4208483 --- L(Val, SINDy): 0.4214167 --- Time: 0.46s; --- Convergence: 3.53e-04; LR: 1.00e-02; Metric: 0.4208483; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.386 1 + 0.002 value_reward_chosen[t] + 0.092 contr_diff + 0.686 reward + -0.018 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + 0.008 value_reward_chosen*reward + -0.095 contr_diff^2 + -0.033 contr_diff*reward + 0.687 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.342 1 + 0.812 value_reward_not_chosen[t] + 0.01 contr_diff + -0.021 value_reward_not_chosen^2 + 0.009 value_reward_not_chosen*contr_diff + -0.022 contr_diff^2 \n",
            "value_choice[t+1] = 0.033 1 + 0.729 value_choice[t] + -0.057 contr_diff + 0.033 choice + -0.178 value_choice^2 + 0.191 value_choice*contr_diff + -0.271 value_choice*choice + -0.016 contr_diff^2 + -0.055 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 4, 4, 4, 0, 4, 0\n",
            "value_reward_not_chosen: 0, 0, 4, 4, 4, 4\n",
            "value_choice: 4, 0, 0, 4, 0, 0, 0, 4, 0, 4\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 204/1000 --- L(Train): 0.4271937 --- L(Val, RNN): 0.4208632 --- L(Val, SINDy): 0.4218394 --- Time: 0.34s; --- Convergence: 1.84e-04; LR: 1.00e-02; Metric: 0.4208483; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.38 1 + 0.008 value_reward_chosen[t] + 0.091 contr_diff + 0.694 reward + -0.011 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.015 value_reward_chosen*reward + -0.089 contr_diff^2 + -0.037 contr_diff*reward + 0.695 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.345 1 + 0.814 value_reward_not_chosen[t] + 0.012 contr_diff + -0.022 value_reward_not_chosen^2 + 0.015 value_reward_not_chosen*contr_diff + -0.027 contr_diff^2 \n",
            "value_choice[t+1] = 0.034 1 + 0.732 value_choice[t] + -0.062 contr_diff + 0.034 choice + -0.174 value_choice^2 + 0.185 value_choice*contr_diff + -0.268 value_choice*choice + -0.012 contr_diff^2 + -0.06 contr_diff*choice + 0.033 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 5, 5, 5, 0, 5, 0\n",
            "value_reward_not_chosen: 0, 0, 5, 5, 5, 5\n",
            "value_choice: 5, 0, 0, 5, 0, 0, 0, 5, 0, 5\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 205/1000 --- L(Train): 0.4204212 --- L(Val, RNN): 0.4209141 --- L(Val, SINDy): 0.4217391 --- Time: 0.38s; --- Convergence: 1.17e-04; LR: 1.00e-02; Metric: 0.4208483; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.375 1 + 0.012 value_reward_chosen[t] + 0.09 contr_diff + 0.7 reward + -0.005 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + 0.02 value_reward_chosen*reward + -0.085 contr_diff^2 + -0.04 contr_diff*reward + 0.701 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.345 1 + 0.813 value_reward_not_chosen[t] + 0.013 contr_diff + -0.021 value_reward_not_chosen^2 + 0.022 value_reward_not_chosen*contr_diff + -0.03 contr_diff^2 \n",
            "value_choice[t+1] = 0.035 1 + 0.735 value_choice[t] + -0.064 contr_diff + 0.034 choice + -0.171 value_choice^2 + 0.183 value_choice*contr_diff + -0.265 value_choice*choice + -0.008 contr_diff^2 + -0.062 contr_diff*choice + 0.034 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 6, 6, 6, 0, 6, 0\n",
            "value_reward_not_chosen: 0, 0, 6, 6, 6, 6\n",
            "value_choice: 6, 0, 0, 6, 0, 0, 0, 6, 0, 6\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 206/1000 --- L(Train): 0.4218383 --- L(Val, RNN): 0.4207184 --- L(Val, SINDy): 0.4213277 --- Time: 0.33s; --- Convergence: 1.57e-04; LR: 1.00e-02; Metric: 0.4207184; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.37 1 + 0.015 value_reward_chosen[t] + 0.091 contr_diff + 0.706 reward + -0.001 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + 0.024 value_reward_chosen*reward + -0.08 contr_diff^2 + -0.041 contr_diff*reward + 0.707 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.345 1 + 0.812 value_reward_not_chosen[t] + 0.013 contr_diff + -0.019 value_reward_not_chosen^2 + 0.026 value_reward_not_chosen*contr_diff + -0.03 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.738 value_choice[t] + -0.062 contr_diff + 0.036 choice + -0.167 value_choice^2 + 0.186 value_choice*contr_diff + -0.262 value_choice*choice + -0.004 contr_diff^2 + -0.06 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 7, 7, 7, 0, 7, 0\n",
            "value_reward_not_chosen: 0, 0, 7, 7, 7, 7\n",
            "value_choice: 7, 0, 0, 7, 0, 0, 0, 7, 0, 7\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 207/1000 --- L(Train): 0.4219377 --- L(Val, RNN): 0.4208101 --- L(Val, SINDy): 0.4210903 --- Time: 0.37s; --- Convergence: 1.24e-04; LR: 1.00e-02; Metric: 0.4207184; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.37 1 + 0.015 value_reward_chosen[t] + 0.096 contr_diff + 0.709 reward + 0.0 value_reward_chosen^2 + 0.01 value_reward_chosen*contr_diff + 0.025 value_reward_chosen*reward + -0.079 contr_diff^2 + -0.041 contr_diff*reward + 0.71 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.344 1 + 0.809 value_reward_not_chosen[t] + 0.012 contr_diff + -0.017 value_reward_not_chosen^2 + 0.025 value_reward_not_chosen*contr_diff + -0.031 contr_diff^2 \n",
            "value_choice[t+1] = 0.038 1 + 0.742 value_choice[t] + -0.058 contr_diff + 0.038 choice + -0.163 value_choice^2 + 0.191 value_choice*contr_diff + -0.259 value_choice*choice + 0.0 contr_diff^2 + -0.056 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 8, 8, 8, 0, 8, 0\n",
            "value_reward_not_chosen: 0, 0, 8, 8, 8, 8\n",
            "value_choice: 8, 0, 0, 8, 0, 0, 0, 8, 0, 8\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 208/1000 --- L(Train): 0.4211220 --- L(Val, RNN): 0.4207366 --- L(Val, SINDy): 0.4209542 --- Time: 0.30s; --- Convergence: 9.89e-05; LR: 1.00e-02; Metric: 0.4207184; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.369 1 + 0.013 value_reward_chosen[t] + 0.103 contr_diff + 0.712 reward + 0.002 value_reward_chosen^2 + 0.01 value_reward_chosen*contr_diff + 0.025 value_reward_chosen*reward + -0.077 contr_diff^2 + -0.038 contr_diff*reward + 0.713 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.344 1 + 0.807 value_reward_not_chosen[t] + 0.011 contr_diff + -0.014 value_reward_not_chosen^2 + 0.02 value_reward_not_chosen*contr_diff + -0.034 contr_diff^2 \n",
            "value_choice[t+1] = 0.041 1 + 0.746 value_choice[t] + -0.053 contr_diff + 0.04 choice + -0.158 value_choice^2 + 0.197 value_choice*contr_diff + -0.255 value_choice*choice + 0.006 contr_diff^2 + -0.051 contr_diff*choice + 0.039 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 9, 9, 9, 0, 9, 0\n",
            "value_reward_not_chosen: 0, 0, 9, 9, 9, 9\n",
            "value_choice: 9, 0, 0, 9, 0, 0, 0, 9, 0, 9\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 209/1000 --- L(Train): 0.4214309 --- L(Val, RNN): 0.4206716 --- L(Val, SINDy): 0.4208766 --- Time: 0.30s; --- Convergence: 8.19e-05; LR: 1.00e-02; Metric: 0.4206716; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.369 1 + 0.01 value_reward_chosen[t] + 0.109 contr_diff + 0.714 reward + 0.001 value_reward_chosen^2 + 0.009 value_reward_chosen*contr_diff + 0.023 value_reward_chosen*reward + -0.074 contr_diff^2 + -0.034 contr_diff*reward + 0.715 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.345 1 + 0.804 value_reward_not_chosen[t] + 0.011 contr_diff + -0.012 value_reward_not_chosen^2 + 0.015 value_reward_not_chosen*contr_diff + -0.038 contr_diff^2 \n",
            "value_choice[t+1] = 0.043 1 + 0.75 value_choice[t] + -0.047 contr_diff + 0.043 choice + -0.153 value_choice^2 + 0.204 value_choice*contr_diff + -0.251 value_choice*choice + 0.01 contr_diff^2 + -0.045 contr_diff*choice + 0.042 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 10, 10, 10, 0, 10, 0\n",
            "value_reward_not_chosen: 0, 0, 10, 10, 10, 10\n",
            "value_choice: 10, 0, 1, 10, 0, 0, 0, 10, 1, 10\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 210/1000 --- L(Train): 0.4308418 --- L(Val, RNN): 0.4207047 --- L(Val, SINDy): 0.4208420 --- Time: 0.39s; --- Convergence: 5.76e-05; LR: 1.00e-02; Metric: 0.4206716; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.371 1 + 0.005 value_reward_chosen[t] + 0.112 contr_diff + 0.713 reward + -0.001 value_reward_chosen^2 + 0.009 value_reward_chosen*contr_diff + 0.019 value_reward_chosen*reward + -0.073 contr_diff^2 + -0.031 contr_diff*reward + 0.714 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.348 1 + 0.803 value_reward_not_chosen[t] + 0.009 contr_diff + -0.01 value_reward_not_chosen^2 + 0.008 value_reward_not_chosen*contr_diff + -0.043 contr_diff^2 \n",
            "value_choice[t+1] = 0.045 1 + 0.752 value_choice[t] + -0.042 contr_diff + 0.044 choice + -0.15 value_choice^2 + 0.211 value_choice*contr_diff + -0.248 value_choice*choice + 0.014 contr_diff^2 + -0.039 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 11, 11, 11, 0, 11, 0\n",
            "value_reward_not_chosen: 0, 0, 11, 11, 11, 11\n",
            "value_choice: 11, 0, 2, 11, 0, 0, 0, 11, 2, 11\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 211/1000 --- L(Train): 0.4125410 --- L(Val, RNN): 0.4207009 --- L(Val, SINDy): 0.4208057 --- Time: 0.30s; --- Convergence: 3.07e-05; LR: 1.00e-02; Metric: 0.4206716; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.373 1 + 0.0 value_reward_chosen[t] + 0.115 contr_diff + 0.713 reward + -0.003 value_reward_chosen^2 + 0.008 value_reward_chosen*contr_diff + 0.015 value_reward_chosen*reward + -0.072 contr_diff^2 + -0.028 contr_diff*reward + 0.714 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.351 1 + 0.801 value_reward_not_chosen[t] + 0.008 contr_diff + -0.01 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.048 contr_diff^2 \n",
            "value_choice[t+1] = 0.046 1 + 0.754 value_choice[t] + -0.037 contr_diff + 0.045 choice + -0.148 value_choice^2 + 0.217 value_choice*contr_diff + -0.246 value_choice*choice + 0.016 contr_diff^2 + -0.035 contr_diff*choice + 0.045 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 12, 12, 12, 0, 12, 0\n",
            "value_reward_not_chosen: 0, 0, 12, 12, 12, 12\n",
            "value_choice: 12, 0, 3, 12, 0, 0, 0, 12, 3, 12\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 212/1000 --- L(Train): 0.4236041 --- L(Val, RNN): 0.4206069 --- L(Val, SINDy): 0.4207964 --- Time: 0.40s; --- Convergence: 6.23e-05; LR: 1.00e-02; Metric: 0.4206069; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.374 1 + -0.004 value_reward_chosen[t] + 0.117 contr_diff + 0.714 reward + -0.004 value_reward_chosen^2 + 0.007 value_reward_chosen*contr_diff + 0.013 value_reward_chosen*reward + -0.071 contr_diff^2 + -0.025 contr_diff*reward + 0.715 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.355 1 + 0.8 value_reward_not_chosen[t] + 0.006 contr_diff + -0.009 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.052 contr_diff^2 \n",
            "value_choice[t+1] = 0.046 1 + 0.754 value_choice[t] + -0.032 contr_diff + 0.045 choice + -0.148 value_choice^2 + 0.221 value_choice*contr_diff + -0.246 value_choice*choice + 0.018 contr_diff^2 + -0.03 contr_diff*choice + 0.045 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 13, 13, 13, 0, 13, 0\n",
            "value_reward_not_chosen: 0, 0, 13, 13, 13, 0\n",
            "value_choice: 13, 0, 4, 13, 0, 0, 0, 13, 4, 13\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 213/1000 --- L(Train): 0.4231736 --- L(Val, RNN): 0.4206401 --- L(Val, SINDy): 0.4208372 --- Time: 0.30s; --- Convergence: 4.77e-05; LR: 1.00e-02; Metric: 0.4206069; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.375 1 + -0.007 value_reward_chosen[t] + 0.116 contr_diff + 0.714 reward + -0.004 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + 0.01 value_reward_chosen*reward + -0.07 contr_diff^2 + -0.024 contr_diff*reward + 0.715 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.359 1 + 0.8 value_reward_not_chosen[t] + 0.003 contr_diff + -0.01 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + -0.057 contr_diff^2 \n",
            "value_choice[t+1] = 0.045 1 + 0.753 value_choice[t] + -0.03 contr_diff + 0.044 choice + -0.149 value_choice^2 + 0.224 value_choice*contr_diff + -0.248 value_choice*choice + 0.018 contr_diff^2 + -0.028 contr_diff*choice + 0.043 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 14, 14, 14, 0, 14, 0\n",
            "value_reward_not_chosen: 0, 0, 14, 14, 14, 0\n",
            "value_choice: 14, 0, 5, 14, 0, 0, 0, 14, 5, 14\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 214/1000 --- L(Train): 0.4277891 --- L(Val, RNN): 0.4206356 --- L(Val, SINDy): 0.4208665 --- Time: 0.34s; --- Convergence: 2.61e-05; LR: 1.00e-02; Metric: 0.4206069; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.376 1 + -0.011 value_reward_chosen[t] + 0.114 contr_diff + 0.714 reward + -0.006 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + 0.007 value_reward_chosen*reward + -0.068 contr_diff^2 + -0.023 contr_diff*reward + 0.715 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.363 1 + 0.8 value_reward_not_chosen[t] + -0.001 contr_diff + -0.011 value_reward_not_chosen^2 + -0.007 value_reward_not_chosen*contr_diff + -0.061 contr_diff^2 \n",
            "value_choice[t+1] = 0.043 1 + 0.751 value_choice[t] + -0.03 contr_diff + 0.042 choice + -0.152 value_choice^2 + 0.225 value_choice*contr_diff + -0.25 value_choice*choice + 0.018 contr_diff^2 + -0.028 contr_diff*choice + 0.041 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 15, 15, 15, 0, 15, 0\n",
            "value_reward_not_chosen: 0, 0, 15, 15, 15, 0\n",
            "value_choice: 15, 0, 6, 15, 0, 0, 0, 15, 6, 15\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 215/1000 --- L(Train): 0.4266143 --- L(Val, RNN): 0.4205410 --- L(Val, SINDy): 0.4208915 --- Time: 0.32s; --- Convergence: 6.04e-05; LR: 1.00e-02; Metric: 0.4205410; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.377 1 + -0.014 value_reward_chosen[t] + 0.11 contr_diff + 0.714 reward + -0.007 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.067 contr_diff^2 + -0.024 contr_diff*reward + 0.715 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.367 1 + 0.799 value_reward_not_chosen[t] + -0.006 contr_diff + -0.011 value_reward_not_chosen^2 + -0.007 value_reward_not_chosen*contr_diff + -0.064 contr_diff^2 \n",
            "value_choice[t+1] = 0.04 1 + 0.748 value_choice[t] + -0.031 contr_diff + 0.039 choice + -0.155 value_choice^2 + 0.224 value_choice*contr_diff + -0.253 value_choice*choice + 0.016 contr_diff^2 + -0.029 contr_diff*choice + 0.039 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 16, 16, 16, 0, 16, 0\n",
            "value_reward_not_chosen: 0, 0, 16, 16, 16, 0\n",
            "value_choice: 16, 0, 7, 16, 0, 0, 0, 16, 7, 16\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 216/1000 --- L(Train): 0.4259273 --- L(Val, RNN): 0.4205403 --- L(Val, SINDy): 0.4209127 --- Time: 0.30s; --- Convergence: 3.05e-05; LR: 1.00e-02; Metric: 0.4205403; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.379 1 + -0.017 value_reward_chosen[t] + 0.106 contr_diff + 0.713 reward + -0.008 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + 0.001 value_reward_chosen*reward + -0.066 contr_diff^2 + -0.025 contr_diff*reward + 0.714 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.37 1 + 0.799 value_reward_not_chosen[t] + -0.012 contr_diff + -0.012 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + -0.064 contr_diff^2 \n",
            "value_choice[t+1] = 0.037 1 + 0.745 value_choice[t] + -0.034 contr_diff + 0.037 choice + -0.158 value_choice^2 + 0.221 value_choice*contr_diff + -0.256 value_choice*choice + 0.013 contr_diff^2 + -0.032 contr_diff*choice + 0.036 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 17, 17, 17, 0, 17, 0\n",
            "value_reward_not_chosen: 0, 0, 17, 17, 17, 0\n",
            "value_choice: 17, 0, 8, 17, 0, 0, 0, 17, 8, 17\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 217/1000 --- L(Train): 0.4293611 --- L(Val, RNN): 0.4204935 --- L(Val, SINDy): 0.4209220 --- Time: 0.30s; --- Convergence: 3.86e-05; LR: 1.00e-02; Metric: 0.4204935; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.378 1 + -0.018 value_reward_chosen[t] + 0.101 contr_diff + 0.714 reward + -0.007 value_reward_chosen^2 + -0.007 value_reward_chosen*contr_diff + 0.001 value_reward_chosen*reward + -0.063 contr_diff^2 + -0.027 contr_diff*reward + 0.715 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.371 1 + 0.797 value_reward_not_chosen[t] + -0.016 contr_diff + -0.011 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.063 contr_diff^2 \n",
            "value_choice[t+1] = 0.035 1 + 0.741 value_choice[t] + -0.039 contr_diff + 0.034 choice + -0.161 value_choice^2 + 0.217 value_choice*contr_diff + -0.259 value_choice*choice + 0.011 contr_diff^2 + -0.037 contr_diff*choice + 0.033 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 18, 18, 18, 0, 18, 0\n",
            "value_reward_not_chosen: 0, 0, 18, 18, 18, 0\n",
            "value_choice: 18, 0, 9, 18, 0, 0, 0, 18, 9, 18\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 218/1000 --- L(Train): 0.4138353 --- L(Val, RNN): 0.4204721 --- L(Val, SINDy): 0.4209335 --- Time: 0.32s; --- Convergence: 3.01e-05; LR: 1.00e-02; Metric: 0.4204721; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.377 1 + -0.017 value_reward_chosen[t] + 0.096 contr_diff + 0.716 reward + -0.006 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + 0.001 value_reward_chosen*reward + -0.06 contr_diff^2 + -0.029 contr_diff*reward + 0.717 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.372 1 + 0.795 value_reward_not_chosen[t] + -0.018 contr_diff + -0.011 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.06 contr_diff^2 \n",
            "value_choice[t+1] = 0.033 1 + 0.739 value_choice[t] + -0.044 contr_diff + 0.032 choice + -0.164 value_choice^2 + 0.213 value_choice*contr_diff + -0.261 value_choice*choice + 0.009 contr_diff^2 + -0.042 contr_diff*choice + 0.031 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 19, 19, 19, 0, 19, 0\n",
            "value_reward_not_chosen: 0, 0, 19, 19, 19, 0\n",
            "value_choice: 19, 0, 10, 19, 0, 0, 0, 19, 10, 19\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 219/1000 --- L(Train): 0.4254930 --- L(Val, RNN): 0.4204308 --- L(Val, SINDy): 0.4209487 --- Time: 0.41s; --- Convergence: 3.57e-05; LR: 1.00e-02; Metric: 0.4204308; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.375 1 + -0.016 value_reward_chosen[t] + 0.092 contr_diff + 0.718 reward + -0.004 value_reward_chosen^2 + -0.014 value_reward_chosen*contr_diff + 0.002 value_reward_chosen*reward + -0.057 contr_diff^2 + -0.031 contr_diff*reward + 0.719 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.373 1 + 0.793 value_reward_not_chosen[t] + -0.018 contr_diff + -0.01 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.057 contr_diff^2 \n",
            "value_choice[t+1] = 0.032 1 + 0.738 value_choice[t] + -0.048 contr_diff + 0.031 choice + -0.165 value_choice^2 + 0.209 value_choice*contr_diff + -0.263 value_choice*choice + 0.008 contr_diff^2 + -0.046 contr_diff*choice + 0.03 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 20, 20, 20, 0, 20, 0\n",
            "value_reward_not_chosen: 0, 0, 20, 20, 20, 0\n",
            "value_choice: 20, 0, 11, 20, 0, 0, 0, 20, 11, 20\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 220/1000 --- L(Train): 0.4234471 --- L(Val, RNN): 0.4203992 --- L(Val, SINDy): 0.4209467 --- Time: 0.32s; --- Convergence: 3.36e-05; LR: 1.00e-02; Metric: 0.4203992; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.373 1 + -0.015 value_reward_chosen[t] + 0.089 contr_diff + 0.721 reward + -0.001 value_reward_chosen^2 + -0.015 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.053 contr_diff^2 + -0.033 contr_diff*reward + 0.722 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.374 1 + 0.792 value_reward_not_chosen[t] + -0.015 contr_diff + -0.01 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + -0.053 contr_diff^2 \n",
            "value_choice[t+1] = 0.032 1 + 0.738 value_choice[t] + -0.053 contr_diff + 0.032 choice + -0.165 value_choice^2 + 0.205 value_choice*contr_diff + -0.262 value_choice*choice + 0.009 contr_diff^2 + -0.051 contr_diff*choice + 0.031 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 21, 21, 21, 0, 21, 0\n",
            "value_reward_not_chosen: 0, 0, 21, 21, 21, 0\n",
            "value_choice: 21, 0, 0, 21, 0, 0, 0, 21, 0, 21\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 221/1000 --- L(Train): 0.4094957 --- L(Val, RNN): 0.4203967 --- L(Val, SINDy): 0.4209347 --- Time: 0.38s; --- Convergence: 1.81e-05; LR: 1.00e-02; Metric: 0.4203967; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.371 1 + -0.013 value_reward_chosen[t] + 0.087 contr_diff + 0.722 reward + 0.0 value_reward_chosen^2 + -0.015 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.049 contr_diff^2 + -0.034 contr_diff*reward + 0.723 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.375 1 + 0.792 value_reward_not_chosen[t] + -0.011 contr_diff + -0.011 value_reward_not_chosen^2 + -0.008 value_reward_not_chosen*contr_diff + -0.048 contr_diff^2 \n",
            "value_choice[t+1] = 0.034 1 + 0.74 value_choice[t] + -0.057 contr_diff + 0.034 choice + -0.162 value_choice^2 + 0.201 value_choice*contr_diff + -0.26 value_choice*choice + 0.01 contr_diff^2 + -0.055 contr_diff*choice + 0.033 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 22, 22, 22, 1, 22, 0\n",
            "value_reward_not_chosen: 0, 0, 22, 22, 22, 1\n",
            "value_choice: 22, 0, 0, 22, 0, 0, 0, 22, 0, 22\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 222/1000 --- L(Train): 0.4250384 --- L(Val, RNN): 0.4203573 --- L(Val, SINDy): 0.4209333 --- Time: 0.35s; --- Convergence: 2.87e-05; LR: 1.00e-02; Metric: 0.4203573; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.372 1 + -0.012 value_reward_chosen[t] + 0.086 contr_diff + 0.722 reward + 0.0 value_reward_chosen^2 + -0.014 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.047 contr_diff^2 + -0.034 contr_diff*reward + 0.723 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.376 1 + 0.792 value_reward_not_chosen[t] + -0.006 contr_diff + -0.013 value_reward_not_chosen^2 + -0.009 value_reward_not_chosen*contr_diff + -0.043 contr_diff^2 \n",
            "value_choice[t+1] = 0.037 1 + 0.743 value_choice[t] + -0.06 contr_diff + 0.036 choice + -0.16 value_choice^2 + 0.199 value_choice*contr_diff + -0.257 value_choice*choice + 0.012 contr_diff^2 + -0.058 contr_diff*choice + 0.036 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 23, 23, 23, 2, 23, 0\n",
            "value_reward_not_chosen: 0, 0, 23, 23, 23, 2\n",
            "value_choice: 23, 0, 0, 23, 0, 0, 0, 23, 0, 23\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 223/1000 --- L(Train): 0.4192916 --- L(Val, RNN): 0.4203358 --- L(Val, SINDy): 0.4209050 --- Time: 0.45s; --- Convergence: 2.51e-05; LR: 1.00e-02; Metric: 0.4203358; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.373 1 + -0.011 value_reward_chosen[t] + 0.085 contr_diff + 0.721 reward + -0.0 value_reward_chosen^2 + -0.013 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.046 contr_diff^2 + -0.033 contr_diff*reward + 0.723 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.377 1 + 0.792 value_reward_not_chosen[t] + -0.002 contr_diff + -0.015 value_reward_not_chosen^2 + -0.011 value_reward_not_chosen*contr_diff + -0.039 contr_diff^2 \n",
            "value_choice[t+1] = 0.039 1 + 0.745 value_choice[t] + -0.062 contr_diff + 0.038 choice + -0.158 value_choice^2 + 0.197 value_choice*contr_diff + -0.256 value_choice*choice + 0.014 contr_diff^2 + -0.06 contr_diff*choice + 0.038 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 24, 24, 24, 3, 24, 0\n",
            "value_reward_not_chosen: 0, 0, 24, 24, 24, 3\n",
            "value_choice: 24, 0, 0, 24, 0, 0, 0, 24, 0, 24\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 224/1000 --- L(Train): 0.4175624 --- L(Val, RNN): 0.4202338 --- L(Val, SINDy): 0.4208211 --- Time: 0.32s; --- Convergence: 6.36e-05; LR: 1.00e-02; Metric: 0.4202338; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.374 1 + -0.01 value_reward_chosen[t] + 0.086 contr_diff + 0.721 reward + -0.0 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.044 contr_diff^2 + -0.032 contr_diff*reward + 0.722 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.378 1 + 0.793 value_reward_not_chosen[t] + 0.001 contr_diff + -0.017 value_reward_not_chosen^2 + -0.011 value_reward_not_chosen*contr_diff + -0.034 contr_diff^2 \n",
            "value_choice[t+1] = 0.04 1 + 0.746 value_choice[t] + -0.064 contr_diff + 0.039 choice + -0.156 value_choice^2 + 0.196 value_choice*contr_diff + -0.255 value_choice*choice + 0.014 contr_diff^2 + -0.062 contr_diff*choice + 0.039 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 25, 25, 25, 4, 25, 0\n",
            "value_reward_not_chosen: 0, 0, 25, 25, 25, 4\n",
            "value_choice: 25, 0, 0, 25, 0, 0, 0, 25, 0, 25\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 225/1000 --- L(Train): 0.4200898 --- L(Val, RNN): 0.4202722 --- L(Val, SINDy): 0.4207337 --- Time: 0.37s; --- Convergence: 5.10e-05; LR: 1.00e-02; Metric: 0.4202338; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.374 1 + -0.009 value_reward_chosen[t] + 0.087 contr_diff + 0.72 reward + -0.001 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.042 contr_diff^2 + -0.031 contr_diff*reward + 0.721 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.378 1 + 0.793 value_reward_not_chosen[t] + 0.001 contr_diff + -0.018 value_reward_not_chosen^2 + -0.009 value_reward_not_chosen*contr_diff + -0.028 contr_diff^2 \n",
            "value_choice[t+1] = 0.039 1 + 0.746 value_choice[t] + -0.064 contr_diff + 0.039 choice + -0.156 value_choice^2 + 0.195 value_choice*contr_diff + -0.255 value_choice*choice + 0.013 contr_diff^2 + -0.062 contr_diff*choice + 0.038 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 26, 26, 26, 5, 26, 0\n",
            "value_reward_not_chosen: 0, 0, 26, 26, 26, 5\n",
            "value_choice: 26, 0, 0, 26, 0, 0, 0, 26, 0, 26\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 226/1000 --- L(Train): 0.4183433 --- L(Val, RNN): 0.4202732 --- L(Val, SINDy): 0.4206532 --- Time: 0.32s; --- Convergence: 2.60e-05; LR: 1.00e-02; Metric: 0.4202338; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.375 1 + -0.008 value_reward_chosen[t] + 0.09 contr_diff + 0.719 reward + -0.001 value_reward_chosen^2 + -0.007 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.041 contr_diff^2 + -0.028 contr_diff*reward + 0.72 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.377 1 + 0.792 value_reward_not_chosen[t] + 0.0 contr_diff + -0.019 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + -0.022 contr_diff^2 \n",
            "value_choice[t+1] = 0.038 1 + 0.745 value_choice[t] + -0.063 contr_diff + 0.038 choice + -0.157 value_choice^2 + 0.196 value_choice*contr_diff + -0.255 value_choice*choice + 0.01 contr_diff^2 + -0.061 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 27, 27, 27, 6, 27, 0\n",
            "value_reward_not_chosen: 0, 0, 27, 27, 27, 6\n",
            "value_choice: 27, 0, 0, 27, 0, 0, 0, 27, 0, 27\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 227/1000 --- L(Train): 0.4107234 --- L(Val, RNN): 0.4202271 --- L(Val, SINDy): 0.4205735 --- Time: 0.45s; --- Convergence: 3.61e-05; LR: 1.00e-02; Metric: 0.4202271; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.376 1 + -0.007 value_reward_chosen[t] + 0.094 contr_diff + 0.718 reward + -0.002 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.039 contr_diff^2 + -0.024 contr_diff*reward + 0.719 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.376 1 + 0.791 value_reward_not_chosen[t] + -0.001 contr_diff + -0.019 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.016 contr_diff^2 \n",
            "value_choice[t+1] = 0.037 1 + 0.744 value_choice[t] + -0.061 contr_diff + 0.037 choice + -0.157 value_choice^2 + 0.197 value_choice*contr_diff + -0.256 value_choice*choice + 0.008 contr_diff^2 + -0.059 contr_diff*choice + 0.036 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 28, 28, 28, 7, 28, 0\n",
            "value_reward_not_chosen: 0, 0, 28, 28, 28, 7\n",
            "value_choice: 28, 0, 0, 28, 0, 0, 0, 28, 0, 28\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 228/1000 --- L(Train): 0.4197267 --- L(Val, RNN): 0.4203010 --- L(Val, SINDy): 0.4205502 --- Time: 0.33s; --- Convergence: 5.50e-05; LR: 1.00e-02; Metric: 0.4202271; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.377 1 + -0.006 value_reward_chosen[t] + 0.1 contr_diff + 0.717 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.002 value_reward_chosen*reward + -0.038 contr_diff^2 + -0.02 contr_diff*reward + 0.718 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.375 1 + 0.789 value_reward_not_chosen[t] + -0.004 contr_diff + -0.018 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.009 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.743 value_choice[t] + -0.059 contr_diff + 0.036 choice + -0.158 value_choice^2 + 0.199 value_choice*contr_diff + -0.257 value_choice*choice + 0.006 contr_diff^2 + -0.057 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 29, 29, 29, 8, 29, 0\n",
            "value_reward_not_chosen: 0, 0, 29, 29, 29, 8\n",
            "value_choice: 29, 0, 0, 29, 0, 0, 0, 29, 0, 29\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 229/1000 --- L(Train): 0.4074208 --- L(Val, RNN): 0.4202241 --- L(Val, SINDy): 0.4205151 --- Time: 0.33s; --- Convergence: 6.60e-05; LR: 1.00e-02; Metric: 0.4202241; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.375 1 + -0.004 value_reward_chosen[t] + 0.104 contr_diff + 0.718 reward + -0.001 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.034 contr_diff^2 + -0.014 contr_diff*reward + 0.719 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.374 1 + 0.787 value_reward_not_chosen[t] + -0.007 contr_diff + -0.018 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.743 value_choice[t] + -0.055 contr_diff + 0.035 choice + -0.158 value_choice^2 + 0.203 value_choice*contr_diff + -0.257 value_choice*choice + 0.004 contr_diff^2 + -0.053 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 30, 30, 30, 9, 30, 0\n",
            "value_reward_not_chosen: 0, 0, 30, 30, 30, 9\n",
            "value_choice: 30, 0, 0, 30, 0, 0, 0, 30, 0, 30\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 230/1000 --- L(Train): 0.4181192 --- L(Val, RNN): 0.4201476 --- L(Val, SINDy): 0.4204864 --- Time: 0.33s; --- Convergence: 7.12e-05; LR: 1.00e-02; Metric: 0.4201476; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.374 1 + -0.003 value_reward_chosen[t] + 0.107 contr_diff + 0.719 reward + 0.001 value_reward_chosen^2 + -0.005 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.029 contr_diff^2 + -0.01 contr_diff*reward + 0.72 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.373 1 + 0.785 value_reward_not_chosen[t] + -0.01 contr_diff + -0.018 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.743 value_choice[t] + -0.051 contr_diff + 0.036 choice + -0.157 value_choice^2 + 0.207 value_choice*contr_diff + -0.257 value_choice*choice + 0.003 contr_diff^2 + -0.049 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 31, 31, 31, 10, 31, 0\n",
            "value_reward_not_chosen: 0, 0, 31, 31, 31, 10\n",
            "value_choice: 31, 0, 0, 31, 0, 0, 0, 31, 1, 31\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 231/1000 --- L(Train): 0.4279128 --- L(Val, RNN): 0.4201801 --- L(Val, SINDy): 0.4204776 --- Time: 0.41s; --- Convergence: 5.19e-05; LR: 1.00e-02; Metric: 0.4201476; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.374 1 + -0.001 value_reward_chosen[t] + 0.109 contr_diff + 0.719 reward + 0.002 value_reward_chosen^2 + -0.005 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.028 contr_diff^2 + -0.008 contr_diff*reward + 0.72 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.373 1 + 0.784 value_reward_not_chosen[t] + -0.01 contr_diff + -0.018 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.037 1 + 0.744 value_choice[t] + -0.047 contr_diff + 0.036 choice + -0.156 value_choice^2 + 0.21 value_choice*contr_diff + -0.256 value_choice*choice + 0.003 contr_diff^2 + -0.045 contr_diff*choice + 0.036 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 32, 32, 32, 11, 32, 0\n",
            "value_reward_not_chosen: 0, 0, 32, 32, 32, 11\n",
            "value_choice: 32, 0, 1, 32, 0, 0, 0, 32, 2, 32\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 232/1000 --- L(Train): 0.4135144 --- L(Val, RNN): 0.4201506 --- L(Val, SINDy): 0.4204196 --- Time: 0.33s; --- Convergence: 4.07e-05; LR: 1.00e-02; Metric: 0.4201476; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.374 1 + -0.001 value_reward_chosen[t] + 0.108 contr_diff + 0.718 reward + 0.001 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.027 contr_diff^2 + -0.008 contr_diff*reward + 0.719 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.375 1 + 0.784 value_reward_not_chosen[t] + -0.009 contr_diff + -0.02 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.038 1 + 0.745 value_choice[t] + -0.044 contr_diff + 0.037 choice + -0.155 value_choice^2 + 0.214 value_choice*contr_diff + -0.255 value_choice*choice + 0.003 contr_diff^2 + -0.042 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 33, 33, 33, 12, 33, 0\n",
            "value_reward_not_chosen: 0, 0, 33, 33, 33, 12\n",
            "value_choice: 33, 0, 2, 33, 0, 0, 0, 33, 3, 33\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 233/1000 --- L(Train): 0.4257932 --- L(Val, RNN): 0.4200740 --- L(Val, SINDy): 0.4203417 --- Time: 0.30s; --- Convergence: 5.87e-05; LR: 1.00e-02; Metric: 0.4200740; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.374 1 + -0.0 value_reward_chosen[t] + 0.107 contr_diff + 0.718 reward + 0.001 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.026 contr_diff^2 + -0.008 contr_diff*reward + 0.719 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.376 1 + 0.784 value_reward_not_chosen[t] + -0.007 contr_diff + -0.021 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + 0.007 contr_diff^2 \n",
            "value_choice[t+1] = 0.039 1 + 0.746 value_choice[t] + -0.042 contr_diff + 0.038 choice + -0.154 value_choice^2 + 0.216 value_choice*contr_diff + -0.254 value_choice*choice + 0.003 contr_diff^2 + -0.04 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 34, 34, 34, 13, 34, 0\n",
            "value_reward_not_chosen: 0, 0, 34, 34, 34, 13\n",
            "value_choice: 34, 0, 3, 34, 0, 0, 0, 34, 4, 34\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 234/1000 --- L(Train): 0.4233509 --- L(Val, RNN): 0.4200197 --- L(Val, SINDy): 0.4202746 --- Time: 0.31s; --- Convergence: 5.65e-05; LR: 1.00e-02; Metric: 0.4200197; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.376 1 + -0.001 value_reward_chosen[t] + 0.105 contr_diff + 0.716 reward + -0.0 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + 0.002 value_reward_chosen*reward + -0.027 contr_diff^2 + -0.009 contr_diff*reward + 0.717 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.377 1 + 0.784 value_reward_not_chosen[t] + -0.006 contr_diff + -0.023 value_reward_not_chosen^2 + -0.008 value_reward_not_chosen*contr_diff + 0.008 contr_diff^2 \n",
            "value_choice[t+1] = 0.039 1 + 0.746 value_choice[t] + -0.04 contr_diff + 0.038 choice + -0.153 value_choice^2 + 0.218 value_choice*contr_diff + -0.254 value_choice*choice + 0.002 contr_diff^2 + -0.038 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 35, 35, 35, 14, 35, 0\n",
            "value_reward_not_chosen: 0, 0, 35, 35, 35, 14\n",
            "value_choice: 35, 0, 4, 35, 0, 0, 0, 35, 5, 35\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 235/1000 --- L(Train): 0.4175076 --- L(Val, RNN): 0.4199580 --- L(Val, SINDy): 0.4201758 --- Time: 0.27s; --- Convergence: 5.91e-05; LR: 1.00e-02; Metric: 0.4199580; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.377 1 + -0.001 value_reward_chosen[t] + 0.103 contr_diff + 0.715 reward + -0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.001 value_reward_chosen*reward + -0.027 contr_diff^2 + -0.011 contr_diff*reward + 0.716 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.378 1 + 0.783 value_reward_not_chosen[t] + -0.005 contr_diff + -0.023 value_reward_not_chosen^2 + -0.01 value_reward_not_chosen*contr_diff + 0.009 contr_diff^2 \n",
            "value_choice[t+1] = 0.039 1 + 0.746 value_choice[t] + -0.039 contr_diff + 0.038 choice + -0.153 value_choice^2 + 0.219 value_choice*contr_diff + -0.254 value_choice*choice + 0.002 contr_diff^2 + -0.037 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 36, 36, 36, 15, 36, 0\n",
            "value_reward_not_chosen: 0, 0, 36, 36, 36, 15\n",
            "value_choice: 36, 0, 5, 36, 0, 0, 0, 36, 6, 36\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 236/1000 --- L(Train): 0.4170941 --- L(Val, RNN): 0.4199662 --- L(Val, SINDy): 0.4200764 --- Time: 0.33s; --- Convergence: 3.36e-05; LR: 1.00e-02; Metric: 0.4199580; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.379 1 + -0.002 value_reward_chosen[t] + 0.102 contr_diff + 0.714 reward + -0.003 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.028 contr_diff^2 + -0.012 contr_diff*reward + 0.715 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.377 1 + 0.782 value_reward_not_chosen[t] + -0.005 contr_diff + -0.023 value_reward_not_chosen^2 + -0.011 value_reward_not_chosen*contr_diff + 0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.038 1 + 0.746 value_choice[t] + -0.039 contr_diff + 0.037 choice + -0.154 value_choice^2 + 0.219 value_choice*contr_diff + -0.255 value_choice*choice + 0.001 contr_diff^2 + -0.037 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 37, 37, 37, 16, 37, 0\n",
            "value_reward_not_chosen: 0, 0, 37, 37, 37, 16\n",
            "value_choice: 37, 0, 6, 37, 0, 0, 0, 37, 7, 37\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 237/1000 --- L(Train): 0.4282243 --- L(Val, RNN): 0.4199152 --- L(Val, SINDy): 0.4200195 --- Time: 0.31s; --- Convergence: 4.23e-05; LR: 1.00e-02; Metric: 0.4199152; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.381 1 + -0.002 value_reward_chosen[t] + 0.101 contr_diff + 0.713 reward + -0.004 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + -0.001 value_reward_chosen*reward + -0.028 contr_diff^2 + -0.013 contr_diff*reward + 0.714 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.377 1 + 0.78 value_reward_not_chosen[t] + -0.005 contr_diff + -0.022 value_reward_not_chosen^2 + -0.011 value_reward_not_chosen*contr_diff + 0.012 contr_diff^2 \n",
            "value_choice[t+1] = 0.037 1 + 0.745 value_choice[t] + -0.038 contr_diff + 0.037 choice + -0.154 value_choice^2 + 0.22 value_choice*contr_diff + -0.256 value_choice*choice + -0.0 contr_diff^2 + -0.036 contr_diff*choice + 0.036 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 38, 38, 38, 17, 38, 0\n",
            "value_reward_not_chosen: 0, 0, 38, 38, 38, 17\n",
            "value_choice: 38, 0, 7, 38, 0, 0, 0, 38, 8, 38\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 238/1000 --- L(Train): 0.4218931 --- L(Val, RNN): 0.4198324 --- L(Val, SINDy): 0.4199756 --- Time: 0.38s; --- Convergence: 6.26e-05; LR: 1.00e-02; Metric: 0.4198324; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.382 1 + -0.002 value_reward_chosen[t] + 0.1 contr_diff + 0.712 reward + -0.005 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + -0.002 value_reward_chosen*reward + -0.028 contr_diff^2 + -0.013 contr_diff*reward + 0.713 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.377 1 + 0.779 value_reward_not_chosen[t] + -0.007 contr_diff + -0.022 value_reward_not_chosen^2 + -0.012 value_reward_not_chosen*contr_diff + 0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.037 1 + 0.744 value_choice[t] + -0.039 contr_diff + 0.036 choice + -0.155 value_choice^2 + 0.219 value_choice*contr_diff + -0.256 value_choice*choice + -0.002 contr_diff^2 + -0.037 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 39, 39, 39, 18, 39, 0\n",
            "value_reward_not_chosen: 0, 0, 39, 39, 39, 18\n",
            "value_choice: 39, 0, 8, 39, 0, 0, 0, 39, 9, 39\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 239/1000 --- L(Train): 0.4303091 --- L(Val, RNN): 0.4198610 --- L(Val, SINDy): 0.4199623 --- Time: 0.36s; --- Convergence: 4.56e-05; LR: 1.00e-02; Metric: 0.4198324; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.382 1 + -0.002 value_reward_chosen[t] + 0.099 contr_diff + 0.713 reward + -0.005 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + -0.002 value_reward_chosen*reward + -0.027 contr_diff^2 + -0.014 contr_diff*reward + 0.714 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.377 1 + 0.777 value_reward_not_chosen[t] + -0.008 contr_diff + -0.022 value_reward_not_chosen^2 + -0.012 value_reward_not_chosen*contr_diff + 0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.743 value_choice[t] + -0.039 contr_diff + 0.035 choice + -0.156 value_choice^2 + 0.219 value_choice*contr_diff + -0.257 value_choice*choice + -0.003 contr_diff^2 + -0.037 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 40, 40, 40, 19, 40, 0\n",
            "value_reward_not_chosen: 0, 0, 40, 40, 40, 19\n",
            "value_choice: 40, 0, 9, 40, 0, 0, 0, 40, 10, 40\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 240/1000 --- L(Train): 0.4249288 --- L(Val, RNN): 0.4199455 --- L(Val, SINDy): 0.4199793 --- Time: 0.35s; --- Convergence: 6.50e-05; LR: 1.00e-02; Metric: 0.4198324; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.381 1 + -0.001 value_reward_chosen[t] + 0.098 contr_diff + 0.714 reward + -0.004 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + -0.001 value_reward_chosen*reward + -0.026 contr_diff^2 + -0.015 contr_diff*reward + 0.715 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.378 1 + 0.776 value_reward_not_chosen[t] + -0.01 contr_diff + -0.022 value_reward_not_chosen^2 + -0.013 value_reward_not_chosen*contr_diff + 0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.743 value_choice[t] + -0.04 contr_diff + 0.035 choice + -0.156 value_choice^2 + 0.218 value_choice*contr_diff + -0.258 value_choice*choice + -0.003 contr_diff^2 + -0.038 contr_diff*choice + 0.034 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 41, 41, 41, 20, 41, 0\n",
            "value_reward_not_chosen: 0, 0, 41, 41, 41, 20\n",
            "value_choice: 41, 0, 10, 41, 0, 0, 0, 41, 11, 41\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 241/1000 --- L(Train): 0.4139111 --- L(Val, RNN): 0.4197940 --- L(Val, SINDy): 0.4199820 --- Time: 0.32s; --- Convergence: 1.08e-04; LR: 1.00e-02; Metric: 0.4197940; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.379 1 + 0.002 value_reward_chosen[t] + 0.097 contr_diff + 0.717 reward + -0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + 0.001 value_reward_chosen*reward + -0.023 contr_diff^2 + -0.016 contr_diff*reward + 0.718 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.379 1 + 0.776 value_reward_not_chosen[t] + -0.012 contr_diff + -0.023 value_reward_not_chosen^2 + -0.013 value_reward_not_chosen*contr_diff + 0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.743 value_choice[t] + -0.042 contr_diff + 0.035 choice + -0.156 value_choice^2 + 0.217 value_choice*contr_diff + -0.258 value_choice*choice + -0.004 contr_diff^2 + -0.04 contr_diff*choice + 0.034 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 42, 42, 42, 21, 42, 0\n",
            "value_reward_not_chosen: 0, 0, 42, 42, 42, 21\n",
            "value_choice: 42, 0, 11, 42, 0, 0, 0, 42, 12, 42\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 242/1000 --- L(Train): 0.4177271 --- L(Val, RNN): 0.4197347 --- L(Val, SINDy): 0.4200082 --- Time: 0.36s; --- Convergence: 8.38e-05; LR: 1.00e-02; Metric: 0.4197347; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.376 1 + 0.004 value_reward_chosen[t] + 0.095 contr_diff + 0.72 reward + 0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.021 contr_diff^2 + -0.018 contr_diff*reward + 0.721 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.381 1 + 0.776 value_reward_not_chosen[t] + -0.015 contr_diff + -0.025 value_reward_not_chosen^2 + -0.014 value_reward_not_chosen*contr_diff + 0.012 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.743 value_choice[t] + -0.043 contr_diff + 0.035 choice + -0.156 value_choice^2 + 0.215 value_choice*contr_diff + -0.258 value_choice*choice + -0.004 contr_diff^2 + -0.041 contr_diff*choice + 0.034 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 43, 43, 43, 22, 43, 0\n",
            "value_reward_not_chosen: 0, 0, 43, 43, 43, 22\n",
            "value_choice: 43, 0, 12, 43, 0, 0, 0, 43, 13, 43\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 243/1000 --- L(Train): 0.4162513 --- L(Val, RNN): 0.4197216 --- L(Val, SINDy): 0.4200391 --- Time: 0.34s; --- Convergence: 4.84e-05; LR: 1.00e-02; Metric: 0.4197216; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.375 1 + 0.005 value_reward_chosen[t] + 0.094 contr_diff + 0.722 reward + 0.003 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.019 contr_diff^2 + -0.018 contr_diff*reward + 0.723 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.382 1 + 0.776 value_reward_not_chosen[t] + -0.017 contr_diff + -0.026 value_reward_not_chosen^2 + -0.014 value_reward_not_chosen*contr_diff + 0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.743 value_choice[t] + -0.045 contr_diff + 0.035 choice + -0.156 value_choice^2 + 0.214 value_choice*contr_diff + -0.258 value_choice*choice + -0.004 contr_diff^2 + -0.043 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 44, 44, 44, 23, 44, 0\n",
            "value_reward_not_chosen: 0, 0, 44, 44, 44, 23\n",
            "value_choice: 44, 0, 13, 44, 0, 0, 0, 44, 14, 44\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 244/1000 --- L(Train): 0.4231050 --- L(Val, RNN): 0.4196604 --- L(Val, SINDy): 0.4200376 --- Time: 0.31s; --- Convergence: 5.48e-05; LR: 1.00e-02; Metric: 0.4196604; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.374 1 + 0.005 value_reward_chosen[t] + 0.093 contr_diff + 0.723 reward + 0.003 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.017 contr_diff^2 + -0.019 contr_diff*reward + 0.724 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.384 1 + 0.775 value_reward_not_chosen[t] + -0.018 contr_diff + -0.026 value_reward_not_chosen^2 + -0.015 value_reward_not_chosen*contr_diff + 0.009 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.743 value_choice[t] + -0.046 contr_diff + 0.035 choice + -0.156 value_choice^2 + 0.212 value_choice*contr_diff + -0.258 value_choice*choice + -0.005 contr_diff^2 + -0.044 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 45, 45, 45, 24, 45, 0\n",
            "value_reward_not_chosen: 0, 0, 45, 45, 45, 24\n",
            "value_choice: 45, 0, 14, 45, 0, 0, 0, 45, 15, 45\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 245/1000 --- L(Train): 0.4251660 --- L(Val, RNN): 0.4196142 --- L(Val, SINDy): 0.4200395 --- Time: 0.38s; --- Convergence: 5.05e-05; LR: 1.00e-02; Metric: 0.4196142; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.374 1 + 0.005 value_reward_chosen[t] + 0.093 contr_diff + 0.723 reward + 0.003 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.017 contr_diff^2 + -0.018 contr_diff*reward + 0.725 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.386 1 + 0.774 value_reward_not_chosen[t] + -0.019 contr_diff + -0.027 value_reward_not_chosen^2 + -0.014 value_reward_not_chosen*contr_diff + 0.008 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.742 value_choice[t] + -0.047 contr_diff + 0.035 choice + -0.156 value_choice^2 + 0.211 value_choice*contr_diff + -0.258 value_choice*choice + -0.005 contr_diff^2 + -0.045 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 46, 46, 46, 25, 46, 0\n",
            "value_reward_not_chosen: 0, 0, 46, 46, 46, 25\n",
            "value_choice: 46, 0, 15, 46, 0, 0, 0, 46, 16, 46\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 246/1000 --- L(Train): 0.4263043 --- L(Val, RNN): 0.4196833 --- L(Val, SINDy): 0.4200495 --- Time: 0.30s; --- Convergence: 5.98e-05; LR: 1.00e-02; Metric: 0.4196142; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.376 1 + 0.004 value_reward_chosen[t] + 0.095 contr_diff + 0.723 reward + 0.002 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + 0.002 value_reward_chosen*reward + -0.018 contr_diff^2 + -0.015 contr_diff*reward + 0.724 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.387 1 + 0.772 value_reward_not_chosen[t] + -0.019 contr_diff + -0.027 value_reward_not_chosen^2 + -0.014 value_reward_not_chosen*contr_diff + 0.007 contr_diff^2 \n",
            "value_choice[t+1] = 0.035 1 + 0.741 value_choice[t] + -0.048 contr_diff + 0.034 choice + -0.157 value_choice^2 + 0.21 value_choice*contr_diff + -0.259 value_choice*choice + -0.007 contr_diff^2 + -0.046 contr_diff*choice + 0.033 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 47, 47, 47, 26, 47, 0\n",
            "value_reward_not_chosen: 0, 0, 47, 47, 47, 26\n",
            "value_choice: 47, 0, 16, 47, 0, 0, 0, 47, 17, 47\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 247/1000 --- L(Train): 0.4127021 --- L(Val, RNN): 0.4195593 --- L(Val, SINDy): 0.4199912 --- Time: 0.32s; --- Convergence: 9.19e-05; LR: 1.00e-02; Metric: 0.4195593; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.376 1 + 0.004 value_reward_chosen[t] + 0.099 contr_diff + 0.723 reward + 0.002 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + 0.002 value_reward_chosen*reward + -0.017 contr_diff^2 + -0.013 contr_diff*reward + 0.724 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.389 1 + 0.77 value_reward_not_chosen[t] + -0.018 contr_diff + -0.026 value_reward_not_chosen^2 + -0.014 value_reward_not_chosen*contr_diff + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.032 1 + 0.739 value_choice[t] + -0.049 contr_diff + 0.031 choice + -0.159 value_choice^2 + 0.209 value_choice*contr_diff + -0.261 value_choice*choice + -0.011 contr_diff^2 + -0.047 contr_diff*choice + 0.031 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 48, 48, 48, 27, 48, 0\n",
            "value_reward_not_chosen: 0, 0, 48, 48, 48, 27\n",
            "value_choice: 48, 0, 17, 48, 0, 0, 0, 48, 18, 48\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 248/1000 --- L(Train): 0.4243127 --- L(Val, RNN): 0.4195038 --- L(Val, SINDy): 0.4199537 --- Time: 0.29s; --- Convergence: 7.37e-05; LR: 1.00e-02; Metric: 0.4195038; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.376 1 + 0.002 value_reward_chosen[t] + 0.105 contr_diff + 0.724 reward + 0.002 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.017 contr_diff^2 + -0.01 contr_diff*reward + 0.725 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.39 1 + 0.767 value_reward_not_chosen[t] + -0.017 contr_diff + -0.026 value_reward_not_chosen^2 + -0.015 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.029 1 + 0.737 value_choice[t] + -0.05 contr_diff + 0.028 choice + -0.161 value_choice^2 + 0.208 value_choice*contr_diff + -0.264 value_choice*choice + -0.015 contr_diff^2 + -0.048 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 49, 49, 49, 28, 49, 0\n",
            "value_reward_not_chosen: 0, 0, 49, 49, 49, 28\n",
            "value_choice: 49, 0, 18, 49, 0, 0, 0, 49, 19, 49\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 249/1000 --- L(Train): 0.4334176 --- L(Val, RNN): 0.4195200 --- L(Val, SINDy): 0.4199245 --- Time: 0.31s; --- Convergence: 4.50e-05; LR: 1.00e-02; Metric: 0.4195038; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.377 1 + 0.001 value_reward_chosen[t] + 0.11 contr_diff + 0.723 reward + 0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.001 value_reward_chosen*reward + -0.017 contr_diff^2 + -0.007 contr_diff*reward + 0.724 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.391 1 + 0.765 value_reward_not_chosen[t] + -0.016 contr_diff + -0.026 value_reward_not_chosen^2 + -0.016 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.026 1 + 0.735 value_choice[t] + -0.05 contr_diff + 0.026 choice + -0.162 value_choice^2 + 0.207 value_choice*contr_diff + -0.266 value_choice*choice + -0.018 contr_diff^2 + -0.048 contr_diff*choice + 0.025 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 50, 50, 50, 29, 50, 0\n",
            "value_reward_not_chosen: 0, 0, 50, 50, 50, 29\n",
            "value_choice: 50, 0, 0, 50, 0, 0, 0, 50, 20, 50\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 250/1000 --- L(Train): 0.4175994 --- L(Val, RNN): 0.4194317 --- L(Val, SINDy): 0.4198520 --- Time: 0.47s; --- Convergence: 6.66e-05; LR: 1.00e-02; Metric: 0.4194317; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.379 1 + -0.002 value_reward_chosen[t] + 0.115 contr_diff + 0.722 reward + -0.002 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.003 value_reward_chosen*reward + -0.019 contr_diff^2 + -0.005 contr_diff*reward + 0.723 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.393 1 + 0.765 value_reward_not_chosen[t] + -0.015 contr_diff + -0.027 value_reward_not_chosen^2 + -0.017 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.025 1 + 0.734 value_choice[t] + -0.051 contr_diff + 0.025 choice + -0.162 value_choice^2 + 0.206 value_choice*contr_diff + -0.266 value_choice*choice + -0.02 contr_diff^2 + -0.049 contr_diff*choice + 0.024 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 51, 51, 51, 30, 51, 0\n",
            "value_reward_not_chosen: 0, 0, 51, 51, 51, 30\n",
            "value_choice: 51, 0, 0, 51, 0, 0, 0, 51, 21, 51\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 251/1000 --- L(Train): 0.4349606 --- L(Val, RNN): 0.4194036 --- L(Val, SINDy): 0.4197927 --- Time: 0.36s; --- Convergence: 4.74e-05; LR: 1.00e-02; Metric: 0.4194036; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.382 1 + -0.004 value_reward_chosen[t] + 0.118 contr_diff + 0.72 reward + -0.004 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + -0.006 value_reward_chosen*reward + -0.021 contr_diff^2 + -0.004 contr_diff*reward + 0.721 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.394 1 + 0.764 value_reward_not_chosen[t] + -0.014 contr_diff + -0.028 value_reward_not_chosen^2 + -0.017 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.025 1 + 0.734 value_choice[t] + -0.051 contr_diff + 0.024 choice + -0.162 value_choice^2 + 0.205 value_choice*contr_diff + -0.266 value_choice*choice + -0.021 contr_diff^2 + -0.049 contr_diff*choice + 0.023 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 52, 52, 52, 31, 52, 0\n",
            "value_reward_not_chosen: 0, 0, 52, 52, 52, 31\n",
            "value_choice: 52, 0, 0, 52, 0, 0, 0, 52, 22, 52\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 252/1000 --- L(Train): 0.4227324 --- L(Val, RNN): 0.4194210 --- L(Val, SINDy): 0.4197449 --- Time: 0.28s; --- Convergence: 3.24e-05; LR: 1.00e-02; Metric: 0.4194036; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.384 1 + -0.006 value_reward_chosen[t] + 0.12 contr_diff + 0.719 reward + -0.007 value_reward_chosen^2 + 0.006 value_reward_chosen*contr_diff + -0.008 value_reward_chosen*reward + -0.023 contr_diff^2 + -0.004 contr_diff*reward + 0.72 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.394 1 + 0.763 value_reward_not_chosen[t] + -0.013 contr_diff + -0.029 value_reward_not_chosen^2 + -0.015 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.024 1 + 0.734 value_choice[t] + -0.051 contr_diff + 0.023 choice + -0.162 value_choice^2 + 0.204 value_choice*contr_diff + -0.266 value_choice*choice + -0.023 contr_diff^2 + -0.049 contr_diff*choice + 0.022 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 53, 53, 53, 32, 53, 0\n",
            "value_reward_not_chosen: 0, 0, 53, 53, 53, 32\n",
            "value_choice: 53, 0, 0, 53, 0, 0, 0, 53, 23, 53\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 253/1000 --- L(Train): 0.4267929 --- L(Val, RNN): 0.4193771 --- L(Val, SINDy): 0.4196855 --- Time: 0.27s; --- Convergence: 3.81e-05; LR: 1.00e-02; Metric: 0.4193771; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.386 1 + -0.007 value_reward_chosen[t] + 0.122 contr_diff + 0.718 reward + -0.008 value_reward_chosen^2 + 0.007 value_reward_chosen*contr_diff + -0.009 value_reward_chosen*reward + -0.024 contr_diff^2 + -0.007 contr_diff*reward + 0.719 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.394 1 + 0.762 value_reward_not_chosen[t] + -0.013 contr_diff + -0.03 value_reward_not_chosen^2 + -0.014 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.022 1 + 0.733 value_choice[t] + -0.052 contr_diff + 0.021 choice + -0.163 value_choice^2 + 0.203 value_choice*contr_diff + -0.267 value_choice*choice + -0.026 contr_diff^2 + -0.05 contr_diff*choice + 0.02 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 54, 54, 54, 33, 54, 0\n",
            "value_reward_not_chosen: 0, 0, 54, 54, 54, 33\n",
            "value_choice: 54, 0, 0, 54, 0, 0, 0, 54, 24, 54\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 254/1000 --- L(Train): 0.4152386 --- L(Val, RNN): 0.4193659 --- L(Val, SINDy): 0.4196571 --- Time: 0.27s; --- Convergence: 2.47e-05; LR: 1.00e-02; Metric: 0.4193659; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.386 1 + -0.007 value_reward_chosen[t] + 0.122 contr_diff + 0.719 reward + -0.008 value_reward_chosen^2 + 0.007 value_reward_chosen*contr_diff + -0.009 value_reward_chosen*reward + -0.025 contr_diff^2 + -0.01 contr_diff*reward + 0.72 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.393 1 + 0.761 value_reward_not_chosen[t] + -0.013 contr_diff + -0.03 value_reward_not_chosen^2 + -0.011 value_reward_not_chosen*contr_diff + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.02 1 + 0.733 value_choice[t] + -0.052 contr_diff + 0.019 choice + -0.163 value_choice^2 + 0.203 value_choice*contr_diff + -0.268 value_choice*choice + -0.029 contr_diff^2 + -0.05 contr_diff*choice + 0.019 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 55, 55, 55, 34, 55, 0\n",
            "value_reward_not_chosen: 0, 0, 55, 55, 55, 34\n",
            "value_choice: 55, 0, 0, 55, 0, 0, 0, 55, 25, 55\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 255/1000 --- L(Train): 0.4138054 --- L(Val, RNN): 0.4193585 --- L(Val, SINDy): 0.4196754 --- Time: 0.37s; --- Convergence: 1.60e-05; LR: 1.00e-02; Metric: 0.4193585; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.386 1 + -0.005 value_reward_chosen[t] + 0.122 contr_diff + 0.721 reward + -0.007 value_reward_chosen^2 + 0.007 value_reward_chosen*contr_diff + -0.006 value_reward_chosen*reward + -0.025 contr_diff^2 + -0.013 contr_diff*reward + 0.722 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.39 1 + 0.76 value_reward_not_chosen[t] + -0.013 contr_diff + -0.029 value_reward_not_chosen^2 + -0.008 value_reward_not_chosen*contr_diff + 0.009 contr_diff^2 \n",
            "value_choice[t+1] = 0.02 1 + 0.734 value_choice[t] + -0.052 contr_diff + 0.019 choice + -0.162 value_choice^2 + 0.202 value_choice*contr_diff + -0.267 value_choice*choice + -0.031 contr_diff^2 + -0.049 contr_diff*choice + 0.018 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 56, 56, 56, 35, 56, 0\n",
            "value_reward_not_chosen: 0, 0, 56, 56, 56, 35\n",
            "value_choice: 56, 0, 0, 56, 0, 0, 0, 56, 26, 56\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 256/1000 --- L(Train): 0.4216775 --- L(Val, RNN): 0.4192613 --- L(Val, SINDy): 0.4196469 --- Time: 0.29s; --- Convergence: 5.66e-05; LR: 1.00e-02; Metric: 0.4192613; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.384 1 + -0.002 value_reward_chosen[t] + 0.122 contr_diff + 0.724 reward + -0.005 value_reward_chosen^2 + 0.006 value_reward_chosen*contr_diff + -0.003 value_reward_chosen*reward + -0.024 contr_diff^2 + -0.016 contr_diff*reward + 0.725 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.388 1 + 0.759 value_reward_not_chosen[t] + -0.011 contr_diff + -0.03 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.02 1 + 0.734 value_choice[t] + -0.051 contr_diff + 0.019 choice + -0.161 value_choice^2 + 0.202 value_choice*contr_diff + -0.266 value_choice*choice + -0.032 contr_diff^2 + -0.049 contr_diff*choice + 0.018 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 57, 57, 57, 36, 57, 0\n",
            "value_reward_not_chosen: 0, 0, 57, 57, 57, 36\n",
            "value_choice: 57, 0, 0, 57, 0, 0, 0, 57, 27, 57\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 257/1000 --- L(Train): 0.4284455 --- L(Val, RNN): 0.4192444 --- L(Val, SINDy): 0.4196152 --- Time: 0.36s; --- Convergence: 3.68e-05; LR: 1.00e-02; Metric: 0.4192444; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.385 1 + -0.001 value_reward_chosen[t] + 0.122 contr_diff + 0.725 reward + -0.004 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.026 contr_diff^2 + -0.018 contr_diff*reward + 0.726 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.387 1 + 0.759 value_reward_not_chosen[t] + -0.008 contr_diff + -0.03 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.016 contr_diff^2 \n",
            "value_choice[t+1] = 0.018 1 + 0.735 value_choice[t] + -0.051 contr_diff + 0.018 choice + -0.161 value_choice^2 + 0.202 value_choice*contr_diff + -0.265 value_choice*choice + -0.035 contr_diff^2 + -0.049 contr_diff*choice + 0.017 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 58, 58, 58, 37, 58, 0\n",
            "value_reward_not_chosen: 0, 0, 58, 58, 58, 37\n",
            "value_choice: 58, 0, 0, 58, 0, 0, 0, 58, 28, 58\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 258/1000 --- L(Train): 0.4250638 --- L(Val, RNN): 0.4192807 --- L(Val, SINDy): 0.4195965 --- Time: 0.36s; --- Convergence: 3.65e-05; LR: 1.00e-02; Metric: 0.4192444; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.386 1 + 0.001 value_reward_chosen[t] + 0.122 contr_diff + 0.725 reward + -0.005 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + 0.001 value_reward_chosen*reward + -0.029 contr_diff^2 + -0.021 contr_diff*reward + 0.727 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.386 1 + 0.76 value_reward_not_chosen[t] + -0.004 contr_diff + -0.032 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.017 contr_diff^2 \n",
            "value_choice[t+1] = 0.018 1 + 0.736 value_choice[t] + -0.05 contr_diff + 0.017 choice + -0.159 value_choice^2 + 0.201 value_choice*contr_diff + -0.264 value_choice*choice + -0.036 contr_diff^2 + -0.048 contr_diff*choice + 0.017 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 59, 59, 59, 38, 59, 0\n",
            "value_reward_not_chosen: 0, 0, 59, 59, 59, 38\n",
            "value_choice: 59, 0, 0, 59, 0, 0, 0, 59, 29, 59\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 259/1000 --- L(Train): 0.4232291 --- L(Val, RNN): 0.4192117 --- L(Val, SINDy): 0.4195595 --- Time: 0.30s; --- Convergence: 5.27e-05; LR: 1.00e-02; Metric: 0.4192117; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.387 1 + 0.002 value_reward_chosen[t] + 0.121 contr_diff + 0.726 reward + -0.005 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.031 contr_diff^2 + -0.024 contr_diff*reward + 0.727 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.385 1 + 0.76 value_reward_not_chosen[t] + -0.001 contr_diff + -0.033 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.019 contr_diff^2 \n",
            "value_choice[t+1] = 0.019 1 + 0.738 value_choice[t] + -0.049 contr_diff + 0.018 choice + -0.158 value_choice^2 + 0.201 value_choice*contr_diff + -0.262 value_choice*choice + -0.036 contr_diff^2 + -0.047 contr_diff*choice + 0.018 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 60, 60, 60, 39, 60, 0\n",
            "value_reward_not_chosen: 0, 0, 60, 60, 60, 39\n",
            "value_choice: 60, 0, 1, 60, 0, 0, 0, 60, 30, 60\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 260/1000 --- L(Train): 0.4172323 --- L(Val, RNN): 0.4191515 --- L(Val, SINDy): 0.4195576 --- Time: 0.28s; --- Convergence: 5.65e-05; LR: 1.00e-02; Metric: 0.4191515; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.388 1 + 0.003 value_reward_chosen[t] + 0.12 contr_diff + 0.727 reward + -0.005 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + 0.005 value_reward_chosen*reward + -0.033 contr_diff^2 + -0.027 contr_diff*reward + 0.728 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.385 1 + 0.761 value_reward_not_chosen[t] + 0.001 contr_diff + -0.034 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.019 contr_diff^2 \n",
            "value_choice[t+1] = 0.019 1 + 0.74 value_choice[t] + -0.048 contr_diff + 0.018 choice + -0.157 value_choice^2 + 0.201 value_choice*contr_diff + -0.261 value_choice*choice + -0.036 contr_diff^2 + -0.046 contr_diff*choice + 0.018 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 61, 61, 61, 40, 61, 0\n",
            "value_reward_not_chosen: 0, 0, 61, 61, 61, 40\n",
            "value_choice: 61, 0, 2, 61, 0, 0, 0, 61, 31, 61\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 261/1000 --- L(Train): 0.4243848 --- L(Val, RNN): 0.4191298 --- L(Val, SINDy): 0.4195512 --- Time: 0.29s; --- Convergence: 3.91e-05; LR: 1.00e-02; Metric: 0.4191298; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.388 1 + 0.003 value_reward_chosen[t] + 0.119 contr_diff + 0.727 reward + -0.005 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + 0.006 value_reward_chosen*reward + -0.034 contr_diff^2 + -0.029 contr_diff*reward + 0.729 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.385 1 + 0.761 value_reward_not_chosen[t] + 0.003 contr_diff + -0.035 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.019 contr_diff^2 \n",
            "value_choice[t+1] = 0.02 1 + 0.742 value_choice[t] + -0.048 contr_diff + 0.02 choice + -0.154 value_choice^2 + 0.201 value_choice*contr_diff + -0.258 value_choice*choice + -0.034 contr_diff^2 + -0.046 contr_diff*choice + 0.019 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 62, 62, 62, 41, 62, 0\n",
            "value_reward_not_chosen: 0, 0, 62, 62, 62, 41\n",
            "value_choice: 62, 0, 3, 62, 0, 0, 0, 62, 32, 62\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 262/1000 --- L(Train): 0.4148427 --- L(Val, RNN): 0.4190685 --- L(Val, SINDy): 0.4195163 --- Time: 0.40s; --- Convergence: 5.02e-05; LR: 1.00e-02; Metric: 0.4190685; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.389 1 + 0.003 value_reward_chosen[t] + 0.119 contr_diff + 0.728 reward + -0.005 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + 0.006 value_reward_chosen*reward + -0.036 contr_diff^2 + -0.031 contr_diff*reward + 0.729 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.385 1 + 0.762 value_reward_not_chosen[t] + 0.003 contr_diff + -0.035 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.018 contr_diff^2 \n",
            "value_choice[t+1] = 0.023 1 + 0.746 value_choice[t] + -0.046 contr_diff + 0.022 choice + -0.151 value_choice^2 + 0.202 value_choice*contr_diff + -0.254 value_choice*choice + -0.031 contr_diff^2 + -0.044 contr_diff*choice + 0.022 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 63, 63, 63, 42, 63, 0\n",
            "value_reward_not_chosen: 0, 0, 63, 63, 63, 42\n",
            "value_choice: 63, 0, 4, 63, 0, 0, 0, 63, 33, 63\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 263/1000 --- L(Train): 0.4220401 --- L(Val, RNN): 0.4190766 --- L(Val, SINDy): 0.4194972 --- Time: 0.35s; --- Convergence: 2.91e-05; LR: 1.00e-02; Metric: 0.4190685; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.391 1 + 0.002 value_reward_chosen[t] + 0.119 contr_diff + 0.728 reward + -0.006 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.006 value_reward_chosen*reward + -0.037 contr_diff^2 + -0.032 contr_diff*reward + 0.729 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.385 1 + 0.762 value_reward_not_chosen[t] + 0.003 contr_diff + -0.035 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.016 contr_diff^2 \n",
            "value_choice[t+1] = 0.025 1 + 0.749 value_choice[t] + -0.046 contr_diff + 0.024 choice + -0.148 value_choice^2 + 0.202 value_choice*contr_diff + -0.251 value_choice*choice + -0.028 contr_diff^2 + -0.044 contr_diff*choice + 0.024 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 64, 64, 64, 43, 64, 0\n",
            "value_reward_not_chosen: 0, 0, 64, 64, 64, 43\n",
            "value_choice: 64, 0, 5, 64, 0, 0, 0, 64, 34, 64\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 264/1000 --- L(Train): 0.4207873 --- L(Val, RNN): 0.4191218 --- L(Val, SINDy): 0.4194819 --- Time: 0.27s; --- Convergence: 3.72e-05; LR: 1.00e-02; Metric: 0.4190685; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.392 1 + 0.001 value_reward_chosen[t] + 0.119 contr_diff + 0.727 reward + -0.007 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + 0.006 value_reward_chosen*reward + -0.039 contr_diff^2 + -0.032 contr_diff*reward + 0.728 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.385 1 + 0.762 value_reward_not_chosen[t] + 0.001 contr_diff + -0.035 value_reward_not_chosen^2 + 0.004 value_reward_not_chosen*contr_diff + 0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.024 1 + 0.751 value_choice[t] + -0.045 contr_diff + 0.024 choice + -0.146 value_choice^2 + 0.202 value_choice*contr_diff + -0.249 value_choice*choice + -0.029 contr_diff^2 + -0.042 contr_diff*choice + 0.023 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 65, 65, 65, 44, 65, 0\n",
            "value_reward_not_chosen: 0, 0, 65, 65, 65, 44\n",
            "value_choice: 65, 0, 6, 65, 0, 0, 0, 65, 35, 65\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 265/1000 --- L(Train): 0.4188304 --- L(Val, RNN): 0.4190523 --- L(Val, SINDy): 0.4194286 --- Time: 0.32s; --- Convergence: 5.33e-05; LR: 1.00e-02; Metric: 0.4190523; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.393 1 + 0.0 value_reward_chosen[t] + 0.119 contr_diff + 0.728 reward + -0.007 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + 0.005 value_reward_chosen*reward + -0.039 contr_diff^2 + -0.033 contr_diff*reward + 0.729 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.386 1 + 0.762 value_reward_not_chosen[t] + -0.001 contr_diff + -0.034 value_reward_not_chosen^2 + 0.005 value_reward_not_chosen*contr_diff + 0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.024 1 + 0.754 value_choice[t] + -0.043 contr_diff + 0.024 choice + -0.144 value_choice^2 + 0.202 value_choice*contr_diff + -0.247 value_choice*choice + -0.029 contr_diff^2 + -0.041 contr_diff*choice + 0.023 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 66, 66, 66, 45, 66, 0\n",
            "value_reward_not_chosen: 0, 0, 66, 66, 66, 45\n",
            "value_choice: 66, 0, 7, 66, 0, 0, 0, 66, 36, 66\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 266/1000 --- L(Train): 0.4076440 --- L(Val, RNN): 0.4190332 --- L(Val, SINDy): 0.4193760 --- Time: 0.31s; --- Convergence: 3.62e-05; LR: 1.00e-02; Metric: 0.4190332; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.393 1 + -0.0 value_reward_chosen[t] + 0.118 contr_diff + 0.729 reward + -0.006 value_reward_chosen^2 + -0.007 value_reward_chosen*contr_diff + 0.005 value_reward_chosen*reward + -0.038 contr_diff^2 + -0.034 contr_diff*reward + 0.73 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.386 1 + 0.761 value_reward_not_chosen[t] + -0.002 contr_diff + -0.033 value_reward_not_chosen^2 + 0.004 value_reward_not_chosen*contr_diff + 0.008 contr_diff^2 \n",
            "value_choice[t+1] = 0.025 1 + 0.757 value_choice[t] + -0.042 contr_diff + 0.025 choice + -0.141 value_choice^2 + 0.202 value_choice*contr_diff + -0.243 value_choice*choice + -0.028 contr_diff^2 + -0.04 contr_diff*choice + 0.024 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 67, 67, 67, 46, 67, 0\n",
            "value_reward_not_chosen: 0, 0, 67, 67, 67, 46\n",
            "value_choice: 67, 0, 8, 67, 0, 0, 0, 67, 37, 67\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 267/1000 --- L(Train): 0.4168695 --- L(Val, RNN): 0.4190471 --- L(Val, SINDy): 0.4193385 --- Time: 0.36s; --- Convergence: 2.50e-05; LR: 1.00e-02; Metric: 0.4190332; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.392 1 + -0.001 value_reward_chosen[t] + 0.118 contr_diff + 0.73 reward + -0.005 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + 0.005 value_reward_chosen*reward + -0.038 contr_diff^2 + -0.034 contr_diff*reward + 0.731 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.387 1 + 0.761 value_reward_not_chosen[t] + -0.004 contr_diff + -0.033 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.027 1 + 0.76 value_choice[t] + -0.041 contr_diff + 0.026 choice + -0.137 value_choice^2 + 0.202 value_choice*contr_diff + -0.24 value_choice*choice + -0.026 contr_diff^2 + -0.039 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 68, 68, 68, 47, 68, 0\n",
            "value_reward_not_chosen: 0, 0, 68, 68, 68, 47\n",
            "value_choice: 68, 0, 9, 68, 0, 0, 0, 68, 38, 68\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 268/1000 --- L(Train): 0.4214418 --- L(Val, RNN): 0.4189722 --- L(Val, SINDy): 0.4193054 --- Time: 0.29s; --- Convergence: 4.99e-05; LR: 1.00e-02; Metric: 0.4189722; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.392 1 + -0.002 value_reward_chosen[t] + 0.118 contr_diff + 0.732 reward + -0.005 value_reward_chosen^2 + -0.01 value_reward_chosen*contr_diff + 0.005 value_reward_chosen*reward + -0.037 contr_diff^2 + -0.034 contr_diff*reward + 0.733 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.388 1 + 0.761 value_reward_not_chosen[t] + -0.005 contr_diff + -0.032 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.029 1 + 0.764 value_choice[t] + -0.04 contr_diff + 0.028 choice + -0.134 value_choice^2 + 0.202 value_choice*contr_diff + -0.236 value_choice*choice + -0.024 contr_diff^2 + -0.038 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 69, 69, 69, 48, 69, 0\n",
            "value_reward_not_chosen: 0, 0, 69, 69, 69, 48\n",
            "value_choice: 69, 0, 10, 69, 0, 0, 0, 69, 39, 69\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 269/1000 --- L(Train): 0.4181094 --- L(Val, RNN): 0.4189707 --- L(Val, SINDy): 0.4192902 --- Time: 0.29s; --- Convergence: 2.57e-05; LR: 1.00e-02; Metric: 0.4189707; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.392 1 + -0.003 value_reward_chosen[t] + 0.118 contr_diff + 0.733 reward + -0.004 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + 0.005 value_reward_chosen*reward + -0.037 contr_diff^2 + -0.034 contr_diff*reward + 0.734 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.39 1 + 0.762 value_reward_not_chosen[t] + -0.006 contr_diff + -0.032 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.031 1 + 0.768 value_choice[t] + -0.039 contr_diff + 0.03 choice + -0.13 value_choice^2 + 0.202 value_choice*contr_diff + -0.232 value_choice*choice + -0.022 contr_diff^2 + -0.037 contr_diff*choice + 0.03 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 70, 70, 70, 49, 70, 0\n",
            "value_reward_not_chosen: 0, 0, 70, 70, 70, 49\n",
            "value_choice: 70, 0, 11, 70, 0, 0, 0, 70, 40, 70\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 270/1000 --- L(Train): 0.4270381 --- L(Val, RNN): 0.4189621 --- L(Val, SINDy): 0.4192672 --- Time: 0.31s; --- Convergence: 1.72e-05; LR: 1.00e-02; Metric: 0.4189621; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.392 1 + -0.003 value_reward_chosen[t] + 0.119 contr_diff + 0.734 reward + -0.004 value_reward_chosen^2 + -0.01 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.036 contr_diff^2 + -0.034 contr_diff*reward + 0.735 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.391 1 + 0.762 value_reward_not_chosen[t] + -0.006 contr_diff + -0.032 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.006 contr_diff^2 \n",
            "value_choice[t+1] = 0.032 1 + 0.771 value_choice[t] + -0.038 contr_diff + 0.031 choice + -0.127 value_choice^2 + 0.202 value_choice*contr_diff + -0.229 value_choice*choice + -0.021 contr_diff^2 + -0.036 contr_diff*choice + 0.031 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 71, 71, 71, 50, 71, 0\n",
            "value_reward_not_chosen: 0, 0, 71, 71, 71, 50\n",
            "value_choice: 71, 0, 12, 71, 0, 0, 0, 71, 41, 71\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 271/1000 --- L(Train): 0.4295215 --- L(Val, RNN): 0.4189104 --- L(Val, SINDy): 0.4192081 --- Time: 0.26s; --- Convergence: 3.44e-05; LR: 1.00e-02; Metric: 0.4189104; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.392 1 + -0.004 value_reward_chosen[t] + 0.119 contr_diff + 0.735 reward + -0.003 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.035 contr_diff^2 + -0.034 contr_diff*reward + 0.736 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.392 1 + 0.762 value_reward_not_chosen[t] + -0.006 contr_diff + -0.032 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.008 contr_diff^2 \n",
            "value_choice[t+1] = 0.032 1 + 0.773 value_choice[t] + -0.037 contr_diff + 0.032 choice + -0.125 value_choice^2 + 0.202 value_choice*contr_diff + -0.227 value_choice*choice + -0.021 contr_diff^2 + -0.035 contr_diff*choice + 0.031 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 72, 72, 72, 51, 72, 0\n",
            "value_reward_not_chosen: 0, 0, 72, 72, 72, 51\n",
            "value_choice: 72, 0, 13, 72, 0, 0, 0, 72, 42, 72\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 272/1000 --- L(Train): 0.4106026 --- L(Val, RNN): 0.4189271 --- L(Val, SINDy): 0.4191678 --- Time: 0.29s; --- Convergence: 2.56e-05; LR: 1.00e-02; Metric: 0.4189104; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.392 1 + -0.005 value_reward_chosen[t] + 0.118 contr_diff + 0.735 reward + -0.003 value_reward_chosen^2 + -0.007 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.035 contr_diff^2 + -0.033 contr_diff*reward + 0.736 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.392 1 + 0.761 value_reward_not_chosen[t] + -0.007 contr_diff + -0.031 value_reward_not_chosen^2 + -0.007 value_reward_not_chosen*contr_diff + -0.01 contr_diff^2 \n",
            "value_choice[t+1] = 0.032 1 + 0.774 value_choice[t] + -0.036 contr_diff + 0.031 choice + -0.123 value_choice^2 + 0.201 value_choice*contr_diff + -0.226 value_choice*choice + -0.022 contr_diff^2 + -0.034 contr_diff*choice + 0.03 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 73, 73, 73, 52, 73, 0\n",
            "value_reward_not_chosen: 0, 0, 73, 73, 73, 52\n",
            "value_choice: 73, 0, 14, 73, 0, 0, 0, 73, 43, 73\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 273/1000 --- L(Train): 0.4239344 --- L(Val, RNN): 0.4188831 --- L(Val, SINDy): 0.4191756 --- Time: 0.27s; --- Convergence: 3.48e-05; LR: 1.00e-02; Metric: 0.4188831; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.393 1 + -0.006 value_reward_chosen[t] + 0.118 contr_diff + 0.735 reward + -0.003 value_reward_chosen^2 + -0.005 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.035 contr_diff^2 + -0.033 contr_diff*reward + 0.737 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.392 1 + 0.76 value_reward_not_chosen[t] + -0.008 contr_diff + -0.029 value_reward_not_chosen^2 + -0.008 value_reward_not_chosen*contr_diff + -0.01 contr_diff^2 \n",
            "value_choice[t+1] = 0.03 1 + 0.774 value_choice[t] + -0.036 contr_diff + 0.029 choice + -0.123 value_choice^2 + 0.201 value_choice*contr_diff + -0.226 value_choice*choice + -0.023 contr_diff^2 + -0.033 contr_diff*choice + 0.029 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 74, 74, 74, 53, 74, 0\n",
            "value_reward_not_chosen: 0, 0, 74, 74, 74, 53\n",
            "value_choice: 74, 0, 15, 74, 0, 0, 0, 74, 44, 74\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 274/1000 --- L(Train): 0.4298138 --- L(Val, RNN): 0.4188790 --- L(Val, SINDy): 0.4192137 --- Time: 0.31s; --- Convergence: 1.94e-05; LR: 1.00e-02; Metric: 0.4188790; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.395 1 + -0.007 value_reward_chosen[t] + 0.118 contr_diff + 0.734 reward + -0.005 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + 0.002 value_reward_chosen*reward + -0.037 contr_diff^2 + -0.031 contr_diff*reward + 0.735 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.391 1 + 0.758 value_reward_not_chosen[t] + -0.01 contr_diff + -0.027 value_reward_not_chosen^2 + -0.009 value_reward_not_chosen*contr_diff + -0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.029 1 + 0.775 value_choice[t] + -0.035 contr_diff + 0.029 choice + -0.122 value_choice^2 + 0.2 value_choice*contr_diff + -0.225 value_choice*choice + -0.023 contr_diff^2 + -0.033 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 75, 75, 75, 54, 75, 0\n",
            "value_reward_not_chosen: 0, 0, 75, 75, 75, 54\n",
            "value_choice: 75, 0, 16, 75, 0, 0, 0, 75, 45, 75\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 275/1000 --- L(Train): 0.4190483 --- L(Val, RNN): 0.4188781 --- L(Val, SINDy): 0.4192307 --- Time: 0.33s; --- Convergence: 1.02e-05; LR: 1.00e-02; Metric: 0.4188781; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.397 1 + -0.009 value_reward_chosen[t] + 0.118 contr_diff + 0.733 reward + -0.007 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.001 value_reward_chosen*reward + -0.039 contr_diff^2 + -0.029 contr_diff*reward + 0.734 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.391 1 + 0.757 value_reward_not_chosen[t] + -0.012 contr_diff + -0.026 value_reward_not_chosen^2 + -0.01 value_reward_not_chosen*contr_diff + -0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.03 1 + 0.777 value_choice[t] + -0.034 contr_diff + 0.029 choice + -0.119 value_choice^2 + 0.2 value_choice*contr_diff + -0.223 value_choice*choice + -0.022 contr_diff^2 + -0.032 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 76, 76, 76, 55, 76, 0\n",
            "value_reward_not_chosen: 0, 0, 76, 76, 76, 55\n",
            "value_choice: 76, 0, 17, 76, 0, 0, 0, 76, 46, 76\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 276/1000 --- L(Train): 0.4189796 --- L(Val, RNN): 0.4188839 --- L(Val, SINDy): 0.4191934 --- Time: 0.29s; --- Convergence: 7.99e-06; LR: 1.00e-02; Metric: 0.4188781; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.398 1 + -0.009 value_reward_chosen[t] + 0.118 contr_diff + 0.733 reward + -0.007 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + 0.001 value_reward_chosen*reward + -0.04 contr_diff^2 + -0.027 contr_diff*reward + 0.734 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.392 1 + 0.755 value_reward_not_chosen[t] + -0.015 contr_diff + -0.026 value_reward_not_chosen^2 + -0.012 value_reward_not_chosen*contr_diff + -0.012 contr_diff^2 \n",
            "value_choice[t+1] = 0.031 1 + 0.78 value_choice[t] + -0.034 contr_diff + 0.03 choice + -0.116 value_choice^2 + 0.199 value_choice*contr_diff + -0.22 value_choice*choice + -0.021 contr_diff^2 + -0.032 contr_diff*choice + 0.029 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 77, 77, 77, 56, 77, 0\n",
            "value_reward_not_chosen: 0, 0, 77, 77, 77, 56\n",
            "value_choice: 77, 0, 18, 77, 0, 0, 0, 77, 47, 77\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 277/1000 --- L(Train): 0.4241377 --- L(Val, RNN): 0.4188698 --- L(Val, SINDy): 0.4191169 --- Time: 0.29s; --- Convergence: 1.10e-05; LR: 1.00e-02; Metric: 0.4188698; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.399 1 + -0.009 value_reward_chosen[t] + 0.119 contr_diff + 0.734 reward + -0.007 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + 0.001 value_reward_chosen*reward + -0.041 contr_diff^2 + -0.025 contr_diff*reward + 0.735 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.393 1 + 0.754 value_reward_not_chosen[t] + -0.016 contr_diff + -0.026 value_reward_not_chosen^2 + -0.013 value_reward_not_chosen*contr_diff + -0.012 contr_diff^2 \n",
            "value_choice[t+1] = 0.033 1 + 0.784 value_choice[t] + -0.033 contr_diff + 0.032 choice + -0.112 value_choice^2 + 0.199 value_choice*contr_diff + -0.217 value_choice*choice + -0.019 contr_diff^2 + -0.031 contr_diff*choice + 0.031 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 78, 78, 78, 57, 78, 0\n",
            "value_reward_not_chosen: 0, 0, 78, 78, 78, 57\n",
            "value_choice: 78, 0, 19, 78, 0, 0, 0, 78, 48, 78\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 278/1000 --- L(Train): 0.4266029 --- L(Val, RNN): 0.4188398 --- L(Val, SINDy): 0.4190591 --- Time: 0.27s; --- Convergence: 2.05e-05; LR: 1.00e-02; Metric: 0.4188398; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.4 1 + -0.008 value_reward_chosen[t] + 0.12 contr_diff + 0.734 reward + -0.008 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + 0.002 value_reward_chosen*reward + -0.042 contr_diff^2 + -0.023 contr_diff*reward + 0.735 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.395 1 + 0.754 value_reward_not_chosen[t] + -0.018 contr_diff + -0.027 value_reward_not_chosen^2 + -0.014 value_reward_not_chosen*contr_diff + -0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.035 1 + 0.787 value_choice[t] + -0.033 contr_diff + 0.034 choice + -0.108 value_choice^2 + 0.199 value_choice*contr_diff + -0.213 value_choice*choice + -0.017 contr_diff^2 + -0.031 contr_diff*choice + 0.033 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 79, 79, 79, 58, 79, 0\n",
            "value_reward_not_chosen: 0, 0, 79, 79, 79, 58\n",
            "value_choice: 79, 0, 20, 79, 0, 0, 0, 79, 49, 79\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 279/1000 --- L(Train): 0.4170529 --- L(Val, RNN): 0.4188033 --- L(Val, SINDy): 0.4190476 --- Time: 0.39s; --- Convergence: 2.85e-05; LR: 1.00e-02; Metric: 0.4188033; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.402 1 + -0.008 value_reward_chosen[t] + 0.122 contr_diff + 0.733 reward + -0.009 value_reward_chosen^2 + 0.006 value_reward_chosen*contr_diff + 0.002 value_reward_chosen*reward + -0.044 contr_diff^2 + -0.02 contr_diff*reward + 0.734 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.396 1 + 0.753 value_reward_not_chosen[t] + -0.02 contr_diff + -0.028 value_reward_not_chosen^2 + -0.017 value_reward_not_chosen*contr_diff + -0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.79 value_choice[t] + -0.031 contr_diff + 0.035 choice + -0.105 value_choice^2 + 0.199 value_choice*contr_diff + -0.211 value_choice*choice + -0.016 contr_diff^2 + -0.029 contr_diff*choice + 0.034 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 80, 80, 80, 59, 80, 0\n",
            "value_reward_not_chosen: 0, 0, 80, 80, 80, 59\n",
            "value_choice: 80, 0, 21, 80, 0, 0, 0, 80, 50, 80\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 280/1000 --- L(Train): 0.4183328 --- L(Val, RNN): 0.4187956 --- L(Val, SINDy): 0.4190446 --- Time: 0.29s; --- Convergence: 1.81e-05; LR: 1.00e-02; Metric: 0.4187956; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.403 1 + -0.007 value_reward_chosen[t] + 0.124 contr_diff + 0.733 reward + -0.009 value_reward_chosen^2 + 0.006 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.046 contr_diff^2 + -0.018 contr_diff*reward + 0.734 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.398 1 + 0.752 value_reward_not_chosen[t] + -0.021 contr_diff + -0.028 value_reward_not_chosen^2 + -0.02 value_reward_not_chosen*contr_diff + -0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.035 1 + 0.79 value_choice[t] + -0.03 contr_diff + 0.034 choice + -0.104 value_choice^2 + 0.199 value_choice*contr_diff + -0.21 value_choice*choice + -0.017 contr_diff^2 + -0.027 contr_diff*choice + 0.034 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 81, 81, 81, 60, 81, 0\n",
            "value_reward_not_chosen: 0, 0, 81, 81, 81, 60\n",
            "value_choice: 81, 0, 22, 81, 0, 0, 0, 81, 51, 81\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 281/1000 --- L(Train): 0.4204429 --- L(Val, RNN): 0.4187619 --- L(Val, SINDy): 0.4190401 --- Time: 0.28s; --- Convergence: 2.59e-05; LR: 1.00e-02; Metric: 0.4187619; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.402 1 + -0.005 value_reward_chosen[t] + 0.126 contr_diff + 0.734 reward + -0.008 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.046 contr_diff^2 + -0.017 contr_diff*reward + 0.735 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.4 1 + 0.751 value_reward_not_chosen[t] + -0.022 contr_diff + -0.029 value_reward_not_chosen^2 + -0.022 value_reward_not_chosen*contr_diff + -0.015 contr_diff^2 \n",
            "value_choice[t+1] = 0.033 1 + 0.79 value_choice[t] + -0.027 contr_diff + 0.033 choice + -0.104 value_choice^2 + 0.199 value_choice*contr_diff + -0.211 value_choice*choice + -0.019 contr_diff^2 + -0.025 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 82, 82, 82, 61, 82, 0\n",
            "value_reward_not_chosen: 0, 0, 82, 82, 82, 61\n",
            "value_choice: 82, 0, 23, 82, 0, 0, 0, 82, 52, 82\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 282/1000 --- L(Train): 0.4313620 --- L(Val, RNN): 0.4187408 --- L(Val, SINDy): 0.4190497 --- Time: 0.30s; --- Convergence: 2.35e-05; LR: 1.00e-02; Metric: 0.4187408; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.402 1 + -0.003 value_reward_chosen[t] + 0.128 contr_diff + 0.736 reward + -0.006 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + 0.006 value_reward_chosen*reward + -0.046 contr_diff^2 + -0.016 contr_diff*reward + 0.737 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.401 1 + 0.749 value_reward_not_chosen[t] + -0.023 contr_diff + -0.029 value_reward_not_chosen^2 + -0.024 value_reward_not_chosen*contr_diff + -0.015 contr_diff^2 \n",
            "value_choice[t+1] = 0.031 1 + 0.789 value_choice[t] + -0.025 contr_diff + 0.03 choice + -0.104 value_choice^2 + 0.199 value_choice*contr_diff + -0.212 value_choice*choice + -0.021 contr_diff^2 + -0.023 contr_diff*choice + 0.03 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 83, 83, 83, 62, 83, 0\n",
            "value_reward_not_chosen: 0, 0, 83, 83, 83, 62\n",
            "value_choice: 83, 0, 24, 83, 0, 0, 0, 83, 53, 83\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 283/1000 --- L(Train): 0.4094176 --- L(Val, RNN): 0.4187159 --- L(Val, SINDy): 0.4190532 --- Time: 0.27s; --- Convergence: 2.42e-05; LR: 1.00e-02; Metric: 0.4187159; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.401 1 + -0.001 value_reward_chosen[t] + 0.13 contr_diff + 0.737 reward + -0.005 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + 0.008 value_reward_chosen*reward + -0.046 contr_diff^2 + -0.014 contr_diff*reward + 0.738 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.403 1 + 0.747 value_reward_not_chosen[t] + -0.024 contr_diff + -0.029 value_reward_not_chosen^2 + -0.026 value_reward_not_chosen*contr_diff + -0.016 contr_diff^2 \n",
            "value_choice[t+1] = 0.029 1 + 0.788 value_choice[t] + -0.024 contr_diff + 0.028 choice + -0.105 value_choice^2 + 0.199 value_choice*contr_diff + -0.213 value_choice*choice + -0.022 contr_diff^2 + -0.022 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 84, 84, 84, 63, 84, 0\n",
            "value_reward_not_chosen: 0, 0, 84, 84, 84, 63\n",
            "value_choice: 84, 0, 25, 84, 0, 0, 0, 84, 54, 84\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 284/1000 --- L(Train): 0.4220492 --- L(Val, RNN): 0.4186701 --- L(Val, SINDy): 0.4190179 --- Time: 0.32s; --- Convergence: 3.50e-05; LR: 1.00e-02; Metric: 0.4186701; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.401 1 + -0.0 value_reward_chosen[t] + 0.132 contr_diff + 0.738 reward + -0.005 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + 0.009 value_reward_chosen*reward + -0.046 contr_diff^2 + -0.013 contr_diff*reward + 0.739 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.404 1 + 0.745 value_reward_not_chosen[t] + -0.026 contr_diff + -0.029 value_reward_not_chosen^2 + -0.028 value_reward_not_chosen*contr_diff + -0.016 contr_diff^2 \n",
            "value_choice[t+1] = 0.028 1 + 0.788 value_choice[t] + -0.023 contr_diff + 0.027 choice + -0.104 value_choice^2 + 0.198 value_choice*contr_diff + -0.213 value_choice*choice + -0.023 contr_diff^2 + -0.021 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 85, 85, 85, 64, 85, 0\n",
            "value_reward_not_chosen: 0, 0, 85, 85, 85, 64\n",
            "value_choice: 85, 0, 26, 85, 0, 0, 0, 85, 55, 85\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 285/1000 --- L(Train): 0.4196337 --- L(Val, RNN): 0.4186727 --- L(Val, SINDy): 0.4189937 --- Time: 0.34s; --- Convergence: 1.87e-05; LR: 1.00e-02; Metric: 0.4186701; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.402 1 + 0.0 value_reward_chosen[t] + 0.134 contr_diff + 0.738 reward + -0.005 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.009 value_reward_chosen*reward + -0.046 contr_diff^2 + -0.013 contr_diff*reward + 0.739 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.406 1 + 0.743 value_reward_not_chosen[t] + -0.028 contr_diff + -0.03 value_reward_not_chosen^2 + -0.029 value_reward_not_chosen*contr_diff + -0.017 contr_diff^2 \n",
            "value_choice[t+1] = 0.028 1 + 0.789 value_choice[t] + -0.022 contr_diff + 0.027 choice + -0.102 value_choice^2 + 0.198 value_choice*contr_diff + -0.211 value_choice*choice + -0.023 contr_diff^2 + -0.02 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 86, 86, 86, 65, 86, 0\n",
            "value_reward_not_chosen: 0, 0, 86, 86, 86, 65\n",
            "value_choice: 86, 0, 27, 86, 0, 0, 0, 86, 56, 86\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 286/1000 --- L(Train): 0.4255156 --- L(Val, RNN): 0.4187112 --- L(Val, SINDy): 0.4189599 --- Time: 0.29s; --- Convergence: 2.87e-05; LR: 1.00e-02; Metric: 0.4186701; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.402 1 + 0.0 value_reward_chosen[t] + 0.136 contr_diff + 0.739 reward + -0.004 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.009 value_reward_chosen*reward + -0.046 contr_diff^2 + -0.013 contr_diff*reward + 0.74 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.408 1 + 0.742 value_reward_not_chosen[t] + -0.03 contr_diff + -0.03 value_reward_not_chosen^2 + -0.03 value_reward_not_chosen*contr_diff + -0.018 contr_diff^2 \n",
            "value_choice[t+1] = 0.029 1 + 0.791 value_choice[t] + -0.022 contr_diff + 0.028 choice + -0.099 value_choice^2 + 0.197 value_choice*contr_diff + -0.209 value_choice*choice + -0.022 contr_diff^2 + -0.02 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 87, 87, 87, 66, 87, 0\n",
            "value_reward_not_chosen: 0, 0, 87, 87, 87, 66\n",
            "value_choice: 87, 0, 28, 87, 0, 0, 0, 87, 57, 87\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 287/1000 --- L(Train): 0.4252401 --- L(Val, RNN): 0.4186910 --- L(Val, SINDy): 0.4189080 --- Time: 0.28s; --- Convergence: 2.45e-05; LR: 1.00e-02; Metric: 0.4186701; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.402 1 + 0.001 value_reward_chosen[t] + 0.136 contr_diff + 0.74 reward + -0.004 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.009 value_reward_chosen*reward + -0.045 contr_diff^2 + -0.014 contr_diff*reward + 0.741 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.41 1 + 0.74 value_reward_not_chosen[t] + -0.031 contr_diff + -0.031 value_reward_not_chosen^2 + -0.03 value_reward_not_chosen*contr_diff + -0.018 contr_diff^2 \n",
            "value_choice[t+1] = 0.029 1 + 0.793 value_choice[t] + -0.022 contr_diff + 0.029 choice + -0.097 value_choice^2 + 0.195 value_choice*contr_diff + -0.207 value_choice*choice + -0.021 contr_diff^2 + -0.02 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 88, 88, 88, 67, 88, 0\n",
            "value_reward_not_chosen: 0, 0, 88, 88, 88, 67\n",
            "value_choice: 88, 0, 29, 88, 0, 0, 0, 88, 58, 88\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 288/1000 --- L(Train): 0.4268967 --- L(Val, RNN): 0.4186468 --- L(Val, SINDy): 0.4188972 --- Time: 0.31s; --- Convergence: 3.43e-05; LR: 1.00e-02; Metric: 0.4186468; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.402 1 + 0.0 value_reward_chosen[t] + 0.137 contr_diff + 0.74 reward + -0.003 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.008 value_reward_chosen*reward + -0.045 contr_diff^2 + -0.017 contr_diff*reward + 0.741 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.412 1 + 0.739 value_reward_not_chosen[t] + -0.032 contr_diff + -0.032 value_reward_not_chosen^2 + -0.03 value_reward_not_chosen*contr_diff + -0.019 contr_diff^2 \n",
            "value_choice[t+1] = 0.031 1 + 0.797 value_choice[t] + -0.022 contr_diff + 0.031 choice + -0.092 value_choice^2 + 0.194 value_choice*contr_diff + -0.204 value_choice*choice + -0.018 contr_diff^2 + -0.02 contr_diff*choice + 0.03 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 89, 89, 89, 68, 89, 0\n",
            "value_reward_not_chosen: 0, 0, 89, 89, 89, 68\n",
            "value_choice: 89, 0, 30, 89, 0, 0, 0, 89, 59, 89\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 289/1000 --- L(Train): 0.4212427 --- L(Val, RNN): 0.4186702 --- L(Val, SINDy): 0.4188822 --- Time: 0.27s; --- Convergence: 2.89e-05; LR: 1.00e-02; Metric: 0.4186468; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.403 1 + -0.001 value_reward_chosen[t] + 0.136 contr_diff + 0.739 reward + -0.004 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.007 value_reward_chosen*reward + -0.045 contr_diff^2 + -0.019 contr_diff*reward + 0.74 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.414 1 + 0.738 value_reward_not_chosen[t] + -0.033 contr_diff + -0.033 value_reward_not_chosen^2 + -0.029 value_reward_not_chosen*contr_diff + -0.019 contr_diff^2 \n",
            "value_choice[t+1] = 0.034 1 + 0.801 value_choice[t] + -0.023 contr_diff + 0.034 choice + -0.086 value_choice^2 + 0.193 value_choice*contr_diff + -0.199 value_choice*choice + -0.015 contr_diff^2 + -0.021 contr_diff*choice + 0.033 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 90, 90, 90, 69, 90, 0\n",
            "value_reward_not_chosen: 0, 0, 90, 90, 90, 69\n",
            "value_choice: 90, 0, 31, 90, 0, 0, 0, 90, 60, 90\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 290/1000 --- L(Train): 0.4194574 --- L(Val, RNN): 0.4186437 --- L(Val, SINDy): 0.4188451 --- Time: 0.28s; --- Convergence: 2.77e-05; LR: 1.00e-02; Metric: 0.4186437; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.404 1 + -0.002 value_reward_chosen[t] + 0.135 contr_diff + 0.739 reward + -0.004 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + 0.006 value_reward_chosen*reward + -0.045 contr_diff^2 + -0.022 contr_diff*reward + 0.74 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.415 1 + 0.737 value_reward_not_chosen[t] + -0.033 contr_diff + -0.033 value_reward_not_chosen^2 + -0.029 value_reward_not_chosen*contr_diff + -0.018 contr_diff^2 \n",
            "value_choice[t+1] = 0.037 1 + 0.805 value_choice[t] + -0.023 contr_diff + 0.036 choice + -0.082 value_choice^2 + 0.192 value_choice*contr_diff + -0.196 value_choice*choice + -0.012 contr_diff^2 + -0.021 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 91, 91, 91, 70, 91, 0\n",
            "value_reward_not_chosen: 0, 0, 91, 91, 91, 70\n",
            "value_choice: 91, 0, 32, 91, 0, 0, 0, 91, 61, 91\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 291/1000 --- L(Train): 0.4323844 --- L(Val, RNN): 0.4186214 --- L(Val, SINDy): 0.4188258 --- Time: 0.34s; --- Convergence: 2.50e-05; LR: 1.00e-02; Metric: 0.4186214; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.405 1 + -0.003 value_reward_chosen[t] + 0.134 contr_diff + 0.739 reward + -0.005 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + 0.005 value_reward_chosen*reward + -0.044 contr_diff^2 + -0.025 contr_diff*reward + 0.74 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.415 1 + 0.735 value_reward_not_chosen[t] + -0.033 contr_diff + -0.033 value_reward_not_chosen^2 + -0.028 value_reward_not_chosen*contr_diff + -0.018 contr_diff^2 \n",
            "value_choice[t+1] = 0.038 1 + 0.808 value_choice[t] + -0.022 contr_diff + 0.038 choice + -0.078 value_choice^2 + 0.192 value_choice*contr_diff + -0.193 value_choice*choice + -0.01 contr_diff^2 + -0.02 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 92, 92, 92, 71, 92, 0\n",
            "value_reward_not_chosen: 0, 0, 92, 92, 92, 71\n",
            "value_choice: 92, 0, 33, 92, 0, 0, 0, 92, 62, 92\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 292/1000 --- L(Train): 0.4312330 --- L(Val, RNN): 0.4186412 --- L(Val, SINDy): 0.4188208 --- Time: 0.30s; --- Convergence: 2.24e-05; LR: 1.00e-02; Metric: 0.4186214; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.405 1 + -0.004 value_reward_chosen[t] + 0.133 contr_diff + 0.739 reward + -0.005 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + 0.004 value_reward_chosen*reward + -0.044 contr_diff^2 + -0.028 contr_diff*reward + 0.74 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.416 1 + 0.733 value_reward_not_chosen[t] + -0.032 contr_diff + -0.032 value_reward_not_chosen^2 + -0.027 value_reward_not_chosen*contr_diff + -0.017 contr_diff^2 \n",
            "value_choice[t+1] = 0.039 1 + 0.809 value_choice[t] + -0.02 contr_diff + 0.038 choice + -0.076 value_choice^2 + 0.192 value_choice*contr_diff + -0.191 value_choice*choice + -0.009 contr_diff^2 + -0.018 contr_diff*choice + 0.038 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 93, 93, 93, 72, 93, 0\n",
            "value_reward_not_chosen: 0, 0, 93, 93, 93, 72\n",
            "value_choice: 93, 0, 34, 93, 0, 0, 0, 93, 63, 93\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 293/1000 --- L(Train): 0.4184884 --- L(Val, RNN): 0.4186102 --- L(Val, SINDy): 0.4187937 --- Time: 0.27s; --- Convergence: 2.67e-05; LR: 1.00e-02; Metric: 0.4186102; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.406 1 + -0.005 value_reward_chosen[t] + 0.132 contr_diff + 0.739 reward + -0.005 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.044 contr_diff^2 + -0.031 contr_diff*reward + 0.74 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.415 1 + 0.731 value_reward_not_chosen[t] + -0.031 contr_diff + -0.031 value_reward_not_chosen^2 + -0.026 value_reward_not_chosen*contr_diff + -0.015 contr_diff^2 \n",
            "value_choice[t+1] = 0.038 1 + 0.809 value_choice[t] + -0.018 contr_diff + 0.038 choice + -0.076 value_choice^2 + 0.193 value_choice*contr_diff + -0.192 value_choice*choice + -0.009 contr_diff^2 + -0.016 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 94, 94, 94, 73, 94, 0\n",
            "value_reward_not_chosen: 0, 0, 94, 94, 94, 73\n",
            "value_choice: 94, 0, 35, 94, 0, 0, 0, 94, 64, 94\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 294/1000 --- L(Train): 0.4145293 --- L(Val, RNN): 0.4186264 --- L(Val, SINDy): 0.4187549 --- Time: 0.44s; --- Convergence: 2.14e-05; LR: 1.00e-02; Metric: 0.4186102; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.406 1 + -0.005 value_reward_chosen[t] + 0.132 contr_diff + 0.739 reward + -0.005 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.044 contr_diff^2 + -0.033 contr_diff*reward + 0.74 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.415 1 + 0.729 value_reward_not_chosen[t] + -0.029 contr_diff + -0.031 value_reward_not_chosen^2 + -0.024 value_reward_not_chosen*contr_diff + -0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.037 1 + 0.807 value_choice[t] + -0.015 contr_diff + 0.036 choice + -0.078 value_choice^2 + 0.194 value_choice*contr_diff + -0.193 value_choice*choice + -0.009 contr_diff^2 + -0.013 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 95, 95, 95, 74, 95, 0\n",
            "value_reward_not_chosen: 0, 0, 95, 95, 95, 74\n",
            "value_choice: 95, 0, 36, 95, 0, 0, 0, 95, 65, 95\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 295/1000 --- L(Train): 0.4311772 --- L(Val, RNN): 0.4185828 --- L(Val, SINDy): 0.4187309 --- Time: 0.33s; --- Convergence: 3.25e-05; LR: 1.00e-02; Metric: 0.4185828; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.406 1 + -0.005 value_reward_chosen[t] + 0.131 contr_diff + 0.74 reward + -0.005 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.043 contr_diff^2 + -0.035 contr_diff*reward + 0.741 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.416 1 + 0.729 value_reward_not_chosen[t] + -0.027 contr_diff + -0.031 value_reward_not_chosen^2 + -0.024 value_reward_not_chosen*contr_diff + -0.012 contr_diff^2 \n",
            "value_choice[t+1] = 0.034 1 + 0.804 value_choice[t] + -0.013 contr_diff + 0.033 choice + -0.081 value_choice^2 + 0.195 value_choice*contr_diff + -0.196 value_choice*choice + -0.011 contr_diff^2 + -0.011 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 96, 96, 96, 75, 96, 0\n",
            "value_reward_not_chosen: 0, 0, 96, 96, 96, 75\n",
            "value_choice: 96, 0, 37, 96, 0, 0, 0, 96, 66, 96\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 296/1000 --- L(Train): 0.4203178 --- L(Val, RNN): 0.4185528 --- L(Val, SINDy): 0.4187162 --- Time: 0.30s; --- Convergence: 3.12e-05; LR: 1.00e-02; Metric: 0.4185528; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.407 1 + -0.005 value_reward_chosen[t] + 0.131 contr_diff + 0.74 reward + -0.005 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + 0.003 value_reward_chosen*reward + -0.043 contr_diff^2 + -0.036 contr_diff*reward + 0.741 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.417 1 + 0.728 value_reward_not_chosen[t] + -0.025 contr_diff + -0.031 value_reward_not_chosen^2 + -0.023 value_reward_not_chosen*contr_diff + -0.012 contr_diff^2 \n",
            "value_choice[t+1] = 0.031 1 + 0.801 value_choice[t] + -0.011 contr_diff + 0.03 choice + -0.084 value_choice^2 + 0.195 value_choice*contr_diff + -0.199 value_choice*choice + -0.012 contr_diff^2 + -0.009 contr_diff*choice + 0.03 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 97, 97, 97, 76, 97, 0\n",
            "value_reward_not_chosen: 0, 0, 97, 97, 97, 76\n",
            "value_choice: 97, 0, 38, 97, 0, 0, 0, 97, 67, 97\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 297/1000 --- L(Train): 0.4152181 --- L(Val, RNN): 0.4185221 --- L(Val, SINDy): 0.4186933 --- Time: 0.45s; --- Convergence: 3.10e-05; LR: 1.00e-02; Metric: 0.4185221; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.408 1 + -0.005 value_reward_chosen[t] + 0.13 contr_diff + 0.74 reward + -0.006 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.002 value_reward_chosen*reward + -0.044 contr_diff^2 + -0.037 contr_diff*reward + 0.741 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.418 1 + 0.728 value_reward_not_chosen[t] + -0.023 contr_diff + -0.032 value_reward_not_chosen^2 + -0.022 value_reward_not_chosen*contr_diff + -0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.029 1 + 0.799 value_choice[t] + -0.011 contr_diff + 0.028 choice + -0.086 value_choice^2 + 0.193 value_choice*contr_diff + -0.202 value_choice*choice + -0.013 contr_diff^2 + -0.009 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 98, 98, 98, 77, 98, 0\n",
            "value_reward_not_chosen: 0, 0, 98, 98, 98, 77\n",
            "value_choice: 98, 0, 39, 98, 0, 0, 0, 98, 68, 98\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 298/1000 --- L(Train): 0.4228285 --- L(Val, RNN): 0.4185084 --- L(Val, SINDy): 0.4186625 --- Time: 0.29s; --- Convergence: 2.24e-05; LR: 1.00e-02; Metric: 0.4185084; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.409 1 + -0.005 value_reward_chosen[t] + 0.129 contr_diff + 0.74 reward + -0.006 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + 0.002 value_reward_chosen*reward + -0.044 contr_diff^2 + -0.038 contr_diff*reward + 0.741 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.419 1 + 0.728 value_reward_not_chosen[t] + -0.021 contr_diff + -0.032 value_reward_not_chosen^2 + -0.02 value_reward_not_chosen*contr_diff + -0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.027 1 + 0.797 value_choice[t] + -0.011 contr_diff + 0.026 choice + -0.088 value_choice^2 + 0.192 value_choice*contr_diff + -0.204 value_choice*choice + -0.014 contr_diff^2 + -0.009 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 99, 99, 99, 78, 99, 0\n",
            "value_reward_not_chosen: 0, 0, 99, 99, 99, 78\n",
            "value_choice: 99, 0, 40, 99, 0, 0, 0, 99, 69, 99\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 299/1000 --- L(Train): 0.4197558 --- L(Val, RNN): 0.4185157 --- L(Val, SINDy): 0.4186655 --- Time: 0.27s; --- Convergence: 1.48e-05; LR: 1.00e-02; Metric: 0.4185084; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 25):\n",
            "value_reward_chosen[t+1] = -0.408 1 + -0.004 value_reward_chosen[t] + 0.127 contr_diff + 0.742 reward + -0.005 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + -0.042 contr_diff^2 + -0.04 contr_diff*reward + 0.743 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.42 1 + 0.727 value_reward_not_chosen[t] + -0.021 contr_diff + -0.031 value_reward_not_chosen^2 + -0.018 value_reward_not_chosen*contr_diff + -0.01 contr_diff^2 \n",
            "value_choice[t+1] = 0.025 1 + 0.795 value_choice[t] + -0.012 contr_diff + 0.025 choice + -0.09 value_choice^2 + 0.189 value_choice*contr_diff + -0.205 value_choice*choice + -0.014 contr_diff^2 + -0.01 contr_diff*choice + 0.024 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 100, 100, -, 79, 100, 0\n",
            "value_reward_not_chosen: 0, 0, 100, 100, 100, 79\n",
            "value_choice: 100, 0, 41, 100, 0, 0, 0, 100, 70, 100\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 300/1000 --- L(Train): 0.4234475 --- L(Val, RNN): 0.4184820 --- L(Val, SINDy): 0.4186548 --- Time: 0.33s; --- Convergence: 2.42e-05; LR: 1.00e-02; Metric: 0.4184820; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 24):\n",
            "value_reward_chosen[t+1] = -0.408 1 + -0.003 value_reward_chosen[t] + 0.126 contr_diff + 0.743 reward + -0.006 value_reward_chosen*contr_diff + -0.041 contr_diff^2 + -0.041 contr_diff*reward + 0.744 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.421 1 + 0.726 value_reward_not_chosen[t] + -0.021 contr_diff + -0.031 value_reward_not_chosen^2 + -0.016 value_reward_not_chosen*contr_diff + -0.01 contr_diff^2 \n",
            "value_choice[t+1] = 0.023 1 + 0.793 value_choice[t] + -0.013 contr_diff + 0.022 choice + -0.092 value_choice^2 + 0.186 value_choice*contr_diff + -0.207 value_choice*choice + -0.016 contr_diff^2 + -0.011 contr_diff*choice + 0.021 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, 101, -, 80, 101, 0\n",
            "value_reward_not_chosen: 0, 0, 101, 101, 101, 80\n",
            "value_choice: 101, 0, 42, 101, 0, 0, 0, 101, 71, 101\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 301/1000 --- L(Train): 0.4231042 --- L(Val, RNN): 0.4184559 --- L(Val, SINDy): 0.4186233 --- Time: 0.40s; --- Convergence: 2.52e-05; LR: 1.00e-02; Metric: 0.4184559; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 23):\n",
            "value_reward_chosen[t+1] = -0.41 1 + -0.003 value_reward_chosen[t] + 0.125 contr_diff + 0.741 reward + -0.041 contr_diff^2 + -0.04 contr_diff*reward + 0.742 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.422 1 + 0.725 value_reward_not_chosen[t] + -0.02 contr_diff + -0.031 value_reward_not_chosen^2 + -0.015 value_reward_not_chosen*contr_diff + -0.01 contr_diff^2 \n",
            "value_choice[t+1] = 0.021 1 + 0.792 value_choice[t] + -0.014 contr_diff + 0.021 choice + -0.092 value_choice^2 + 0.183 value_choice*contr_diff + -0.208 value_choice*choice + -0.016 contr_diff^2 + -0.012 contr_diff*choice + 0.02 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 81, 102, 0\n",
            "value_reward_not_chosen: 0, 0, 102, 102, 102, 81\n",
            "value_choice: 102, 0, 43, 102, 0, 0, 0, 102, 72, 102\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 302/1000 --- L(Train): 0.4116508 --- L(Val, RNN): 0.4184166 --- L(Val, SINDy): 0.4186160 --- Time: 0.33s; --- Convergence: 3.22e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 22):\n",
            "value_reward_chosen[t+1] = -0.413 1 + -0.004 value_reward_chosen[t] + 0.125 contr_diff + 0.74 reward + -0.042 contr_diff^2 + -0.037 contr_diff*reward + 0.741 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.424 1 + 0.725 value_reward_not_chosen[t] + -0.02 contr_diff + -0.031 value_reward_not_chosen^2 + -0.01 contr_diff^2 \n",
            "value_choice[t+1] = 0.021 1 + 0.793 value_choice[t] + -0.016 contr_diff + 0.02 choice + -0.092 value_choice^2 + 0.18 value_choice*contr_diff + -0.208 value_choice*choice + -0.015 contr_diff^2 + -0.014 contr_diff*choice + 0.02 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 82, 103, 0\n",
            "value_reward_not_chosen: 0, 0, 103, 103, -, 82\n",
            "value_choice: 103, 0, 44, 103, 0, 0, 0, 103, 73, 103\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 303/1000 --- L(Train): 0.4257302 --- L(Val, RNN): 0.4184230 --- L(Val, SINDy): 0.4185673 --- Time: 0.35s; --- Convergence: 1.93e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 21):\n",
            "value_reward_chosen[t+1] = -0.416 1 + -0.005 value_reward_chosen[t] + 0.125 contr_diff + 0.738 reward + -0.043 contr_diff^2 + -0.034 contr_diff*reward + 0.739 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.425 1 + 0.725 value_reward_not_chosen[t] + -0.017 contr_diff + -0.031 value_reward_not_chosen^2 + -0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.021 1 + 0.793 value_choice[t] + -0.018 contr_diff + 0.02 choice + -0.091 value_choice^2 + 0.176 value_choice*contr_diff + -0.207 value_choice*choice + -0.016 contr_diff*choice + 0.019 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 83, 104, 0\n",
            "value_reward_not_chosen: 0, 0, 104, 104, -, 83\n",
            "value_choice: 104, 0, 45, 104, 0, 0, 0, -, 74, 104\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 304/1000 --- L(Train): 0.4205886 --- L(Val, RNN): 0.4184587 --- L(Val, SINDy): 0.4186091 --- Time: 0.36s; --- Convergence: 2.75e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 20):\n",
            "value_reward_chosen[t+1] = -0.417 1 + -0.005 value_reward_chosen[t] + 0.126 contr_diff + 0.738 reward + -0.042 contr_diff^2 + -0.032 contr_diff*reward + 0.739 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.427 1 + 0.726 value_reward_not_chosen[t] + -0.032 value_reward_not_chosen^2 + -0.01 contr_diff^2 \n",
            "value_choice[t+1] = 0.019 1 + 0.793 value_choice[t] + -0.02 contr_diff + 0.018 choice + -0.091 value_choice^2 + 0.173 value_choice*contr_diff + -0.207 value_choice*choice + -0.018 contr_diff*choice + 0.018 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 84, 105, 0\n",
            "value_reward_not_chosen: 0, 0, -, 105, -, 84\n",
            "value_choice: 105, 0, 46, 105, 0, 0, 0, -, 75, 105\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 305/1000 --- L(Train): 0.4181367 --- L(Val, RNN): 0.4184915 --- L(Val, SINDy): 0.4185740 --- Time: 0.31s; --- Convergence: 3.02e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 19):\n",
            "value_reward_chosen[t+1] = -0.417 1 + -0.004 value_reward_chosen[t] + 0.128 contr_diff + 0.739 reward + -0.04 contr_diff^2 + -0.029 contr_diff*reward + 0.74 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.428 1 + 0.727 value_reward_not_chosen[t] + -0.033 value_reward_not_chosen^2 + -0.008 contr_diff^2 \n",
            "value_choice[t+1] = 0.02 1 + 0.796 value_choice[t] + -0.022 contr_diff + 0.019 choice + -0.087 value_choice^2 + 0.17 value_choice*contr_diff + -0.204 value_choice*choice + -0.02 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 85, 106, 0\n",
            "value_reward_not_chosen: 0, 0, -, 106, -, 85\n",
            "value_choice: 106, 0, 47, 106, 0, 0, 0, -, 76, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 306/1000 --- L(Train): 0.4259442 --- L(Val, RNN): 0.4184626 --- L(Val, SINDy): 0.4186412 --- Time: 0.36s; --- Convergence: 2.96e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 18):\n",
            "value_reward_chosen[t+1] = -0.417 1 + -0.003 value_reward_chosen[t] + 0.129 contr_diff + 0.74 reward + -0.039 contr_diff^2 + -0.026 contr_diff*reward + 0.741 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.428 1 + 0.727 value_reward_not_chosen[t] + -0.034 value_reward_not_chosen^2 + -0.006 contr_diff^2 \n",
            "value_choice[t+1] = 0.023 1 + 0.801 value_choice[t] + -0.024 contr_diff + -0.081 value_choice^2 + 0.167 value_choice*contr_diff + -0.199 value_choice*choice + -0.022 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 86, 107, 0\n",
            "value_reward_not_chosen: 0, 0, -, 107, -, 86\n",
            "value_choice: 107, 0, 48, -, 0, 0, 0, -, 77, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 307/1000 --- L(Train): 0.4200295 --- L(Val, RNN): 0.4184439 --- L(Val, SINDy): 0.4187212 --- Time: 0.28s; --- Convergence: 2.41e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 18):\n",
            "value_reward_chosen[t+1] = -0.418 1 + -0.002 value_reward_chosen[t] + 0.13 contr_diff + 0.741 reward + -0.038 contr_diff^2 + 0.742 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.426 1 + 0.727 value_reward_not_chosen[t] + -0.034 value_reward_not_chosen^2 + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.028 1 + 0.809 value_choice[t] + -0.025 contr_diff + -0.072 value_choice^2 + 0.166 value_choice*contr_diff + -0.191 value_choice*choice + -0.023 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 87, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, 108, -, 87\n",
            "value_choice: 108, 0, 49, -, 0, 0, 0, -, 78, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 308/1000 --- L(Train): 0.4150532 --- L(Val, RNN): 0.4184531 --- L(Val, SINDy): 0.4186550 --- Time: 0.43s; --- Convergence: 1.67e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 17):\n",
            "value_reward_chosen[t+1] = -0.417 1 + -0.0 value_reward_chosen[t] + 0.127 contr_diff + 0.743 reward + -0.035 contr_diff^2 + 0.744 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.424 1 + 0.727 value_reward_not_chosen[t] + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.82 value_choice[t] + -0.023 contr_diff + -0.06 value_choice^2 + 0.17 value_choice*contr_diff + -0.181 value_choice*choice + -0.02 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 88, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 88\n",
            "value_choice: 109, 0, 50, -, 0, 0, 0, -, 79, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 309/1000 --- L(Train): 0.4261651 --- L(Val, RNN): 0.4185276 --- L(Val, SINDy): 0.4186091 --- Time: 0.28s; --- Convergence: 4.56e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 17):\n",
            "value_reward_chosen[t+1] = -0.416 1 + 0.001 value_reward_chosen[t] + 0.124 contr_diff + 0.745 reward + -0.033 contr_diff^2 + 0.746 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.426 1 + 0.732 value_reward_not_chosen[t] + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.83 value_choice[t] + -0.017 contr_diff + -0.048 value_choice^2 + 0.175 value_choice*contr_diff + -0.17 value_choice*choice + -0.015 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 89, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 89\n",
            "value_choice: -, 0, 51, -, 1, 0, 0, -, 80, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 310/1000 --- L(Train): 0.4208801 --- L(Val, RNN): 0.4185118 --- L(Val, SINDy): 0.4185311 --- Time: 0.27s; --- Convergence: 3.07e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.416 1 + 0.002 value_reward_chosen[t] + 0.121 contr_diff + 0.746 reward + -0.031 contr_diff^2 + 0.748 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.432 1 + 0.741 value_reward_not_chosen[t] + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.841 value_choice[t] + -0.007 contr_diff + -0.039 value_choice^2 + 0.177 value_choice*contr_diff + -0.16 value_choice*choice + -0.005 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 90, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 90\n",
            "value_choice: -, 0, 52, -, 2, 0, 0, -, 81, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 311/1000 --- L(Train): 0.4166074 --- L(Val, RNN): 0.4185353 --- L(Val, SINDy): 0.4184504 --- Time: 0.31s; --- Convergence: 2.71e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.418 1 + 0.002 value_reward_chosen[t] + 0.121 contr_diff + 0.746 reward + -0.032 contr_diff^2 + 0.747 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.441 1 + 0.752 value_reward_not_chosen[t] + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.855 value_choice[t] + -0.002 contr_diff + -0.025 value_choice^2 + 0.176 value_choice*contr_diff + -0.146 value_choice*choice + -0.0 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 91, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 91\n",
            "value_choice: -, 0, 53, -, 3, 0, 0, -, 82, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 312/1000 --- L(Train): 0.4136464 --- L(Val, RNN): 0.4185105 --- L(Val, SINDy): 0.4184259 --- Time: 0.30s; --- Convergence: 2.59e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.42 1 + 0.001 value_reward_chosen[t] + 0.121 contr_diff + 0.745 reward + -0.033 contr_diff^2 + 0.746 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.448 1 + 0.763 value_reward_not_chosen[t] + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.871 value_choice[t] + -0.002 contr_diff + -0.009 value_choice^2 + 0.172 value_choice*contr_diff + -0.129 value_choice*choice + -0.0 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 92, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 92\n",
            "value_choice: -, 0, 54, -, 4, 0, 0, -, 83, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 313/1000 --- L(Train): 0.4221133 --- L(Val, RNN): 0.4185436 --- L(Val, SINDy): 0.4184603 --- Time: 0.30s; --- Convergence: 2.95e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.422 1 + -0.0 value_reward_chosen[t] + 0.122 contr_diff + 0.745 reward + -0.034 contr_diff^2 + 0.746 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.77 value_reward_not_chosen[t] + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.889 value_choice[t] + -0.01 contr_diff + 0.008 value_choice^2 + 0.162 value_choice*contr_diff + -0.111 value_choice*choice + -0.007 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 93, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 93\n",
            "value_choice: -, 0, 55, -, 5, 0, 0, -, 84, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 314/1000 --- L(Train): 0.4303359 --- L(Val, RNN): 0.4185007 --- L(Val, SINDy): 0.4185104 --- Time: 0.38s; --- Convergence: 3.62e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.424 1 + -0.001 value_reward_chosen[t] + 0.123 contr_diff + 0.745 reward + -0.035 contr_diff^2 + 0.746 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.449 1 + 0.773 value_reward_not_chosen[t] + 0.01 contr_diff^2 \n",
            "value_choice[t+1] = 0.907 value_choice[t] + -0.017 contr_diff + 0.027 value_choice^2 + 0.151 value_choice*contr_diff + -0.093 value_choice*choice + -0.015 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 94, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 94\n",
            "value_choice: -, 0, 56, -, 6, 0, 0, -, 85, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 315/1000 --- L(Train): 0.4275360 --- L(Val, RNN): 0.4184738 --- L(Val, SINDy): 0.4185172 --- Time: 0.28s; --- Convergence: 3.16e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 13/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.425 1 + -0.002 value_reward_chosen[t] + 0.122 contr_diff + 0.745 reward + -0.035 contr_diff^2 + 0.746 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.446 1 + 0.773 value_reward_not_chosen[t] + 0.019 contr_diff^2 \n",
            "value_choice[t+1] = 0.926 value_choice[t] + -0.026 contr_diff + 0.046 value_choice^2 + 0.14 value_choice*contr_diff + -0.074 value_choice*choice + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 95, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 95\n",
            "value_choice: -, 0, 57, -, 7, 0, 0, -, 86, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 316/1000 --- L(Train): 0.4322408 --- L(Val, RNN): 0.4185414 --- L(Val, SINDy): 0.4184866 --- Time: 0.30s; --- Convergence: 4.96e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 14/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.428 1 + -0.004 value_reward_chosen[t] + 0.12 contr_diff + 0.744 reward + -0.037 contr_diff^2 + 0.745 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.441 1 + 0.77 value_reward_not_chosen[t] + 0.029 contr_diff^2 \n",
            "value_choice[t+1] = 0.945 value_choice[t] + -0.036 contr_diff + 0.065 value_choice^2 + 0.131 value_choice*contr_diff + -0.055 value_choice*choice + -0.034 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 96, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 96\n",
            "value_choice: -, 0, 58, -, 0, 0, 0, -, 87, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 317/1000 --- L(Train): 0.4132491 --- L(Val, RNN): 0.4185806 --- L(Val, SINDy): 0.4184509 --- Time: 0.32s; --- Convergence: 4.44e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 15/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.431 1 + -0.005 value_reward_chosen[t] + 0.117 contr_diff + 0.743 reward + -0.039 contr_diff^2 + 0.744 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.766 value_reward_not_chosen[t] + 0.039 contr_diff^2 \n",
            "value_choice[t+1] = 0.963 value_choice[t] + -0.046 contr_diff + 0.083 value_choice^2 + 0.125 value_choice*contr_diff + -0.038 value_choice*choice + -0.043 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 97, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 97\n",
            "value_choice: -, 1, 59, -, 0, 0, 1, -, 88, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 318/1000 --- L(Train): 0.4249561 --- L(Val, RNN): 0.4184828 --- L(Val, SINDy): 0.4183850 --- Time: 0.31s; --- Convergence: 7.11e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 16/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.432 1 + -0.005 value_reward_chosen[t] + 0.114 contr_diff + 0.744 reward + -0.04 contr_diff^2 + 0.745 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.43 1 + 0.763 value_reward_not_chosen[t] + 0.048 contr_diff^2 \n",
            "value_choice[t+1] = 0.979 value_choice[t] + -0.054 contr_diff + 0.1 value_choice^2 + 0.121 value_choice*contr_diff + -0.021 value_choice*choice + -0.052 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 98, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 98\n",
            "value_choice: -, 2, 0, -, 0, 0, 2, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 319/1000 --- L(Train): 0.4117825 --- L(Val, RNN): 0.4184476 --- L(Val, SINDy): 0.4183539 --- Time: 0.36s; --- Convergence: 5.32e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 17/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.432 1 + -0.004 value_reward_chosen[t] + 0.112 contr_diff + 0.746 reward + -0.04 contr_diff^2 + 0.747 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.427 1 + 0.759 value_reward_not_chosen[t] + 0.054 contr_diff^2 \n",
            "value_choice[t+1] = 0.994 value_choice[t] + -0.061 contr_diff + 0.115 value_choice^2 + 0.122 value_choice*contr_diff + -0.006 value_choice*choice + -0.059 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, 99, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 0\n",
            "value_choice: -, 3, 0, -, 0, 0, 3, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 320/1000 --- L(Train): 0.4304827 --- L(Val, RNN): 0.4184280 --- L(Val, SINDy): 0.4183670 --- Time: 0.40s; --- Convergence: 3.64e-05; LR: 1.00e-02; Metric: 0.4184166; Bad epochs: 18/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.433 1 + -0.003 value_reward_chosen[t] + 0.11 contr_diff + 0.747 reward + 0.748 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.425 1 + 0.756 value_reward_not_chosen[t] + 0.058 contr_diff^2 \n",
            "value_choice[t+1] = 1.008 value_choice[t] + -0.066 contr_diff + 0.129 value_choice^2 + 0.126 value_choice*contr_diff + 0.007 value_choice*choice + -0.064 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 0\n",
            "value_choice: -, 4, 0, -, 0, 0, 4, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 321/1000 --- L(Train): 0.4239318 --- L(Val, RNN): 0.4183478 --- L(Val, SINDy): 0.4183381 --- Time: 0.30s; --- Convergence: 5.83e-05; LR: 1.00e-02; Metric: 0.4183478; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.435 1 + -0.003 value_reward_chosen[t] + 0.109 contr_diff + 0.747 reward + 0.748 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.426 1 + 0.755 value_reward_not_chosen[t] + 0.061 contr_diff^2 \n",
            "value_choice[t+1] = 1.019 value_choice[t] + -0.07 contr_diff + 0.141 value_choice^2 + 0.131 value_choice*contr_diff + 0.019 value_choice*choice + -0.068 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 0\n",
            "value_choice: -, 5, 0, -, 0, 0, 5, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 322/1000 --- L(Train): 0.4221975 --- L(Val, RNN): 0.4183241 --- L(Val, SINDy): 0.4183058 --- Time: 0.27s; --- Convergence: 4.10e-05; LR: 1.00e-02; Metric: 0.4183241; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.439 1 + -0.004 value_reward_chosen[t] + 0.111 contr_diff + 0.746 reward + 0.747 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.43 1 + 0.756 value_reward_not_chosen[t] + 0.06 contr_diff^2 \n",
            "value_choice[t+1] = 1.028 value_choice[t] + -0.071 contr_diff + 0.15 value_choice^2 + 0.142 value_choice*contr_diff + 0.028 value_choice*choice + -0.069 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 0\n",
            "value_choice: -, 6, 0, -, 0, 0, 6, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 323/1000 --- L(Train): 0.4076790 --- L(Val, RNN): 0.4183446 --- L(Val, SINDy): 0.4182751 --- Time: 0.31s; --- Convergence: 3.07e-05; LR: 1.00e-02; Metric: 0.4183241; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.445 1 + -0.006 value_reward_chosen[t] + 0.114 contr_diff + 0.742 reward + 0.743 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.758 value_reward_not_chosen[t] + 0.057 contr_diff^2 \n",
            "value_choice[t+1] = 1.035 value_choice[t] + -0.07 contr_diff + 0.157 value_choice^2 + 0.154 value_choice*contr_diff + 0.034 value_choice*choice + -0.068 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 0\n",
            "value_choice: -, 7, 0, -, 0, 0, 7, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 324/1000 --- L(Train): 0.4229760 --- L(Val, RNN): 0.4182967 --- L(Val, SINDy): 0.4182412 --- Time: 0.30s; --- Convergence: 3.93e-05; LR: 1.00e-02; Metric: 0.4182967; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.449 1 + -0.007 value_reward_chosen[t] + 0.116 contr_diff + 0.74 reward + 0.741 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.441 1 + 0.76 value_reward_not_chosen[t] + 0.052 contr_diff^2 \n",
            "value_choice[t+1] = 1.039 value_choice[t] + -0.066 contr_diff + 0.161 value_choice^2 + 0.169 value_choice*contr_diff + 0.039 value_choice*choice + -0.064 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 0\n",
            "value_choice: -, 8, 0, -, 0, 0, 8, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 325/1000 --- L(Train): 0.4257369 --- L(Val, RNN): 0.4182735 --- L(Val, SINDy): 0.4182430 --- Time: 0.31s; --- Convergence: 3.12e-05; LR: 1.00e-02; Metric: 0.4182735; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.453 1 + -0.007 value_reward_chosen[t] + 0.117 contr_diff + 0.74 reward + 0.741 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.446 1 + 0.763 value_reward_not_chosen[t] + 0.048 contr_diff^2 \n",
            "value_choice[t+1] = 1.042 value_choice[t] + -0.062 contr_diff + 0.164 value_choice^2 + 0.185 value_choice*contr_diff + 0.042 value_choice*choice + -0.06 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 1\n",
            "value_choice: -, 9, 0, -, 0, 0, 9, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 326/1000 --- L(Train): 0.4209398 --- L(Val, RNN): 0.4182763 --- L(Val, SINDy): 0.4182793 --- Time: 0.30s; --- Convergence: 1.70e-05; LR: 1.00e-02; Metric: 0.4182735; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.455 1 + -0.005 value_reward_chosen[t] + 0.118 contr_diff + 0.741 reward + 0.742 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.764 value_reward_not_chosen[t] + 0.044 contr_diff^2 \n",
            "value_choice[t+1] = 1.043 value_choice[t] + -0.058 contr_diff + 0.165 value_choice^2 + 0.2 value_choice*contr_diff + 0.043 value_choice*choice + -0.056 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 2\n",
            "value_choice: -, 10, 0, -, 0, 0, 10, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 327/1000 --- L(Train): 0.4169634 --- L(Val, RNN): 0.4182668 --- L(Val, SINDy): 0.4182611 --- Time: 0.31s; --- Convergence: 1.32e-05; LR: 1.00e-02; Metric: 0.4182668; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.456 1 + -0.003 value_reward_chosen[t] + 0.117 contr_diff + 0.744 reward + 0.745 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.764 value_reward_not_chosen[t] + 0.042 contr_diff^2 \n",
            "value_choice[t+1] = 1.042 value_choice[t] + -0.054 contr_diff + 0.164 value_choice^2 + 0.216 value_choice*contr_diff + 0.042 value_choice*choice + -0.052 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 3\n",
            "value_choice: -, 11, 0, -, 0, 0, 11, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 328/1000 --- L(Train): 0.4162454 --- L(Val, RNN): 0.4182099 --- L(Val, SINDy): 0.4182684 --- Time: 0.31s; --- Convergence: 3.51e-05; LR: 1.00e-02; Metric: 0.4182099; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.456 1 + -0.0 value_reward_chosen[t] + 0.116 contr_diff + 0.747 reward + 0.748 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.764 value_reward_not_chosen[t] + 0.041 contr_diff^2 \n",
            "value_choice[t+1] = 1.041 value_choice[t] + -0.05 contr_diff + 0.163 value_choice^2 + 0.231 value_choice*contr_diff + 0.04 value_choice*choice + -0.048 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 4\n",
            "value_choice: -, 12, 0, -, 0, 0, 12, -, 1, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 329/1000 --- L(Train): 0.4181412 --- L(Val, RNN): 0.4181830 --- L(Val, SINDy): 0.4182854 --- Time: 0.31s; --- Convergence: 3.10e-05; LR: 1.00e-02; Metric: 0.4181830; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.457 1 + 0.002 value_reward_chosen[t] + 0.115 contr_diff + 0.749 reward + 0.75 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.762 value_reward_not_chosen[t] + 0.041 contr_diff^2 \n",
            "value_choice[t+1] = 1.038 value_choice[t] + -0.047 contr_diff + 0.16 value_choice^2 + 0.245 value_choice*contr_diff + 0.038 value_choice*choice + -0.045 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 5\n",
            "value_choice: -, 13, 1, -, 0, 0, 13, -, 2, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 330/1000 --- L(Train): 0.4220097 --- L(Val, RNN): 0.4181488 --- L(Val, SINDy): 0.4182704 --- Time: 0.37s; --- Convergence: 3.26e-05; LR: 1.00e-02; Metric: 0.4181488; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.458 1 + 0.004 value_reward_chosen[t] + 0.113 contr_diff + 0.752 reward + 0.753 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.761 value_reward_not_chosen[t] + 0.041 contr_diff^2 \n",
            "value_choice[t+1] = 1.035 value_choice[t] + -0.044 contr_diff + 0.157 value_choice^2 + 0.258 value_choice*contr_diff + 0.035 value_choice*choice + -0.042 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 6\n",
            "value_choice: -, 14, 2, -, 0, 0, 14, -, 3, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 331/1000 --- L(Train): 0.4191395 --- L(Val, RNN): 0.4181320 --- L(Val, SINDy): 0.4182593 --- Time: 0.37s; --- Convergence: 2.47e-05; LR: 1.00e-02; Metric: 0.4181320; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.459 1 + 0.005 value_reward_chosen[t] + 0.112 contr_diff + 0.754 reward + 0.755 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.759 value_reward_not_chosen[t] + 0.041 contr_diff^2 \n",
            "value_choice[t+1] = 1.032 value_choice[t] + -0.041 contr_diff + 0.154 value_choice^2 + 0.27 value_choice*contr_diff + 0.032 value_choice*choice + -0.039 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 7\n",
            "value_choice: -, 15, 3, -, 0, 0, 15, -, 4, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 332/1000 --- L(Train): 0.4211645 --- L(Val, RNN): 0.4181186 --- L(Val, SINDy): 0.4182312 --- Time: 0.28s; --- Convergence: 1.91e-05; LR: 1.00e-02; Metric: 0.4181186; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.461 1 + 0.006 value_reward_chosen[t] + 0.112 contr_diff + 0.755 reward + 0.756 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.454 1 + 0.757 value_reward_not_chosen[t] + 0.042 contr_diff^2 \n",
            "value_choice[t+1] = 1.029 value_choice[t] + -0.039 contr_diff + 0.151 value_choice^2 + 0.28 value_choice*contr_diff + 0.028 value_choice*choice + -0.037 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 8\n",
            "value_choice: -, 16, 4, -, 0, 0, 16, -, 5, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 333/1000 --- L(Train): 0.4240373 --- L(Val, RNN): 0.4181198 --- L(Val, SINDy): 0.4181496 --- Time: 0.27s; --- Convergence: 1.01e-05; LR: 1.00e-02; Metric: 0.4181186; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.462 1 + 0.007 value_reward_chosen[t] + 0.112 contr_diff + 0.757 reward + 0.758 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.755 value_reward_not_chosen[t] + 0.043 contr_diff^2 \n",
            "value_choice[t+1] = 1.025 value_choice[t] + -0.039 contr_diff + 0.148 value_choice^2 + 0.29 value_choice*contr_diff + 0.024 value_choice*choice + -0.037 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 9\n",
            "value_choice: -, 17, 5, -, 0, 0, 17, -, 6, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 334/1000 --- L(Train): 0.4082732 --- L(Val, RNN): 0.4180788 --- L(Val, SINDy): 0.4181530 --- Time: 0.33s; --- Convergence: 2.56e-05; LR: 1.00e-02; Metric: 0.4180788; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.463 1 + 0.007 value_reward_chosen[t] + 0.113 contr_diff + 0.758 reward + 0.759 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.451 1 + 0.753 value_reward_not_chosen[t] + 0.044 contr_diff^2 \n",
            "value_choice[t+1] = 1.021 value_choice[t] + -0.04 contr_diff + 0.144 value_choice^2 + 0.299 value_choice*contr_diff + 0.02 value_choice*choice + -0.038 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 10\n",
            "value_choice: -, 18, 6, -, 0, 0, 18, -, 7, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 335/1000 --- L(Train): 0.4161422 --- L(Val, RNN): 0.4180611 --- L(Val, SINDy): 0.4181048 --- Time: 0.40s; --- Convergence: 2.16e-05; LR: 1.00e-02; Metric: 0.4180611; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.465 1 + 0.006 value_reward_chosen[t] + 0.115 contr_diff + 0.759 reward + 0.76 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.751 value_reward_not_chosen[t] + 0.045 contr_diff^2 \n",
            "value_choice[t+1] = 1.016 value_choice[t] + -0.042 contr_diff + 0.14 value_choice^2 + 0.306 value_choice*contr_diff + 0.016 value_choice*choice + -0.039 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 11\n",
            "value_choice: -, 19, 7, -, 0, 0, 19, -, 8, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 336/1000 --- L(Train): 0.4020832 --- L(Val, RNN): 0.4180420 --- L(Val, SINDy): 0.4181013 --- Time: 0.40s; --- Convergence: 2.04e-05; LR: 1.00e-02; Metric: 0.4180420; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.466 1 + 0.005 value_reward_chosen[t] + 0.118 contr_diff + 0.76 reward + 0.762 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.449 1 + 0.75 value_reward_not_chosen[t] + 0.046 contr_diff^2 \n",
            "value_choice[t+1] = 1.012 value_choice[t] + -0.043 contr_diff + 0.136 value_choice^2 + 0.313 value_choice*contr_diff + 0.011 value_choice*choice + -0.041 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 12\n",
            "value_choice: -, 20, 8, -, 0, 0, 20, -, 9, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 337/1000 --- L(Train): 0.4188787 --- L(Val, RNN): 0.4180244 --- L(Val, SINDy): 0.4181365 --- Time: 0.47s; --- Convergence: 1.90e-05; LR: 1.00e-02; Metric: 0.4180244; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.468 1 + 0.004 value_reward_chosen[t] + 0.119 contr_diff + 0.761 reward + 0.762 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.449 1 + 0.75 value_reward_not_chosen[t] + 0.047 contr_diff^2 \n",
            "value_choice[t+1] = 1.007 value_choice[t] + -0.046 contr_diff + 0.132 value_choice^2 + 0.32 value_choice*contr_diff + 0.007 value_choice*choice + -0.044 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 13\n",
            "value_choice: -, 21, 9, -, 0, 0, 21, -, 10, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 338/1000 --- L(Train): 0.4201368 --- L(Val, RNN): 0.4180018 --- L(Val, SINDy): 0.4181948 --- Time: 0.30s; --- Convergence: 2.08e-05; LR: 1.00e-02; Metric: 0.4180018; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.469 1 + 0.004 value_reward_chosen[t] + 0.119 contr_diff + 0.763 reward + 0.764 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.751 value_reward_not_chosen[t] + 0.046 contr_diff^2 \n",
            "value_choice[t+1] = 1.003 value_choice[t] + -0.05 contr_diff + 0.128 value_choice^2 + 0.326 value_choice*contr_diff + 0.003 value_choice*choice + -0.048 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 14\n",
            "value_choice: -, 22, 10, -, 0, 0, 22, -, 11, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 339/1000 --- L(Train): 0.4042138 --- L(Val, RNN): 0.4179414 --- L(Val, SINDy): 0.4182411 --- Time: 0.35s; --- Convergence: 4.06e-05; LR: 1.00e-02; Metric: 0.4179414; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.47 1 + 0.003 value_reward_chosen[t] + 0.118 contr_diff + 0.764 reward + 0.766 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.451 1 + 0.753 value_reward_not_chosen[t] + 0.045 contr_diff^2 \n",
            "value_choice[t+1] = 0.999 value_choice[t] + -0.054 contr_diff + 0.124 value_choice^2 + 0.331 value_choice*contr_diff + -0.001 value_choice*choice + -0.052 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 15\n",
            "value_choice: -, 23, 0, -, 0, 0, 23, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 340/1000 --- L(Train): 0.4146500 --- L(Val, RNN): 0.4179515 --- L(Val, SINDy): 0.4182842 --- Time: 0.30s; --- Convergence: 2.53e-05; LR: 1.00e-02; Metric: 0.4179414; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.471 1 + 0.002 value_reward_chosen[t] + 0.116 contr_diff + 0.765 reward + 0.766 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.452 1 + 0.756 value_reward_not_chosen[t] + 0.044 contr_diff^2 \n",
            "value_choice[t+1] = 0.995 value_choice[t] + -0.058 contr_diff + 0.12 value_choice^2 + 0.337 value_choice*contr_diff + -0.005 value_choice*choice + -0.056 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 16\n",
            "value_choice: -, 24, 0, -, 0, 0, 24, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 341/1000 --- L(Train): 0.4254953 --- L(Val, RNN): 0.4179559 --- L(Val, SINDy): 0.4182650 --- Time: 0.35s; --- Convergence: 1.49e-05; LR: 1.00e-02; Metric: 0.4179414; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.474 1 + 0.001 value_reward_chosen[t] + 0.114 contr_diff + 0.765 reward + 0.766 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.454 1 + 0.758 value_reward_not_chosen[t] + 0.043 contr_diff^2 \n",
            "value_choice[t+1] = 0.991 value_choice[t] + -0.062 contr_diff + 0.117 value_choice^2 + 0.343 value_choice*contr_diff + -0.009 value_choice*choice + -0.06 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 17\n",
            "value_choice: -, 25, 0, -, 0, 0, 25, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 342/1000 --- L(Train): 0.4036899 --- L(Val, RNN): 0.4179053 --- L(Val, SINDy): 0.4181717 --- Time: 0.26s; --- Convergence: 3.27e-05; LR: 1.00e-02; Metric: 0.4179053; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.477 1 + -0.002 value_reward_chosen[t] + 0.114 contr_diff + 0.764 reward + 0.765 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.76 value_reward_not_chosen[t] + 0.042 contr_diff^2 \n",
            "value_choice[t+1] = 0.988 value_choice[t] + -0.065 contr_diff + 0.114 value_choice^2 + 0.35 value_choice*contr_diff + -0.013 value_choice*choice + -0.063 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 18\n",
            "value_choice: -, 26, 0, -, 0, 0, 26, -, 0, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 343/1000 --- L(Train): 0.4101789 --- L(Val, RNN): 0.4179068 --- L(Val, SINDy): 0.4180794 --- Time: 0.30s; --- Convergence: 1.71e-05; LR: 1.00e-02; Metric: 0.4179053; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.48 1 + -0.004 value_reward_chosen[t] + 0.114 contr_diff + 0.763 reward + 0.764 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.762 value_reward_not_chosen[t] + 0.042 contr_diff^2 \n",
            "value_choice[t+1] = 0.985 value_choice[t] + -0.067 contr_diff + 0.111 value_choice^2 + 0.356 value_choice*contr_diff + -0.016 value_choice*choice + -0.065 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 19\n",
            "value_choice: -, 27, 0, -, 0, 0, 27, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 344/1000 --- L(Train): 0.4082949 --- L(Val, RNN): 0.4179173 --- L(Val, SINDy): 0.4179982 --- Time: 0.40s; --- Convergence: 1.38e-05; LR: 1.00e-02; Metric: 0.4179053; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.483 1 + -0.006 value_reward_chosen[t] + 0.114 contr_diff + 0.762 reward + 0.763 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.764 value_reward_not_chosen[t] + 0.042 contr_diff^2 \n",
            "value_choice[t+1] = 0.982 value_choice[t] + -0.069 contr_diff + 0.108 value_choice^2 + 0.363 value_choice*contr_diff + -0.019 value_choice*choice + -0.067 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 20\n",
            "value_choice: -, 28, 0, -, 0, 0, 28, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 345/1000 --- L(Train): 0.4253414 --- L(Val, RNN): 0.4178646 --- L(Val, SINDy): 0.4179411 --- Time: 0.27s; --- Convergence: 3.33e-05; LR: 1.00e-02; Metric: 0.4178646; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.485 1 + -0.006 value_reward_chosen[t] + 0.114 contr_diff + 0.763 reward + 0.764 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.764 value_reward_not_chosen[t] + 0.042 contr_diff^2 \n",
            "value_choice[t+1] = 0.979 value_choice[t] + -0.071 contr_diff + 0.106 value_choice^2 + 0.37 value_choice*contr_diff + -0.021 value_choice*choice + -0.068 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 21\n",
            "value_choice: -, 29, 0, -, 0, 0, 29, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 346/1000 --- L(Train): 0.4205732 --- L(Val, RNN): 0.4178339 --- L(Val, SINDy): 0.4179370 --- Time: 0.30s; --- Convergence: 3.20e-05; LR: 1.00e-02; Metric: 0.4178339; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.485 1 + -0.006 value_reward_chosen[t] + 0.115 contr_diff + 0.764 reward + 0.765 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.764 value_reward_not_chosen[t] + 0.043 contr_diff^2 \n",
            "value_choice[t+1] = 0.977 value_choice[t] + -0.072 contr_diff + 0.104 value_choice^2 + 0.376 value_choice*contr_diff + -0.023 value_choice*choice + -0.07 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 22\n",
            "value_choice: -, 30, 0, -, 0, 0, 30, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 347/1000 --- L(Train): 0.4199528 --- L(Val, RNN): 0.4177917 --- L(Val, SINDy): 0.4179331 --- Time: 0.28s; --- Convergence: 3.71e-05; LR: 1.00e-02; Metric: 0.4177917; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.484 1 + -0.004 value_reward_chosen[t] + 0.115 contr_diff + 0.767 reward + 0.768 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.452 1 + 0.762 value_reward_not_chosen[t] + 0.043 contr_diff^2 \n",
            "value_choice[t+1] = 0.975 value_choice[t] + -0.073 contr_diff + 0.102 value_choice^2 + 0.383 value_choice*contr_diff + -0.025 value_choice*choice + -0.071 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 23\n",
            "value_choice: -, 31, 0, -, 0, 0, 31, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 348/1000 --- L(Train): 0.4194531 --- L(Val, RNN): 0.4177571 --- L(Val, SINDy): 0.4179468 --- Time: 0.26s; --- Convergence: 3.58e-05; LR: 1.00e-02; Metric: 0.4177571; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.483 1 + -0.003 value_reward_chosen[t] + 0.115 contr_diff + 0.77 reward + 0.771 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.758 value_reward_not_chosen[t] + 0.042 contr_diff^2 \n",
            "value_choice[t+1] = 0.974 value_choice[t] + -0.074 contr_diff + 0.101 value_choice^2 + 0.391 value_choice*contr_diff + -0.026 value_choice*choice + -0.071 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 24\n",
            "value_choice: -, 32, 0, -, 0, 0, 32, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 349/1000 --- L(Train): 0.4214120 --- L(Val, RNN): 0.4177509 --- L(Val, SINDy): 0.4179567 --- Time: 0.36s; --- Convergence: 2.10e-05; LR: 1.00e-02; Metric: 0.4177509; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.484 1 + -0.001 value_reward_chosen[t] + 0.116 contr_diff + 0.772 reward + 0.773 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.448 1 + 0.754 value_reward_not_chosen[t] + 0.041 contr_diff^2 \n",
            "value_choice[t+1] = 0.973 value_choice[t] + -0.074 contr_diff + 0.1 value_choice^2 + 0.399 value_choice*contr_diff + -0.027 value_choice*choice + -0.072 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 25\n",
            "value_choice: -, 33, 0, -, 0, 0, 33, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 350/1000 --- L(Train): 0.4214712 --- L(Val, RNN): 0.4177336 --- L(Val, SINDy): 0.4179007 --- Time: 0.31s; --- Convergence: 1.91e-05; LR: 1.00e-02; Metric: 0.4177336; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.484 1 + -0.001 value_reward_chosen[t] + 0.116 contr_diff + 0.774 reward + 0.775 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.447 1 + 0.75 value_reward_not_chosen[t] + 0.04 contr_diff^2 \n",
            "value_choice[t+1] = 0.972 value_choice[t] + -0.074 contr_diff + 0.1 value_choice^2 + 0.408 value_choice*contr_diff + -0.028 value_choice*choice + -0.072 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 26\n",
            "value_choice: -, 34, 0, -, 0, 0, 34, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 351/1000 --- L(Train): 0.4086026 --- L(Val, RNN): 0.4177152 --- L(Val, SINDy): 0.4178446 --- Time: 0.30s; --- Convergence: 1.88e-05; LR: 1.00e-02; Metric: 0.4177152; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.485 1 + -0.0 value_reward_chosen[t] + 0.116 contr_diff + 0.775 reward + 0.776 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.446 1 + 0.746 value_reward_not_chosen[t] + 0.038 contr_diff^2 \n",
            "value_choice[t+1] = 0.972 value_choice[t] + -0.073 contr_diff + 0.099 value_choice^2 + 0.416 value_choice*contr_diff + -0.028 value_choice*choice + -0.071 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 27\n",
            "value_choice: -, 35, 0, -, 0, 0, 35, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 352/1000 --- L(Train): 0.4115552 --- L(Val, RNN): 0.4177049 --- L(Val, SINDy): 0.4178163 --- Time: 0.30s; --- Convergence: 1.46e-05; LR: 1.00e-02; Metric: 0.4177049; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.486 1 + 0.0 value_reward_chosen[t] + 0.117 contr_diff + 0.775 reward + 0.776 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.447 1 + 0.743 value_reward_not_chosen[t] + 0.035 contr_diff^2 \n",
            "value_choice[t+1] = 0.972 value_choice[t] + -0.072 contr_diff + 0.099 value_choice^2 + 0.425 value_choice*contr_diff + -0.028 value_choice*choice + -0.07 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 28\n",
            "value_choice: -, 36, 0, -, 0, 0, 36, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 353/1000 --- L(Train): 0.4125721 --- L(Val, RNN): 0.4176586 --- L(Val, SINDy): 0.4177578 --- Time: 0.32s; --- Convergence: 3.04e-05; LR: 1.00e-02; Metric: 0.4176586; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.488 1 + 0.0 value_reward_chosen[t] + 0.119 contr_diff + 0.775 reward + 0.776 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.449 1 + 0.742 value_reward_not_chosen[t] + 0.031 contr_diff^2 \n",
            "value_choice[t+1] = 0.972 value_choice[t] + -0.071 contr_diff + 0.099 value_choice^2 + 0.433 value_choice*contr_diff + -0.028 value_choice*choice + -0.069 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 29\n",
            "value_choice: -, 37, 0, -, 0, 0, 37, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 354/1000 --- L(Train): 0.4115566 --- L(Val, RNN): 0.4176274 --- L(Val, SINDy): 0.4177393 --- Time: 0.33s; --- Convergence: 3.08e-05; LR: 1.00e-02; Metric: 0.4176274; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.49 1 + -0.001 value_reward_chosen[t] + 0.122 contr_diff + 0.774 reward + 0.775 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.452 1 + 0.741 value_reward_not_chosen[t] + 0.026 contr_diff^2 \n",
            "value_choice[t+1] = 0.973 value_choice[t] + -0.07 contr_diff + 0.099 value_choice^2 + 0.442 value_choice*contr_diff + -0.028 value_choice*choice + -0.068 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 30\n",
            "value_choice: -, 38, 0, -, 0, 0, 38, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 355/1000 --- L(Train): 0.4264816 --- L(Val, RNN): 0.4176223 --- L(Val, SINDy): 0.4177310 --- Time: 0.29s; --- Convergence: 1.79e-05; LR: 1.00e-02; Metric: 0.4176223; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.492 1 + -0.001 value_reward_chosen[t] + 0.124 contr_diff + 0.773 reward + 0.774 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.456 1 + 0.742 value_reward_not_chosen[t] + 0.021 contr_diff^2 \n",
            "value_choice[t+1] = 0.973 value_choice[t] + -0.069 contr_diff + 0.1 value_choice^2 + 0.45 value_choice*contr_diff + -0.027 value_choice*choice + -0.066 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 31\n",
            "value_choice: -, 39, 0, -, 0, 0, 39, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 356/1000 --- L(Train): 0.4197007 --- L(Val, RNN): 0.4175959 --- L(Val, SINDy): 0.4177341 --- Time: 0.35s; --- Convergence: 2.22e-05; LR: 1.00e-02; Metric: 0.4175959; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.493 1 + -0.001 value_reward_chosen[t] + 0.125 contr_diff + 0.773 reward + 0.775 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.743 value_reward_not_chosen[t] + 0.017 contr_diff^2 \n",
            "value_choice[t+1] = 0.974 value_choice[t] + -0.067 contr_diff + 0.1 value_choice^2 + 0.458 value_choice*contr_diff + -0.026 value_choice*choice + -0.065 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 32\n",
            "value_choice: -, 40, 0, -, 0, 0, 40, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 357/1000 --- L(Train): 0.4180639 --- L(Val, RNN): 0.4175862 --- L(Val, SINDy): 0.4177493 --- Time: 0.36s; --- Convergence: 1.59e-05; LR: 1.00e-02; Metric: 0.4175862; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.494 1 + -0.001 value_reward_chosen[t] + 0.126 contr_diff + 0.774 reward + 0.775 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.746 value_reward_not_chosen[t] + 0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.975 value_choice[t] + -0.066 contr_diff + 0.101 value_choice^2 + 0.466 value_choice*contr_diff + -0.026 value_choice*choice + -0.064 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 33\n",
            "value_choice: -, 41, 0, -, 0, 0, 41, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 358/1000 --- L(Train): 0.4141201 --- L(Val, RNN): 0.4175477 --- L(Val, SINDy): 0.4177642 --- Time: 0.30s; --- Convergence: 2.72e-05; LR: 1.00e-02; Metric: 0.4175477; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.495 1 + -0.001 value_reward_chosen[t] + 0.126 contr_diff + 0.774 reward + 0.775 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.749 value_reward_not_chosen[t] + 0.012 contr_diff^2 \n",
            "value_choice[t+1] = 0.976 value_choice[t] + -0.065 contr_diff + 0.102 value_choice^2 + 0.473 value_choice*contr_diff + -0.024 value_choice*choice + -0.063 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 34\n",
            "value_choice: -, 42, 0, -, 0, 0, 42, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 359/1000 --- L(Train): 0.4170473 --- L(Val, RNN): 0.4175136 --- L(Val, SINDy): 0.4177609 --- Time: 0.33s; --- Convergence: 3.07e-05; LR: 1.00e-02; Metric: 0.4175136; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.496 1 + -0.001 value_reward_chosen[t] + 0.125 contr_diff + 0.774 reward + 0.775 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.753 value_reward_not_chosen[t] + 0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.977 value_choice[t] + -0.065 contr_diff + 0.103 value_choice^2 + 0.48 value_choice*contr_diff + -0.023 value_choice*choice + -0.063 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 35\n",
            "value_choice: -, 43, 0, -, 0, 0, 43, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 360/1000 --- L(Train): 0.4055015 --- L(Val, RNN): 0.4174983 --- L(Val, SINDy): 0.4177398 --- Time: 0.34s; --- Convergence: 2.30e-05; LR: 1.00e-02; Metric: 0.4174983; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.497 1 + -0.001 value_reward_chosen[t] + 0.124 contr_diff + 0.774 reward + 0.775 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.756 value_reward_not_chosen[t] + 0.012 contr_diff^2 \n",
            "value_choice[t+1] = 0.978 value_choice[t] + -0.066 contr_diff + 0.104 value_choice^2 + 0.487 value_choice*contr_diff + -0.022 value_choice*choice + -0.064 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 36\n",
            "value_choice: -, 44, 0, -, 0, 0, 44, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 361/1000 --- L(Train): 0.4264762 --- L(Val, RNN): 0.4174704 --- L(Val, SINDy): 0.4177166 --- Time: 0.40s; --- Convergence: 2.54e-05; LR: 1.00e-02; Metric: 0.4174704; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.497 1 + -0.001 value_reward_chosen[t] + 0.123 contr_diff + 0.775 reward + 0.776 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.758 value_reward_not_chosen[t] + 0.016 contr_diff^2 \n",
            "value_choice[t+1] = 0.98 value_choice[t] + -0.067 contr_diff + 0.105 value_choice^2 + 0.494 value_choice*contr_diff + -0.021 value_choice*choice + -0.065 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 37\n",
            "value_choice: -, 45, 0, -, 0, 0, 45, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 362/1000 --- L(Train): 0.4096743 --- L(Val, RNN): 0.4174900 --- L(Val, SINDy): 0.4176911 --- Time: 0.38s; --- Convergence: 2.25e-05; LR: 1.00e-02; Metric: 0.4174704; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.498 1 + -0.001 value_reward_chosen[t] + 0.122 contr_diff + 0.775 reward + 0.776 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.76 value_reward_not_chosen[t] + 0.02 contr_diff^2 \n",
            "value_choice[t+1] = 0.981 value_choice[t] + -0.068 contr_diff + 0.106 value_choice^2 + 0.501 value_choice*contr_diff + -0.019 value_choice*choice + -0.066 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 38\n",
            "value_choice: -, 46, 0, -, 0, 0, 46, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 363/1000 --- L(Train): 0.4139533 --- L(Val, RNN): 0.4175026 --- L(Val, SINDy): 0.4176844 --- Time: 0.31s; --- Convergence: 1.76e-05; LR: 1.00e-02; Metric: 0.4174704; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.498 1 + -0.001 value_reward_chosen[t] + 0.121 contr_diff + 0.776 reward + 0.777 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.46 1 + 0.76 value_reward_not_chosen[t] + 0.025 contr_diff^2 \n",
            "value_choice[t+1] = 0.982 value_choice[t] + -0.069 contr_diff + 0.107 value_choice^2 + 0.507 value_choice*contr_diff + -0.018 value_choice*choice + -0.067 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 39\n",
            "value_choice: -, 47, 0, -, 0, 0, 47, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 364/1000 --- L(Train): 0.4293030 --- L(Val, RNN): 0.4174624 --- L(Val, SINDy): 0.4176415 --- Time: 0.40s; --- Convergence: 2.89e-05; LR: 1.00e-02; Metric: 0.4174624; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.5 1 + -0.001 value_reward_chosen[t] + 0.12 contr_diff + 0.775 reward + 0.776 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.457 1 + 0.76 value_reward_not_chosen[t] + 0.029 contr_diff^2 \n",
            "value_choice[t+1] = 0.983 value_choice[t] + -0.07 contr_diff + 0.108 value_choice^2 + 0.513 value_choice*contr_diff + -0.017 value_choice*choice + -0.068 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 40\n",
            "value_choice: -, 48, 0, -, 0, 0, 48, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 365/1000 --- L(Train): 0.4156595 --- L(Val, RNN): 0.4174332 --- L(Val, SINDy): 0.4176095 --- Time: 0.39s; --- Convergence: 2.91e-05; LR: 1.00e-02; Metric: 0.4174332; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.502 1 + -0.003 value_reward_chosen[t] + 0.119 contr_diff + 0.773 reward + 0.774 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.759 value_reward_not_chosen[t] + 0.033 contr_diff^2 \n",
            "value_choice[t+1] = 0.985 value_choice[t] + -0.071 contr_diff + 0.109 value_choice^2 + 0.518 value_choice*contr_diff + -0.016 value_choice*choice + -0.069 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 41\n",
            "value_choice: -, 49, 0, -, 0, 0, 49, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 366/1000 --- L(Train): 0.4270087 --- L(Val, RNN): 0.4174218 --- L(Val, SINDy): 0.4176040 --- Time: 0.43s; --- Convergence: 2.02e-05; LR: 1.00e-02; Metric: 0.4174218; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.505 1 + -0.005 value_reward_chosen[t] + 0.119 contr_diff + 0.771 reward + 0.773 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.449 1 + 0.759 value_reward_not_chosen[t] + 0.037 contr_diff^2 \n",
            "value_choice[t+1] = 0.986 value_choice[t] + -0.073 contr_diff + 0.11 value_choice^2 + 0.523 value_choice*contr_diff + -0.015 value_choice*choice + -0.071 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 42\n",
            "value_choice: -, 50, 0, -, 0, 0, 50, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 367/1000 --- L(Train): 0.4124989 --- L(Val, RNN): 0.4173802 --- L(Val, SINDy): 0.4176269 --- Time: 0.32s; --- Convergence: 3.09e-05; LR: 1.00e-02; Metric: 0.4173802; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.507 1 + -0.006 value_reward_chosen[t] + 0.119 contr_diff + 0.77 reward + 0.772 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.446 1 + 0.758 value_reward_not_chosen[t] + 0.04 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.074 contr_diff + 0.11 value_choice^2 + 0.528 value_choice*contr_diff + -0.014 value_choice*choice + -0.072 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 43\n",
            "value_choice: -, 51, 0, -, 0, 0, 51, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 368/1000 --- L(Train): 0.4173192 --- L(Val, RNN): 0.4173481 --- L(Val, SINDy): 0.4176768 --- Time: 0.29s; --- Convergence: 3.15e-05; LR: 1.00e-02; Metric: 0.4173481; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.508 1 + -0.005 value_reward_chosen[t] + 0.118 contr_diff + 0.771 reward + 0.772 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.444 1 + 0.758 value_reward_not_chosen[t] + 0.043 contr_diff^2 \n",
            "value_choice[t+1] = 0.988 value_choice[t] + -0.075 contr_diff + 0.111 value_choice^2 + 0.533 value_choice*contr_diff + -0.013 value_choice*choice + -0.073 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 44\n",
            "value_choice: -, 52, 0, -, 0, 0, 52, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 369/1000 --- L(Train): 0.4187238 --- L(Val, RNN): 0.4173076 --- L(Val, SINDy): 0.4177233 --- Time: 0.28s; --- Convergence: 3.60e-05; LR: 1.00e-02; Metric: 0.4173076; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.508 1 + -0.004 value_reward_chosen[t] + 0.119 contr_diff + 0.772 reward + 0.773 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.441 1 + 0.758 value_reward_not_chosen[t] + 0.046 contr_diff^2 \n",
            "value_choice[t+1] = 0.988 value_choice[t] + -0.076 contr_diff + 0.112 value_choice^2 + 0.538 value_choice*contr_diff + -0.012 value_choice*choice + -0.074 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 45\n",
            "value_choice: -, 53, 0, -, 0, 0, 53, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 370/1000 --- L(Train): 0.4280925 --- L(Val, RNN): 0.4172738 --- L(Val, SINDy): 0.4177206 --- Time: 0.32s; --- Convergence: 3.49e-05; LR: 1.00e-02; Metric: 0.4172738; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.508 1 + -0.003 value_reward_chosen[t] + 0.12 contr_diff + 0.773 reward + 0.774 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.44 1 + 0.759 value_reward_not_chosen[t] + 0.047 contr_diff^2 \n",
            "value_choice[t+1] = 0.989 value_choice[t] + -0.077 contr_diff + 0.112 value_choice^2 + 0.542 value_choice*contr_diff + -0.011 value_choice*choice + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 46\n",
            "value_choice: -, 54, 0, -, 0, 0, 54, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 371/1000 --- L(Train): 0.4188947 --- L(Val, RNN): 0.4172611 --- L(Val, SINDy): 0.4176888 --- Time: 0.35s; --- Convergence: 2.38e-05; LR: 1.00e-02; Metric: 0.4172611; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.508 1 + -0.001 value_reward_chosen[t] + 0.122 contr_diff + 0.774 reward + 0.776 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.439 1 + 0.759 value_reward_not_chosen[t] + 0.047 contr_diff^2 \n",
            "value_choice[t+1] = 0.99 value_choice[t] + -0.079 contr_diff + 0.113 value_choice^2 + 0.548 value_choice*contr_diff + -0.01 value_choice*choice + -0.076 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 47\n",
            "value_choice: -, 55, 0, -, 0, 0, 55, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 372/1000 --- L(Train): 0.4093619 --- L(Val, RNN): 0.4172109 --- L(Val, SINDy): 0.4176701 --- Time: 0.31s; --- Convergence: 3.70e-05; LR: 1.00e-02; Metric: 0.4172109; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.508 1 + -0.0 value_reward_chosen[t] + 0.124 contr_diff + 0.776 reward + 0.777 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.439 1 + 0.76 value_reward_not_chosen[t] + 0.047 contr_diff^2 \n",
            "value_choice[t+1] = 0.99 value_choice[t] + -0.079 contr_diff + 0.113 value_choice^2 + 0.552 value_choice*contr_diff + -0.01 value_choice*choice + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 48\n",
            "value_choice: -, 56, 0, -, 0, 0, 56, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 373/1000 --- L(Train): 0.4162770 --- L(Val, RNN): 0.4171914 --- L(Val, SINDy): 0.4176739 --- Time: 0.39s; --- Convergence: 2.82e-05; LR: 1.00e-02; Metric: 0.4171914; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.509 1 + 0.0 value_reward_chosen[t] + 0.126 contr_diff + 0.776 reward + 0.777 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.439 1 + 0.761 value_reward_not_chosen[t] + 0.045 contr_diff^2 \n",
            "value_choice[t+1] = 0.991 value_choice[t] + -0.08 contr_diff + 0.114 value_choice^2 + 0.558 value_choice*contr_diff + -0.009 value_choice*choice + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 49\n",
            "value_choice: -, 57, 0, -, 0, 0, 57, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 374/1000 --- L(Train): 0.4312749 --- L(Val, RNN): 0.4171488 --- L(Val, SINDy): 0.4176579 --- Time: 0.33s; --- Convergence: 3.54e-05; LR: 1.00e-02; Metric: 0.4171488; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.51 1 + 0.0 value_reward_chosen[t] + 0.129 contr_diff + 0.776 reward + 0.777 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.44 1 + 0.762 value_reward_not_chosen[t] + 0.044 contr_diff^2 \n",
            "value_choice[t+1] = 0.991 value_choice[t] + -0.08 contr_diff + 0.114 value_choice^2 + 0.563 value_choice*contr_diff + -0.009 value_choice*choice + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 50\n",
            "value_choice: -, 58, 0, -, 0, 0, 58, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 375/1000 --- L(Train): 0.4262557 --- L(Val, RNN): 0.4171164 --- L(Val, SINDy): 0.4176079 --- Time: 0.36s; --- Convergence: 3.39e-05; LR: 1.00e-02; Metric: 0.4171164; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.509 1 + 0.002 value_reward_chosen[t] + 0.13 contr_diff + 0.778 reward + 0.779 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.441 1 + 0.763 value_reward_not_chosen[t] + 0.042 contr_diff^2 \n",
            "value_choice[t+1] = 0.992 value_choice[t] + -0.08 contr_diff + 0.114 value_choice^2 + 0.568 value_choice*contr_diff + -0.009 value_choice*choice + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 51\n",
            "value_choice: -, 59, 0, -, 0, 0, 59, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 376/1000 --- L(Train): 0.4169946 --- L(Val, RNN): 0.4171356 --- L(Val, SINDy): 0.4175258 --- Time: 0.31s; --- Convergence: 2.65e-05; LR: 1.00e-02; Metric: 0.4171164; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.508 1 + 0.004 value_reward_chosen[t] + 0.13 contr_diff + 0.781 reward + 0.782 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.442 1 + 0.763 value_reward_not_chosen[t] + 0.039 contr_diff^2 \n",
            "value_choice[t+1] = 0.992 value_choice[t] + -0.08 contr_diff + 0.114 value_choice^2 + 0.573 value_choice*contr_diff + -0.009 value_choice*choice + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 52\n",
            "value_choice: -, 60, 0, -, 0, 0, 60, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 377/1000 --- L(Train): 0.4117751 --- L(Val, RNN): 0.4170923 --- L(Val, SINDy): 0.4174513 --- Time: 0.33s; --- Convergence: 3.49e-05; LR: 1.00e-02; Metric: 0.4170923; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.507 1 + 0.005 value_reward_chosen[t] + 0.13 contr_diff + 0.782 reward + 0.783 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.443 1 + 0.763 value_reward_not_chosen[t] + 0.036 contr_diff^2 \n",
            "value_choice[t+1] = 0.992 value_choice[t] + -0.08 contr_diff + 0.114 value_choice^2 + 0.578 value_choice*contr_diff + -0.009 value_choice*choice + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 53\n",
            "value_choice: -, 61, 0, -, 0, 0, 61, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 378/1000 --- L(Train): 0.4169136 --- L(Val, RNN): 0.4170273 --- L(Val, SINDy): 0.4173653 --- Time: 0.36s; --- Convergence: 4.99e-05; LR: 1.00e-02; Metric: 0.4170273; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.506 1 + 0.004 value_reward_chosen[t] + 0.131 contr_diff + 0.783 reward + 0.784 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.444 1 + 0.764 value_reward_not_chosen[t] + 0.034 contr_diff^2 \n",
            "value_choice[t+1] = 0.992 value_choice[t] + -0.08 contr_diff + 0.114 value_choice^2 + 0.583 value_choice*contr_diff + -0.009 value_choice*choice + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 54\n",
            "value_choice: -, 62, 0, -, 0, 0, 62, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 379/1000 --- L(Train): 0.4181980 --- L(Val, RNN): 0.4169836 --- L(Val, SINDy): 0.4172787 --- Time: 0.27s; --- Convergence: 4.68e-05; LR: 1.00e-02; Metric: 0.4169836; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.506 1 + 0.004 value_reward_chosen[t] + 0.132 contr_diff + 0.783 reward + 0.784 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.445 1 + 0.764 value_reward_not_chosen[t] + 0.032 contr_diff^2 \n",
            "value_choice[t+1] = 0.992 value_choice[t] + -0.08 contr_diff + 0.114 value_choice^2 + 0.588 value_choice*contr_diff + -0.009 value_choice*choice + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 55\n",
            "value_choice: -, 63, 0, -, 0, 0, 63, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 380/1000 --- L(Train): 0.4224473 --- L(Val, RNN): 0.4169921 --- L(Val, SINDy): 0.4172026 --- Time: 0.30s; --- Convergence: 2.77e-05; LR: 1.00e-02; Metric: 0.4169836; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.508 1 + 0.002 value_reward_chosen[t] + 0.134 contr_diff + 0.782 reward + 0.783 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.445 1 + 0.764 value_reward_not_chosen[t] + 0.03 contr_diff^2 \n",
            "value_choice[t+1] = 0.992 value_choice[t] + -0.08 contr_diff + 0.114 value_choice^2 + 0.594 value_choice*contr_diff + -0.009 value_choice*choice + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 56\n",
            "value_choice: -, 64, 0, -, 0, 0, 64, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 381/1000 --- L(Train): 0.4225717 --- L(Val, RNN): 0.4169262 --- L(Val, SINDy): 0.4171578 --- Time: 0.36s; --- Convergence: 4.68e-05; LR: 1.00e-02; Metric: 0.4169262; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.51 1 + -0.001 value_reward_chosen[t] + 0.136 contr_diff + 0.78 reward + 0.781 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.446 1 + 0.765 value_reward_not_chosen[t] + 0.028 contr_diff^2 \n",
            "value_choice[t+1] = 0.991 value_choice[t] + -0.08 contr_diff + 0.114 value_choice^2 + 0.6 value_choice*contr_diff + -0.009 value_choice*choice + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 57\n",
            "value_choice: -, 65, 0, -, 0, 0, 65, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 382/1000 --- L(Train): 0.4209694 --- L(Val, RNN): 0.4168858 --- L(Val, SINDy): 0.4171205 --- Time: 0.36s; --- Convergence: 4.36e-05; LR: 1.00e-02; Metric: 0.4168858; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.513 1 + -0.003 value_reward_chosen[t] + 0.137 contr_diff + 0.777 reward + 0.778 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.445 1 + 0.766 value_reward_not_chosen[t] + 0.027 contr_diff^2 \n",
            "value_choice[t+1] = 0.991 value_choice[t] + -0.08 contr_diff + 0.114 value_choice^2 + 0.605 value_choice*contr_diff + -0.009 value_choice*choice + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 58\n",
            "value_choice: -, 66, 0, -, 0, 0, 66, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 383/1000 --- L(Train): 0.4211953 --- L(Val, RNN): 0.4169050 --- L(Val, SINDy): 0.4171088 --- Time: 0.32s; --- Convergence: 3.14e-05; LR: 1.00e-02; Metric: 0.4168858; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.515 1 + -0.005 value_reward_chosen[t] + 0.137 contr_diff + 0.775 reward + 0.776 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.445 1 + 0.768 value_reward_not_chosen[t] + 0.027 contr_diff^2 \n",
            "value_choice[t+1] = 0.991 value_choice[t] + -0.08 contr_diff + 0.114 value_choice^2 + 0.611 value_choice*contr_diff + -0.009 value_choice*choice + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 59\n",
            "value_choice: -, 67, 0, -, 0, 0, 67, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 384/1000 --- L(Train): 0.4140019 --- L(Val, RNN): 0.4169030 --- L(Val, SINDy): 0.4171100 --- Time: 0.29s; --- Convergence: 1.67e-05; LR: 1.00e-02; Metric: 0.4168858; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.515 1 + -0.005 value_reward_chosen[t] + 0.136 contr_diff + 0.776 reward + 0.777 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.444 1 + 0.769 value_reward_not_chosen[t] + 0.027 contr_diff^2 \n",
            "value_choice[t+1] = 0.991 value_choice[t] + -0.081 contr_diff + 0.114 value_choice^2 + 0.617 value_choice*contr_diff + -0.01 value_choice*choice + -0.079 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 60\n",
            "value_choice: -, 68, 0, -, 0, 0, 68, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 385/1000 --- L(Train): 0.4357966 --- L(Val, RNN): 0.4168216 --- L(Val, SINDy): 0.4171095 --- Time: 0.34s; --- Convergence: 4.90e-05; LR: 1.00e-02; Metric: 0.4168216; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.513 1 + -0.002 value_reward_chosen[t] + 0.132 contr_diff + 0.779 reward + 0.78 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.442 1 + 0.771 value_reward_not_chosen[t] + 0.028 contr_diff^2 \n",
            "value_choice[t+1] = 0.99 value_choice[t] + -0.081 contr_diff + 0.113 value_choice^2 + 0.622 value_choice*contr_diff + -0.01 value_choice*choice + -0.079 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 61\n",
            "value_choice: -, 69, 0, -, 0, 0, 69, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 386/1000 --- L(Train): 0.4221693 --- L(Val, RNN): 0.4167827 --- L(Val, SINDy): 0.4171411 --- Time: 0.31s; --- Convergence: 4.40e-05; LR: 1.00e-02; Metric: 0.4167827; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.512 1 + -0.001 value_reward_chosen[t] + 0.128 contr_diff + 0.78 reward + 0.781 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.441 1 + 0.773 value_reward_not_chosen[t] + 0.028 contr_diff^2 \n",
            "value_choice[t+1] = 0.99 value_choice[t] + -0.082 contr_diff + 0.113 value_choice^2 + 0.628 value_choice*contr_diff + -0.01 value_choice*choice + -0.08 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 62\n",
            "value_choice: -, 70, 0, -, 0, 0, 70, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 387/1000 --- L(Train): 0.4046040 --- L(Val, RNN): 0.4167809 --- L(Val, SINDy): 0.4171481 --- Time: 0.36s; --- Convergence: 2.29e-05; LR: 1.00e-02; Metric: 0.4167809; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.514 1 + -0.001 value_reward_chosen[t] + 0.124 contr_diff + 0.78 reward + 0.781 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.439 1 + 0.775 value_reward_not_chosen[t] + 0.029 contr_diff^2 \n",
            "value_choice[t+1] = 0.99 value_choice[t] + -0.083 contr_diff + 0.113 value_choice^2 + 0.633 value_choice*contr_diff + -0.011 value_choice*choice + -0.081 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 63\n",
            "value_choice: -, 71, 0, -, 0, 0, 71, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 388/1000 --- L(Train): 0.4207459 --- L(Val, RNN): 0.4167903 --- L(Val, SINDy): 0.4171355 --- Time: 0.32s; --- Convergence: 1.61e-05; LR: 1.00e-02; Metric: 0.4167809; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.516 1 + -0.002 value_reward_chosen[t] + 0.121 contr_diff + 0.778 reward + 0.779 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.437 1 + 0.777 value_reward_not_chosen[t] + 0.029 contr_diff^2 \n",
            "value_choice[t+1] = 0.989 value_choice[t] + -0.084 contr_diff + 0.113 value_choice^2 + 0.639 value_choice*contr_diff + -0.011 value_choice*choice + -0.082 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 64\n",
            "value_choice: -, 72, 0, -, 0, 0, 72, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 389/1000 --- L(Train): 0.4239346 --- L(Val, RNN): 0.4167422 --- L(Val, SINDy): 0.4171142 --- Time: 0.36s; --- Convergence: 3.21e-05; LR: 1.00e-02; Metric: 0.4167422; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.518 1 + -0.002 value_reward_chosen[t] + 0.119 contr_diff + 0.777 reward + 0.778 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.778 value_reward_not_chosen[t] + 0.03 contr_diff^2 \n",
            "value_choice[t+1] = 0.989 value_choice[t] + -0.084 contr_diff + 0.112 value_choice^2 + 0.646 value_choice*contr_diff + -0.011 value_choice*choice + -0.082 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 65\n",
            "value_choice: -, 73, 0, -, 0, 0, 73, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 390/1000 --- L(Train): 0.4226472 --- L(Val, RNN): 0.4166973 --- L(Val, SINDy): 0.4170894 --- Time: 0.35s; --- Convergence: 3.85e-05; LR: 1.00e-02; Metric: 0.4166973; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.518 1 + -0.001 value_reward_chosen[t] + 0.117 contr_diff + 0.777 reward + 0.778 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.433 1 + 0.779 value_reward_not_chosen[t] + 0.029 contr_diff^2 \n",
            "value_choice[t+1] = 0.989 value_choice[t] + -0.085 contr_diff + 0.112 value_choice^2 + 0.652 value_choice*contr_diff + -0.012 value_choice*choice + -0.083 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 66\n",
            "value_choice: -, 74, 0, -, 0, 0, 74, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 391/1000 --- L(Train): 0.4185115 --- L(Val, RNN): 0.4166664 --- L(Val, SINDy): 0.4170547 --- Time: 0.34s; --- Convergence: 3.47e-05; LR: 1.00e-02; Metric: 0.4166664; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.519 1 + -0.0 value_reward_chosen[t] + 0.117 contr_diff + 0.778 reward + 0.779 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.432 1 + 0.778 value_reward_not_chosen[t] + 0.029 contr_diff^2 \n",
            "value_choice[t+1] = 0.988 value_choice[t] + -0.086 contr_diff + 0.112 value_choice^2 + 0.658 value_choice*contr_diff + -0.012 value_choice*choice + -0.083 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 67\n",
            "value_choice: -, 75, 0, -, 0, 0, 75, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 392/1000 --- L(Train): 0.4099500 --- L(Val, RNN): 0.4166797 --- L(Val, SINDy): 0.4170266 --- Time: 0.35s; --- Convergence: 2.40e-05; LR: 1.00e-02; Metric: 0.4166664; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.519 1 + 0.001 value_reward_chosen[t] + 0.118 contr_diff + 0.778 reward + 0.78 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.43 1 + 0.777 value_reward_not_chosen[t] + 0.028 contr_diff^2 \n",
            "value_choice[t+1] = 0.988 value_choice[t] + -0.086 contr_diff + 0.112 value_choice^2 + 0.664 value_choice*contr_diff + -0.012 value_choice*choice + -0.084 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 68\n",
            "value_choice: -, 76, 0, -, 0, 0, 76, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 393/1000 --- L(Train): 0.4162765 --- L(Val, RNN): 0.4166396 --- L(Val, SINDy): 0.4169922 --- Time: 0.32s; --- Convergence: 3.21e-05; LR: 1.00e-02; Metric: 0.4166396; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.52 1 + 0.002 value_reward_chosen[t] + 0.119 contr_diff + 0.779 reward + 0.781 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.429 1 + 0.776 value_reward_not_chosen[t] + 0.027 contr_diff^2 \n",
            "value_choice[t+1] = 0.988 value_choice[t] + -0.086 contr_diff + 0.111 value_choice^2 + 0.67 value_choice*contr_diff + -0.013 value_choice*choice + -0.084 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 69\n",
            "value_choice: -, 77, 0, -, 0, 0, 77, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 394/1000 --- L(Train): 0.4212835 --- L(Val, RNN): 0.4165978 --- L(Val, SINDy): 0.4169535 --- Time: 0.29s; --- Convergence: 3.69e-05; LR: 1.00e-02; Metric: 0.4165978; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.52 1 + 0.003 value_reward_chosen[t] + 0.12 contr_diff + 0.78 reward + 0.781 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.429 1 + 0.775 value_reward_not_chosen[t] + 0.025 contr_diff^2 \n",
            "value_choice[t+1] = 0.988 value_choice[t] + -0.086 contr_diff + 0.111 value_choice^2 + 0.675 value_choice*contr_diff + -0.013 value_choice*choice + -0.084 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 70\n",
            "value_choice: -, 78, 0, -, 0, 0, 78, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 395/1000 --- L(Train): 0.4238383 --- L(Val, RNN): 0.4165649 --- L(Val, SINDy): 0.4169292 --- Time: 0.47s; --- Convergence: 3.49e-05; LR: 1.00e-02; Metric: 0.4165649; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.521 1 + 0.003 value_reward_chosen[t] + 0.121 contr_diff + 0.781 reward + 0.782 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.43 1 + 0.775 value_reward_not_chosen[t] + 0.023 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.085 contr_diff + 0.111 value_choice^2 + 0.68 value_choice*contr_diff + -0.013 value_choice*choice + -0.083 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 71\n",
            "value_choice: -, 79, 0, -, 0, 0, 79, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 396/1000 --- L(Train): 0.4176835 --- L(Val, RNN): 0.4165805 --- L(Val, SINDy): 0.4169196 --- Time: 0.33s; --- Convergence: 2.52e-05; LR: 1.00e-02; Metric: 0.4165649; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.521 1 + 0.003 value_reward_chosen[t] + 0.122 contr_diff + 0.781 reward + 0.783 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.432 1 + 0.775 value_reward_not_chosen[t] + 0.02 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.085 contr_diff + 0.111 value_choice^2 + 0.684 value_choice*contr_diff + -0.013 value_choice*choice + -0.083 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 72\n",
            "value_choice: -, 80, 0, -, 0, 0, 80, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 397/1000 --- L(Train): 0.4123278 --- L(Val, RNN): 0.4165419 --- L(Val, SINDy): 0.4169021 --- Time: 0.35s; --- Convergence: 3.19e-05; LR: 1.00e-02; Metric: 0.4165419; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.522 1 + 0.003 value_reward_chosen[t] + 0.122 contr_diff + 0.782 reward + 0.783 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.434 1 + 0.776 value_reward_not_chosen[t] + 0.015 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.085 contr_diff + 0.111 value_choice^2 + 0.689 value_choice*contr_diff + -0.013 value_choice*choice + -0.083 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 73\n",
            "value_choice: -, 81, 0, -, 0, 0, 81, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 398/1000 --- L(Train): 0.4128527 --- L(Val, RNN): 0.4164906 --- L(Val, SINDy): 0.4168908 --- Time: 0.27s; --- Convergence: 4.16e-05; LR: 1.00e-02; Metric: 0.4164906; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.522 1 + 0.003 value_reward_chosen[t] + 0.123 contr_diff + 0.783 reward + 0.784 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.437 1 + 0.777 value_reward_not_chosen[t] + 0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.086 contr_diff + 0.111 value_choice^2 + 0.694 value_choice*contr_diff + -0.013 value_choice*choice + -0.084 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 74\n",
            "value_choice: -, 82, 0, -, 0, 0, 82, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 399/1000 --- L(Train): 0.4120698 --- L(Val, RNN): 0.4164875 --- L(Val, SINDy): 0.4168838 --- Time: 0.34s; --- Convergence: 2.23e-05; LR: 1.00e-02; Metric: 0.4164875; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.522 1 + 0.003 value_reward_chosen[t] + 0.125 contr_diff + 0.784 reward + 0.785 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.44 1 + 0.778 value_reward_not_chosen[t] + 0.006 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.087 contr_diff + 0.111 value_choice^2 + 0.699 value_choice*contr_diff + -0.014 value_choice*choice + -0.085 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 75\n",
            "value_choice: -, 83, 0, -, 0, 0, 83, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 400/1000 --- L(Train): 0.4159024 --- L(Val, RNN): 0.4164906 --- L(Val, SINDy): 0.4168287 --- Time: 0.30s; --- Convergence: 1.27e-05; LR: 1.00e-02; Metric: 0.4164875; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.522 1 + 0.002 value_reward_chosen[t] + 0.127 contr_diff + 0.785 reward + 0.786 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.443 1 + 0.78 value_reward_not_chosen[t] + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.088 contr_diff + 0.111 value_choice^2 + 0.704 value_choice*contr_diff + -0.014 value_choice*choice + -0.086 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 76\n",
            "value_choice: -, 84, 0, -, 0, 0, 84, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 401/1000 --- L(Train): 0.4193845 --- L(Val, RNN): 0.4164399 --- L(Val, SINDy): 0.4167908 --- Time: 0.33s; --- Convergence: 3.17e-05; LR: 1.00e-02; Metric: 0.4164399; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.524 1 + 0.001 value_reward_chosen[t] + 0.13 contr_diff + 0.785 reward + 0.786 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.445 1 + 0.781 value_reward_not_chosen[t] + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.089 contr_diff + 0.111 value_choice^2 + 0.708 value_choice*contr_diff + -0.014 value_choice*choice + -0.087 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 77\n",
            "value_choice: -, 85, 0, -, 0, 0, 85, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 402/1000 --- L(Train): 0.4142926 --- L(Val, RNN): 0.4164072 --- L(Val, SINDy): 0.4167463 --- Time: 0.38s; --- Convergence: 3.22e-05; LR: 1.00e-02; Metric: 0.4164072; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.524 1 + -0.0 value_reward_chosen[t] + 0.133 contr_diff + 0.785 reward + 0.786 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.446 1 + 0.783 value_reward_not_chosen[t] + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.09 contr_diff + 0.111 value_choice^2 + 0.714 value_choice*contr_diff + -0.014 value_choice*choice + -0.088 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 78\n",
            "value_choice: -, 86, 0, -, 0, 0, 86, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 403/1000 --- L(Train): 0.4218021 --- L(Val, RNN): 0.4164129 --- L(Val, SINDy): 0.4166931 --- Time: 0.37s; --- Convergence: 1.89e-05; LR: 1.00e-02; Metric: 0.4164072; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.525 1 + -0.001 value_reward_chosen[t] + 0.135 contr_diff + 0.785 reward + 0.786 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.446 1 + 0.783 value_reward_not_chosen[t] + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.09 contr_diff + 0.111 value_choice^2 + 0.719 value_choice*contr_diff + -0.014 value_choice*choice + -0.088 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 79\n",
            "value_choice: -, 87, 0, -, 0, 0, 87, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 404/1000 --- L(Train): 0.4178872 --- L(Val, RNN): 0.4164457 --- L(Val, SINDy): 0.4166809 --- Time: 0.32s; --- Convergence: 2.59e-05; LR: 1.00e-02; Metric: 0.4164072; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.529 1 + -0.004 value_reward_chosen[t] + 0.135 contr_diff + 0.782 reward + 0.783 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.444 1 + 0.781 value_reward_not_chosen[t] + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.09 contr_diff + 0.111 value_choice^2 + 0.725 value_choice*contr_diff + -0.014 value_choice*choice + -0.088 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 80\n",
            "value_choice: -, 88, 0, -, 0, 0, 88, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 405/1000 --- L(Train): 0.4271765 --- L(Val, RNN): 0.4163634 --- L(Val, SINDy): 0.4166721 --- Time: 0.26s; --- Convergence: 5.41e-05; LR: 1.00e-02; Metric: 0.4163634; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.531 1 + -0.006 value_reward_chosen[t] + 0.131 contr_diff + 0.78 reward + 0.781 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.441 1 + 0.778 value_reward_not_chosen[t] + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.09 contr_diff + 0.111 value_choice^2 + 0.731 value_choice*contr_diff + -0.014 value_choice*choice + -0.088 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 81\n",
            "value_choice: -, 89, 0, -, 0, 0, 89, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 406/1000 --- L(Train): 0.4189448 --- L(Val, RNN): 0.4163290 --- L(Val, SINDy): 0.4166922 --- Time: 0.32s; --- Convergence: 4.42e-05; LR: 1.00e-02; Metric: 0.4163290; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.533 1 + -0.007 value_reward_chosen[t] + 0.126 contr_diff + 0.78 reward + 0.781 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.438 1 + 0.776 value_reward_not_chosen[t] + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.09 contr_diff + 0.111 value_choice^2 + 0.737 value_choice*contr_diff + -0.013 value_choice*choice + -0.088 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 82\n",
            "value_choice: -, 90, 0, -, 0, 0, 90, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 407/1000 --- L(Train): 0.4098020 --- L(Val, RNN): 0.4163707 --- L(Val, SINDy): 0.4167236 --- Time: 0.31s; --- Convergence: 4.30e-05; LR: 1.00e-02; Metric: 0.4163290; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.534 1 + -0.007 value_reward_chosen[t] + 0.121 contr_diff + 0.78 reward + 0.781 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.436 1 + 0.775 value_reward_not_chosen[t] + 0.008 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.091 contr_diff + 0.111 value_choice^2 + 0.743 value_choice*contr_diff + -0.013 value_choice*choice + -0.088 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 83\n",
            "value_choice: -, 91, 0, -, 0, 0, 91, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 408/1000 --- L(Train): 0.4134843 --- L(Val, RNN): 0.4163160 --- L(Val, SINDy): 0.4167217 --- Time: 0.29s; --- Convergence: 4.88e-05; LR: 1.00e-02; Metric: 0.4163160; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.534 1 + -0.005 value_reward_chosen[t] + 0.116 contr_diff + 0.781 reward + 0.783 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.776 value_reward_not_chosen[t] + 0.01 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.091 contr_diff + 0.111 value_choice^2 + 0.748 value_choice*contr_diff + -0.013 value_choice*choice + -0.089 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 84\n",
            "value_choice: -, 92, 0, -, 0, 0, 92, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 409/1000 --- L(Train): 0.4307839 --- L(Val, RNN): 0.4162656 --- L(Val, SINDy): 0.4167169 --- Time: 0.37s; --- Convergence: 4.96e-05; LR: 1.00e-02; Metric: 0.4162656; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.534 1 + -0.004 value_reward_chosen[t] + 0.113 contr_diff + 0.782 reward + 0.783 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.777 value_reward_not_chosen[t] + 0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.091 contr_diff + 0.111 value_choice^2 + 0.753 value_choice*contr_diff + -0.013 value_choice*choice + -0.089 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 85\n",
            "value_choice: -, 93, 0, -, 0, 0, 93, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 410/1000 --- L(Train): 0.4197632 --- L(Val, RNN): 0.4162800 --- L(Val, SINDy): 0.4166830 --- Time: 0.32s; --- Convergence: 3.20e-05; LR: 1.00e-02; Metric: 0.4162656; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.536 1 + -0.003 value_reward_chosen[t] + 0.112 contr_diff + 0.782 reward + 0.783 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.778 value_reward_not_chosen[t] + 0.012 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.091 contr_diff + 0.111 value_choice^2 + 0.758 value_choice*contr_diff + -0.013 value_choice*choice + -0.089 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 86\n",
            "value_choice: -, 94, 0, -, 0, 0, 94, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 411/1000 --- L(Train): 0.4189723 --- L(Val, RNN): 0.4162793 --- L(Val, SINDy): 0.4165780 --- Time: 0.37s; --- Convergence: 1.63e-05; LR: 1.00e-02; Metric: 0.4162656; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.538 1 + -0.001 value_reward_chosen[t] + 0.112 contr_diff + 0.783 reward + 0.784 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.779 value_reward_not_chosen[t] + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.092 contr_diff + 0.111 value_choice^2 + 0.763 value_choice*contr_diff + -0.013 value_choice*choice + -0.09 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 87\n",
            "value_choice: -, 95, 0, -, 0, 0, 95, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 412/1000 --- L(Train): 0.4245020 --- L(Val, RNN): 0.4162342 --- L(Val, SINDy): 0.4164989 --- Time: 0.31s; --- Convergence: 3.07e-05; LR: 1.00e-02; Metric: 0.4162342; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.538 1 + 0.001 value_reward_chosen[t] + 0.114 contr_diff + 0.784 reward + 0.785 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.779 value_reward_not_chosen[t] + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.987 value_choice[t] + -0.094 contr_diff + 0.111 value_choice^2 + 0.767 value_choice*contr_diff + -0.013 value_choice*choice + -0.091 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 88\n",
            "value_choice: -, 96, 0, -, 0, 0, 96, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 413/1000 --- L(Train): 0.4199885 --- L(Val, RNN): 0.4161921 --- L(Val, SINDy): 0.4164534 --- Time: 0.27s; --- Convergence: 3.64e-05; LR: 1.00e-02; Metric: 0.4161921; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.537 1 + 0.003 value_reward_chosen[t] + 0.116 contr_diff + 0.787 reward + 0.788 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.778 value_reward_not_chosen[t] + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.988 value_choice[t] + -0.095 contr_diff + 0.111 value_choice^2 + 0.772 value_choice*contr_diff + -0.013 value_choice*choice + -0.093 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 89\n",
            "value_choice: -, 97, 0, -, 0, 0, 97, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 414/1000 --- L(Train): 0.4234390 --- L(Val, RNN): 0.4161921 --- L(Val, SINDy): 0.4164103 --- Time: 0.28s; --- Convergence: 1.82e-05; LR: 1.00e-02; Metric: 0.4161921; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.537 1 + 0.005 value_reward_chosen[t] + 0.118 contr_diff + 0.789 reward + 0.79 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.778 value_reward_not_chosen[t] + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.988 value_choice[t] + -0.096 contr_diff + 0.111 value_choice^2 + 0.777 value_choice*contr_diff + -0.013 value_choice*choice + -0.094 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 90\n",
            "value_choice: -, 98, 0, -, 0, 0, 98, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 415/1000 --- L(Train): 0.4172297 --- L(Val, RNN): 0.4161732 --- L(Val, SINDy): 0.4163760 --- Time: 0.28s; --- Convergence: 1.86e-05; LR: 1.00e-02; Metric: 0.4161732; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.537 1 + 0.006 value_reward_chosen[t] + 0.12 contr_diff + 0.79 reward + 0.791 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.778 value_reward_not_chosen[t] + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.988 value_choice[t] + -0.098 contr_diff + 0.111 value_choice^2 + 0.781 value_choice*contr_diff + -0.013 value_choice*choice + -0.096 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 91\n",
            "value_choice: -, 99, 0, -, 0, 0, 99, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 416/1000 --- L(Train): 0.4251902 --- L(Val, RNN): 0.4161800 --- L(Val, SINDy): 0.4163420 --- Time: 0.31s; --- Convergence: 1.27e-05; LR: 1.00e-02; Metric: 0.4161732; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.538 1 + 0.007 value_reward_chosen[t] + 0.121 contr_diff + 0.791 reward + 0.792 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.778 value_reward_not_chosen[t] + 0.012 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.099 contr_diff + 0.111 value_choice^2 + 0.786 value_choice*contr_diff + -0.012 value_choice*choice + -0.097 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 92\n",
            "value_choice: -, -, 0, -, 0, 0, 100, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 417/1000 --- L(Train): 0.4064426 --- L(Val, RNN): 0.4161579 --- L(Val, SINDy): 0.4163384 --- Time: 0.29s; --- Convergence: 1.74e-05; LR: 1.00e-02; Metric: 0.4161579; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.538 1 + 0.007 value_reward_chosen[t] + 0.121 contr_diff + 0.792 reward + 0.793 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.778 value_reward_not_chosen[t] + 0.012 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.1 contr_diff + 0.111 value_choice^2 + 0.79 value_choice*contr_diff + -0.098 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 93\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 418/1000 --- L(Train): 0.4171163 --- L(Val, RNN): 0.4161319 --- L(Val, SINDy): 0.4163497 --- Time: 0.30s; --- Convergence: 2.17e-05; LR: 1.00e-02; Metric: 0.4161319; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.538 1 + 0.007 value_reward_chosen[t] + 0.122 contr_diff + 0.793 reward + 0.794 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.777 value_reward_not_chosen[t] + 0.012 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.1 contr_diff + 0.111 value_choice^2 + 0.795 value_choice*contr_diff + -0.098 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 94\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 419/1000 --- L(Train): 0.4235156 --- L(Val, RNN): 0.4161025 --- L(Val, SINDy): 0.4163475 --- Time: 0.33s; --- Convergence: 2.56e-05; LR: 1.00e-02; Metric: 0.4161025; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.539 1 + 0.005 value_reward_chosen[t] + 0.122 contr_diff + 0.793 reward + 0.794 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.777 value_reward_not_chosen[t] + 0.011 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.101 contr_diff + 0.111 value_choice^2 + 0.8 value_choice*contr_diff + -0.098 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 95\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 420/1000 --- L(Train): 0.4156384 --- L(Val, RNN): 0.4160784 --- L(Val, SINDy): 0.4163800 --- Time: 0.31s; --- Convergence: 2.48e-05; LR: 1.00e-02; Metric: 0.4160784; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.541 1 + 0.003 value_reward_chosen[t] + 0.124 contr_diff + 0.791 reward + 0.792 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.777 value_reward_not_chosen[t] + 0.01 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.101 contr_diff + 0.111 value_choice^2 + 0.805 value_choice*contr_diff + -0.098 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 96\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 421/1000 --- L(Train): 0.4135704 --- L(Val, RNN): 0.4160362 --- L(Val, SINDy): 0.4164066 --- Time: 0.28s; --- Convergence: 3.35e-05; LR: 1.00e-02; Metric: 0.4160362; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.543 1 + 0.0 value_reward_chosen[t] + 0.126 contr_diff + 0.79 reward + 0.791 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.436 1 + 0.777 value_reward_not_chosen[t] + 0.009 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.1 contr_diff + 0.11 value_choice^2 + 0.81 value_choice*contr_diff + -0.098 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 97\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 422/1000 --- L(Train): 0.4061646 --- L(Val, RNN): 0.4160171 --- L(Val, SINDy): 0.4164396 --- Time: 0.30s; --- Convergence: 2.63e-05; LR: 1.00e-02; Metric: 0.4160171; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.544 1 + -0.001 value_reward_chosen[t] + 0.128 contr_diff + 0.79 reward + 0.791 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.437 1 + 0.778 value_reward_not_chosen[t] + 0.007 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.099 contr_diff + 0.11 value_choice^2 + 0.815 value_choice*contr_diff + -0.097 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 98\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 423/1000 --- L(Train): 0.4164224 --- L(Val, RNN): 0.4160204 --- L(Val, SINDy): 0.4165630 --- Time: 0.35s; --- Convergence: 1.48e-05; LR: 1.00e-02; Metric: 0.4160171; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.545 1 + -0.002 value_reward_chosen[t] + 0.13 contr_diff + 0.79 reward + 0.791 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.438 1 + 0.78 value_reward_not_chosen[t] + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.099 contr_diff + 0.109 value_choice^2 + 0.819 value_choice*contr_diff + -0.097 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, 99\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 424/1000 --- L(Train): 0.4174499 --- L(Val, RNN): 0.4159918 --- L(Val, SINDy): 0.4165196 --- Time: 0.38s; --- Convergence: 2.17e-05; LR: 1.00e-02; Metric: 0.4159918; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.546 1 + -0.003 value_reward_chosen[t] + 0.129 contr_diff + 0.789 reward + 0.79 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.439 1 + 0.782 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.1 contr_diff + 0.108 value_choice^2 + 0.824 value_choice*contr_diff + -0.097 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 425/1000 --- L(Train): 0.4132874 --- L(Val, RNN): 0.4159612 --- L(Val, SINDy): 0.4163619 --- Time: 0.41s; --- Convergence: 2.62e-05; LR: 1.00e-02; Metric: 0.4159612; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.549 1 + -0.005 value_reward_chosen[t] + 0.127 contr_diff + 0.788 reward + 0.789 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.44 1 + 0.783 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.1 contr_diff + 0.108 value_choice^2 + 0.829 value_choice*contr_diff + -0.098 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 426/1000 --- L(Train): 0.4236156 --- L(Val, RNN): 0.4159833 --- L(Val, SINDy): 0.4161876 --- Time: 0.34s; --- Convergence: 2.42e-05; LR: 1.00e-02; Metric: 0.4159612; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.551 1 + -0.006 value_reward_chosen[t] + 0.125 contr_diff + 0.786 reward + 0.787 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.441 1 + 0.785 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.1 contr_diff + 0.107 value_choice^2 + 0.833 value_choice*contr_diff + -0.098 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 427/1000 --- L(Train): 0.4071182 --- L(Val, RNN): 0.4159851 --- L(Val, SINDy): 0.4160911 --- Time: 0.39s; --- Convergence: 1.30e-05; LR: 1.00e-02; Metric: 0.4159612; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.551 1 + -0.004 value_reward_chosen[t] + 0.119 contr_diff + 0.787 reward + 0.788 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.441 1 + 0.785 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.101 contr_diff + 0.106 value_choice^2 + 0.837 value_choice*contr_diff + -0.099 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 428/1000 --- L(Train): 0.4160601 --- L(Val, RNN): 0.4159258 --- L(Val, SINDy): 0.4160748 --- Time: 0.31s; --- Convergence: 3.61e-05; LR: 1.00e-02; Metric: 0.4159258; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.552 1 + -0.003 value_reward_chosen[t] + 0.112 contr_diff + 0.788 reward + 0.789 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.441 1 + 0.784 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.103 contr_diff + 0.105 value_choice^2 + 0.842 value_choice*contr_diff + -0.101 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 429/1000 --- L(Train): 0.4077264 --- L(Val, RNN): 0.4159266 --- L(Val, SINDy): 0.4160907 --- Time: 0.34s; --- Convergence: 1.85e-05; LR: 1.00e-02; Metric: 0.4159258; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.553 1 + -0.002 value_reward_chosen[t] + 0.106 contr_diff + 0.789 reward + 0.79 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.44 1 + 0.783 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.104 contr_diff + 0.104 value_choice^2 + 0.847 value_choice*contr_diff + -0.102 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 430/1000 --- L(Train): 0.4170427 --- L(Val, RNN): 0.4158974 --- L(Val, SINDy): 0.4161060 --- Time: 0.29s; --- Convergence: 2.38e-05; LR: 1.00e-02; Metric: 0.4158974; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.554 1 + -0.001 value_reward_chosen[t] + 0.103 contr_diff + 0.789 reward + 0.791 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.44 1 + 0.782 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.106 contr_diff + 0.103 value_choice^2 + 0.852 value_choice*contr_diff + -0.104 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 431/1000 --- L(Train): 0.4223001 --- L(Val, RNN): 0.4158864 --- L(Val, SINDy): 0.4161432 --- Time: 0.35s; --- Convergence: 1.74e-05; LR: 1.00e-02; Metric: 0.4158864; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.555 1 + -0.0 value_reward_chosen[t] + 0.101 contr_diff + 0.79 reward + 0.791 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.439 1 + 0.781 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.107 contr_diff + 0.102 value_choice^2 + 0.857 value_choice*contr_diff + -0.105 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 432/1000 --- L(Train): 0.4203567 --- L(Val, RNN): 0.4158956 --- L(Val, SINDy): 0.4161878 --- Time: 0.32s; --- Convergence: 1.33e-05; LR: 1.00e-02; Metric: 0.4158864; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.556 1 + 0.001 value_reward_chosen[t] + 0.102 contr_diff + 0.79 reward + 0.791 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.439 1 + 0.78 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.107 contr_diff + 0.101 value_choice^2 + 0.862 value_choice*contr_diff + -0.105 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 433/1000 --- L(Train): 0.4215192 --- L(Val, RNN): 0.4158713 --- L(Val, SINDy): 0.4161853 --- Time: 0.36s; --- Convergence: 1.88e-05; LR: 1.00e-02; Metric: 0.4158713; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.556 1 + 0.002 value_reward_chosen[t] + 0.103 contr_diff + 0.791 reward + 0.793 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.439 1 + 0.779 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.108 contr_diff + 0.1 value_choice^2 + 0.866 value_choice*contr_diff + -0.106 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 434/1000 --- L(Train): 0.4097625 --- L(Val, RNN): 0.4158220 --- L(Val, SINDy): 0.4161786 --- Time: 0.36s; --- Convergence: 3.40e-05; LR: 1.00e-02; Metric: 0.4158220; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.557 1 + 0.003 value_reward_chosen[t] + 0.105 contr_diff + 0.792 reward + 0.793 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.438 1 + 0.778 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.107 contr_diff + 0.099 value_choice^2 + 0.871 value_choice*contr_diff + -0.105 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 435/1000 --- L(Train): 0.4190050 --- L(Val, RNN): 0.4158217 --- L(Val, SINDy): 0.4161777 --- Time: 0.34s; --- Convergence: 1.72e-05; LR: 1.00e-02; Metric: 0.4158217; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.559 1 + 0.003 value_reward_chosen[t] + 0.107 contr_diff + 0.792 reward + 0.793 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.438 1 + 0.776 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.106 contr_diff + 0.098 value_choice^2 + 0.876 value_choice*contr_diff + -0.104 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 436/1000 --- L(Train): 0.4263421 --- L(Val, RNN): 0.4158303 --- L(Val, SINDy): 0.4161828 --- Time: 0.41s; --- Convergence: 1.29e-05; LR: 1.00e-02; Metric: 0.4158217; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.561 1 + 0.002 value_reward_chosen[t] + 0.109 contr_diff + 0.792 reward + 0.793 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.438 1 + 0.774 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.106 contr_diff + 0.097 value_choice^2 + 0.881 value_choice*contr_diff + -0.103 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 437/1000 --- L(Train): 0.4236857 --- L(Val, RNN): 0.4158282 --- L(Val, SINDy): 0.4162027 --- Time: 0.34s; --- Convergence: 7.49e-06; LR: 1.00e-02; Metric: 0.4158217; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.562 1 + 0.002 value_reward_chosen[t] + 0.11 contr_diff + 0.792 reward + 0.793 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.438 1 + 0.771 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.105 contr_diff + 0.096 value_choice^2 + 0.885 value_choice*contr_diff + -0.103 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 438/1000 --- L(Train): 0.4211859 --- L(Val, RNN): 0.4158723 --- L(Val, SINDy): 0.4162329 --- Time: 0.36s; --- Convergence: 2.58e-05; LR: 1.00e-02; Metric: 0.4158217; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.564 1 + 0.001 value_reward_chosen[t] + 0.112 contr_diff + 0.792 reward + 0.793 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.437 1 + 0.768 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.104 contr_diff + 0.095 value_choice^2 + 0.889 value_choice*contr_diff + -0.102 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 439/1000 --- L(Train): 0.4091716 --- L(Val, RNN): 0.4159062 --- L(Val, SINDy): 0.4162378 --- Time: 0.39s; --- Convergence: 2.98e-05; LR: 1.00e-02; Metric: 0.4158217; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.566 1 + 0.0 value_reward_chosen[t] + 0.114 contr_diff + 0.792 reward + 0.793 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.437 1 + 0.765 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.103 contr_diff + 0.094 value_choice^2 + 0.893 value_choice*contr_diff + -0.101 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 440/1000 --- L(Train): 0.4188121 --- L(Val, RNN): 0.4158786 --- L(Val, SINDy): 0.4162551 --- Time: 0.32s; --- Convergence: 2.87e-05; LR: 1.00e-02; Metric: 0.4158217; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.568 1 + -0.001 value_reward_chosen[t] + 0.118 contr_diff + 0.792 reward + 0.793 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.438 1 + 0.761 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.102 contr_diff + 0.093 value_choice^2 + 0.898 value_choice*contr_diff + -0.1 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 441/1000 --- L(Train): 0.4116660 --- L(Val, RNN): 0.4158296 --- L(Val, SINDy): 0.4162505 --- Time: 0.40s; --- Convergence: 3.89e-05; LR: 1.00e-02; Metric: 0.4158217; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.568 1 + -0.0 value_reward_chosen[t] + 0.122 contr_diff + 0.793 reward + 0.794 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.441 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.102 contr_diff + 0.092 value_choice^2 + 0.902 value_choice*contr_diff + -0.1 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 442/1000 --- L(Train): 0.4253722 --- L(Val, RNN): 0.4157858 --- L(Val, SINDy): 0.4162080 --- Time: 0.32s; --- Convergence: 4.13e-05; LR: 1.00e-02; Metric: 0.4157858; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.567 1 + 0.002 value_reward_chosen[t] + 0.125 contr_diff + 0.797 reward + 0.798 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.445 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.101 contr_diff + 0.091 value_choice^2 + 0.906 value_choice*contr_diff + -0.099 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 443/1000 --- L(Train): 0.4188792 --- L(Val, RNN): 0.4157749 --- L(Val, SINDy): 0.4161632 --- Time: 0.33s; --- Convergence: 2.61e-05; LR: 1.00e-02; Metric: 0.4157749; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.565 1 + 0.003 value_reward_chosen[t] + 0.127 contr_diff + 0.8 reward + 0.801 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.101 contr_diff + 0.09 value_choice^2 + 0.91 value_choice*contr_diff + -0.099 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 444/1000 --- L(Train): 0.4122811 --- L(Val, RNN): 0.4157859 --- L(Val, SINDy): 0.4161059 --- Time: 0.36s; --- Convergence: 1.85e-05; LR: 1.00e-02; Metric: 0.4157749; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.565 1 + 0.003 value_reward_chosen[t] + 0.128 contr_diff + 0.801 reward + 0.802 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.101 contr_diff + 0.089 value_choice^2 + 0.914 value_choice*contr_diff + -0.099 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 445/1000 --- L(Train): 0.4198447 --- L(Val, RNN): 0.4157354 --- L(Val, SINDy): 0.4160496 --- Time: 0.34s; --- Convergence: 3.45e-05; LR: 1.00e-02; Metric: 0.4157354; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.566 1 + 0.002 value_reward_chosen[t] + 0.129 contr_diff + 0.802 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.102 contr_diff + 0.088 value_choice^2 + 0.917 value_choice*contr_diff + -0.1 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 446/1000 --- L(Train): 0.4146156 --- L(Val, RNN): 0.4157003 --- L(Val, SINDy): 0.4160318 --- Time: 0.41s; --- Convergence: 3.48e-05; LR: 1.00e-02; Metric: 0.4157003; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.567 1 + 0.001 value_reward_chosen[t] + 0.128 contr_diff + 0.802 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.102 contr_diff + 0.087 value_choice^2 + 0.921 value_choice*contr_diff + -0.1 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 447/1000 --- L(Train): 0.4047419 --- L(Val, RNN): 0.4157013 --- L(Val, SINDy): 0.4160079 --- Time: 0.34s; --- Convergence: 1.79e-05; LR: 1.00e-02; Metric: 0.4157003; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.569 1 + 0.0 value_reward_chosen[t] + 0.126 contr_diff + 0.802 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.103 contr_diff + 0.086 value_choice^2 + 0.925 value_choice*contr_diff + -0.101 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 448/1000 --- L(Train): 0.4103247 --- L(Val, RNN): 0.4156792 --- L(Val, SINDy): 0.4160205 --- Time: 0.37s; --- Convergence: 2.00e-05; LR: 1.00e-02; Metric: 0.4156792; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.57 1 + -0.001 value_reward_chosen[t] + 0.123 contr_diff + 0.801 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.104 contr_diff + 0.085 value_choice^2 + 0.928 value_choice*contr_diff + -0.102 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 449/1000 --- L(Train): 0.4185929 --- L(Val, RNN): 0.4156862 --- L(Val, SINDy): 0.4160381 --- Time: 0.33s; --- Convergence: 1.35e-05; LR: 1.00e-02; Metric: 0.4156792; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.57 1 + -0.001 value_reward_chosen[t] + 0.12 contr_diff + 0.802 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.104 contr_diff + 0.084 value_choice^2 + 0.932 value_choice*contr_diff + -0.102 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 450/1000 --- L(Train): 0.4225832 --- L(Val, RNN): 0.4157068 --- L(Val, SINDy): 0.4160556 --- Time: 0.33s; --- Convergence: 1.71e-05; LR: 1.00e-02; Metric: 0.4156792; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.57 1 + -0.001 value_reward_chosen[t] + 0.118 contr_diff + 0.803 reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.103 contr_diff + 0.083 value_choice^2 + 0.937 value_choice*contr_diff + -0.101 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 451/1000 --- L(Train): 0.4222499 --- L(Val, RNN): 0.4157069 --- L(Val, SINDy): 0.4160679 --- Time: 0.39s; --- Convergence: 8.58e-06; LR: 1.00e-02; Metric: 0.4156792; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.571 1 + -0.001 value_reward_chosen[t] + 0.117 contr_diff + 0.803 reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.101 contr_diff + 0.082 value_choice^2 + 0.942 value_choice*contr_diff + -0.099 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 452/1000 --- L(Train): 0.4144768 --- L(Val, RNN): 0.4157110 --- L(Val, SINDy): 0.4160859 --- Time: 0.31s; --- Convergence: 6.35e-06; LR: 1.00e-02; Metric: 0.4156792; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.572 1 + -0.001 value_reward_chosen[t] + 0.115 contr_diff + 0.803 reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.099 contr_diff + 0.081 value_choice^2 + 0.947 value_choice*contr_diff + -0.096 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 453/1000 --- L(Train): 0.4069209 --- L(Val, RNN): 0.4157124 --- L(Val, SINDy): 0.4160399 --- Time: 0.39s; --- Convergence: 3.84e-06; LR: 1.00e-02; Metric: 0.4156792; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.573 1 + -0.001 value_reward_chosen[t] + 0.112 contr_diff + 0.803 reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.097 contr_diff + 0.081 value_choice^2 + 0.953 value_choice*contr_diff + -0.095 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 454/1000 --- L(Train): 0.4173023 --- L(Val, RNN): 0.4157397 --- L(Val, SINDy): 0.4160000 --- Time: 0.33s; --- Convergence: 1.56e-05; LR: 1.00e-02; Metric: 0.4156792; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.574 1 + -0.001 value_reward_chosen[t] + 0.107 contr_diff + 0.803 reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.095 contr_diff + 0.08 value_choice^2 + 0.958 value_choice*contr_diff + -0.093 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 455/1000 --- L(Train): 0.4271199 --- L(Val, RNN): 0.4156838 --- L(Val, SINDy): 0.4159565 --- Time: 0.34s; --- Convergence: 3.57e-05; LR: 1.00e-02; Metric: 0.4156792; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.575 1 + -0.001 value_reward_chosen[t] + 0.102 contr_diff + 0.803 reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.094 contr_diff + 0.079 value_choice^2 + 0.963 value_choice*contr_diff + -0.092 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 456/1000 --- L(Train): 0.4171775 --- L(Val, RNN): 0.4156266 --- L(Val, SINDy): 0.4160270 --- Time: 0.29s; --- Convergence: 4.64e-05; LR: 1.00e-02; Metric: 0.4156266; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.576 1 + -0.001 value_reward_chosen[t] + 0.098 contr_diff + 0.802 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.093 contr_diff + 0.078 value_choice^2 + 0.967 value_choice*contr_diff + -0.091 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 457/1000 --- L(Train): 0.4278600 --- L(Val, RNN): 0.4156260 --- L(Val, SINDy): 0.4160170 --- Time: 0.31s; --- Convergence: 2.35e-05; LR: 1.00e-02; Metric: 0.4156260; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.577 1 + -0.001 value_reward_chosen[t] + 0.095 contr_diff + 0.802 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.093 contr_diff + 0.077 value_choice^2 + 0.972 value_choice*contr_diff + -0.091 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 458/1000 --- L(Train): 0.4202844 --- L(Val, RNN): 0.4156696 --- L(Val, SINDy): 0.4159733 --- Time: 0.36s; --- Convergence: 3.36e-05; LR: 1.00e-02; Metric: 0.4156260; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.577 1 + 0.0 value_reward_chosen[t] + 0.092 contr_diff + 0.804 reward + 0.805 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.093 contr_diff + 0.077 value_choice^2 + 0.975 value_choice*contr_diff + -0.091 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 459/1000 --- L(Train): 0.4143507 --- L(Val, RNN): 0.4156649 --- L(Val, SINDy): 0.4159353 --- Time: 0.32s; --- Convergence: 1.91e-05; LR: 1.00e-02; Metric: 0.4156260; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.576 1 + 0.002 value_reward_chosen[t] + 0.091 contr_diff + 0.805 reward + 0.806 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.093 contr_diff + 0.076 value_choice^2 + 0.979 value_choice*contr_diff + -0.091 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 460/1000 --- L(Train): 0.4194306 --- L(Val, RNN): 0.4156655 --- L(Val, SINDy): 0.4158781 --- Time: 0.34s; --- Convergence: 9.84e-06; LR: 1.00e-02; Metric: 0.4156260; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.575 1 + 0.003 value_reward_chosen[t] + 0.092 contr_diff + 0.807 reward + 0.808 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.466 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.094 contr_diff + 0.075 value_choice^2 + 0.982 value_choice*contr_diff + -0.092 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 461/1000 --- L(Train): 0.4205708 --- L(Val, RNN): 0.4156174 --- L(Val, SINDy): 0.4158963 --- Time: 0.35s; --- Convergence: 2.90e-05; LR: 1.00e-02; Metric: 0.4156174; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.574 1 + 0.004 value_reward_chosen[t] + 0.093 contr_diff + 0.809 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.094 contr_diff + 0.075 value_choice^2 + 0.986 value_choice*contr_diff + -0.092 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 462/1000 --- L(Train): 0.4135384 --- L(Val, RNN): 0.4155730 --- L(Val, SINDy): 0.4159478 --- Time: 0.44s; --- Convergence: 3.67e-05; LR: 1.00e-02; Metric: 0.4155730; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.573 1 + 0.004 value_reward_chosen[t] + 0.096 contr_diff + 0.81 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.095 contr_diff + 0.074 value_choice^2 + 0.989 value_choice*contr_diff + -0.093 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 463/1000 --- L(Train): 0.4015142 --- L(Val, RNN): 0.4155851 --- L(Val, SINDy): 0.4159579 --- Time: 0.42s; --- Convergence: 2.44e-05; LR: 1.00e-02; Metric: 0.4155730; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.574 1 + 0.003 value_reward_chosen[t] + 0.1 contr_diff + 0.81 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.096 contr_diff + 0.073 value_choice^2 + 0.992 value_choice*contr_diff + -0.094 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 464/1000 --- L(Train): 0.4161151 --- L(Val, RNN): 0.4155834 --- L(Val, SINDy): 0.4160028 --- Time: 0.32s; --- Convergence: 1.30e-05; LR: 1.00e-02; Metric: 0.4155730; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.575 1 + 0.002 value_reward_chosen[t] + 0.103 contr_diff + 0.809 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.097 contr_diff + 0.073 value_choice^2 + 0.996 value_choice*contr_diff + -0.095 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 465/1000 --- L(Train): 0.4211441 --- L(Val, RNN): 0.4155783 --- L(Val, SINDy): 0.4160396 --- Time: 0.36s; --- Convergence: 9.07e-06; LR: 1.00e-02; Metric: 0.4155730; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.576 1 + 0.001 value_reward_chosen[t] + 0.106 contr_diff + 0.808 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.098 contr_diff + 0.072 value_choice^2 + 0.999 value_choice*contr_diff + -0.096 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 466/1000 --- L(Train): 0.4140252 --- L(Val, RNN): 0.4155345 --- L(Val, SINDy): 0.4160373 --- Time: 0.38s; --- Convergence: 2.64e-05; LR: 1.00e-02; Metric: 0.4155345; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.576 1 + 0.0 value_reward_chosen[t] + 0.109 contr_diff + 0.808 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.098 contr_diff + 0.072 value_choice^2 + 1.003 value_choice*contr_diff + -0.096 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 467/1000 --- L(Train): 0.4097846 --- L(Val, RNN): 0.4155364 --- L(Val, SINDy): 0.4159997 --- Time: 0.29s; --- Convergence: 1.42e-05; LR: 1.00e-02; Metric: 0.4155345; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.577 1 + -0.0 value_reward_chosen[t] + 0.11 contr_diff + 0.808 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.098 contr_diff + 0.071 value_choice^2 + 1.006 value_choice*contr_diff + -0.096 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 468/1000 --- L(Train): 0.4106183 --- L(Val, RNN): 0.4155631 --- L(Val, SINDy): 0.4159220 --- Time: 0.31s; --- Convergence: 2.04e-05; LR: 1.00e-02; Metric: 0.4155345; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.578 1 + -0.001 value_reward_chosen[t] + 0.112 contr_diff + 0.807 reward + 0.808 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.097 contr_diff + 0.071 value_choice^2 + 1.011 value_choice*contr_diff + -0.095 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 469/1000 --- L(Train): 0.4163631 --- L(Val, RNN): 0.4155602 --- L(Val, SINDy): 0.4158680 --- Time: 0.37s; --- Convergence: 1.17e-05; LR: 1.00e-02; Metric: 0.4155345; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.579 1 + -0.002 value_reward_chosen[t] + 0.113 contr_diff + 0.806 reward + 0.807 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.097 contr_diff + 0.07 value_choice^2 + 1.015 value_choice*contr_diff + -0.095 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 470/1000 --- L(Train): 0.4127508 --- L(Val, RNN): 0.4154884 --- L(Val, SINDy): 0.4158691 --- Time: 0.33s; --- Convergence: 4.17e-05; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.581 1 + -0.004 value_reward_chosen[t] + 0.113 contr_diff + 0.804 reward + 0.805 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.096 contr_diff + 0.07 value_choice^2 + 1.018 value_choice*contr_diff + -0.094 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 471/1000 --- L(Train): 0.4273787 --- L(Val, RNN): 0.4156161 --- L(Val, SINDy): 0.4158733 --- Time: 0.32s; --- Convergence: 8.47e-05; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.582 1 + -0.004 value_reward_chosen[t] + 0.113 contr_diff + 0.804 reward + 0.805 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.095 contr_diff + 0.069 value_choice^2 + 1.022 value_choice*contr_diff + -0.093 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 472/1000 --- L(Train): 0.4226770 --- L(Val, RNN): 0.4154888 --- L(Val, SINDy): 0.4158544 --- Time: 0.37s; --- Convergence: 1.06e-04; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.581 1 + -0.004 value_reward_chosen[t] + 0.111 contr_diff + 0.804 reward + 0.806 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.094 contr_diff + 0.069 value_choice^2 + 1.026 value_choice*contr_diff + -0.092 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 473/1000 --- L(Train): 0.4238203 --- L(Val, RNN): 0.4155875 --- L(Val, SINDy): 0.4159235 --- Time: 0.38s; --- Convergence: 1.02e-04; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.581 1 + -0.003 value_reward_chosen[t] + 0.109 contr_diff + 0.805 reward + 0.807 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.092 contr_diff + 0.068 value_choice^2 + 1.03 value_choice*contr_diff + -0.09 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 474/1000 --- L(Train): 0.4121023 --- L(Val, RNN): 0.4156006 --- L(Val, SINDy): 0.4159270 --- Time: 0.37s; --- Convergence: 5.77e-05; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.581 1 + -0.002 value_reward_chosen[t] + 0.106 contr_diff + 0.806 reward + 0.807 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.091 contr_diff + 0.068 value_choice^2 + 1.033 value_choice*contr_diff + -0.089 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 475/1000 --- L(Train): 0.4085592 --- L(Val, RNN): 0.4155173 --- L(Val, SINDy): 0.4158926 --- Time: 0.41s; --- Convergence: 7.05e-05; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.582 1 + -0.001 value_reward_chosen[t] + 0.102 contr_diff + 0.806 reward + 0.807 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.09 contr_diff + 0.068 value_choice^2 + 1.037 value_choice*contr_diff + -0.088 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 476/1000 --- L(Train): 0.4199554 --- L(Val, RNN): 0.4155180 --- L(Val, SINDy): 0.4158777 --- Time: 0.31s; --- Convergence: 3.56e-05; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.582 1 + 0.001 value_reward_chosen[t] + 0.099 contr_diff + 0.807 reward + 0.808 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.76 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.088 contr_diff + 0.067 value_choice^2 + 1.04 value_choice*contr_diff + -0.086 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 477/1000 --- L(Train): 0.4189999 --- L(Val, RNN): 0.4156048 --- L(Val, SINDy): 0.4158811 --- Time: 0.36s; --- Convergence: 6.12e-05; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.581 1 + 0.002 value_reward_chosen[t] + 0.096 contr_diff + 0.807 reward + 0.808 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.762 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.085 contr_diff + 0.067 value_choice^2 + 1.044 value_choice*contr_diff + -0.083 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 478/1000 --- L(Train): 0.4103390 --- L(Val, RNN): 0.4155951 --- L(Val, SINDy): 0.4158728 --- Time: 0.28s; --- Convergence: 3.54e-05; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.581 1 + 0.003 value_reward_chosen[t] + 0.093 contr_diff + 0.808 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.083 contr_diff + 0.066 value_choice^2 + 1.047 value_choice*contr_diff + -0.081 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 479/1000 --- L(Train): 0.4234656 --- L(Val, RNN): 0.4154952 --- L(Val, SINDy): 0.4158486 --- Time: 0.38s; --- Convergence: 6.77e-05; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.582 1 + 0.003 value_reward_chosen[t] + 0.093 contr_diff + 0.808 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.764 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.082 contr_diff + 0.066 value_choice^2 + 1.05 value_choice*contr_diff + -0.08 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 480/1000 --- L(Train): 0.4215419 --- L(Val, RNN): 0.4155222 --- L(Val, SINDy): 0.4158628 --- Time: 0.31s; --- Convergence: 4.73e-05; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.583 1 + 0.003 value_reward_chosen[t] + 0.093 contr_diff + 0.808 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.466 1 + 0.764 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.081 contr_diff + 0.066 value_choice^2 + 1.053 value_choice*contr_diff + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 481/1000 --- L(Train): 0.4094273 --- L(Val, RNN): 0.4155475 --- L(Val, SINDy): 0.4158396 --- Time: 0.38s; --- Convergence: 3.64e-05; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.582 1 + 0.003 value_reward_chosen[t] + 0.095 contr_diff + 0.809 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.764 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.079 contr_diff + 0.066 value_choice^2 + 1.056 value_choice*contr_diff + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 482/1000 --- L(Train): 0.4172946 --- L(Val, RNN): 0.4155326 --- L(Val, SINDy): 0.4158013 --- Time: 0.46s; --- Convergence: 2.57e-05; LR: 1.00e-02; Metric: 0.4154884; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.582 1 + 0.003 value_reward_chosen[t] + 0.096 contr_diff + 0.81 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.765 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.078 contr_diff + 0.065 value_choice^2 + 1.059 value_choice*contr_diff + -0.076 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 483/1000 --- L(Train): 0.4128078 --- L(Val, RNN): 0.4154799 --- L(Val, SINDy): 0.4157955 --- Time: 0.32s; --- Convergence: 3.92e-05; LR: 1.00e-02; Metric: 0.4154799; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.582 1 + 0.002 value_reward_chosen[t] + 0.098 contr_diff + 0.809 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.767 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.065 value_choice^2 + 1.062 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 484/1000 --- L(Train): 0.4179122 --- L(Val, RNN): 0.4154971 --- L(Val, SINDy): 0.4157853 --- Time: 0.30s; --- Convergence: 2.82e-05; LR: 1.00e-02; Metric: 0.4154799; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.582 1 + 0.001 value_reward_chosen[t] + 0.1 contr_diff + 0.81 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.769 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.076 contr_diff + 0.065 value_choice^2 + 1.064 value_choice*contr_diff + -0.073 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 485/1000 --- L(Train): 0.4175784 --- L(Val, RNN): 0.4154868 --- L(Val, SINDy): 0.4157754 --- Time: 0.34s; --- Convergence: 1.92e-05; LR: 1.00e-02; Metric: 0.4154799; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.581 1 + 0.001 value_reward_chosen[t] + 0.102 contr_diff + 0.811 reward + 0.812 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.77 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.074 contr_diff + 0.065 value_choice^2 + 1.067 value_choice*contr_diff + -0.072 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 486/1000 --- L(Train): 0.4146818 --- L(Val, RNN): 0.4154758 --- L(Val, SINDy): 0.4157844 --- Time: 0.31s; --- Convergence: 1.51e-05; LR: 1.00e-02; Metric: 0.4154758; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.581 1 + 0.0 value_reward_chosen[t] + 0.103 contr_diff + 0.812 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.456 1 + 0.771 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.073 contr_diff + 0.064 value_choice^2 + 1.069 value_choice*contr_diff + -0.071 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 487/1000 --- L(Train): 0.4142425 --- L(Val, RNN): 0.4154272 --- L(Val, SINDy): 0.4157723 --- Time: 0.29s; --- Convergence: 3.19e-05; LR: 1.00e-02; Metric: 0.4154272; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.581 1 + -0.001 value_reward_chosen[t] + 0.105 contr_diff + 0.812 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.454 1 + 0.773 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.071 contr_diff + 0.064 value_choice^2 + 1.071 value_choice*contr_diff + -0.069 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 488/1000 --- L(Train): 0.4258496 --- L(Val, RNN): 0.4154027 --- L(Val, SINDy): 0.4157641 --- Time: 0.32s; --- Convergence: 2.82e-05; LR: 1.00e-02; Metric: 0.4154027; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.583 1 + -0.002 value_reward_chosen[t] + 0.106 contr_diff + 0.81 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.452 1 + 0.774 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.069 contr_diff + 0.064 value_choice^2 + 1.073 value_choice*contr_diff + -0.067 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 489/1000 --- L(Train): 0.4185398 --- L(Val, RNN): 0.4153964 --- L(Val, SINDy): 0.4157692 --- Time: 0.31s; --- Convergence: 1.72e-05; LR: 1.00e-02; Metric: 0.4153964; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.003 value_reward_chosen[t] + 0.106 contr_diff + 0.809 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.451 1 + 0.775 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.068 contr_diff + 0.064 value_choice^2 + 1.075 value_choice*contr_diff + -0.066 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 490/1000 --- L(Train): 0.4219131 --- L(Val, RNN): 0.4154545 --- L(Val, SINDy): 0.4157862 --- Time: 0.40s; --- Convergence: 3.77e-05; LR: 1.00e-02; Metric: 0.4153964; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.585 1 + -0.004 value_reward_chosen[t] + 0.106 contr_diff + 0.809 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.451 1 + 0.776 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.067 contr_diff + 0.064 value_choice^2 + 1.077 value_choice*contr_diff + -0.065 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 491/1000 --- L(Train): 0.4259447 --- L(Val, RNN): 0.4154069 --- L(Val, SINDy): 0.4157802 --- Time: 0.31s; --- Convergence: 4.26e-05; LR: 1.00e-02; Metric: 0.4153964; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.004 value_reward_chosen[t] + 0.106 contr_diff + 0.809 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.776 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.067 contr_diff + 0.063 value_choice^2 + 1.079 value_choice*contr_diff + -0.065 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 492/1000 --- L(Train): 0.4109170 --- L(Val, RNN): 0.4153434 --- L(Val, SINDy): 0.4157846 --- Time: 0.34s; --- Convergence: 5.31e-05; LR: 1.00e-02; Metric: 0.4153434; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.003 value_reward_chosen[t] + 0.106 contr_diff + 0.81 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.451 1 + 0.775 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.067 contr_diff + 0.063 value_choice^2 + 1.081 value_choice*contr_diff + -0.065 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 493/1000 --- L(Train): 0.4230338 --- L(Val, RNN): 0.4153285 --- L(Val, SINDy): 0.4157947 --- Time: 0.33s; --- Convergence: 3.40e-05; LR: 1.00e-02; Metric: 0.4153285; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.003 value_reward_chosen[t] + 0.107 contr_diff + 0.81 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.451 1 + 0.775 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.067 contr_diff + 0.063 value_choice^2 + 1.082 value_choice*contr_diff + -0.065 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 494/1000 --- L(Train): 0.4178880 --- L(Val, RNN): 0.4153623 --- L(Val, SINDy): 0.4157828 --- Time: 0.31s; --- Convergence: 3.39e-05; LR: 1.00e-02; Metric: 0.4153285; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.585 1 + -0.002 value_reward_chosen[t] + 0.107 contr_diff + 0.809 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.452 1 + 0.774 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.068 contr_diff + 0.063 value_choice^2 + 1.084 value_choice*contr_diff + -0.066 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 495/1000 --- L(Train): 0.4176535 --- L(Val, RNN): 0.4152637 --- L(Val, SINDy): 0.4157476 --- Time: 0.32s; --- Convergence: 6.62e-05; LR: 1.00e-02; Metric: 0.4152637; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.589 1 + -0.003 value_reward_chosen[t] + 0.107 contr_diff + 0.806 reward + 0.807 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.775 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.069 contr_diff + 0.063 value_choice^2 + 1.085 value_choice*contr_diff + -0.067 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 496/1000 --- L(Train): 0.4234084 --- L(Val, RNN): 0.4151975 --- L(Val, SINDy): 0.4157041 --- Time: 0.32s; --- Convergence: 6.62e-05; LR: 1.00e-02; Metric: 0.4151975; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.59 1 + -0.002 value_reward_chosen[t] + 0.105 contr_diff + 0.805 reward + 0.806 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.457 1 + 0.774 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.07 contr_diff + 0.063 value_choice^2 + 1.086 value_choice*contr_diff + -0.068 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 497/1000 --- L(Train): 0.4193113 --- L(Val, RNN): 0.4151945 --- L(Val, SINDy): 0.4156770 --- Time: 0.36s; --- Convergence: 3.46e-05; LR: 1.00e-02; Metric: 0.4151945; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.59 1 + -0.0 value_reward_chosen[t] + 0.101 contr_diff + 0.806 reward + 0.807 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.772 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.071 contr_diff + 0.062 value_choice^2 + 1.088 value_choice*contr_diff + -0.069 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 498/1000 --- L(Train): 0.4170956 --- L(Val, RNN): 0.4151690 --- L(Val, SINDy): 0.4156871 --- Time: 0.31s; --- Convergence: 3.00e-05; LR: 1.00e-02; Metric: 0.4151690; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.002 value_reward_chosen[t] + 0.096 contr_diff + 0.807 reward + 0.808 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.768 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.073 contr_diff + 0.062 value_choice^2 + 1.09 value_choice*contr_diff + -0.07 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 499/1000 --- L(Train): 0.4190007 --- L(Val, RNN): 0.4152014 --- L(Val, SINDy): 0.4157216 --- Time: 0.32s; --- Convergence: 3.12e-05; LR: 1.00e-02; Metric: 0.4151690; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.003 value_reward_chosen[t] + 0.093 contr_diff + 0.808 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.764 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.074 contr_diff + 0.062 value_choice^2 + 1.091 value_choice*contr_diff + -0.071 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 500/1000 --- L(Train): 0.4116724 --- L(Val, RNN): 0.4152403 --- L(Val, SINDy): 0.4157564 --- Time: 0.33s; --- Convergence: 3.50e-05; LR: 1.00e-02; Metric: 0.4151690; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.004 value_reward_chosen[t] + 0.09 contr_diff + 0.808 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.761 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.074 contr_diff + 0.062 value_choice^2 + 1.093 value_choice*contr_diff + -0.072 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 501/1000 --- L(Train): 0.4167406 --- L(Val, RNN): 0.4152002 --- L(Val, SINDy): 0.4157730 --- Time: 0.29s; --- Convergence: 3.76e-05; LR: 1.00e-02; Metric: 0.4151690; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.004 value_reward_chosen[t] + 0.09 contr_diff + 0.81 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.76 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.075 contr_diff + 0.062 value_choice^2 + 1.095 value_choice*contr_diff + -0.073 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 502/1000 --- L(Train): 0.4153560 --- L(Val, RNN): 0.4151867 --- L(Val, SINDy): 0.4157803 --- Time: 0.38s; --- Convergence: 2.56e-05; LR: 1.00e-02; Metric: 0.4151690; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.005 value_reward_chosen[t] + 0.092 contr_diff + 0.812 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.074 contr_diff + 0.062 value_choice^2 + 1.097 value_choice*contr_diff + -0.072 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 503/1000 --- L(Train): 0.4290207 --- L(Val, RNN): 0.4151861 --- L(Val, SINDy): 0.4157601 --- Time: 0.31s; --- Convergence: 1.31e-05; LR: 1.00e-02; Metric: 0.4151690; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.586 1 + 0.005 value_reward_chosen[t] + 0.096 contr_diff + 0.814 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.074 contr_diff + 0.062 value_choice^2 + 1.099 value_choice*contr_diff + -0.072 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 504/1000 --- L(Train): 0.4323129 --- L(Val, RNN): 0.4151414 --- L(Val, SINDy): 0.4157247 --- Time: 0.34s; --- Convergence: 2.89e-05; LR: 1.00e-02; Metric: 0.4151414; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.586 1 + 0.004 value_reward_chosen[t] + 0.101 contr_diff + 0.814 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.073 contr_diff + 0.062 value_choice^2 + 1.101 value_choice*contr_diff + -0.071 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 505/1000 --- L(Train): 0.4205134 --- L(Val, RNN): 0.4151153 --- L(Val, SINDy): 0.4157046 --- Time: 0.31s; --- Convergence: 2.75e-05; LR: 1.00e-02; Metric: 0.4151153; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.003 value_reward_chosen[t] + 0.105 contr_diff + 0.813 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.753 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.071 contr_diff + 0.061 value_choice^2 + 1.104 value_choice*contr_diff + -0.069 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 506/1000 --- L(Train): 0.4044883 --- L(Val, RNN): 0.4151421 --- L(Val, SINDy): 0.4156791 --- Time: 0.36s; --- Convergence: 2.72e-05; LR: 1.00e-02; Metric: 0.4151153; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.002 value_reward_chosen[t] + 0.108 contr_diff + 0.813 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.482 1 + 0.749 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.07 contr_diff + 0.061 value_choice^2 + 1.106 value_choice*contr_diff + -0.068 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 507/1000 --- L(Train): 0.4104262 --- L(Val, RNN): 0.4151452 --- L(Val, SINDy): 0.4156195 --- Time: 0.31s; --- Convergence: 1.51e-05; LR: 1.00e-02; Metric: 0.4151153; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.001 value_reward_chosen[t] + 0.109 contr_diff + 0.814 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.069 contr_diff + 0.061 value_choice^2 + 1.108 value_choice*contr_diff + -0.067 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 508/1000 --- L(Train): 0.4190570 --- L(Val, RNN): 0.4151235 --- L(Val, SINDy): 0.4155703 --- Time: 0.33s; --- Convergence: 1.84e-05; LR: 1.00e-02; Metric: 0.4151153; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.0 value_reward_chosen[t] + 0.11 contr_diff + 0.814 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.068 contr_diff + 0.061 value_choice^2 + 1.11 value_choice*contr_diff + -0.066 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 509/1000 --- L(Train): 0.4288342 --- L(Val, RNN): 0.4151076 --- L(Val, SINDy): 0.4155540 --- Time: 0.32s; --- Convergence: 1.72e-05; LR: 1.00e-02; Metric: 0.4151076; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.588 1 + -0.001 value_reward_chosen[t] + 0.109 contr_diff + 0.813 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.484 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.068 contr_diff + 0.061 value_choice^2 + 1.112 value_choice*contr_diff + -0.066 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 510/1000 --- L(Train): 0.3982188 --- L(Val, RNN): 0.4150729 --- L(Val, SINDy): 0.4155520 --- Time: 0.30s; --- Convergence: 2.59e-05; LR: 1.00e-02; Metric: 0.4150729; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.589 1 + -0.002 value_reward_chosen[t] + 0.108 contr_diff + 0.812 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.484 1 + 0.74 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.068 contr_diff + 0.061 value_choice^2 + 1.113 value_choice*contr_diff + -0.065 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 511/1000 --- L(Train): 0.4099503 --- L(Val, RNN): 0.4150788 --- L(Val, SINDy): 0.4155527 --- Time: 0.48s; --- Convergence: 1.59e-05; LR: 1.00e-02; Metric: 0.4150729; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.59 1 + -0.003 value_reward_chosen[t] + 0.107 contr_diff + 0.811 reward + 0.812 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.484 1 + 0.74 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.068 contr_diff + 0.061 value_choice^2 + 1.115 value_choice*contr_diff + -0.066 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 512/1000 --- L(Train): 0.4208883 --- L(Val, RNN): 0.4150700 --- L(Val, SINDy): 0.4155668 --- Time: 0.38s; --- Convergence: 1.24e-05; LR: 1.00e-02; Metric: 0.4150700; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.591 1 + -0.004 value_reward_chosen[t] + 0.105 contr_diff + 0.81 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.741 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.068 contr_diff + 0.061 value_choice^2 + 1.116 value_choice*contr_diff + -0.066 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 513/1000 --- L(Train): 0.4177529 --- L(Val, RNN): 0.4150427 --- L(Val, SINDy): 0.4155437 --- Time: 0.27s; --- Convergence: 1.98e-05; LR: 1.00e-02; Metric: 0.4150427; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.592 1 + -0.004 value_reward_chosen[t] + 0.104 contr_diff + 0.81 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.486 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.069 contr_diff + 0.061 value_choice^2 + 1.117 value_choice*contr_diff + -0.067 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 514/1000 --- L(Train): 0.4183259 --- L(Val, RNN): 0.4150635 --- L(Val, SINDy): 0.4155110 --- Time: 0.30s; --- Convergence: 2.03e-05; LR: 1.00e-02; Metric: 0.4150427; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.591 1 + -0.003 value_reward_chosen[t] + 0.104 contr_diff + 0.81 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.487 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.07 contr_diff + 0.061 value_choice^2 + 1.119 value_choice*contr_diff + -0.068 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 515/1000 --- L(Train): 0.4200248 --- L(Val, RNN): 0.4150190 --- L(Val, SINDy): 0.4155119 --- Time: 0.44s; --- Convergence: 3.24e-05; LR: 1.00e-02; Metric: 0.4150190; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.591 1 + -0.002 value_reward_chosen[t] + 0.103 contr_diff + 0.811 reward + 0.812 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.487 1 + 0.75 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.071 contr_diff + 0.061 value_choice^2 + 1.12 value_choice*contr_diff + -0.069 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 516/1000 --- L(Train): 0.4259717 --- L(Val, RNN): 0.4149952 --- L(Val, SINDy): 0.4155101 --- Time: 0.31s; --- Convergence: 2.81e-05; LR: 1.00e-02; Metric: 0.4149952; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.59 1 + -0.001 value_reward_chosen[t] + 0.101 contr_diff + 0.812 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.751 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.072 contr_diff + 0.06 value_choice^2 + 1.121 value_choice*contr_diff + -0.07 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 517/1000 --- L(Train): 0.4193122 --- L(Val, RNN): 0.4150028 --- L(Val, SINDy): 0.4155211 --- Time: 0.29s; --- Convergence: 1.79e-05; LR: 1.00e-02; Metric: 0.4149952; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.59 1 + -0.0 value_reward_chosen[t] + 0.1 contr_diff + 0.812 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.751 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.073 contr_diff + 0.06 value_choice^2 + 1.122 value_choice*contr_diff + -0.071 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 518/1000 --- L(Train): 0.4159273 --- L(Val, RNN): 0.4150784 --- L(Val, SINDy): 0.4155391 --- Time: 0.28s; --- Convergence: 4.67e-05; LR: 1.00e-02; Metric: 0.4149952; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.592 1 + -0.0 value_reward_chosen[t] + 0.099 contr_diff + 0.811 reward + 0.812 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.75 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.073 contr_diff + 0.06 value_choice^2 + 1.123 value_choice*contr_diff + -0.071 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 519/1000 --- L(Train): 0.4108015 --- L(Val, RNN): 0.4150673 --- L(Val, SINDy): 0.4155220 --- Time: 0.29s; --- Convergence: 2.89e-05; LR: 1.00e-02; Metric: 0.4149952; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.592 1 + 0.001 value_reward_chosen[t] + 0.097 contr_diff + 0.811 reward + 0.812 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.749 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.074 contr_diff + 0.06 value_choice^2 + 1.124 value_choice*contr_diff + -0.072 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 520/1000 --- L(Train): 0.4158657 --- L(Val, RNN): 0.4149790 --- L(Val, SINDy): 0.4155401 --- Time: 0.28s; --- Convergence: 5.86e-05; LR: 1.00e-02; Metric: 0.4149790; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.592 1 + 0.002 value_reward_chosen[t] + 0.097 contr_diff + 0.812 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.75 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.075 contr_diff + 0.06 value_choice^2 + 1.125 value_choice*contr_diff + -0.072 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 521/1000 --- L(Train): 0.4160798 --- L(Val, RNN): 0.4149186 --- L(Val, SINDy): 0.4155554 --- Time: 0.29s; --- Convergence: 5.95e-05; LR: 1.00e-02; Metric: 0.4149186; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.592 1 + 0.003 value_reward_chosen[t] + 0.098 contr_diff + 0.813 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.75 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.075 contr_diff + 0.06 value_choice^2 + 1.127 value_choice*contr_diff + -0.072 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 522/1000 --- L(Train): 0.4050867 --- L(Val, RNN): 0.4149503 --- L(Val, SINDy): 0.4155250 --- Time: 0.34s; --- Convergence: 4.56e-05; LR: 1.00e-02; Metric: 0.4149186; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.102 contr_diff + 0.814 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.752 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.075 contr_diff + 0.06 value_choice^2 + 1.128 value_choice*contr_diff + -0.073 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 523/1000 --- L(Train): 0.4198060 --- L(Val, RNN): 0.4149513 --- L(Val, SINDy): 0.4155153 --- Time: 0.30s; --- Convergence: 2.33e-05; LR: 1.00e-02; Metric: 0.4149186; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.593 1 + 0.002 value_reward_chosen[t] + 0.108 contr_diff + 0.814 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.075 contr_diff + 0.06 value_choice^2 + 1.13 value_choice*contr_diff + -0.073 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 524/1000 --- L(Train): 0.4102201 --- L(Val, RNN): 0.4150093 --- L(Val, SINDy): 0.4154891 --- Time: 0.34s; --- Convergence: 4.07e-05; LR: 1.00e-02; Metric: 0.4149186; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.594 1 + 0.001 value_reward_chosen[t] + 0.114 contr_diff + 0.813 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.076 contr_diff + 0.06 value_choice^2 + 1.131 value_choice*contr_diff + -0.074 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 525/1000 --- L(Train): 0.4323160 --- L(Val, RNN): 0.4149785 --- L(Val, SINDy): 0.4154176 --- Time: 0.33s; --- Convergence: 3.57e-05; LR: 1.00e-02; Metric: 0.4149186; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.595 1 + 0.001 value_reward_chosen[t] + 0.119 contr_diff + 0.813 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.76 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.076 contr_diff + 0.06 value_choice^2 + 1.133 value_choice*contr_diff + -0.074 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 526/1000 --- L(Train): 0.4189746 --- L(Val, RNN): 0.4149519 --- L(Val, SINDy): 0.4153775 --- Time: 0.29s; --- Convergence: 3.12e-05; LR: 1.00e-02; Metric: 0.4149186; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.596 1 + -0.0 value_reward_chosen[t] + 0.122 contr_diff + 0.813 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.06 value_choice^2 + 1.134 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 527/1000 --- L(Train): 0.4174439 --- L(Val, RNN): 0.4148968 --- L(Val, SINDy): 0.4153222 --- Time: 0.30s; --- Convergence: 4.31e-05; LR: 1.00e-02; Metric: 0.4148968; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.597 1 + -0.0 value_reward_chosen[t] + 0.122 contr_diff + 0.814 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.766 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.078 contr_diff + 0.059 value_choice^2 + 1.135 value_choice*contr_diff + -0.076 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 528/1000 --- L(Train): 0.4157610 --- L(Val, RNN): 0.4148873 --- L(Val, SINDy): 0.4152810 --- Time: 0.29s; --- Convergence: 2.63e-05; LR: 1.00e-02; Metric: 0.4148873; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.597 1 + -0.0 value_reward_chosen[t] + 0.12 contr_diff + 0.815 reward + 0.816 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.768 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.079 contr_diff + 0.059 value_choice^2 + 1.137 value_choice*contr_diff + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 529/1000 --- L(Train): 0.4233758 --- L(Val, RNN): 0.4148389 --- L(Val, SINDy): 0.4152713 --- Time: 0.31s; --- Convergence: 3.74e-05; LR: 1.00e-02; Metric: 0.4148389; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.597 1 + 0.0 value_reward_chosen[t] + 0.116 contr_diff + 0.816 reward + 0.817 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.768 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.08 contr_diff + 0.059 value_choice^2 + 1.138 value_choice*contr_diff + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 530/1000 --- L(Train): 0.4163037 --- L(Val, RNN): 0.4148650 --- L(Val, SINDy): 0.4152717 --- Time: 0.31s; --- Convergence: 3.18e-05; LR: 1.00e-02; Metric: 0.4148389; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.001 value_reward_chosen[t] + 0.113 contr_diff + 0.818 reward + 0.819 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.767 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.08 contr_diff + 0.059 value_choice^2 + 1.14 value_choice*contr_diff + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 531/1000 --- L(Train): 0.4197140 --- L(Val, RNN): 0.4149224 --- L(Val, SINDy): 0.4152841 --- Time: 0.28s; --- Convergence: 4.46e-05; LR: 1.00e-02; Metric: 0.4148389; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.595 1 + 0.001 value_reward_chosen[t] + 0.109 contr_diff + 0.82 reward + 0.821 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.767 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.08 contr_diff + 0.059 value_choice^2 + 1.142 value_choice*contr_diff + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 532/1000 --- L(Train): 0.4163436 --- L(Val, RNN): 0.4148903 --- L(Val, SINDy): 0.4153259 --- Time: 0.33s; --- Convergence: 3.84e-05; LR: 1.00e-02; Metric: 0.4148389; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.595 1 + 0.001 value_reward_chosen[t] + 0.106 contr_diff + 0.822 reward + 0.823 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.456 1 + 0.768 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.08 contr_diff + 0.059 value_choice^2 + 1.143 value_choice*contr_diff + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 533/1000 --- L(Train): 0.4141812 --- L(Val, RNN): 0.4148437 --- L(Val, SINDy): 0.4153663 --- Time: 0.31s; --- Convergence: 4.25e-05; LR: 1.00e-02; Metric: 0.4148389; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.594 1 + 0.001 value_reward_chosen[t] + 0.103 contr_diff + 0.823 reward + 0.825 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.77 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.079 contr_diff + 0.059 value_choice^2 + 1.145 value_choice*contr_diff + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 534/1000 --- L(Train): 0.4206356 --- L(Val, RNN): 0.4148600 --- L(Val, SINDy): 0.4153940 --- Time: 0.27s; --- Convergence: 2.93e-05; LR: 1.00e-02; Metric: 0.4148389; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.594 1 + 0.001 value_reward_chosen[t] + 0.1 contr_diff + 0.825 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.774 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.078 contr_diff + 0.059 value_choice^2 + 1.147 value_choice*contr_diff + -0.076 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 535/1000 --- L(Train): 0.4128836 --- L(Val, RNN): 0.4148389 --- L(Val, SINDy): 0.4154132 --- Time: 0.31s; --- Convergence: 2.52e-05; LR: 1.00e-02; Metric: 0.4148389; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.595 1 + -0.0 value_reward_chosen[t] + 0.097 contr_diff + 0.825 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.776 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.078 contr_diff + 0.059 value_choice^2 + 1.148 value_choice*contr_diff + -0.076 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 536/1000 --- L(Train): 0.4129836 --- L(Val, RNN): 0.4148588 --- L(Val, SINDy): 0.4154177 --- Time: 0.28s; --- Convergence: 2.25e-05; LR: 1.00e-02; Metric: 0.4148389; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.596 1 + -0.001 value_reward_chosen[t] + 0.093 contr_diff + 0.824 reward + 0.825 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.779 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.078 contr_diff + 0.059 value_choice^2 + 1.15 value_choice*contr_diff + -0.076 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 537/1000 --- L(Train): 0.4090285 --- L(Val, RNN): 0.4148766 --- L(Val, SINDy): 0.4154137 --- Time: 0.30s; --- Convergence: 2.02e-05; LR: 1.00e-02; Metric: 0.4148389; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.598 1 + -0.003 value_reward_chosen[t] + 0.089 contr_diff + 0.823 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.78 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.059 value_choice^2 + 1.152 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 538/1000 --- L(Train): 0.4179753 --- L(Val, RNN): 0.4148234 --- L(Val, SINDy): 0.4154199 --- Time: 0.32s; --- Convergence: 3.67e-05; LR: 1.00e-02; Metric: 0.4148234; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.6 1 + -0.004 value_reward_chosen[t] + 0.086 contr_diff + 0.822 reward + 0.823 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.454 1 + 0.781 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.058 value_choice^2 + 1.153 value_choice*contr_diff + -0.074 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 539/1000 --- L(Train): 0.4116045 --- L(Val, RNN): 0.4147838 --- L(Val, SINDy): 0.4154223 --- Time: 0.28s; --- Convergence: 3.82e-05; LR: 1.00e-02; Metric: 0.4147838; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.6 1 + -0.003 value_reward_chosen[t] + 0.089 contr_diff + 0.823 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.452 1 + 0.78 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.058 value_choice^2 + 1.155 value_choice*contr_diff + -0.074 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 540/1000 --- L(Train): 0.4179209 --- L(Val, RNN): 0.4147784 --- L(Val, SINDy): 0.4154255 --- Time: 0.45s; --- Convergence: 2.18e-05; LR: 1.00e-02; Metric: 0.4147784; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.598 1 + -0.001 value_reward_chosen[t] + 0.093 contr_diff + 0.825 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.778 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.058 value_choice^2 + 1.157 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 541/1000 --- L(Train): 0.4154205 --- L(Val, RNN): 0.4147951 --- L(Val, SINDy): 0.4154376 --- Time: 0.39s; --- Convergence: 1.93e-05; LR: 1.00e-02; Metric: 0.4147784; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.597 1 + 0.0 value_reward_chosen[t] + 0.097 contr_diff + 0.826 reward + 0.827 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.448 1 + 0.776 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.058 value_choice^2 + 1.158 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 542/1000 --- L(Train): 0.4078262 --- L(Val, RNN): 0.4148226 --- L(Val, SINDy): 0.4154531 --- Time: 0.35s; --- Convergence: 2.34e-05; LR: 1.00e-02; Metric: 0.4147784; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.001 value_reward_chosen[t] + 0.101 contr_diff + 0.827 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.447 1 + 0.774 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.058 value_choice^2 + 1.159 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 543/1000 --- L(Train): 0.4176746 --- L(Val, RNN): 0.4147883 --- L(Val, SINDy): 0.4154358 --- Time: 0.34s; --- Convergence: 2.89e-05; LR: 1.00e-02; Metric: 0.4147784; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.597 1 + 0.0 value_reward_chosen[t] + 0.106 contr_diff + 0.827 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.448 1 + 0.775 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.058 value_choice^2 + 1.161 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 544/1000 --- L(Train): 0.4203738 --- L(Val, RNN): 0.4147932 --- L(Val, SINDy): 0.4153963 --- Time: 0.29s; --- Convergence: 1.69e-05; LR: 1.00e-02; Metric: 0.4147784; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.597 1 + 0.0 value_reward_chosen[t] + 0.111 contr_diff + 0.827 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.451 1 + 0.777 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.058 value_choice^2 + 1.162 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 545/1000 --- L(Train): 0.4207998 --- L(Val, RNN): 0.4148042 --- L(Val, SINDy): 0.4154084 --- Time: 0.37s; --- Convergence: 1.40e-05; LR: 1.00e-02; Metric: 0.4147784; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.001 value_reward_chosen[t] + 0.115 contr_diff + 0.828 reward + 0.829 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.779 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.058 value_choice^2 + 1.163 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 546/1000 --- L(Train): 0.4054174 --- L(Val, RNN): 0.4147999 --- L(Val, SINDy): 0.4153673 --- Time: 0.30s; --- Convergence: 9.12e-06; LR: 1.00e-02; Metric: 0.4147784; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.595 1 + 0.001 value_reward_chosen[t] + 0.117 contr_diff + 0.829 reward + 0.83 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.457 1 + 0.779 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.057 value_choice^2 + 1.165 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 547/1000 --- L(Train): 0.4203098 --- L(Val, RNN): 0.4147505 --- L(Val, SINDy): 0.4153230 --- Time: 0.30s; --- Convergence: 2.93e-05; LR: 1.00e-02; Metric: 0.4147505; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.594 1 + 0.001 value_reward_chosen[t] + 0.118 contr_diff + 0.829 reward + 0.83 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.778 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.057 value_choice^2 + 1.166 value_choice*contr_diff + -0.074 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 548/1000 --- L(Train): 0.4095047 --- L(Val, RNN): 0.4147430 --- L(Val, SINDy): 0.4153119 --- Time: 0.31s; --- Convergence: 1.84e-05; LR: 1.00e-02; Metric: 0.4147430; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.595 1 + 0.001 value_reward_chosen[t] + 0.118 contr_diff + 0.829 reward + 0.83 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.776 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.057 value_choice^2 + 1.168 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 549/1000 --- L(Train): 0.4203898 --- L(Val, RNN): 0.4147850 --- L(Val, SINDy): 0.4153638 --- Time: 0.30s; --- Convergence: 3.01e-05; LR: 1.00e-02; Metric: 0.4147430; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.597 1 + -0.0 value_reward_chosen[t] + 0.117 contr_diff + 0.827 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.774 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.057 value_choice^2 + 1.169 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 550/1000 --- L(Train): 0.4232143 --- L(Val, RNN): 0.4148020 --- L(Val, SINDy): 0.4153446 --- Time: 0.30s; --- Convergence: 2.36e-05; LR: 1.00e-02; Metric: 0.4147430; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.598 1 + -0.001 value_reward_chosen[t] + 0.116 contr_diff + 0.826 reward + 0.827 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.771 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.078 contr_diff + 0.057 value_choice^2 + 1.17 value_choice*contr_diff + -0.076 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 551/1000 --- L(Train): 0.4124878 --- L(Val, RNN): 0.4147817 --- L(Val, SINDy): 0.4153341 --- Time: 0.34s; --- Convergence: 2.19e-05; LR: 1.00e-02; Metric: 0.4147430; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.599 1 + -0.002 value_reward_chosen[t] + 0.115 contr_diff + 0.825 reward + 0.827 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.769 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.079 contr_diff + 0.057 value_choice^2 + 1.172 value_choice*contr_diff + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 552/1000 --- L(Train): 0.4144754 --- L(Val, RNN): 0.4147545 --- L(Val, SINDy): 0.4153157 --- Time: 0.28s; --- Convergence: 2.46e-05; LR: 1.00e-02; Metric: 0.4147430; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.6 1 + -0.001 value_reward_chosen[t] + 0.114 contr_diff + 0.825 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.767 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.08 contr_diff + 0.057 value_choice^2 + 1.173 value_choice*contr_diff + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 553/1000 --- L(Train): 0.4147863 --- L(Val, RNN): 0.4147504 --- L(Val, SINDy): 0.4153087 --- Time: 0.42s; --- Convergence: 1.43e-05; LR: 1.00e-02; Metric: 0.4147430; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.601 1 + -0.001 value_reward_chosen[t] + 0.113 contr_diff + 0.825 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.766 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.08 contr_diff + 0.057 value_choice^2 + 1.175 value_choice*contr_diff + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 554/1000 --- L(Train): 0.4166337 --- L(Val, RNN): 0.4147553 --- L(Val, SINDy): 0.4153443 --- Time: 0.37s; --- Convergence: 9.62e-06; LR: 1.00e-02; Metric: 0.4147430; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.603 1 + -0.002 value_reward_chosen[t] + 0.112 contr_diff + 0.823 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.765 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.08 contr_diff + 0.057 value_choice^2 + 1.177 value_choice*contr_diff + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 555/1000 --- L(Train): 0.4088383 --- L(Val, RNN): 0.4147383 --- L(Val, SINDy): 0.4153555 --- Time: 0.29s; --- Convergence: 1.33e-05; LR: 1.00e-02; Metric: 0.4147383; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.605 1 + -0.003 value_reward_chosen[t] + 0.109 contr_diff + 0.823 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.764 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.079 contr_diff + 0.056 value_choice^2 + 1.179 value_choice*contr_diff + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 556/1000 --- L(Train): 0.4158339 --- L(Val, RNN): 0.4147865 --- L(Val, SINDy): 0.4153514 --- Time: 0.27s; --- Convergence: 3.08e-05; LR: 1.00e-02; Metric: 0.4147383; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.606 1 + -0.002 value_reward_chosen[t] + 0.106 contr_diff + 0.823 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.079 contr_diff + 0.056 value_choice^2 + 1.181 value_choice*contr_diff + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 557/1000 --- L(Train): 0.4096804 --- L(Val, RNN): 0.4148020 --- L(Val, SINDy): 0.4153670 --- Time: 0.30s; --- Convergence: 2.31e-05; LR: 1.00e-02; Metric: 0.4147383; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.607 1 + -0.002 value_reward_chosen[t] + 0.103 contr_diff + 0.823 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.762 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.078 contr_diff + 0.056 value_choice^2 + 1.183 value_choice*contr_diff + -0.076 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 558/1000 --- L(Train): 0.4171249 --- L(Val, RNN): 0.4147528 --- L(Val, SINDy): 0.4153410 --- Time: 0.28s; --- Convergence: 3.62e-05; LR: 1.00e-02; Metric: 0.4147383; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.607 1 + -0.001 value_reward_chosen[t] + 0.1 contr_diff + 0.824 reward + 0.825 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.76 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.056 value_choice^2 + 1.185 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 559/1000 --- L(Train): 0.4253047 --- L(Val, RNN): 0.4147367 --- L(Val, SINDy): 0.4153351 --- Time: 0.30s; --- Convergence: 2.61e-05; LR: 1.00e-02; Metric: 0.4147367; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.608 1 + 0.0 value_reward_chosen[t] + 0.099 contr_diff + 0.825 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.056 value_choice^2 + 1.187 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 560/1000 --- L(Train): 0.4152957 --- L(Val, RNN): 0.4147641 --- L(Val, SINDy): 0.4153661 --- Time: 0.30s; --- Convergence: 2.67e-05; LR: 1.00e-02; Metric: 0.4147367; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.608 1 + 0.001 value_reward_chosen[t] + 0.1 contr_diff + 0.825 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.056 value_choice^2 + 1.189 value_choice*contr_diff + -0.074 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 561/1000 --- L(Train): 0.4179505 --- L(Val, RNN): 0.4147287 --- L(Val, SINDy): 0.4153405 --- Time: 0.31s; --- Convergence: 3.11e-05; LR: 1.00e-02; Metric: 0.4147287; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.608 1 + 0.003 value_reward_chosen[t] + 0.101 contr_diff + 0.827 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.056 value_choice^2 + 1.191 value_choice*contr_diff + -0.074 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 562/1000 --- L(Train): 0.4208707 --- L(Val, RNN): 0.4147300 --- L(Val, SINDy): 0.4153434 --- Time: 0.31s; --- Convergence: 1.61e-05; LR: 1.00e-02; Metric: 0.4147287; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.609 1 + 0.004 value_reward_chosen[t] + 0.105 contr_diff + 0.827 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.077 contr_diff + 0.056 value_choice^2 + 1.192 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 563/1000 --- L(Train): 0.4060893 --- L(Val, RNN): 0.4148016 --- L(Val, SINDy): 0.4153713 --- Time: 0.33s; --- Convergence: 4.39e-05; LR: 1.00e-02; Metric: 0.4147287; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.612 1 + 0.004 value_reward_chosen[t] + 0.11 contr_diff + 0.826 reward + 0.827 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.078 contr_diff + 0.056 value_choice^2 + 1.194 value_choice*contr_diff + -0.075 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 564/1000 --- L(Train): 0.4286247 --- L(Val, RNN): 0.4147434 --- L(Val, SINDy): 0.4153344 --- Time: 0.28s; --- Convergence: 5.11e-05; LR: 1.00e-02; Metric: 0.4147287; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.613 1 + 0.004 value_reward_chosen[t] + 0.114 contr_diff + 0.827 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.078 contr_diff + 0.056 value_choice^2 + 1.196 value_choice*contr_diff + -0.076 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 565/1000 --- L(Train): 0.4315191 --- L(Val, RNN): 0.4147078 --- L(Val, SINDy): 0.4153138 --- Time: 0.28s; --- Convergence: 4.33e-05; LR: 1.00e-02; Metric: 0.4147078; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.614 1 + 0.005 value_reward_chosen[t] + 0.118 contr_diff + 0.827 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.079 contr_diff + 0.055 value_choice^2 + 1.197 value_choice*contr_diff + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 566/1000 --- L(Train): 0.4120036 --- L(Val, RNN): 0.4147077 --- L(Val, SINDy): 0.4153084 --- Time: 0.30s; --- Convergence: 2.17e-05; LR: 1.00e-02; Metric: 0.4147077; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.616 1 + 0.005 value_reward_chosen[t] + 0.12 contr_diff + 0.827 reward + 0.829 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.079 contr_diff + 0.055 value_choice^2 + 1.199 value_choice*contr_diff + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 567/1000 --- L(Train): 0.4115002 --- L(Val, RNN): 0.4146840 --- L(Val, SINDy): 0.4152598 --- Time: 0.29s; --- Convergence: 2.27e-05; LR: 1.00e-02; Metric: 0.4146840; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.617 1 + 0.004 value_reward_chosen[t] + 0.12 contr_diff + 0.828 reward + 0.829 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.079 contr_diff + 0.055 value_choice^2 + 1.2 value_choice*contr_diff + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 568/1000 --- L(Train): 0.4239385 --- L(Val, RNN): 0.4146969 --- L(Val, SINDy): 0.4152369 --- Time: 0.31s; --- Convergence: 1.78e-05; LR: 1.00e-02; Metric: 0.4146840; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.004 value_reward_chosen[t] + 0.119 contr_diff + 0.828 reward + 0.829 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.079 contr_diff + 0.055 value_choice^2 + 1.202 value_choice*contr_diff + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 569/1000 --- L(Train): 0.4099830 --- L(Val, RNN): 0.4146997 --- L(Val, SINDy): 0.4152472 --- Time: 0.43s; --- Convergence: 1.03e-05; LR: 1.00e-02; Metric: 0.4146840; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.619 1 + 0.003 value_reward_chosen[t] + 0.116 contr_diff + 0.829 reward + 0.83 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.079 contr_diff + 0.055 value_choice^2 + 1.203 value_choice*contr_diff + -0.077 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 570/1000 --- L(Train): 0.4161168 --- L(Val, RNN): 0.4146475 --- L(Val, SINDy): 0.4151979 --- Time: 0.37s; --- Convergence: 3.13e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.617 1 + 0.004 value_reward_chosen[t] + 0.111 contr_diff + 0.832 reward + 0.834 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.08 contr_diff + 0.055 value_choice^2 + 1.205 value_choice*contr_diff + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 571/1000 --- L(Train): 0.3985407 --- L(Val, RNN): 0.4146917 --- L(Val, SINDy): 0.4151776 --- Time: 0.31s; --- Convergence: 3.78e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.615 1 + 0.005 value_reward_chosen[t] + 0.106 contr_diff + 0.836 reward + 0.837 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.081 contr_diff + 0.055 value_choice^2 + 1.206 value_choice*contr_diff + -0.079 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 572/1000 --- L(Train): 0.4129009 --- L(Val, RNN): 0.4148038 --- L(Val, SINDy): 0.4152086 --- Time: 0.29s; --- Convergence: 7.49e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.614 1 + 0.005 value_reward_chosen[t] + 0.102 contr_diff + 0.837 reward + 0.838 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.082 contr_diff + 0.055 value_choice^2 + 1.207 value_choice*contr_diff + -0.08 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 573/1000 --- L(Train): 0.4141159 --- L(Val, RNN): 0.4146916 --- L(Val, SINDy): 0.4151914 --- Time: 0.32s; --- Convergence: 9.35e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.614 1 + 0.004 value_reward_chosen[t] + 0.099 contr_diff + 0.839 reward + 0.84 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.083 contr_diff + 0.055 value_choice^2 + 1.209 value_choice*contr_diff + -0.081 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 574/1000 --- L(Train): 0.4184778 --- L(Val, RNN): 0.4146504 --- L(Val, SINDy): 0.4151964 --- Time: 0.34s; --- Convergence: 6.74e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.615 1 + 0.003 value_reward_chosen[t] + 0.097 contr_diff + 0.839 reward + 0.841 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.762 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.083 contr_diff + 0.055 value_choice^2 + 1.211 value_choice*contr_diff + -0.081 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 575/1000 --- L(Train): 0.4162841 --- L(Val, RNN): 0.4146995 --- L(Val, SINDy): 0.4152304 --- Time: 0.39s; --- Convergence: 5.82e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.615 1 + 0.002 value_reward_chosen[t] + 0.094 contr_diff + 0.84 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.766 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.084 contr_diff + 0.055 value_choice^2 + 1.213 value_choice*contr_diff + -0.082 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 576/1000 --- L(Train): 0.4265464 --- L(Val, RNN): 0.4147354 --- L(Val, SINDy): 0.4152216 --- Time: 0.35s; --- Convergence: 4.71e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.613 1 + 0.002 value_reward_chosen[t] + 0.092 contr_diff + 0.843 reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.769 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.084 contr_diff + 0.054 value_choice^2 + 1.214 value_choice*contr_diff + -0.081 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 577/1000 --- L(Train): 0.4168793 --- L(Val, RNN): 0.4147619 --- L(Val, SINDy): 0.4152310 --- Time: 0.33s; --- Convergence: 3.68e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.613 1 + 0.001 value_reward_chosen[t] + 0.089 contr_diff + 0.843 reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.771 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.082 contr_diff + 0.054 value_choice^2 + 1.216 value_choice*contr_diff + -0.08 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 578/1000 --- L(Train): 0.4090126 --- L(Val, RNN): 0.4147917 --- L(Val, SINDy): 0.4152561 --- Time: 0.37s; --- Convergence: 3.33e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.615 1 + -0.002 value_reward_chosen[t] + 0.087 contr_diff + 0.841 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.772 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.08 contr_diff + 0.054 value_choice^2 + 1.218 value_choice*contr_diff + -0.078 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 579/1000 --- L(Train): 0.4310932 --- L(Val, RNN): 0.4147145 --- L(Val, SINDy): 0.4152252 --- Time: 0.36s; --- Convergence: 5.52e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.617 1 + -0.004 value_reward_chosen[t] + 0.085 contr_diff + 0.84 reward + 0.841 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.774 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.079 contr_diff + 0.054 value_choice^2 + 1.22 value_choice*contr_diff + -0.076 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 580/1000 --- L(Train): 0.4247770 --- L(Val, RNN): 0.4146984 --- L(Val, SINDy): 0.4152314 --- Time: 0.42s; --- Convergence: 3.57e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.619 1 + -0.006 value_reward_chosen[t] + 0.084 contr_diff + 0.838 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.775 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.073 contr_diff + 0.054 value_choice^2 + 1.223 value_choice*contr_diff + -0.071 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 581/1000 --- L(Train): 0.4108767 --- L(Val, RNN): 0.4147426 --- L(Val, SINDy): 0.4152867 --- Time: 0.29s; --- Convergence: 3.99e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.621 1 + -0.007 value_reward_chosen[t] + 0.084 contr_diff + 0.836 reward + 0.837 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.454 1 + 0.776 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.065 contr_diff + 0.054 value_choice^2 + 1.226 value_choice*contr_diff + -0.063 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 582/1000 --- L(Train): 0.4124565 --- L(Val, RNN): 0.4147624 --- L(Val, SINDy): 0.4152968 --- Time: 0.31s; --- Convergence: 2.98e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.621 1 + -0.006 value_reward_chosen[t] + 0.084 contr_diff + 0.837 reward + 0.838 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.777 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.055 contr_diff + 0.054 value_choice^2 + 1.23 value_choice*contr_diff + -0.053 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 0, -, 0, 0, -, -, 0, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 583/1000 --- L(Train): 0.4231695 --- L(Val, RNN): 0.4147429 --- L(Val, SINDy): 0.4153045 --- Time: 0.34s; --- Convergence: 2.46e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 13/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.621 1 + -0.004 value_reward_chosen[t] + 0.086 contr_diff + 0.837 reward + 0.838 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.447 1 + 0.779 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.045 contr_diff + 0.054 value_choice^2 + 1.233 value_choice*contr_diff + -0.043 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 1, -, 0, 0, -, -, 1, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 584/1000 --- L(Train): 0.4072604 --- L(Val, RNN): 0.4147390 --- L(Val, SINDy): 0.4153494 --- Time: 0.34s; --- Convergence: 1.43e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 14/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.622 1 + -0.003 value_reward_chosen[t] + 0.088 contr_diff + 0.836 reward + 0.838 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.445 1 + 0.783 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.036 contr_diff + 0.054 value_choice^2 + 1.236 value_choice*contr_diff + -0.034 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 2, -, 0, 0, -, -, 2, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 585/1000 --- L(Train): 0.4130896 --- L(Val, RNN): 0.4147384 --- L(Val, SINDy): 0.4153472 --- Time: 0.31s; --- Convergence: 7.42e-06; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 15/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.62 1 + -0.0 value_reward_chosen[t] + 0.091 contr_diff + 0.838 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.444 1 + 0.788 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.029 contr_diff + 0.054 value_choice^2 + 1.239 value_choice*contr_diff + -0.027 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 3, -, 0, 0, -, -, 3, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 586/1000 --- L(Train): 0.4118894 --- L(Val, RNN): 0.4147513 --- L(Val, SINDy): 0.4153254 --- Time: 0.27s; --- Convergence: 1.01e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 16/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.619 1 + 0.003 value_reward_chosen[t] + 0.094 contr_diff + 0.84 reward + 0.841 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.444 1 + 0.793 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.054 value_choice^2 + 1.24 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 4, -, 0, 0, -, -, 4, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 587/1000 --- L(Train): 0.4232893 --- L(Val, RNN): 0.4146815 --- L(Val, SINDy): 0.4153363 --- Time: 0.32s; --- Convergence: 3.99e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 17/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.004 value_reward_chosen[t] + 0.096 contr_diff + 0.841 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.442 1 + 0.796 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.025 contr_diff + 0.054 value_choice^2 + 1.241 value_choice*contr_diff + -0.022 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 5, -, 0, 0, -, -, 5, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 588/1000 --- L(Train): 0.4016228 --- L(Val, RNN): 0.4147094 --- L(Val, SINDy): 0.4152982 --- Time: 0.31s; --- Convergence: 3.39e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 18/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.005 value_reward_chosen[t] + 0.098 contr_diff + 0.842 reward + 0.843 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.439 1 + 0.797 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.024 contr_diff + 0.054 value_choice^2 + 1.242 value_choice*contr_diff + -0.022 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 6, -, 0, 0, -, -, 6, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 589/1000 --- L(Train): 0.4154752 --- L(Val, RNN): 0.4147891 --- L(Val, SINDy): 0.4152626 --- Time: 0.39s; --- Convergence: 5.68e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 19/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.617 1 + 0.006 value_reward_chosen[t] + 0.099 contr_diff + 0.843 reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.436 1 + 0.796 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.024 contr_diff + 0.054 value_choice^2 + 1.243 value_choice*contr_diff + -0.022 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 7, -, 0, 0, -, -, 7, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 590/1000 --- L(Train): 0.4178399 --- L(Val, RNN): 0.4147458 --- L(Val, SINDy): 0.4152670 --- Time: 0.37s; --- Convergence: 5.00e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 20/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.617 1 + 0.004 value_reward_chosen[t] + 0.1 contr_diff + 0.842 reward + 0.843 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.434 1 + 0.793 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.024 contr_diff + 0.054 value_choice^2 + 1.244 value_choice*contr_diff + -0.022 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 8, -, 0, 0, -, -, 8, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 591/1000 --- L(Train): 0.4233934 --- L(Val, RNN): 0.4147052 --- L(Val, SINDy): 0.4152488 --- Time: 0.32s; --- Convergence: 4.53e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 21/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.003 value_reward_chosen[t] + 0.1 contr_diff + 0.841 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.432 1 + 0.79 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.025 contr_diff + 0.054 value_choice^2 + 1.244 value_choice*contr_diff + -0.023 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 9, -, 0, 0, -, -, 9, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 592/1000 --- L(Train): 0.4216682 --- L(Val, RNN): 0.4146841 --- L(Val, SINDy): 0.4151974 --- Time: 0.29s; --- Convergence: 3.32e-05; LR: 1.00e-02; Metric: 0.4146475; Bad epochs: 22/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.619 1 + 0.002 value_reward_chosen[t] + 0.102 contr_diff + 0.841 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.431 1 + 0.787 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.027 contr_diff + 0.054 value_choice^2 + 1.245 value_choice*contr_diff + -0.025 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 10, -, 0, 0, -, -, 10, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 593/1000 --- L(Train): 0.4122467 --- L(Val, RNN): 0.4146428 --- L(Val, SINDy): 0.4151925 --- Time: 0.36s; --- Convergence: 3.72e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.62 1 + -0.0 value_reward_chosen[t] + 0.104 contr_diff + 0.839 reward + 0.84 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.432 1 + 0.786 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.029 contr_diff + 0.054 value_choice^2 + 1.245 value_choice*contr_diff + -0.027 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 11, -, 0, 0, -, -, 11, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 594/1000 --- L(Train): 0.4137381 --- L(Val, RNN): 0.4146716 --- L(Val, SINDy): 0.4152165 --- Time: 0.27s; --- Convergence: 3.30e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.622 1 + -0.002 value_reward_chosen[t] + 0.106 contr_diff + 0.838 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.434 1 + 0.785 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.031 contr_diff + 0.053 value_choice^2 + 1.245 value_choice*contr_diff + -0.029 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 12, -, 0, 0, -, -, 12, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 595/1000 --- L(Train): 0.4332146 --- L(Val, RNN): 0.4147191 --- L(Val, SINDy): 0.4152452 --- Time: 0.31s; --- Convergence: 4.02e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.622 1 + -0.003 value_reward_chosen[t] + 0.108 contr_diff + 0.838 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.437 1 + 0.784 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.033 contr_diff + 0.053 value_choice^2 + 1.245 value_choice*contr_diff + -0.03 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 13, -, 0, 0, -, -, 13, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 596/1000 --- L(Train): 0.4094889 --- L(Val, RNN): 0.4147730 --- L(Val, SINDy): 0.4153049 --- Time: 0.29s; --- Convergence: 4.71e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.622 1 + -0.003 value_reward_chosen[t] + 0.108 contr_diff + 0.838 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.44 1 + 0.784 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.032 contr_diff + 0.053 value_choice^2 + 1.246 value_choice*contr_diff + -0.03 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 14, -, 0, 0, -, -, 14, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 597/1000 --- L(Train): 0.4198400 --- L(Val, RNN): 0.4147446 --- L(Val, SINDy): 0.4153593 --- Time: 0.35s; --- Convergence: 3.77e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.621 1 + -0.003 value_reward_chosen[t] + 0.105 contr_diff + 0.839 reward + 0.84 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.445 1 + 0.784 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.03 contr_diff + 0.053 value_choice^2 + 1.246 value_choice*contr_diff + -0.028 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 15, -, 0, 0, -, -, 15, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 598/1000 --- L(Train): 0.4060616 --- L(Val, RNN): 0.4147304 --- L(Val, SINDy): 0.4153742 --- Time: 0.44s; --- Convergence: 2.60e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.62 1 + -0.001 value_reward_chosen[t] + 0.101 contr_diff + 0.841 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.448 1 + 0.783 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.027 contr_diff + 0.053 value_choice^2 + 1.247 value_choice*contr_diff + -0.025 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 16, -, 0, 0, -, -, 16, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 599/1000 --- L(Train): 0.4170905 --- L(Val, RNN): 0.4147218 --- L(Val, SINDy): 0.4153902 --- Time: 0.36s; --- Convergence: 1.73e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.619 1 + 0.001 value_reward_chosen[t] + 0.097 contr_diff + 0.843 reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.451 1 + 0.781 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.023 contr_diff + 0.053 value_choice^2 + 1.248 value_choice*contr_diff + -0.021 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 17, -, 0, 0, -, -, 17, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 600/1000 --- L(Train): 0.4122982 --- L(Val, RNN): 0.4147375 --- L(Val, SINDy): 0.4154207 --- Time: 0.30s; --- Convergence: 1.65e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.001 value_reward_chosen[t] + 0.093 contr_diff + 0.844 reward + 0.845 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.452 1 + 0.778 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.02 contr_diff + 0.053 value_choice^2 + 1.248 value_choice*contr_diff + -0.018 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 18, -, 0, 0, -, -, 18, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 601/1000 --- L(Train): 0.4133780 --- L(Val, RNN): 0.4147398 --- L(Val, SINDy): 0.4153810 --- Time: 0.36s; --- Convergence: 9.35e-06; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.617 1 + 0.002 value_reward_chosen[t] + 0.089 contr_diff + 0.845 reward + 0.846 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.774 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.017 contr_diff + 0.053 value_choice^2 + 1.249 value_choice*contr_diff + -0.015 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 19, -, 0, 0, -, -, 19, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 602/1000 --- L(Train): 0.4038307 --- L(Val, RNN): 0.4147364 --- L(Val, SINDy): 0.4153319 --- Time: 0.29s; --- Convergence: 6.33e-06; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.617 1 + 0.002 value_reward_chosen[t] + 0.086 contr_diff + 0.846 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.77 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.015 contr_diff + 0.053 value_choice^2 + 1.249 value_choice*contr_diff + -0.013 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 20, -, 0, 0, -, -, 20, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 603/1000 --- L(Train): 0.4210361 --- L(Val, RNN): 0.4146694 --- L(Val, SINDy): 0.4153349 --- Time: 0.31s; --- Convergence: 3.67e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.001 value_reward_chosen[t] + 0.084 contr_diff + 0.845 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.454 1 + 0.766 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.013 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.011 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 21, -, 0, 0, -, -, 21, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 604/1000 --- L(Train): 0.4224531 --- L(Val, RNN): 0.4146607 --- L(Val, SINDy): 0.4153123 --- Time: 0.30s; --- Convergence: 2.27e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.001 value_reward_chosen[t] + 0.082 contr_diff + 0.846 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.012 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.01 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 22, -, 0, 0, -, -, 22, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 605/1000 --- L(Train): 0.4155045 --- L(Val, RNN): 0.4146767 --- L(Val, SINDy): 0.4152781 --- Time: 0.33s; --- Convergence: 1.94e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.001 value_reward_chosen[t] + 0.082 contr_diff + 0.846 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.457 1 + 0.761 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.011 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.009 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 23, -, 0, 0, -, -, 23, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 606/1000 --- L(Train): 0.4232125 --- L(Val, RNN): 0.4146469 --- L(Val, SINDy): 0.4152835 --- Time: 0.30s; --- Convergence: 2.46e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 13/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.0 value_reward_chosen[t] + 0.082 contr_diff + 0.847 reward + 0.848 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.011 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.009 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 24, -, 0, 0, -, -, 24, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 607/1000 --- L(Train): 0.4061114 --- L(Val, RNN): 0.4146526 --- L(Val, SINDy): 0.4152923 --- Time: 0.29s; --- Convergence: 1.51e-05; LR: 1.00e-02; Metric: 0.4146428; Bad epochs: 14/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + -0.0 value_reward_chosen[t] + 0.083 contr_diff + 0.847 reward + 0.848 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.011 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.009 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 25, -, 0, 0, -, -, 25, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 608/1000 --- L(Train): 0.4104648 --- L(Val, RNN): 0.4146366 --- L(Val, SINDy): 0.4152991 --- Time: 0.28s; --- Convergence: 1.56e-05; LR: 1.00e-02; Metric: 0.4146366; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.619 1 + -0.001 value_reward_chosen[t] + 0.084 contr_diff + 0.847 reward + 0.848 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.012 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.01 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 26, -, 0, 0, -, -, 26, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 609/1000 --- L(Train): 0.4133115 --- L(Val, RNN): 0.4146235 --- L(Val, SINDy): 0.4153066 --- Time: 0.39s; --- Convergence: 1.43e-05; LR: 1.00e-02; Metric: 0.4146235; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.62 1 + -0.001 value_reward_chosen[t] + 0.086 contr_diff + 0.846 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.014 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.012 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 27, -, 0, 0, -, -, 27, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 610/1000 --- L(Train): 0.4015798 --- L(Val, RNN): 0.4146511 --- L(Val, SINDy): 0.4152787 --- Time: 0.30s; --- Convergence: 2.09e-05; LR: 1.00e-02; Metric: 0.4146235; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.619 1 + -0.001 value_reward_chosen[t] + 0.087 contr_diff + 0.847 reward + 0.849 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.016 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.014 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 28, -, 0, 0, -, -, 28, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 611/1000 --- L(Train): 0.4120158 --- L(Val, RNN): 0.4146607 --- L(Val, SINDy): 0.4152630 --- Time: 0.37s; --- Convergence: 1.53e-05; LR: 1.00e-02; Metric: 0.4146235; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.0 value_reward_chosen[t] + 0.088 contr_diff + 0.849 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.019 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.017 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 29, -, 0, 0, -, -, 29, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 612/1000 --- L(Train): 0.4145827 --- L(Val, RNN): 0.4146105 --- L(Val, SINDy): 0.4152821 --- Time: 0.34s; --- Convergence: 3.27e-05; LR: 1.00e-02; Metric: 0.4146105; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.617 1 + 0.001 value_reward_chosen[t] + 0.089 contr_diff + 0.85 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.022 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.019 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 30, -, 0, 0, -, -, 30, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 613/1000 --- L(Train): 0.4226089 --- L(Val, RNN): 0.4145594 --- L(Val, SINDy): 0.4153115 --- Time: 0.32s; --- Convergence: 4.19e-05; LR: 1.00e-02; Metric: 0.4145594; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.001 value_reward_chosen[t] + 0.09 contr_diff + 0.85 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.024 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.022 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 31, -, 0, 0, -, -, 31, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 614/1000 --- L(Train): 0.4137227 --- L(Val, RNN): 0.4145909 --- L(Val, SINDy): 0.4153240 --- Time: 0.29s; --- Convergence: 3.67e-05; LR: 1.00e-02; Metric: 0.4145594; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.001 value_reward_chosen[t] + 0.092 contr_diff + 0.85 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.025 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.023 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 32, -, 0, 0, -, -, 32, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 615/1000 --- L(Train): 0.4098386 --- L(Val, RNN): 0.4146119 --- L(Val, SINDy): 0.4153455 --- Time: 0.33s; --- Convergence: 2.89e-05; LR: 1.00e-02; Metric: 0.4145594; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.619 1 + 0.0 value_reward_chosen[t] + 0.094 contr_diff + 0.85 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.752 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 33, -, 0, 0, -, -, 33, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 616/1000 --- L(Train): 0.4354137 --- L(Val, RNN): 0.4146350 --- L(Val, SINDy): 0.4153453 --- Time: 0.35s; --- Convergence: 2.60e-05; LR: 1.00e-02; Metric: 0.4145594; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.619 1 + 0.001 value_reward_chosen[t] + 0.096 contr_diff + 0.85 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.752 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.027 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.025 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 34, -, 0, 0, -, -, 34, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 617/1000 --- L(Train): 0.4159190 --- L(Val, RNN): 0.4146003 --- L(Val, SINDy): 0.4153173 --- Time: 0.28s; --- Convergence: 3.04e-05; LR: 1.00e-02; Metric: 0.4145594; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.619 1 + 0.001 value_reward_chosen[t] + 0.098 contr_diff + 0.851 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.752 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.027 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.025 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 35, -, 0, 0, -, -, 35, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 618/1000 --- L(Train): 0.4094130 --- L(Val, RNN): 0.4145878 --- L(Val, SINDy): 0.4153182 --- Time: 0.29s; --- Convergence: 2.14e-05; LR: 1.00e-02; Metric: 0.4145594; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.619 1 + 0.001 value_reward_chosen[t] + 0.1 contr_diff + 0.851 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.751 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 36, -, 0, 0, -, -, 36, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 619/1000 --- L(Train): 0.4106173 --- L(Val, RNN): 0.4145626 --- L(Val, SINDy): 0.4153414 --- Time: 0.32s; --- Convergence: 2.33e-05; LR: 1.00e-02; Metric: 0.4145594; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.621 1 + -0.0 value_reward_chosen[t] + 0.102 contr_diff + 0.849 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.749 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.25 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 37, -, 0, 0, -, -, 37, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 620/1000 --- L(Train): 0.4150026 --- L(Val, RNN): 0.4146162 --- L(Val, SINDy): 0.4153296 --- Time: 0.29s; --- Convergence: 3.85e-05; LR: 1.00e-02; Metric: 0.4145594; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.622 1 + -0.001 value_reward_chosen[t] + 0.104 contr_diff + 0.849 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.025 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.023 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 38, -, 0, 0, -, -, 38, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 621/1000 --- L(Train): 0.4275878 --- L(Val, RNN): 0.4146197 --- L(Val, SINDy): 0.4152966 --- Time: 0.32s; --- Convergence: 2.10e-05; LR: 1.00e-02; Metric: 0.4145594; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.623 1 + -0.001 value_reward_chosen[t] + 0.103 contr_diff + 0.848 reward + 0.849 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.024 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.022 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 39, -, 0, 0, -, -, 39, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 622/1000 --- L(Train): 0.4250772 --- L(Val, RNN): 0.4145544 --- L(Val, SINDy): 0.4152513 --- Time: 0.31s; --- Convergence: 4.31e-05; LR: 1.00e-02; Metric: 0.4145544; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.624 1 + -0.001 value_reward_chosen[t] + 0.101 contr_diff + 0.848 reward + 0.849 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.024 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.022 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 40, -, 0, 0, -, -, 40, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 623/1000 --- L(Train): 0.4112326 --- L(Val, RNN): 0.4145550 --- L(Val, SINDy): 0.4151858 --- Time: 0.30s; --- Convergence: 2.18e-05; LR: 1.00e-02; Metric: 0.4145544; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.624 1 + -0.001 value_reward_chosen[t] + 0.098 contr_diff + 0.848 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.023 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.021 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 41, -, 0, 0, -, -, 41, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 624/1000 --- L(Train): 0.4212212 --- L(Val, RNN): 0.4145472 --- L(Val, SINDy): 0.4151380 --- Time: 0.34s; --- Convergence: 1.48e-05; LR: 1.00e-02; Metric: 0.4145472; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.624 1 + -0.0 value_reward_chosen[t] + 0.095 contr_diff + 0.849 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.023 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.02 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 42, -, 0, 0, -, -, 42, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 625/1000 --- L(Train): 0.4125864 --- L(Val, RNN): 0.4145460 --- L(Val, SINDy): 0.4151048 --- Time: 0.38s; --- Convergence: 8.00e-06; LR: 1.00e-02; Metric: 0.4145460; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.624 1 + 0.001 value_reward_chosen[t] + 0.092 contr_diff + 0.85 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.749 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.022 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.02 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 43, -, 0, 0, -, -, 43, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 626/1000 --- L(Train): 0.4161264 --- L(Val, RNN): 0.4145241 --- L(Val, SINDy): 0.4150668 --- Time: 0.43s; --- Convergence: 1.50e-05; LR: 1.00e-02; Metric: 0.4145241; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.623 1 + 0.001 value_reward_chosen[t] + 0.089 contr_diff + 0.851 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.753 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.023 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.021 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 44, -, 0, 0, -, -, 44, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 627/1000 --- L(Train): 0.4220119 --- L(Val, RNN): 0.4145512 --- L(Val, SINDy): 0.4150694 --- Time: 0.38s; --- Convergence: 2.11e-05; LR: 1.00e-02; Metric: 0.4145241; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.622 1 + 0.002 value_reward_chosen[t] + 0.087 contr_diff + 0.852 reward + 0.853 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.023 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.021 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 45, -, 0, 0, -, -, 45, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 628/1000 --- L(Train): 0.4188683 --- L(Val, RNN): 0.4145806 --- L(Val, SINDy): 0.4151004 --- Time: 0.30s; --- Convergence: 2.52e-05; LR: 1.00e-02; Metric: 0.4145241; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.623 1 + 0.001 value_reward_chosen[t] + 0.086 contr_diff + 0.852 reward + 0.853 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.024 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.022 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 46, -, 0, 0, -, -, 46, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 629/1000 --- L(Train): 0.4172464 --- L(Val, RNN): 0.4145791 --- L(Val, SINDy): 0.4151218 --- Time: 0.30s; --- Convergence: 1.34e-05; LR: 1.00e-02; Metric: 0.4145241; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.623 1 + 0.0 value_reward_chosen[t] + 0.084 contr_diff + 0.852 reward + 0.853 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.761 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.025 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.023 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 47, -, 0, 0, -, -, 47, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 630/1000 --- L(Train): 0.4043547 --- L(Val, RNN): 0.4145039 --- L(Val, SINDy): 0.4151427 --- Time: 0.29s; --- Convergence: 4.43e-05; LR: 1.00e-02; Metric: 0.4145039; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.622 1 + 0.0 value_reward_chosen[t] + 0.083 contr_diff + 0.853 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.76 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 48, -, 0, 0, -, -, 48, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 631/1000 --- L(Train): 0.4148463 --- L(Val, RNN): 0.4145200 --- L(Val, SINDy): 0.4151574 --- Time: 0.27s; --- Convergence: 3.02e-05; LR: 1.00e-02; Metric: 0.4145039; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.621 1 + 0.0 value_reward_chosen[t] + 0.081 contr_diff + 0.854 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.027 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 49, -, 0, 0, -, -, 49, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 632/1000 --- L(Train): 0.4122895 --- L(Val, RNN): 0.4145884 --- L(Val, SINDy): 0.4151686 --- Time: 0.33s; --- Convergence: 4.93e-05; LR: 1.00e-02; Metric: 0.4145039; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.621 1 + 0.0 value_reward_chosen[t] + 0.079 contr_diff + 0.854 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.027 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.025 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 50, -, 0, 0, -, -, 50, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 633/1000 --- L(Train): 0.4186144 --- L(Val, RNN): 0.4146189 --- L(Val, SINDy): 0.4151838 --- Time: 0.34s; --- Convergence: 3.99e-05; LR: 1.00e-02; Metric: 0.4145039; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.622 1 + -0.001 value_reward_chosen[t] + 0.077 contr_diff + 0.853 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.028 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.026 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 51, -, 0, 0, -, -, 51, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 634/1000 --- L(Train): 0.4060456 --- L(Val, RNN): 0.4146162 --- L(Val, SINDy): 0.4152232 --- Time: 0.30s; --- Convergence: 2.13e-05; LR: 1.00e-02; Metric: 0.4145039; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.623 1 + -0.001 value_reward_chosen[t] + 0.076 contr_diff + 0.853 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.46 1 + 0.761 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.028 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.026 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 52, -, 0, 0, -, -, 52, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 635/1000 --- L(Train): 0.4163080 --- L(Val, RNN): 0.4145815 --- L(Val, SINDy): 0.4152639 --- Time: 0.30s; --- Convergence: 2.80e-05; LR: 1.00e-02; Metric: 0.4145039; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.623 1 + -0.001 value_reward_chosen[t] + 0.075 contr_diff + 0.853 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.765 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.029 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.027 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 53, -, 0, 0, -, -, 53, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 636/1000 --- L(Train): 0.4166538 --- L(Val, RNN): 0.4145513 --- L(Val, SINDy): 0.4152544 --- Time: 0.32s; --- Convergence: 2.91e-05; LR: 1.00e-02; Metric: 0.4145039; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.621 1 + -0.0 value_reward_chosen[t] + 0.074 contr_diff + 0.855 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.772 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.029 contr_diff + 0.053 value_choice^2 + 1.251 value_choice*contr_diff + -0.027 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 54, -, 0, 0, -, -, 54, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 637/1000 --- L(Train): 0.4066336 --- L(Val, RNN): 0.4145690 --- L(Val, SINDy): 0.4152179 --- Time: 0.32s; --- Convergence: 2.34e-05; LR: 1.00e-02; Metric: 0.4145039; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.619 1 + 0.001 value_reward_chosen[t] + 0.076 contr_diff + 0.856 reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.457 1 + 0.778 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.029 contr_diff + 0.053 value_choice^2 + 1.252 value_choice*contr_diff + -0.027 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 55, -, 0, 0, -, -, 55, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 638/1000 --- L(Train): 0.4128480 --- L(Val, RNN): 0.4145376 --- L(Val, SINDy): 0.4152030 --- Time: 0.37s; --- Convergence: 2.74e-05; LR: 1.00e-02; Metric: 0.4145039; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.619 1 + 0.001 value_reward_chosen[t] + 0.077 contr_diff + 0.857 reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.782 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.029 contr_diff + 0.053 value_choice^2 + 1.252 value_choice*contr_diff + -0.027 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 56, -, 0, 0, -, -, 56, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 639/1000 --- L(Train): 0.4089612 --- L(Val, RNN): 0.4144878 --- L(Val, SINDy): 0.4152126 --- Time: 0.32s; --- Convergence: 3.86e-05; LR: 1.00e-02; Metric: 0.4144878; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.621 1 + -0.001 value_reward_chosen[t] + 0.081 contr_diff + 0.855 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.451 1 + 0.782 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.029 contr_diff + 0.053 value_choice^2 + 1.252 value_choice*contr_diff + -0.027 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 57, -, 0, 0, -, -, 57, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 640/1000 --- L(Train): 0.4143989 --- L(Val, RNN): 0.4145337 --- L(Val, SINDy): 0.4151987 --- Time: 0.37s; --- Convergence: 4.22e-05; LR: 1.00e-02; Metric: 0.4144878; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.622 1 + -0.004 value_reward_chosen[t] + 0.085 contr_diff + 0.853 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.447 1 + 0.779 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.029 contr_diff + 0.053 value_choice^2 + 1.252 value_choice*contr_diff + -0.026 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 58, -, 0, 0, -, -, 58, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 641/1000 --- L(Train): 0.4203859 --- L(Val, RNN): 0.4145460 --- L(Val, SINDy): 0.4151613 --- Time: 0.34s; --- Convergence: 2.73e-05; LR: 1.00e-02; Metric: 0.4144878; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.624 1 + -0.005 value_reward_chosen[t] + 0.089 contr_diff + 0.851 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.443 1 + 0.776 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.028 contr_diff + 0.053 value_choice^2 + 1.252 value_choice*contr_diff + -0.026 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 59, -, 0, 0, -, -, 59, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 642/1000 --- L(Train): 0.4184905 --- L(Val, RNN): 0.4145096 --- L(Val, SINDy): 0.4151641 --- Time: 0.35s; --- Convergence: 3.19e-05; LR: 1.00e-02; Metric: 0.4144878; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.624 1 + -0.005 value_reward_chosen[t] + 0.091 contr_diff + 0.851 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.442 1 + 0.772 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.028 contr_diff + 0.053 value_choice^2 + 1.253 value_choice*contr_diff + -0.026 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 60, -, 0, 0, -, -, 60, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 643/1000 --- L(Train): 0.4189897 --- L(Val, RNN): 0.4144690 --- L(Val, SINDy): 0.4151953 --- Time: 0.35s; --- Convergence: 3.62e-05; LR: 1.00e-02; Metric: 0.4144690; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.623 1 + -0.004 value_reward_chosen[t] + 0.092 contr_diff + 0.853 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.441 1 + 0.769 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.027 contr_diff + 0.053 value_choice^2 + 1.253 value_choice*contr_diff + -0.025 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 61, -, 0, 0, -, -, 61, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 644/1000 --- L(Train): 0.4213784 --- L(Val, RNN): 0.4144751 --- L(Val, SINDy): 0.4152068 --- Time: 0.36s; --- Convergence: 2.11e-05; LR: 1.00e-02; Metric: 0.4144690; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.62 1 + -0.001 value_reward_chosen[t] + 0.092 contr_diff + 0.856 reward + 0.857 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.443 1 + 0.766 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.253 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 62, -, 0, 0, -, -, 62, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 645/1000 --- L(Train): 0.4186493 --- L(Val, RNN): 0.4144529 --- L(Val, SINDy): 0.4151950 --- Time: 0.35s; --- Convergence: 2.16e-05; LR: 1.00e-02; Metric: 0.4144529; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.62 1 + 0.001 value_reward_chosen[t] + 0.092 contr_diff + 0.857 reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.446 1 + 0.764 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.253 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 63, -, 0, 0, -, -, 63, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 646/1000 --- L(Train): 0.4160251 --- L(Val, RNN): 0.4145499 --- L(Val, SINDy): 0.4152003 --- Time: 0.30s; --- Convergence: 5.93e-05; LR: 1.00e-02; Metric: 0.4144529; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.621 1 + 0.001 value_reward_chosen[t] + 0.092 contr_diff + 0.856 reward + 0.857 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.254 value_choice*contr_diff + -0.023 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 64, -, 0, 0, -, -, 64, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 647/1000 --- L(Train): 0.4152359 --- L(Val, RNN): 0.4145181 --- L(Val, SINDy): 0.4152264 --- Time: 0.29s; --- Convergence: 4.56e-05; LR: 1.00e-02; Metric: 0.4144529; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.623 1 + 0.001 value_reward_chosen[t] + 0.092 contr_diff + 0.855 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.456 1 + 0.765 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.025 contr_diff + 0.053 value_choice^2 + 1.254 value_choice*contr_diff + -0.023 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 65, -, 0, 0, -, -, 65, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 648/1000 --- L(Train): 0.4161804 --- L(Val, RNN): 0.4144863 --- L(Val, SINDy): 0.4152468 --- Time: 0.28s; --- Convergence: 3.87e-05; LR: 1.00e-02; Metric: 0.4144529; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.624 1 + 0.001 value_reward_chosen[t] + 0.091 contr_diff + 0.855 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.766 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.254 value_choice*contr_diff + -0.023 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 66, -, 0, 0, -, -, 66, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 649/1000 --- L(Train): 0.4198283 --- L(Val, RNN): 0.4144790 --- L(Val, SINDy): 0.4152561 --- Time: 0.31s; --- Convergence: 2.30e-05; LR: 1.00e-02; Metric: 0.4144529; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.626 1 + 0.001 value_reward_chosen[t] + 0.091 contr_diff + 0.854 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.767 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.254 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 67, -, 0, 0, -, -, 67, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 650/1000 --- L(Train): 0.4273540 --- L(Val, RNN): 0.4145942 --- L(Val, SINDy): 0.4152236 --- Time: 0.32s; --- Convergence: 6.91e-05; LR: 1.00e-02; Metric: 0.4144529; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.627 1 + 0.001 value_reward_chosen[t] + 0.091 contr_diff + 0.854 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.764 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.254 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 68, -, 0, 0, -, -, 68, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 651/1000 --- L(Train): 0.4118641 --- L(Val, RNN): 0.4145403 --- L(Val, SINDy): 0.4152468 --- Time: 0.28s; --- Convergence: 6.15e-05; LR: 1.00e-02; Metric: 0.4144529; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.628 1 + 0.001 value_reward_chosen[t] + 0.091 contr_diff + 0.854 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.254 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 69, -, 0, 0, -, -, 69, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 652/1000 --- L(Train): 0.4179526 --- L(Val, RNN): 0.4145624 --- L(Val, SINDy): 0.4152874 --- Time: 0.27s; --- Convergence: 4.18e-05; LR: 1.00e-02; Metric: 0.4144529; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.628 1 + 0.002 value_reward_chosen[t] + 0.091 contr_diff + 0.855 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.023 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 70, -, 0, 0, -, -, 70, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 653/1000 --- L(Train): 0.4256687 --- L(Val, RNN): 0.4145406 --- L(Val, SINDy): 0.4153000 --- Time: 0.38s; --- Convergence: 3.18e-05; LR: 1.00e-02; Metric: 0.4144529; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.627 1 + 0.002 value_reward_chosen[t] + 0.09 contr_diff + 0.856 reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.457 1 + 0.751 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.025 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.023 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 71, -, 0, 0, -, -, 71, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 654/1000 --- L(Train): 0.4162883 --- L(Val, RNN): 0.4145129 --- L(Val, SINDy): 0.4153135 --- Time: 0.36s; --- Convergence: 2.98e-05; LR: 1.00e-02; Metric: 0.4144529; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.626 1 + 0.004 value_reward_chosen[t] + 0.089 contr_diff + 0.859 reward + 0.86 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.457 1 + 0.75 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 72, -, 0, 0, -, -, 72, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 655/1000 --- L(Train): 0.4127153 --- L(Val, RNN): 0.4145243 --- L(Val, SINDy): 0.4153569 --- Time: 0.34s; --- Convergence: 2.06e-05; LR: 1.00e-02; Metric: 0.4144529; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.623 1 + 0.006 value_reward_chosen[t] + 0.087 contr_diff + 0.862 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.751 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 73, -, 0, 0, -, -, 73, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 656/1000 --- L(Train): 0.4061089 --- L(Val, RNN): 0.4145162 --- L(Val, SINDy): 0.4153803 --- Time: 0.41s; --- Convergence: 1.43e-05; LR: 1.00e-02; Metric: 0.4144529; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.621 1 + 0.007 value_reward_chosen[t] + 0.085 contr_diff + 0.865 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.026 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.024 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 74, -, 0, 0, -, -, 74, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 657/1000 --- L(Train): 0.4134158 --- L(Val, RNN): 0.4144404 --- L(Val, SINDy): 0.4154035 --- Time: 0.28s; --- Convergence: 4.51e-05; LR: 1.00e-02; Metric: 0.4144404; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.621 1 + 0.005 value_reward_chosen[t] + 0.085 contr_diff + 0.865 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.466 1 + 0.76 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.027 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.025 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 75, -, 0, 0, -, -, 75, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 658/1000 --- L(Train): 0.4089845 --- L(Val, RNN): 0.4144263 --- L(Val, SINDy): 0.4153943 --- Time: 0.34s; --- Convergence: 2.96e-05; LR: 1.00e-02; Metric: 0.4144263; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.622 1 + 0.003 value_reward_chosen[t] + 0.085 contr_diff + 0.865 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.765 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.028 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.025 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 76, -, 0, 0, -, -, 76, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 659/1000 --- L(Train): 0.4064462 --- L(Val, RNN): 0.4144829 --- L(Val, SINDy): 0.4153352 --- Time: 0.32s; --- Convergence: 4.31e-05; LR: 1.00e-02; Metric: 0.4144263; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.62 1 + 0.002 value_reward_chosen[t] + 0.085 contr_diff + 0.866 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.77 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.028 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.026 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 77, -, 0, 0, -, -, 77, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 660/1000 --- L(Train): 0.4107775 --- L(Val, RNN): 0.4144992 --- L(Val, SINDy): 0.4152761 --- Time: 0.30s; --- Convergence: 2.97e-05; LR: 1.00e-02; Metric: 0.4144263; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.618 1 + 0.001 value_reward_chosen[t] + 0.085 contr_diff + 0.868 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.773 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.029 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.027 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 78, -, 0, 0, -, -, 78, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 661/1000 --- L(Train): 0.4149955 --- L(Val, RNN): 0.4144342 --- L(Val, SINDy): 0.4152345 --- Time: 0.29s; --- Convergence: 4.73e-05; LR: 1.00e-02; Metric: 0.4144263; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.616 1 + 0.001 value_reward_chosen[t] + 0.085 contr_diff + 0.869 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.773 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.03 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.028 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 79, -, 0, 0, -, -, 79, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 662/1000 --- L(Train): 0.4114609 --- L(Val, RNN): 0.4144161 --- L(Val, SINDy): 0.4151723 --- Time: 0.30s; --- Convergence: 3.27e-05; LR: 1.00e-02; Metric: 0.4144161; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.614 1 + -0.001 value_reward_chosen[t] + 0.086 contr_diff + 0.87 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.772 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.03 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.028 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 80, -, 0, 0, -, -, 80, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 663/1000 --- L(Train): 0.4135306 --- L(Val, RNN): 0.4143925 --- L(Val, SINDy): 0.4151249 --- Time: 0.33s; --- Convergence: 2.81e-05; LR: 1.00e-02; Metric: 0.4143925; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.613 1 + -0.003 value_reward_chosen[t] + 0.086 contr_diff + 0.87 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.457 1 + 0.77 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.031 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.029 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 81, -, 0, 0, -, -, 81, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 664/1000 --- L(Train): 0.4189461 --- L(Val, RNN): 0.4144165 --- L(Val, SINDy): 0.4151134 --- Time: 0.36s; --- Convergence: 2.60e-05; LR: 1.00e-02; Metric: 0.4143925; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.612 1 + -0.005 value_reward_chosen[t] + 0.087 contr_diff + 0.869 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.452 1 + 0.767 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.032 contr_diff + 0.053 value_choice^2 + 1.255 value_choice*contr_diff + -0.03 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 82, -, 0, 0, -, -, 82, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 665/1000 --- L(Train): 0.4139202 --- L(Val, RNN): 0.4143984 --- L(Val, SINDy): 0.4151056 --- Time: 0.34s; --- Convergence: 2.20e-05; LR: 1.00e-02; Metric: 0.4143925; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.61 1 + -0.005 value_reward_chosen[t] + 0.087 contr_diff + 0.869 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.449 1 + 0.766 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.033 contr_diff + 0.053 value_choice^2 + 1.256 value_choice*contr_diff + -0.031 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 83, -, 0, 0, -, -, 83, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 666/1000 --- L(Train): 0.4108900 --- L(Val, RNN): 0.4144065 --- L(Val, SINDy): 0.4150927 --- Time: 0.32s; --- Convergence: 1.51e-05; LR: 1.00e-02; Metric: 0.4143925; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.609 1 + -0.006 value_reward_chosen[t] + 0.088 contr_diff + 0.869 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.448 1 + 0.767 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.033 contr_diff + 0.053 value_choice^2 + 1.256 value_choice*contr_diff + -0.031 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 84, -, 0, 0, -, -, 84, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 667/1000 --- L(Train): 0.4169927 --- L(Val, RNN): 0.4143809 --- L(Val, SINDy): 0.4150937 --- Time: 0.31s; --- Convergence: 2.03e-05; LR: 1.00e-02; Metric: 0.4143809; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.61 1 + -0.008 value_reward_chosen[t] + 0.089 contr_diff + 0.866 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.449 1 + 0.77 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.034 contr_diff + 0.053 value_choice^2 + 1.256 value_choice*contr_diff + -0.032 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 85, -, 0, 0, -, -, 85, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 668/1000 --- L(Train): 0.4094769 --- L(Val, RNN): 0.4143959 --- L(Val, SINDy): 0.4150706 --- Time: 0.27s; --- Convergence: 1.77e-05; LR: 1.00e-02; Metric: 0.4143809; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.609 1 + -0.007 value_reward_chosen[t] + 0.09 contr_diff + 0.865 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.452 1 + 0.774 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.034 contr_diff + 0.053 value_choice^2 + 1.256 value_choice*contr_diff + -0.032 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 86, -, 0, 0, -, -, 86, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 669/1000 --- L(Train): 0.4203874 --- L(Val, RNN): 0.4143955 --- L(Val, SINDy): 0.4150470 --- Time: 0.33s; --- Convergence: 9.04e-06; LR: 1.00e-02; Metric: 0.4143809; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.607 1 + -0.004 value_reward_chosen[t] + 0.091 contr_diff + 0.866 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.456 1 + 0.778 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.034 contr_diff + 0.053 value_choice^2 + 1.256 value_choice*contr_diff + -0.032 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 87, -, 0, 0, -, -, 87, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 670/1000 --- L(Train): 0.4197680 --- L(Val, RNN): 0.4143348 --- L(Val, SINDy): 0.4150646 --- Time: 0.31s; --- Convergence: 3.49e-05; LR: 1.00e-02; Metric: 0.4143348; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.606 1 + -0.002 value_reward_chosen[t] + 0.092 contr_diff + 0.865 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.781 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.034 contr_diff + 0.053 value_choice^2 + 1.256 value_choice*contr_diff + -0.032 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 88, -, 0, 0, -, -, 88, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 671/1000 --- L(Train): 0.4166911 --- L(Val, RNN): 0.4143318 --- L(Val, SINDy): 0.4150582 --- Time: 0.26s; --- Convergence: 1.89e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.605 1 + 0.001 value_reward_chosen[t] + 0.092 contr_diff + 0.865 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.783 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.034 contr_diff + 0.053 value_choice^2 + 1.257 value_choice*contr_diff + -0.032 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 89, -, 0, 0, -, -, 89, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 672/1000 --- L(Train): 0.4093066 --- L(Val, RNN): 0.4143987 --- L(Val, SINDy): 0.4150175 --- Time: 0.27s; --- Convergence: 4.29e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.604 1 + 0.003 value_reward_chosen[t] + 0.092 contr_diff + 0.865 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.784 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.034 contr_diff + 0.053 value_choice^2 + 1.257 value_choice*contr_diff + -0.032 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 90, -, 0, 0, -, -, 90, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 673/1000 --- L(Train): 0.4059660 --- L(Val, RNN): 0.4144253 --- L(Val, SINDy): 0.4150091 --- Time: 0.32s; --- Convergence: 3.47e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.605 1 + 0.003 value_reward_chosen[t] + 0.091 contr_diff + 0.864 reward + 0.865 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.781 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.033 contr_diff + 0.053 value_choice^2 + 1.257 value_choice*contr_diff + -0.031 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 91, -, 0, 0, -, -, 91, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 674/1000 --- L(Train): 0.4228629 --- L(Val, RNN): 0.4143948 --- L(Val, SINDy): 0.4150229 --- Time: 0.33s; --- Convergence: 3.26e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.605 1 + 0.003 value_reward_chosen[t] + 0.089 contr_diff + 0.862 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.775 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.032 contr_diff + 0.053 value_choice^2 + 1.258 value_choice*contr_diff + -0.03 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 92, -, 0, 0, -, -, 92, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 675/1000 --- L(Train): 0.4225545 --- L(Val, RNN): 0.4143464 --- L(Val, SINDy): 0.4150310 --- Time: 0.30s; --- Convergence: 4.05e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.604 1 + 0.003 value_reward_chosen[t] + 0.086 contr_diff + 0.862 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.77 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.031 contr_diff + 0.053 value_choice^2 + 1.258 value_choice*contr_diff + -0.029 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 93, -, 0, 0, -, -, 93, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 676/1000 --- L(Train): 0.4168017 --- L(Val, RNN): 0.4143656 --- L(Val, SINDy): 0.4150529 --- Time: 0.37s; --- Convergence: 2.99e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.603 1 + 0.003 value_reward_chosen[t] + 0.085 contr_diff + 0.862 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.765 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.031 contr_diff + 0.053 value_choice^2 + 1.258 value_choice*contr_diff + -0.029 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 94, -, 0, 0, -, -, 94, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 677/1000 --- L(Train): 0.4157332 --- L(Val, RNN): 0.4143942 --- L(Val, SINDy): 0.4150598 --- Time: 0.35s; --- Convergence: 2.92e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.603 1 + 0.003 value_reward_chosen[t] + 0.085 contr_diff + 0.862 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.452 1 + 0.761 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.03 contr_diff + 0.053 value_choice^2 + 1.259 value_choice*contr_diff + -0.028 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 95, -, 0, 0, -, -, 95, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 678/1000 --- L(Train): 0.4202112 --- L(Val, RNN): 0.4143791 --- L(Val, SINDy): 0.4150306 --- Time: 0.30s; --- Convergence: 2.22e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.603 1 + 0.002 value_reward_chosen[t] + 0.084 contr_diff + 0.862 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.451 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.03 contr_diff + 0.053 value_choice^2 + 1.259 value_choice*contr_diff + -0.028 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 96, -, 0, 0, -, -, 96, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 679/1000 --- L(Train): 0.4101367 --- L(Val, RNN): 0.4143666 --- L(Val, SINDy): 0.4150056 --- Time: 0.34s; --- Convergence: 1.73e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.603 1 + 0.001 value_reward_chosen[t] + 0.085 contr_diff + 0.861 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.029 contr_diff + 0.053 value_choice^2 + 1.259 value_choice*contr_diff + -0.027 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 97, -, 0, 0, -, -, 97, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 680/1000 --- L(Train): 0.4193051 --- L(Val, RNN): 0.4143919 --- L(Val, SINDy): 0.4149893 --- Time: 0.31s; --- Convergence: 2.13e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.603 1 + 0.001 value_reward_chosen[t] + 0.085 contr_diff + 0.861 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.457 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.029 contr_diff + 0.053 value_choice^2 + 1.259 value_choice*contr_diff + -0.027 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 98, -, 0, 0, -, -, 98, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 681/1000 --- L(Train): 0.4248534 --- L(Val, RNN): 0.4143593 --- L(Val, SINDy): 0.4149678 --- Time: 0.39s; --- Convergence: 2.69e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.603 1 + -0.0 value_reward_chosen[t] + 0.086 contr_diff + 0.861 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.761 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.029 contr_diff + 0.053 value_choice^2 + 1.259 value_choice*contr_diff + -0.027 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 99, -, 0, 0, -, -, 99, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 682/1000 --- L(Train): 0.4218466 --- L(Val, RNN): 0.4143623 --- L(Val, SINDy): 0.4149762 --- Time: 0.35s; --- Convergence: 1.49e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.602 1 + 0.0 value_reward_chosen[t] + 0.086 contr_diff + 0.861 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.03 contr_diff + 0.053 value_choice^2 + 1.26 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, 100, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 683/1000 --- L(Train): 0.4267823 --- L(Val, RNN): 0.4143703 --- L(Val, SINDy): 0.4150244 --- Time: 0.29s; --- Convergence: 1.15e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.601 1 + 0.001 value_reward_chosen[t] + 0.087 contr_diff + 0.862 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.765 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.26 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 684/1000 --- L(Train): 0.4145952 --- L(Val, RNN): 0.4143732 --- L(Val, SINDy): 0.4151072 --- Time: 0.31s; --- Convergence: 7.19e-06; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 13/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + 0.0 value_reward_chosen[t] + 0.088 contr_diff + 0.863 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.259 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 685/1000 --- L(Train): 0.4124440 --- L(Val, RNN): 0.4144244 --- L(Val, SINDy): 0.4151703 --- Time: 0.49s; --- Convergence: 2.92e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 14/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + 0.0 value_reward_chosen[t] + 0.089 contr_diff + 0.863 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.761 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.259 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 686/1000 --- L(Train): 0.4072215 --- L(Val, RNN): 0.4144192 --- L(Val, SINDy): 0.4151813 --- Time: 0.40s; --- Convergence: 1.72e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 15/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + -0.0 value_reward_chosen[t] + 0.09 contr_diff + 0.863 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.259 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 687/1000 --- L(Train): 0.4223368 --- L(Val, RNN): 0.4144067 --- L(Val, SINDy): 0.4151807 --- Time: 0.31s; --- Convergence: 1.48e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 16/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + -0.0 value_reward_chosen[t] + 0.089 contr_diff + 0.863 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.258 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 688/1000 --- L(Train): 0.4240915 --- L(Val, RNN): 0.4143794 --- L(Val, SINDy): 0.4152047 --- Time: 0.37s; --- Convergence: 2.11e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 17/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + -0.001 value_reward_chosen[t] + 0.089 contr_diff + 0.863 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.258 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 689/1000 --- L(Train): 0.4157075 --- L(Val, RNN): 0.4144392 --- L(Val, SINDy): 0.4151944 --- Time: 0.40s; --- Convergence: 4.05e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 18/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + -0.001 value_reward_chosen[t] + 0.088 contr_diff + 0.863 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.258 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 690/1000 --- L(Train): 0.4196719 --- L(Val, RNN): 0.4144473 --- L(Val, SINDy): 0.4151820 --- Time: 0.41s; --- Convergence: 2.42e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 19/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.601 1 + -0.001 value_reward_chosen[t] + 0.088 contr_diff + 0.863 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.257 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 691/1000 --- L(Train): 0.4102399 --- L(Val, RNN): 0.4144521 --- L(Val, SINDy): 0.4151919 --- Time: 0.32s; --- Convergence: 1.45e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 20/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.603 1 + -0.001 value_reward_chosen[t] + 0.088 contr_diff + 0.862 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.46 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.257 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 692/1000 --- L(Train): 0.4047305 --- L(Val, RNN): 0.4144771 --- L(Val, SINDy): 0.4151631 --- Time: 0.30s; --- Convergence: 1.97e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 21/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.603 1 + -0.0 value_reward_chosen[t] + 0.088 contr_diff + 0.863 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.46 1 + 0.76 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.256 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 693/1000 --- L(Train): 0.4132493 --- L(Val, RNN): 0.4144420 --- L(Val, SINDy): 0.4151112 --- Time: 0.29s; --- Convergence: 2.74e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 22/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.602 1 + 0.001 value_reward_chosen[t] + 0.088 contr_diff + 0.864 reward + 0.865 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.46 1 + 0.764 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.256 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 694/1000 --- L(Train): 0.4069027 --- L(Val, RNN): 0.4144194 --- L(Val, SINDy): 0.4151084 --- Time: 0.44s; --- Convergence: 2.50e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 23/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.601 1 + 0.002 value_reward_chosen[t] + 0.088 contr_diff + 0.865 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.767 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.256 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 695/1000 --- L(Train): 0.4127643 --- L(Val, RNN): 0.4143895 --- L(Val, SINDy): 0.4150981 --- Time: 0.32s; --- Convergence: 2.74e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 24/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.599 1 + 0.004 value_reward_chosen[t] + 0.087 contr_diff + 0.868 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.768 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.256 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 696/1000 --- L(Train): 0.4130437 --- L(Val, RNN): 0.4143421 --- L(Val, SINDy): 0.4150980 --- Time: 0.30s; --- Convergence: 3.74e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 25/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.597 1 + 0.006 value_reward_chosen[t] + 0.084 contr_diff + 0.871 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.768 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.255 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 697/1000 --- L(Train): 0.4154610 --- L(Val, RNN): 0.4144312 --- L(Val, SINDy): 0.4151241 --- Time: 0.29s; --- Convergence: 6.33e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 26/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.005 value_reward_chosen[t] + 0.082 contr_diff + 0.872 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.767 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.255 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 698/1000 --- L(Train): 0.4160888 --- L(Val, RNN): 0.4144680 --- L(Val, SINDy): 0.4151202 --- Time: 0.38s; --- Convergence: 5.00e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 27/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.003 value_reward_chosen[t] + 0.08 contr_diff + 0.873 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.766 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.255 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 699/1000 --- L(Train): 0.4124263 --- L(Val, RNN): 0.4143682 --- L(Val, SINDy): 0.4150878 --- Time: 0.29s; --- Convergence: 7.49e-05; LR: 1.00e-02; Metric: 0.4143318; Bad epochs: 28/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.597 1 + -0.001 value_reward_chosen[t] + 0.078 contr_diff + 0.871 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.765 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.255 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 700/1000 --- L(Train): 0.4125941 --- L(Val, RNN): 0.4143162 --- L(Val, SINDy): 0.4150602 --- Time: 0.27s; --- Convergence: 6.34e-05; LR: 1.00e-02; Metric: 0.4143162; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.599 1 + -0.005 value_reward_chosen[t] + 0.078 contr_diff + 0.869 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.765 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.254 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 701/1000 --- L(Train): 0.4093910 --- L(Val, RNN): 0.4143095 --- L(Val, SINDy): 0.4150594 --- Time: 0.28s; --- Convergence: 3.51e-05; LR: 1.00e-02; Metric: 0.4143095; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + -0.008 value_reward_chosen[t] + 0.078 contr_diff + 0.868 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.254 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 702/1000 --- L(Train): 0.4148596 --- L(Val, RNN): 0.4143969 --- L(Val, SINDy): 0.4150611 --- Time: 0.32s; --- Convergence: 6.12e-05; LR: 1.00e-02; Metric: 0.4143095; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.601 1 + -0.009 value_reward_chosen[t] + 0.079 contr_diff + 0.867 reward + 0.868 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.762 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.254 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 703/1000 --- L(Train): 0.4237384 --- L(Val, RNN): 0.4144717 --- L(Val, SINDy): 0.4150823 --- Time: 0.31s; --- Convergence: 6.80e-05; LR: 1.00e-02; Metric: 0.4143095; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.601 1 + -0.009 value_reward_chosen[t] + 0.08 contr_diff + 0.867 reward + 0.868 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.761 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.254 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 704/1000 --- L(Train): 0.4102046 --- L(Val, RNN): 0.4144397 --- L(Val, SINDy): 0.4150829 --- Time: 0.31s; --- Convergence: 5.01e-05; LR: 1.00e-02; Metric: 0.4143095; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + -0.008 value_reward_chosen[t] + 0.08 contr_diff + 0.869 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.761 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.254 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 705/1000 --- L(Train): 0.4124605 --- L(Val, RNN): 0.4143062 --- L(Val, SINDy): 0.4150783 --- Time: 0.35s; --- Convergence: 9.18e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + -0.004 value_reward_chosen[t] + 0.079 contr_diff + 0.871 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.254 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 706/1000 --- L(Train): 0.4156423 --- L(Val, RNN): 0.4143260 --- L(Val, SINDy): 0.4150814 --- Time: 0.30s; --- Convergence: 5.58e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + -0.001 value_reward_chosen[t] + 0.079 contr_diff + 0.873 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.764 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.254 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 707/1000 --- L(Train): 0.4126279 --- L(Val, RNN): 0.4143333 --- L(Val, SINDy): 0.4150774 --- Time: 0.36s; --- Convergence: 3.15e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + 0.002 value_reward_chosen[t] + 0.079 contr_diff + 0.874 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 708/1000 --- L(Train): 0.4174176 --- L(Val, RNN): 0.4143401 --- L(Val, SINDy): 0.4150561 --- Time: 0.29s; --- Convergence: 1.92e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + 0.004 value_reward_chosen[t] + 0.079 contr_diff + 0.875 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.761 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 709/1000 --- L(Train): 0.4212249 --- L(Val, RNN): 0.4143483 --- L(Val, SINDy): 0.4150626 --- Time: 0.33s; --- Convergence: 1.37e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + 0.005 value_reward_chosen[t] + 0.081 contr_diff + 0.874 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 710/1000 --- L(Train): 0.4148837 --- L(Val, RNN): 0.4143800 --- L(Val, SINDy): 0.4150805 --- Time: 0.45s; --- Convergence: 2.27e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.005 value_reward_chosen[t] + 0.083 contr_diff + 0.874 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 711/1000 --- L(Train): 0.4077701 --- L(Val, RNN): 0.4143362 --- L(Val, SINDy): 0.4150736 --- Time: 0.30s; --- Convergence: 3.32e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.005 value_reward_chosen[t] + 0.085 contr_diff + 0.874 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 712/1000 --- L(Train): 0.4029799 --- L(Val, RNN): 0.4143589 --- L(Val, SINDy): 0.4150634 --- Time: 0.28s; --- Convergence: 2.79e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.597 1 + 0.004 value_reward_chosen[t] + 0.087 contr_diff + 0.873 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.753 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 713/1000 --- L(Train): 0.4240961 --- L(Val, RNN): 0.4143632 --- L(Val, SINDy): 0.4150830 --- Time: 0.33s; --- Convergence: 1.61e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + 0.002 value_reward_chosen[t] + 0.089 contr_diff + 0.872 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.466 1 + 0.751 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 714/1000 --- L(Train): 0.4133571 --- L(Val, RNN): 0.4143260 --- L(Val, SINDy): 0.4151240 --- Time: 0.38s; --- Convergence: 2.66e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + 0.0 value_reward_chosen[t] + 0.089 contr_diff + 0.87 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.749 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 715/1000 --- L(Train): 0.4141923 --- L(Val, RNN): 0.4143417 --- L(Val, SINDy): 0.4151499 --- Time: 0.39s; --- Convergence: 2.11e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.599 1 + -0.001 value_reward_chosen[t] + 0.089 contr_diff + 0.87 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 716/1000 --- L(Train): 0.4128635 --- L(Val, RNN): 0.4143663 --- L(Val, SINDy): 0.4151815 --- Time: 0.42s; --- Convergence: 2.29e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.599 1 + -0.0 value_reward_chosen[t] + 0.088 contr_diff + 0.871 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 717/1000 --- L(Train): 0.4181770 --- L(Val, RNN): 0.4143393 --- L(Val, SINDy): 0.4152148 --- Time: 0.43s; --- Convergence: 2.49e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + -0.0 value_reward_chosen[t] + 0.087 contr_diff + 0.872 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 718/1000 --- L(Train): 0.4025942 --- L(Val, RNN): 0.4143483 --- L(Val, SINDy): 0.4152241 --- Time: 0.30s; --- Convergence: 1.70e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 13/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + -0.0 value_reward_chosen[t] + 0.086 contr_diff + 0.872 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 719/1000 --- L(Train): 0.4180209 --- L(Val, RNN): 0.4143349 --- L(Val, SINDy): 0.4151863 --- Time: 0.32s; --- Convergence: 1.52e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 14/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + -0.001 value_reward_chosen[t] + 0.085 contr_diff + 0.873 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.741 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 720/1000 --- L(Train): 0.4175406 --- L(Val, RNN): 0.4143099 --- L(Val, SINDy): 0.4151514 --- Time: 0.36s; --- Convergence: 2.01e-05; LR: 1.00e-02; Metric: 0.4143062; Bad epochs: 15/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + -0.0 value_reward_chosen[t] + 0.085 contr_diff + 0.873 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.739 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.253 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 721/1000 --- L(Train): 0.4151908 --- L(Val, RNN): 0.4143045 --- L(Val, SINDy): 0.4151095 --- Time: 0.37s; --- Convergence: 1.27e-05; LR: 1.00e-02; Metric: 0.4143045; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + 0.0 value_reward_chosen[t] + 0.084 contr_diff + 0.874 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.737 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 722/1000 --- L(Train): 0.4082408 --- L(Val, RNN): 0.4143599 --- L(Val, SINDy): 0.4150333 --- Time: 0.40s; --- Convergence: 3.41e-05; LR: 1.00e-02; Metric: 0.4143045; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.597 1 + 0.001 value_reward_chosen[t] + 0.083 contr_diff + 0.875 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.484 1 + 0.735 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 723/1000 --- L(Train): 0.4030611 --- L(Val, RNN): 0.4144012 --- L(Val, SINDy): 0.4149653 --- Time: 0.39s; --- Convergence: 3.77e-05; LR: 1.00e-02; Metric: 0.4143045; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.597 1 + 0.0 value_reward_chosen[t] + 0.084 contr_diff + 0.875 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.487 1 + 0.734 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 724/1000 --- L(Train): 0.4063942 --- L(Val, RNN): 0.4143082 --- L(Val, SINDy): 0.4149336 --- Time: 0.33s; --- Convergence: 6.54e-05; LR: 1.00e-02; Metric: 0.4143045; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.599 1 + -0.001 value_reward_chosen[t] + 0.084 contr_diff + 0.874 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.49 1 + 0.733 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 725/1000 --- L(Train): 0.4205081 --- L(Val, RNN): 0.4142809 --- L(Val, SINDy): 0.4149523 --- Time: 0.47s; --- Convergence: 4.63e-05; LR: 1.00e-02; Metric: 0.4142809; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + -0.003 value_reward_chosen[t] + 0.085 contr_diff + 0.873 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.492 1 + 0.733 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 726/1000 --- L(Train): 0.4199339 --- L(Val, RNN): 0.4143181 --- L(Val, SINDy): 0.4149884 --- Time: 0.43s; --- Convergence: 4.18e-05; LR: 1.00e-02; Metric: 0.4142809; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.601 1 + -0.003 value_reward_chosen[t] + 0.085 contr_diff + 0.873 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.492 1 + 0.731 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 727/1000 --- L(Train): 0.4163687 --- L(Val, RNN): 0.4143113 --- L(Val, SINDy): 0.4150168 --- Time: 0.32s; --- Convergence: 2.43e-05; LR: 1.00e-02; Metric: 0.4142809; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + -0.002 value_reward_chosen[t] + 0.085 contr_diff + 0.873 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.492 1 + 0.73 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 728/1000 --- L(Train): 0.4128346 --- L(Val, RNN): 0.4144041 --- L(Val, SINDy): 0.4150464 --- Time: 0.36s; --- Convergence: 5.86e-05; LR: 1.00e-02; Metric: 0.4142809; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.601 1 + -0.002 value_reward_chosen[t] + 0.086 contr_diff + 0.873 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.491 1 + 0.728 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 729/1000 --- L(Train): 0.4102033 --- L(Val, RNN): 0.4144219 --- L(Val, SINDy): 0.4150649 --- Time: 0.30s; --- Convergence: 3.82e-05; LR: 1.00e-02; Metric: 0.4142809; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.601 1 + -0.001 value_reward_chosen[t] + 0.086 contr_diff + 0.874 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.491 1 + 0.729 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 730/1000 --- L(Train): 0.3979427 --- L(Val, RNN): 0.4143292 --- L(Val, SINDy): 0.4150743 --- Time: 0.37s; --- Convergence: 6.54e-05; LR: 1.00e-02; Metric: 0.4142809; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + -0.0 value_reward_chosen[t] + 0.086 contr_diff + 0.875 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.492 1 + 0.731 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 731/1000 --- L(Train): 0.4321555 --- L(Val, RNN): 0.4142748 --- L(Val, SINDy): 0.4151030 --- Time: 0.29s; --- Convergence: 5.99e-05; LR: 1.00e-02; Metric: 0.4142748; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.599 1 + 0.001 value_reward_chosen[t] + 0.086 contr_diff + 0.876 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.493 1 + 0.734 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 732/1000 --- L(Train): 0.4076749 --- L(Val, RNN): 0.4142575 --- L(Val, SINDy): 0.4151257 --- Time: 0.31s; --- Convergence: 3.86e-05; LR: 1.00e-02; Metric: 0.4142575; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + 0.002 value_reward_chosen[t] + 0.085 contr_diff + 0.877 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.495 1 + 0.737 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 733/1000 --- L(Train): 0.4088278 --- L(Val, RNN): 0.4142473 --- L(Val, SINDy): 0.4151304 --- Time: 0.32s; --- Convergence: 2.44e-05; LR: 1.00e-02; Metric: 0.4142473; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + 0.002 value_reward_chosen[t] + 0.084 contr_diff + 0.877 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.495 1 + 0.739 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 734/1000 --- L(Train): 0.4097713 --- L(Val, RNN): 0.4143177 --- L(Val, SINDy): 0.4151246 --- Time: 0.31s; --- Convergence: 4.74e-05; LR: 1.00e-02; Metric: 0.4142473; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.599 1 + 0.001 value_reward_chosen[t] + 0.082 contr_diff + 0.876 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.495 1 + 0.741 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 735/1000 --- L(Train): 0.4102746 --- L(Val, RNN): 0.4143956 --- L(Val, SINDy): 0.4151189 --- Time: 0.39s; --- Convergence: 6.27e-05; LR: 1.00e-02; Metric: 0.4142473; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + 0.0 value_reward_chosen[t] + 0.08 contr_diff + 0.876 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.493 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 736/1000 --- L(Train): 0.4136986 --- L(Val, RNN): 0.4143082 --- L(Val, SINDy): 0.4150864 --- Time: 0.33s; --- Convergence: 7.50e-05; LR: 1.00e-02; Metric: 0.4142473; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + 0.001 value_reward_chosen[t] + 0.079 contr_diff + 0.876 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.492 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 737/1000 --- L(Train): 0.4151277 --- L(Val, RNN): 0.4142970 --- L(Val, SINDy): 0.4150699 --- Time: 0.30s; --- Convergence: 4.31e-05; LR: 1.00e-02; Metric: 0.4142473; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.599 1 + 0.001 value_reward_chosen[t] + 0.079 contr_diff + 0.877 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.489 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 738/1000 --- L(Train): 0.4177701 --- L(Val, RNN): 0.4143086 --- L(Val, SINDy): 0.4150698 --- Time: 0.33s; --- Convergence: 2.73e-05; LR: 1.00e-02; Metric: 0.4142473; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + 0.002 value_reward_chosen[t] + 0.08 contr_diff + 0.878 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 739/1000 --- L(Train): 0.4276525 --- L(Val, RNN): 0.4142576 --- L(Val, SINDy): 0.4150605 --- Time: 0.41s; --- Convergence: 3.92e-05; LR: 1.00e-02; Metric: 0.4142473; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + 0.001 value_reward_chosen[t] + 0.082 contr_diff + 0.878 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.482 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 740/1000 --- L(Train): 0.4226484 --- L(Val, RNN): 0.4142448 --- L(Val, SINDy): 0.4150586 --- Time: 0.34s; --- Convergence: 2.60e-05; LR: 1.00e-02; Metric: 0.4142448; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + 0.0 value_reward_chosen[t] + 0.084 contr_diff + 0.878 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 741/1000 --- L(Train): 0.4055231 --- L(Val, RNN): 0.4142771 --- L(Val, SINDy): 0.4150696 --- Time: 0.32s; --- Convergence: 2.92e-05; LR: 1.00e-02; Metric: 0.4142448; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + -0.0 value_reward_chosen[t] + 0.085 contr_diff + 0.878 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.252 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 742/1000 --- L(Train): 0.4217264 --- L(Val, RNN): 0.4143043 --- L(Val, SINDy): 0.4150535 --- Time: 0.38s; --- Convergence: 2.82e-05; LR: 1.00e-02; Metric: 0.4142448; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.597 1 + 0.001 value_reward_chosen[t] + 0.086 contr_diff + 0.879 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 743/1000 --- L(Train): 0.4085832 --- L(Val, RNN): 0.4143012 --- L(Val, SINDy): 0.4150347 --- Time: 0.38s; --- Convergence: 1.57e-05; LR: 1.00e-02; Metric: 0.4142448; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.001 value_reward_chosen[t] + 0.086 contr_diff + 0.879 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.749 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 744/1000 --- L(Train): 0.4216598 --- L(Val, RNN): 0.4142367 --- L(Val, SINDy): 0.4150515 --- Time: 0.32s; --- Convergence: 4.01e-05; LR: 1.00e-02; Metric: 0.4142367; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.0 value_reward_chosen[t] + 0.085 contr_diff + 0.879 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.751 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 745/1000 --- L(Train): 0.4144286 --- L(Val, RNN): 0.4142261 --- L(Val, SINDy): 0.4150635 --- Time: 0.37s; --- Convergence: 2.53e-05; LR: 1.00e-02; Metric: 0.4142261; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.597 1 + -0.001 value_reward_chosen[t] + 0.085 contr_diff + 0.878 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 746/1000 --- L(Train): 0.4109191 --- L(Val, RNN): 0.4142748 --- L(Val, SINDy): 0.4150368 --- Time: 0.30s; --- Convergence: 3.70e-05; LR: 1.00e-02; Metric: 0.4142261; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + -0.001 value_reward_chosen[t] + 0.083 contr_diff + 0.879 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 747/1000 --- L(Train): 0.4221251 --- L(Val, RNN): 0.4142604 --- L(Val, SINDy): 0.4150307 --- Time: 0.34s; --- Convergence: 2.57e-05; LR: 1.00e-02; Metric: 0.4142261; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + -0.002 value_reward_chosen[t] + 0.081 contr_diff + 0.878 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 748/1000 --- L(Train): 0.4062721 --- L(Val, RNN): 0.4142196 --- L(Val, SINDy): 0.4150292 --- Time: 0.35s; --- Convergence: 3.32e-05; LR: 1.00e-02; Metric: 0.4142196; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + -0.003 value_reward_chosen[t] + 0.08 contr_diff + 0.876 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.753 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 749/1000 --- L(Train): 0.4092431 --- L(Val, RNN): 0.4142085 --- L(Val, SINDy): 0.4150113 --- Time: 0.37s; --- Convergence: 2.22e-05; LR: 1.00e-02; Metric: 0.4142085; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.599 1 + -0.004 value_reward_chosen[t] + 0.078 contr_diff + 0.874 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.752 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 750/1000 --- L(Train): 0.4237821 --- L(Val, RNN): 0.4142695 --- L(Val, SINDy): 0.4150148 --- Time: 0.29s; --- Convergence: 4.16e-05; LR: 1.00e-02; Metric: 0.4142085; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.599 1 + -0.004 value_reward_chosen[t] + 0.077 contr_diff + 0.874 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.751 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 751/1000 --- L(Train): 0.4134844 --- L(Val, RNN): 0.4142677 --- L(Val, SINDy): 0.4150547 --- Time: 0.30s; --- Convergence: 2.17e-05; LR: 1.00e-02; Metric: 0.4142085; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.598 1 + -0.001 value_reward_chosen[t] + 0.076 contr_diff + 0.875 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.75 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 752/1000 --- L(Train): 0.4141267 --- L(Val, RNN): 0.4142323 --- L(Val, SINDy): 0.4150459 --- Time: 0.33s; --- Convergence: 2.86e-05; LR: 1.00e-02; Metric: 0.4142085; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + 0.002 value_reward_chosen[t] + 0.075 contr_diff + 0.878 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.749 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 753/1000 --- L(Train): 0.4227146 --- L(Val, RNN): 0.4142451 --- L(Val, SINDy): 0.4150294 --- Time: 0.36s; --- Convergence: 2.07e-05; LR: 1.00e-02; Metric: 0.4142085; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + 0.004 value_reward_chosen[t] + 0.076 contr_diff + 0.881 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.748 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 754/1000 --- L(Train): 0.4161164 --- L(Val, RNN): 0.4142015 --- L(Val, SINDy): 0.4150100 --- Time: 0.28s; --- Convergence: 3.22e-05; LR: 1.00e-02; Metric: 0.4142015; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.006 value_reward_chosen[t] + 0.078 contr_diff + 0.881 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.748 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 755/1000 --- L(Train): 0.4024396 --- L(Val, RNN): 0.4141569 --- L(Val, SINDy): 0.4149675 --- Time: 0.31s; --- Convergence: 3.84e-05; LR: 1.00e-02; Metric: 0.4141569; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.006 value_reward_chosen[t] + 0.08 contr_diff + 0.881 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.748 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 756/1000 --- L(Train): 0.4155816 --- L(Val, RNN): 0.4142213 --- L(Val, SINDy): 0.4149364 --- Time: 0.34s; --- Convergence: 5.14e-05; LR: 1.00e-02; Metric: 0.4141569; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + 0.005 value_reward_chosen[t] + 0.082 contr_diff + 0.881 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.75 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 757/1000 --- L(Train): 0.4163754 --- L(Val, RNN): 0.4141460 --- L(Val, SINDy): 0.4149475 --- Time: 0.32s; --- Convergence: 6.33e-05; LR: 1.00e-02; Metric: 0.4141460; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + 0.004 value_reward_chosen[t] + 0.084 contr_diff + 0.88 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.752 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 758/1000 --- L(Train): 0.4123209 --- L(Val, RNN): 0.4142287 --- L(Val, SINDy): 0.4149628 --- Time: 0.34s; --- Convergence: 7.30e-05; LR: 1.00e-02; Metric: 0.4141460; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + 0.003 value_reward_chosen[t] + 0.086 contr_diff + 0.879 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.753 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 759/1000 --- L(Train): 0.4176846 --- L(Val, RNN): 0.4142472 --- L(Val, SINDy): 0.4149511 --- Time: 0.33s; --- Convergence: 4.58e-05; LR: 1.00e-02; Metric: 0.4141460; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.594 1 + 0.001 value_reward_chosen[t] + 0.088 contr_diff + 0.878 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 760/1000 --- L(Train): 0.4197944 --- L(Val, RNN): 0.4142373 --- L(Val, SINDy): 0.4149683 --- Time: 0.29s; --- Convergence: 2.79e-05; LR: 1.00e-02; Metric: 0.4141460; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + -0.001 value_reward_chosen[t] + 0.088 contr_diff + 0.877 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.251 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 761/1000 --- L(Train): 0.4200502 --- L(Val, RNN): 0.4142968 --- L(Val, SINDy): 0.4150143 --- Time: 0.32s; --- Convergence: 4.37e-05; LR: 1.00e-02; Metric: 0.4141460; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + -0.002 value_reward_chosen[t] + 0.088 contr_diff + 0.877 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.753 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 762/1000 --- L(Train): 0.4189441 --- L(Val, RNN): 0.4142892 --- L(Val, SINDy): 0.4150111 --- Time: 0.41s; --- Convergence: 2.57e-05; LR: 1.00e-02; Metric: 0.4141460; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + -0.001 value_reward_chosen[t] + 0.087 contr_diff + 0.879 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.751 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 763/1000 --- L(Train): 0.4160006 --- L(Val, RNN): 0.4142898 --- L(Val, SINDy): 0.4150043 --- Time: 0.34s; --- Convergence: 1.31e-05; LR: 1.00e-02; Metric: 0.4141460; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + -0.0 value_reward_chosen[t] + 0.086 contr_diff + 0.88 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.749 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 764/1000 --- L(Train): 0.4131565 --- L(Val, RNN): 0.4143001 --- L(Val, SINDy): 0.4150124 --- Time: 0.29s; --- Convergence: 1.17e-05; LR: 1.00e-02; Metric: 0.4141460; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + -0.0 value_reward_chosen[t] + 0.085 contr_diff + 0.88 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.748 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 765/1000 --- L(Train): 0.4152703 --- L(Val, RNN): 0.4143052 --- L(Val, SINDy): 0.4149841 --- Time: 0.33s; --- Convergence: 8.37e-06; LR: 1.00e-02; Metric: 0.4141460; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + -0.0 value_reward_chosen[t] + 0.085 contr_diff + 0.88 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.748 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 766/1000 --- L(Train): 0.4157923 --- L(Val, RNN): 0.4142463 --- L(Val, SINDy): 0.4149449 --- Time: 0.33s; --- Convergence: 3.36e-05; LR: 1.00e-02; Metric: 0.4141460; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.001 value_reward_chosen[t] + 0.084 contr_diff + 0.882 reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 767/1000 --- L(Train): 0.4180223 --- L(Val, RNN): 0.4141369 --- L(Val, SINDy): 0.4149267 --- Time: 0.30s; --- Convergence: 7.15e-05; LR: 1.00e-02; Metric: 0.4141369; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.004 value_reward_chosen[t] + 0.083 contr_diff + 0.884 reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 768/1000 --- L(Train): 0.3990453 --- L(Val, RNN): 0.4141739 --- L(Val, SINDy): 0.4149318 --- Time: 0.32s; --- Convergence: 5.42e-05; LR: 1.00e-02; Metric: 0.4141369; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.586 1 + 0.005 value_reward_chosen[t] + 0.083 contr_diff + 0.886 reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 769/1000 --- L(Train): 0.4006389 --- L(Val, RNN): 0.4142753 --- L(Val, SINDy): 0.4149371 --- Time: 0.29s; --- Convergence: 7.78e-05; LR: 1.00e-02; Metric: 0.4141369; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + 0.006 value_reward_chosen[t] + 0.083 contr_diff + 0.887 reward + 0.888 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 770/1000 --- L(Train): 0.4155637 --- L(Val, RNN): 0.4142035 --- L(Val, SINDy): 0.4149641 --- Time: 0.30s; --- Convergence: 7.48e-05; LR: 1.00e-02; Metric: 0.4141369; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.004 value_reward_chosen[t] + 0.083 contr_diff + 0.885 reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 771/1000 --- L(Train): 0.4101151 --- L(Val, RNN): 0.4141329 --- L(Val, SINDy): 0.4149669 --- Time: 0.30s; --- Convergence: 7.27e-05; LR: 1.00e-02; Metric: 0.4141329; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.001 value_reward_chosen[t] + 0.084 contr_diff + 0.882 reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 772/1000 --- L(Train): 0.4006703 --- L(Val, RNN): 0.4141141 --- L(Val, SINDy): 0.4149667 --- Time: 0.39s; --- Convergence: 4.57e-05; LR: 1.00e-02; Metric: 0.4141141; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + -0.001 value_reward_chosen[t] + 0.083 contr_diff + 0.88 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 773/1000 --- L(Train): 0.4128315 --- L(Val, RNN): 0.4141129 --- L(Val, SINDy): 0.4149975 --- Time: 0.28s; --- Convergence: 2.35e-05; LR: 1.00e-02; Metric: 0.4141129; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + -0.002 value_reward_chosen[t] + 0.082 contr_diff + 0.88 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 774/1000 --- L(Train): 0.4101318 --- L(Val, RNN): 0.4141771 --- L(Val, SINDy): 0.4150239 --- Time: 0.28s; --- Convergence: 4.39e-05; LR: 1.00e-02; Metric: 0.4141129; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + -0.002 value_reward_chosen[t] + 0.081 contr_diff + 0.88 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 775/1000 --- L(Train): 0.4173626 --- L(Val, RNN): 0.4141831 --- L(Val, SINDy): 0.4150313 --- Time: 0.33s; --- Convergence: 2.49e-05; LR: 1.00e-02; Metric: 0.4141129; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.59 1 + -0.001 value_reward_chosen[t] + 0.079 contr_diff + 0.88 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 776/1000 --- L(Train): 0.4196864 --- L(Val, RNN): 0.4141130 --- L(Val, SINDy): 0.4150330 --- Time: 0.30s; --- Convergence: 4.75e-05; LR: 1.00e-02; Metric: 0.4141129; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.59 1 + -0.0 value_reward_chosen[t] + 0.078 contr_diff + 0.881 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 777/1000 --- L(Train): 0.4183998 --- L(Val, RNN): 0.4141044 --- L(Val, SINDy): 0.4150363 --- Time: 0.39s; --- Convergence: 2.81e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.0 value_reward_chosen[t] + 0.077 contr_diff + 0.881 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.482 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 778/1000 --- L(Train): 0.4131459 --- L(Val, RNN): 0.4141252 --- L(Val, SINDy): 0.4150290 --- Time: 0.30s; --- Convergence: 2.44e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.001 value_reward_chosen[t] + 0.079 contr_diff + 0.882 reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.482 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 779/1000 --- L(Train): 0.4133203 --- L(Val, RNN): 0.4141434 --- L(Val, SINDy): 0.4150032 --- Time: 0.29s; --- Convergence: 2.13e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.001 value_reward_chosen[t] + 0.082 contr_diff + 0.883 reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 780/1000 --- L(Train): 0.4327941 --- L(Val, RNN): 0.4141403 --- L(Val, SINDy): 0.4149886 --- Time: 0.36s; --- Convergence: 1.22e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.002 value_reward_chosen[t] + 0.084 contr_diff + 0.883 reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 781/1000 --- L(Train): 0.4212423 --- L(Val, RNN): 0.4141377 --- L(Val, SINDy): 0.4149873 --- Time: 0.36s; --- Convergence: 7.38e-06; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.003 value_reward_chosen[t] + 0.086 contr_diff + 0.884 reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.25 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 782/1000 --- L(Train): 0.4203631 --- L(Val, RNN): 0.4141373 --- L(Val, SINDy): 0.4149867 --- Time: 0.40s; --- Convergence: 3.90e-06; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + 0.004 value_reward_chosen[t] + 0.087 contr_diff + 0.886 reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 783/1000 --- L(Train): 0.4050965 --- L(Val, RNN): 0.4141616 --- L(Val, SINDy): 0.4150034 --- Time: 0.31s; --- Convergence: 1.41e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.583 1 + 0.005 value_reward_chosen[t] + 0.089 contr_diff + 0.888 reward + 0.889 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 784/1000 --- L(Train): 0.4212020 --- L(Val, RNN): 0.4141731 --- L(Val, SINDy): 0.4150257 --- Time: 0.37s; --- Convergence: 1.28e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.582 1 + 0.004 value_reward_chosen[t] + 0.089 contr_diff + 0.888 reward + 0.889 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 785/1000 --- L(Train): 0.4328307 --- L(Val, RNN): 0.4141527 --- L(Val, SINDy): 0.4150272 --- Time: 0.32s; --- Convergence: 1.66e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.583 1 + 0.003 value_reward_chosen[t] + 0.088 contr_diff + 0.888 reward + 0.889 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.75 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 786/1000 --- L(Train): 0.4187273 --- L(Val, RNN): 0.4141383 --- L(Val, SINDy): 0.4150262 --- Time: 0.34s; --- Convergence: 1.55e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + 0.0 value_reward_chosen[t] + 0.088 contr_diff + 0.887 reward + 0.888 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 787/1000 --- L(Train): 0.4141783 --- L(Val, RNN): 0.4141233 --- L(Val, SINDy): 0.4150328 --- Time: 0.36s; --- Convergence: 1.53e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.002 value_reward_chosen[t] + 0.087 contr_diff + 0.885 reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 788/1000 --- L(Train): 0.4205089 --- L(Val, RNN): 0.4141246 --- L(Val, SINDy): 0.4150416 --- Time: 0.35s; --- Convergence: 8.28e-06; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.004 value_reward_chosen[t] + 0.087 contr_diff + 0.885 reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.762 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 789/1000 --- L(Train): 0.4220454 --- L(Val, RNN): 0.4141377 --- L(Val, SINDy): 0.4150532 --- Time: 0.35s; --- Convergence: 1.07e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.004 value_reward_chosen[t] + 0.086 contr_diff + 0.885 reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.764 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 790/1000 --- L(Train): 0.4154935 --- L(Val, RNN): 0.4141179 --- L(Val, SINDy): 0.4150497 --- Time: 0.32s; --- Convergence: 1.52e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 13/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.583 1 + -0.004 value_reward_chosen[t] + 0.085 contr_diff + 0.885 reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 791/1000 --- L(Train): 0.4243905 --- L(Val, RNN): 0.4141082 --- L(Val, SINDy): 0.4150308 --- Time: 0.31s; --- Convergence: 1.25e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 14/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.583 1 + -0.004 value_reward_chosen[t] + 0.084 contr_diff + 0.885 reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.76 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 792/1000 --- L(Train): 0.4178481 --- L(Val, RNN): 0.4141618 --- L(Val, SINDy): 0.4150161 --- Time: 0.30s; --- Convergence: 3.30e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 15/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.582 1 + -0.003 value_reward_chosen[t] + 0.083 contr_diff + 0.885 reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.46 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 793/1000 --- L(Train): 0.4102486 --- L(Val, RNN): 0.4141745 --- L(Val, SINDy): 0.4150255 --- Time: 0.29s; --- Convergence: 2.29e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 16/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.582 1 + -0.003 value_reward_chosen[t] + 0.081 contr_diff + 0.885 reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.456 1 + 0.752 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 794/1000 --- L(Train): 0.4118194 --- L(Val, RNN): 0.4141486 --- L(Val, SINDy): 0.4150293 --- Time: 0.37s; --- Convergence: 2.44e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 17/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.582 1 + -0.002 value_reward_chosen[t] + 0.08 contr_diff + 0.885 reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.749 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 795/1000 --- L(Train): 0.4176613 --- L(Val, RNN): 0.4141122 --- L(Val, SINDy): 0.4150171 --- Time: 0.34s; --- Convergence: 3.04e-05; LR: 1.00e-02; Metric: 0.4141044; Bad epochs: 18/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.58 1 + 0.0 value_reward_chosen[t] + 0.078 contr_diff + 0.886 reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 796/1000 --- L(Train): 0.4198554 --- L(Val, RNN): 0.4140981 --- L(Val, SINDy): 0.4149997 --- Time: 0.38s; --- Convergence: 2.22e-05; LR: 1.00e-02; Metric: 0.4140981; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.578 1 + 0.002 value_reward_chosen[t] + 0.076 contr_diff + 0.887 reward + 0.888 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 797/1000 --- L(Train): 0.4011064 --- L(Val, RNN): 0.4141276 --- L(Val, SINDy): 0.4149711 --- Time: 0.31s; --- Convergence: 2.59e-05; LR: 1.00e-02; Metric: 0.4140981; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.577 1 + 0.003 value_reward_chosen[t] + 0.076 contr_diff + 0.888 reward + 0.889 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.75 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 798/1000 --- L(Train): 0.4137607 --- L(Val, RNN): 0.4141000 --- L(Val, SINDy): 0.4149407 --- Time: 0.37s; --- Convergence: 2.68e-05; LR: 1.00e-02; Metric: 0.4140981; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.575 1 + 0.003 value_reward_chosen[t] + 0.076 contr_diff + 0.888 reward + 0.889 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.753 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 799/1000 --- L(Train): 0.4200130 --- L(Val, RNN): 0.4140752 --- L(Val, SINDy): 0.4149213 --- Time: 0.40s; --- Convergence: 2.58e-05; LR: 1.00e-02; Metric: 0.4140752; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.575 1 + 0.002 value_reward_chosen[t] + 0.077 contr_diff + 0.888 reward + 0.889 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 800/1000 --- L(Train): 0.4121521 --- L(Val, RNN): 0.4140626 --- L(Val, SINDy): 0.4149189 --- Time: 0.33s; --- Convergence: 1.92e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.575 1 + 0.001 value_reward_chosen[t] + 0.079 contr_diff + 0.887 reward + 0.888 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.759 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 801/1000 --- L(Train): 0.4225310 --- L(Val, RNN): 0.4140948 --- L(Val, SINDy): 0.4149249 --- Time: 0.34s; --- Convergence: 2.57e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.574 1 + -0.0 value_reward_chosen[t] + 0.08 contr_diff + 0.886 reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.76 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 802/1000 --- L(Train): 0.4169487 --- L(Val, RNN): 0.4141788 --- L(Val, SINDy): 0.4149372 --- Time: 0.30s; --- Convergence: 5.49e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.574 1 + -0.001 value_reward_chosen[t] + 0.081 contr_diff + 0.886 reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 803/1000 --- L(Train): 0.4092461 --- L(Val, RNN): 0.4141799 --- L(Val, SINDy): 0.4149648 --- Time: 0.34s; --- Convergence: 2.80e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.574 1 + -0.002 value_reward_chosen[t] + 0.081 contr_diff + 0.885 reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 804/1000 --- L(Train): 0.4110089 --- L(Val, RNN): 0.4141053 --- L(Val, SINDy): 0.4149869 --- Time: 0.33s; --- Convergence: 5.13e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.575 1 + -0.004 value_reward_chosen[t] + 0.081 contr_diff + 0.883 reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.751 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 805/1000 --- L(Train): 0.4063803 --- L(Val, RNN): 0.4140936 --- L(Val, SINDy): 0.4149745 --- Time: 0.34s; --- Convergence: 3.15e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.576 1 + -0.005 value_reward_chosen[t] + 0.081 contr_diff + 0.881 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 806/1000 --- L(Train): 0.4210013 --- L(Val, RNN): 0.4140930 --- L(Val, SINDy): 0.4149647 --- Time: 0.41s; --- Convergence: 1.61e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.577 1 + -0.006 value_reward_chosen[t] + 0.082 contr_diff + 0.88 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 807/1000 --- L(Train): 0.4165053 --- L(Val, RNN): 0.4141113 --- L(Val, SINDy): 0.4149562 --- Time: 0.39s; --- Convergence: 1.72e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.578 1 + -0.007 value_reward_chosen[t] + 0.084 contr_diff + 0.878 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.249 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 808/1000 --- L(Train): 0.4197930 --- L(Val, RNN): 0.4141358 --- L(Val, SINDy): 0.4149307 --- Time: 0.33s; --- Convergence: 2.08e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.579 1 + -0.005 value_reward_chosen[t] + 0.085 contr_diff + 0.878 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 809/1000 --- L(Train): 0.4173199 --- L(Val, RNN): 0.4141022 --- L(Val, SINDy): 0.4149151 --- Time: 0.29s; --- Convergence: 2.72e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.578 1 + -0.003 value_reward_chosen[t] + 0.087 contr_diff + 0.879 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 810/1000 --- L(Train): 0.4164707 --- L(Val, RNN): 0.4141420 --- L(Val, SINDy): 0.4149109 --- Time: 0.31s; --- Convergence: 3.35e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.578 1 + -0.001 value_reward_chosen[t] + 0.088 contr_diff + 0.879 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 811/1000 --- L(Train): 0.4039170 --- L(Val, RNN): 0.4141664 --- L(Val, SINDy): 0.4148990 --- Time: 0.31s; --- Convergence: 2.89e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.579 1 + 0.001 value_reward_chosen[t] + 0.09 contr_diff + 0.88 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 812/1000 --- L(Train): 0.4180668 --- L(Val, RNN): 0.4141038 --- L(Val, SINDy): 0.4148899 --- Time: 0.33s; --- Convergence: 4.57e-05; LR: 1.00e-02; Metric: 0.4140626; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.581 1 + 0.001 value_reward_chosen[t] + 0.091 contr_diff + 0.878 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 813/1000 --- L(Train): 0.4119398 --- L(Val, RNN): 0.4140611 --- L(Val, SINDy): 0.4148844 --- Time: 0.34s; --- Convergence: 4.42e-05; LR: 1.00e-02; Metric: 0.4140611; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + 0.001 value_reward_chosen[t] + 0.091 contr_diff + 0.877 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 814/1000 --- L(Train): 0.4140193 --- L(Val, RNN): 0.4140606 --- L(Val, SINDy): 0.4148781 --- Time: 0.38s; --- Convergence: 2.24e-05; LR: 1.00e-02; Metric: 0.4140606; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + 0.001 value_reward_chosen[t] + 0.088 contr_diff + 0.877 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 815/1000 --- L(Train): 0.4261509 --- L(Val, RNN): 0.4140522 --- L(Val, SINDy): 0.4148768 --- Time: 0.41s; --- Convergence: 1.54e-05; LR: 1.00e-02; Metric: 0.4140522; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.002 value_reward_chosen[t] + 0.086 contr_diff + 0.877 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.484 1 + 0.741 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 816/1000 --- L(Train): 0.4038909 --- L(Val, RNN): 0.4140694 --- L(Val, SINDy): 0.4148900 --- Time: 0.33s; --- Convergence: 1.63e-05; LR: 1.00e-02; Metric: 0.4140522; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.004 value_reward_chosen[t] + 0.083 contr_diff + 0.878 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.737 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 817/1000 --- L(Train): 0.4187738 --- L(Val, RNN): 0.4141354 --- L(Val, SINDy): 0.4148861 --- Time: 0.32s; --- Convergence: 4.12e-05; LR: 1.00e-02; Metric: 0.4140522; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.005 value_reward_chosen[t] + 0.081 contr_diff + 0.881 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.734 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 818/1000 --- L(Train): 0.4249877 --- L(Val, RNN): 0.4141459 --- L(Val, SINDy): 0.4148687 --- Time: 0.36s; --- Convergence: 2.59e-05; LR: 1.00e-02; Metric: 0.4140522; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.006 value_reward_chosen[t] + 0.081 contr_diff + 0.883 reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.733 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 819/1000 --- L(Train): 0.4156453 --- L(Val, RNN): 0.4140783 --- L(Val, SINDy): 0.4148721 --- Time: 0.29s; --- Convergence: 4.68e-05; LR: 1.00e-02; Metric: 0.4140522; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.082 contr_diff + 0.884 reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.733 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 820/1000 --- L(Train): 0.4151708 --- L(Val, RNN): 0.4140301 --- L(Val, SINDy): 0.4148930 --- Time: 0.34s; --- Convergence: 4.75e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.004 value_reward_chosen[t] + 0.084 contr_diff + 0.885 reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.734 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 821/1000 --- L(Train): 0.4096797 --- L(Val, RNN): 0.4140604 --- L(Val, SINDy): 0.4149066 --- Time: 0.32s; --- Convergence: 3.89e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.003 value_reward_chosen[t] + 0.085 contr_diff + 0.887 reward + 0.888 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.482 1 + 0.735 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 822/1000 --- L(Train): 0.4113798 --- L(Val, RNN): 0.4140556 --- L(Val, SINDy): 0.4149247 --- Time: 0.34s; --- Convergence: 2.19e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.002 value_reward_chosen[t] + 0.085 contr_diff + 0.888 reward + 0.889 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.736 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 823/1000 --- L(Train): 0.4160551 --- L(Val, RNN): 0.4140400 --- L(Val, SINDy): 0.4149407 --- Time: 0.35s; --- Convergence: 1.87e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.001 value_reward_chosen[t] + 0.085 contr_diff + 0.89 reward + 0.891 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.738 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 824/1000 --- L(Train): 0.4335703 --- L(Val, RNN): 0.4140472 --- L(Val, SINDy): 0.4149530 --- Time: 0.29s; --- Convergence: 1.30e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.001 value_reward_chosen[t] + 0.083 contr_diff + 0.893 reward + 0.894 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.484 1 + 0.74 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 825/1000 --- L(Train): 0.4175466 --- L(Val, RNN): 0.4140640 --- L(Val, SINDy): 0.4149559 --- Time: 0.41s; --- Convergence: 1.49e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.001 value_reward_chosen[t] + 0.081 contr_diff + 0.895 reward + 0.896 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.484 1 + 0.741 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 826/1000 --- L(Train): 0.4085984 --- L(Val, RNN): 0.4140575 --- L(Val, SINDy): 0.4149356 --- Time: 0.42s; --- Convergence: 1.06e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.586 1 + 0.001 value_reward_chosen[t] + 0.079 contr_diff + 0.897 reward + 0.898 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.482 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.248 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 827/1000 --- L(Train): 0.4085105 --- L(Val, RNN): 0.4141279 --- L(Val, SINDy): 0.4149146 --- Time: 0.37s; --- Convergence: 4.05e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + -0.0 value_reward_chosen[t] + 0.078 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 828/1000 --- L(Train): 0.4039335 --- L(Val, RNN): 0.4141289 --- L(Val, SINDy): 0.4148764 --- Time: 0.35s; --- Convergence: 2.07e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + -0.001 value_reward_chosen[t] + 0.077 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 829/1000 --- L(Train): 0.4166068 --- L(Val, RNN): 0.4140520 --- L(Val, SINDy): 0.4148636 --- Time: 0.29s; --- Convergence: 4.88e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + -0.002 value_reward_chosen[t] + 0.076 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.748 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 830/1000 --- L(Train): 0.4071021 --- L(Val, RNN): 0.4140386 --- L(Val, SINDy): 0.4148579 --- Time: 0.43s; --- Convergence: 3.11e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + -0.003 value_reward_chosen[t] + 0.076 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.75 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 831/1000 --- L(Train): 0.4316924 --- L(Val, RNN): 0.4140745 --- L(Val, SINDy): 0.4148218 --- Time: 0.34s; --- Convergence: 3.35e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + -0.004 value_reward_chosen[t] + 0.076 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.752 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 832/1000 --- L(Train): 0.4160394 --- L(Val, RNN): 0.4140859 --- L(Val, SINDy): 0.4148268 --- Time: 0.31s; --- Convergence: 2.25e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + -0.004 value_reward_chosen[t] + 0.077 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.754 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 833/1000 --- L(Train): 0.4193589 --- L(Val, RNN): 0.4141284 --- L(Val, SINDy): 0.4148630 --- Time: 0.29s; --- Convergence: 3.25e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 13/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.003 value_reward_chosen[t] + 0.078 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 834/1000 --- L(Train): 0.4045961 --- L(Val, RNN): 0.4141360 --- L(Val, SINDy): 0.4148437 --- Time: 0.30s; --- Convergence: 2.01e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 14/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.581 1 + -0.002 value_reward_chosen[t] + 0.078 contr_diff + 0.899 reward + 0.9 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 835/1000 --- L(Train): 0.4174358 --- L(Val, RNN): 0.4140335 --- L(Val, SINDy): 0.4148166 --- Time: 0.32s; --- Convergence: 6.13e-05; LR: 1.00e-02; Metric: 0.4140301; Bad epochs: 15/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.58 1 + -0.001 value_reward_chosen[t] + 0.079 contr_diff + 0.899 reward + 0.9 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.76 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 836/1000 --- L(Train): 0.4115903 --- L(Val, RNN): 0.4139959 --- L(Val, SINDy): 0.4148269 --- Time: 0.28s; --- Convergence: 4.95e-05; LR: 1.00e-02; Metric: 0.4139959; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.579 1 + -0.0 value_reward_chosen[t] + 0.079 contr_diff + 0.899 reward + 0.9 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.762 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 837/1000 --- L(Train): 0.4013491 --- L(Val, RNN): 0.4140112 --- L(Val, SINDy): 0.4148543 --- Time: 0.40s; --- Convergence: 3.24e-05; LR: 1.00e-02; Metric: 0.4139959; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.579 1 + -0.001 value_reward_chosen[t] + 0.08 contr_diff + 0.897 reward + 0.898 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 838/1000 --- L(Train): 0.4108699 --- L(Val, RNN): 0.4140084 --- L(Val, SINDy): 0.4148678 --- Time: 0.38s; --- Convergence: 1.76e-05; LR: 1.00e-02; Metric: 0.4139959; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.58 1 + -0.003 value_reward_chosen[t] + 0.081 contr_diff + 0.895 reward + 0.896 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 839/1000 --- L(Train): 0.4216749 --- L(Val, RNN): 0.4140566 --- L(Val, SINDy): 0.4148922 --- Time: 0.30s; --- Convergence: 3.29e-05; LR: 1.00e-02; Metric: 0.4139959; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.581 1 + -0.003 value_reward_chosen[t] + 0.081 contr_diff + 0.893 reward + 0.894 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.46 1 + 0.763 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 840/1000 --- L(Train): 0.3976328 --- L(Val, RNN): 0.4140696 --- L(Val, SINDy): 0.4149178 --- Time: 0.30s; --- Convergence: 2.29e-05; LR: 1.00e-02; Metric: 0.4139959; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.579 1 + -0.001 value_reward_chosen[t] + 0.082 contr_diff + 0.894 reward + 0.895 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.762 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 841/1000 --- L(Train): 0.4229608 --- L(Val, RNN): 0.4140143 --- L(Val, SINDy): 0.4149185 --- Time: 0.35s; --- Convergence: 3.91e-05; LR: 1.00e-02; Metric: 0.4139959; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.576 1 + 0.001 value_reward_chosen[t] + 0.081 contr_diff + 0.895 reward + 0.896 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.762 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 842/1000 --- L(Train): 0.4032823 --- L(Val, RNN): 0.4139961 --- L(Val, SINDy): 0.4148927 --- Time: 0.29s; --- Convergence: 2.87e-05; LR: 1.00e-02; Metric: 0.4139959; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.572 1 + 0.005 value_reward_chosen[t] + 0.081 contr_diff + 0.897 reward + 0.898 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.762 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 843/1000 --- L(Train): 0.4149247 --- L(Val, RNN): 0.4139945 --- L(Val, SINDy): 0.4148875 --- Time: 0.27s; --- Convergence: 1.51e-05; LR: 1.00e-02; Metric: 0.4139945; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.569 1 + 0.007 value_reward_chosen[t] + 0.081 contr_diff + 0.899 reward + 0.9 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.761 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 844/1000 --- L(Train): 0.4108040 --- L(Val, RNN): 0.4140549 --- L(Val, SINDy): 0.4148903 --- Time: 0.37s; --- Convergence: 3.77e-05; LR: 1.00e-02; Metric: 0.4139945; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.568 1 + 0.006 value_reward_chosen[t] + 0.081 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.76 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 845/1000 --- L(Train): 0.4175163 --- L(Val, RNN): 0.4140762 --- L(Val, SINDy): 0.4148574 --- Time: 0.34s; --- Convergence: 2.95e-05; LR: 1.00e-02; Metric: 0.4139945; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.567 1 + 0.005 value_reward_chosen[t] + 0.082 contr_diff + 0.897 reward + 0.898 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.758 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 846/1000 --- L(Train): 0.4134699 --- L(Val, RNN): 0.4140098 --- L(Val, SINDy): 0.4148363 --- Time: 0.37s; --- Convergence: 4.80e-05; LR: 1.00e-02; Metric: 0.4139945; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.568 1 + 0.001 value_reward_chosen[t] + 0.082 contr_diff + 0.895 reward + 0.896 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.756 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 847/1000 --- L(Train): 0.4137684 --- L(Val, RNN): 0.4139666 --- L(Val, SINDy): 0.4148169 --- Time: 0.33s; --- Convergence: 4.56e-05; LR: 1.00e-02; Metric: 0.4139666; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.569 1 + -0.002 value_reward_chosen[t] + 0.083 contr_diff + 0.892 reward + 0.893 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.755 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 848/1000 --- L(Train): 0.4157452 --- L(Val, RNN): 0.4139992 --- L(Val, SINDy): 0.4148060 --- Time: 0.46s; --- Convergence: 3.91e-05; LR: 1.00e-02; Metric: 0.4139666; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.569 1 + -0.005 value_reward_chosen[t] + 0.084 contr_diff + 0.891 reward + 0.892 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.752 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 849/1000 --- L(Train): 0.4160657 --- L(Val, RNN): 0.4140000 --- L(Val, SINDy): 0.4148272 --- Time: 0.43s; --- Convergence: 1.99e-05; LR: 1.00e-02; Metric: 0.4139666; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.569 1 + -0.006 value_reward_chosen[t] + 0.085 contr_diff + 0.889 reward + 0.89 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.75 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 850/1000 --- L(Train): 0.4065900 --- L(Val, RNN): 0.4139517 --- L(Val, SINDy): 0.4148550 --- Time: 0.41s; --- Convergence: 3.42e-05; LR: 1.00e-02; Metric: 0.4139517; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.57 1 + -0.007 value_reward_chosen[t] + 0.085 contr_diff + 0.888 reward + 0.889 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 851/1000 --- L(Train): 0.4149805 --- L(Val, RNN): 0.4139500 --- L(Val, SINDy): 0.4148667 --- Time: 0.38s; --- Convergence: 1.79e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.569 1 + -0.006 value_reward_chosen[t] + 0.086 contr_diff + 0.888 reward + 0.889 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 852/1000 --- L(Train): 0.4232160 --- L(Val, RNN): 0.4140178 --- L(Val, SINDy): 0.4148624 --- Time: 0.30s; --- Convergence: 4.28e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.568 1 + -0.003 value_reward_chosen[t] + 0.085 contr_diff + 0.889 reward + 0.89 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.741 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 853/1000 --- L(Train): 0.4156498 --- L(Val, RNN): 0.4140483 --- L(Val, SINDy): 0.4148653 --- Time: 0.37s; --- Convergence: 3.67e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.567 1 + -0.0 value_reward_chosen[t] + 0.084 contr_diff + 0.89 reward + 0.891 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.482 1 + 0.74 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 854/1000 --- L(Train): 0.4106816 --- L(Val, RNN): 0.4140437 --- L(Val, SINDy): 0.4148746 --- Time: 0.27s; --- Convergence: 2.06e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.567 1 + 0.002 value_reward_chosen[t] + 0.083 contr_diff + 0.891 reward + 0.892 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.738 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 855/1000 --- L(Train): 0.4127592 --- L(Val, RNN): 0.4140609 --- L(Val, SINDy): 0.4148932 --- Time: 0.27s; --- Convergence: 1.89e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.567 1 + 0.004 value_reward_chosen[t] + 0.083 contr_diff + 0.891 reward + 0.892 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.484 1 + 0.737 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 856/1000 --- L(Train): 0.4192763 --- L(Val, RNN): 0.4140807 --- L(Val, SINDy): 0.4149416 --- Time: 0.30s; --- Convergence: 1.94e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.568 1 + 0.004 value_reward_chosen[t] + 0.083 contr_diff + 0.891 reward + 0.892 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.484 1 + 0.736 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 857/1000 --- L(Train): 0.4064422 --- L(Val, RNN): 0.4140129 --- L(Val, SINDy): 0.4150186 --- Time: 0.34s; --- Convergence: 4.36e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.569 1 + 0.004 value_reward_chosen[t] + 0.083 contr_diff + 0.89 reward + 0.891 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.734 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 858/1000 --- L(Train): 0.4095987 --- L(Val, RNN): 0.4140277 --- L(Val, SINDy): 0.4150654 --- Time: 0.34s; --- Convergence: 2.92e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.57 1 + 0.004 value_reward_chosen[t] + 0.082 contr_diff + 0.89 reward + 0.891 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.482 1 + 0.732 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 859/1000 --- L(Train): 0.4016970 --- L(Val, RNN): 0.4141014 --- L(Val, SINDy): 0.4150378 --- Time: 0.41s; --- Convergence: 5.14e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.57 1 + 0.004 value_reward_chosen[t] + 0.082 contr_diff + 0.891 reward + 0.892 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.482 1 + 0.73 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.247 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 860/1000 --- L(Train): 0.4173775 --- L(Val, RNN): 0.4140389 --- L(Val, SINDy): 0.4149984 --- Time: 0.31s; --- Convergence: 5.69e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.571 1 + 0.002 value_reward_chosen[t] + 0.081 contr_diff + 0.89 reward + 0.892 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.73 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 861/1000 --- L(Train): 0.4091020 --- L(Val, RNN): 0.4140210 --- L(Val, SINDy): 0.4149291 --- Time: 0.32s; --- Convergence: 3.74e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.572 1 + 0.0 value_reward_chosen[t] + 0.081 contr_diff + 0.89 reward + 0.891 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.486 1 + 0.729 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 862/1000 --- L(Train): 0.4037935 --- L(Val, RNN): 0.4140367 --- L(Val, SINDy): 0.4148535 --- Time: 0.30s; --- Convergence: 2.66e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.573 1 + -0.002 value_reward_chosen[t] + 0.081 contr_diff + 0.889 reward + 0.89 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.488 1 + 0.729 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 863/1000 --- L(Train): 0.4244470 --- L(Val, RNN): 0.4140261 --- L(Val, SINDy): 0.4148295 --- Time: 0.33s; --- Convergence: 1.86e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.575 1 + -0.004 value_reward_chosen[t] + 0.081 contr_diff + 0.889 reward + 0.89 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.491 1 + 0.728 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 864/1000 --- L(Train): 0.4162236 --- L(Val, RNN): 0.4140157 --- L(Val, SINDy): 0.4148456 --- Time: 0.32s; --- Convergence: 1.45e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 13/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.577 1 + -0.005 value_reward_chosen[t] + 0.081 contr_diff + 0.888 reward + 0.889 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.492 1 + 0.727 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 865/1000 --- L(Train): 0.4050415 --- L(Val, RNN): 0.4140404 --- L(Val, SINDy): 0.4148485 --- Time: 0.35s; --- Convergence: 1.96e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 14/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.578 1 + -0.005 value_reward_chosen[t] + 0.08 contr_diff + 0.889 reward + 0.89 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.493 1 + 0.724 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 866/1000 --- L(Train): 0.4222575 --- L(Val, RNN): 0.4141154 --- L(Val, SINDy): 0.4148396 --- Time: 0.29s; --- Convergence: 4.73e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 15/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.578 1 + -0.004 value_reward_chosen[t] + 0.079 contr_diff + 0.891 reward + 0.892 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.493 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 867/1000 --- L(Train): 0.4170211 --- L(Val, RNN): 0.4140560 --- L(Val, SINDy): 0.4148632 --- Time: 0.48s; --- Convergence: 5.34e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 16/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.579 1 + -0.003 value_reward_chosen[t] + 0.077 contr_diff + 0.892 reward + 0.893 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.494 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 868/1000 --- L(Train): 0.4180244 --- L(Val, RNN): 0.4140446 --- L(Val, SINDy): 0.4148846 --- Time: 0.43s; --- Convergence: 3.24e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 17/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.58 1 + -0.002 value_reward_chosen[t] + 0.075 contr_diff + 0.892 reward + 0.893 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.495 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 869/1000 --- L(Train): 0.4195688 --- L(Val, RNN): 0.4140350 --- L(Val, SINDy): 0.4148569 --- Time: 0.40s; --- Convergence: 2.10e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 18/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.58 1 + -0.001 value_reward_chosen[t] + 0.074 contr_diff + 0.893 reward + 0.894 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.495 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 870/1000 --- L(Train): 0.4134683 --- L(Val, RNN): 0.4140756 --- L(Val, SINDy): 0.4148362 --- Time: 0.31s; --- Convergence: 3.08e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 19/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.582 1 + -0.001 value_reward_chosen[t] + 0.074 contr_diff + 0.894 reward + 0.895 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.495 1 + 0.723 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 871/1000 --- L(Train): 0.4241112 --- L(Val, RNN): 0.4140260 --- L(Val, SINDy): 0.4148681 --- Time: 0.43s; --- Convergence: 4.02e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 20/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.0 value_reward_chosen[t] + 0.075 contr_diff + 0.894 reward + 0.895 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.494 1 + 0.725 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 872/1000 --- L(Train): 0.4206538 --- L(Val, RNN): 0.4140326 --- L(Val, SINDy): 0.4149052 --- Time: 0.37s; --- Convergence: 2.34e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 21/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + 0.001 value_reward_chosen[t] + 0.076 contr_diff + 0.895 reward + 0.896 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.492 1 + 0.727 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 873/1000 --- L(Train): 0.4218980 --- L(Val, RNN): 0.4140811 --- L(Val, SINDy): 0.4149015 --- Time: 0.36s; --- Convergence: 3.59e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 22/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + 0.002 value_reward_chosen[t] + 0.077 contr_diff + 0.897 reward + 0.898 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.49 1 + 0.73 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 874/1000 --- L(Train): 0.4097556 --- L(Val, RNN): 0.4139888 --- L(Val, SINDy): 0.4149106 --- Time: 0.29s; --- Convergence: 6.41e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 23/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + 0.003 value_reward_chosen[t] + 0.078 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.487 1 + 0.734 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.246 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 875/1000 --- L(Train): 0.4324575 --- L(Val, RNN): 0.4140725 --- L(Val, SINDy): 0.4148981 --- Time: 0.35s; --- Convergence: 7.39e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 24/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + 0.004 value_reward_chosen[t] + 0.078 contr_diff + 0.9 reward + 0.901 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.736 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 876/1000 --- L(Train): 0.4164570 --- L(Val, RNN): 0.4141382 --- L(Val, SINDy): 0.4148763 --- Time: 0.43s; --- Convergence: 6.98e-05; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 25/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + 0.004 value_reward_chosen[t] + 0.078 contr_diff + 0.901 reward + 0.902 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.737 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 877/1000 --- L(Train): 0.4139825 --- L(Val, RNN): 0.4139830 --- L(Val, SINDy): 0.4148884 --- Time: 0.38s; --- Convergence: 1.13e-04; LR: 1.00e-02; Metric: 0.4139500; Bad epochs: 26/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + 0.002 value_reward_chosen[t] + 0.077 contr_diff + 0.902 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.736 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 878/1000 --- L(Train): 0.4104390 --- L(Val, RNN): 0.4139393 --- L(Val, SINDy): 0.4149299 --- Time: 0.39s; --- Convergence: 7.81e-05; LR: 1.00e-02; Metric: 0.4139393; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + 0.001 value_reward_chosen[t] + 0.076 contr_diff + 0.902 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.736 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 879/1000 --- L(Train): 0.4127864 --- L(Val, RNN): 0.4139197 --- L(Val, SINDy): 0.4149260 --- Time: 0.40s; --- Convergence: 4.88e-05; LR: 1.00e-02; Metric: 0.4139197; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.586 1 + -0.001 value_reward_chosen[t] + 0.076 contr_diff + 0.902 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.466 1 + 0.737 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 880/1000 --- L(Train): 0.4245426 --- L(Val, RNN): 0.4138975 --- L(Val, SINDy): 0.4149293 --- Time: 0.38s; --- Convergence: 3.55e-05; LR: 1.00e-02; Metric: 0.4138975; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.588 1 + -0.004 value_reward_chosen[t] + 0.076 contr_diff + 0.9 reward + 0.901 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.739 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 881/1000 --- L(Train): 0.4100541 --- L(Val, RNN): 0.4138740 --- L(Val, SINDy): 0.4149482 --- Time: 0.31s; --- Convergence: 2.95e-05; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + -0.008 value_reward_chosen[t] + 0.077 contr_diff + 0.897 reward + 0.898 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 882/1000 --- L(Train): 0.4221735 --- L(Val, RNN): 0.4139252 --- L(Val, SINDy): 0.4149211 --- Time: 0.31s; --- Convergence: 4.03e-05; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + -0.009 value_reward_chosen[t] + 0.079 contr_diff + 0.896 reward + 0.897 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 883/1000 --- L(Train): 0.4245000 --- L(Val, RNN): 0.4140892 --- L(Val, SINDy): 0.4148798 --- Time: 0.32s; --- Convergence: 1.02e-04; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + -0.009 value_reward_chosen[t] + 0.079 contr_diff + 0.896 reward + 0.897 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 884/1000 --- L(Train): 0.4108572 --- L(Val, RNN): 0.4139662 --- L(Val, SINDy): 0.4148785 --- Time: 0.35s; --- Convergence: 1.13e-04; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + -0.006 value_reward_chosen[t] + 0.08 contr_diff + 0.896 reward + 0.897 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.466 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 885/1000 --- L(Train): 0.4073820 --- L(Val, RNN): 0.4139735 --- L(Val, SINDy): 0.4148619 --- Time: 0.33s; --- Convergence: 5.99e-05; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + -0.002 value_reward_chosen[t] + 0.08 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.741 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 886/1000 --- L(Train): 0.4210137 --- L(Val, RNN): 0.4140011 --- L(Val, SINDy): 0.4148310 --- Time: 0.34s; --- Convergence: 4.38e-05; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.002 value_reward_chosen[t] + 0.08 contr_diff + 0.9 reward + 0.901 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.738 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 887/1000 --- L(Train): 0.4131406 --- L(Val, RNN): 0.4139430 --- L(Val, SINDy): 0.4148352 --- Time: 0.36s; --- Convergence: 5.10e-05; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.005 value_reward_chosen[t] + 0.08 contr_diff + 0.901 reward + 0.902 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.737 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 888/1000 --- L(Train): 0.4094597 --- L(Val, RNN): 0.4139568 --- L(Val, SINDy): 0.4148355 --- Time: 0.46s; --- Convergence: 3.24e-05; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.007 value_reward_chosen[t] + 0.08 contr_diff + 0.901 reward + 0.902 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.737 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 889/1000 --- L(Train): 0.4157953 --- L(Val, RNN): 0.4139718 --- L(Val, SINDy): 0.4148135 --- Time: 0.30s; --- Convergence: 2.37e-05; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.008 value_reward_chosen[t] + 0.08 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.738 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 890/1000 --- L(Train): 0.4137012 --- L(Val, RNN): 0.4139312 --- L(Val, SINDy): 0.4148293 --- Time: 0.39s; --- Convergence: 3.22e-05; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.009 value_reward_chosen[t] + 0.079 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.74 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 891/1000 --- L(Train): 0.4253355 --- L(Val, RNN): 0.4139264 --- L(Val, SINDy): 0.4148497 --- Time: 0.33s; --- Convergence: 1.84e-05; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.007 value_reward_chosen[t] + 0.079 contr_diff + 0.902 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.741 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.245 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 892/1000 --- L(Train): 0.4114249 --- L(Val, RNN): 0.4139018 --- L(Val, SINDy): 0.4148249 --- Time: 0.34s; --- Convergence: 2.15e-05; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.079 contr_diff + 0.901 reward + 0.902 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.741 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 893/1000 --- L(Train): 0.3977425 --- L(Val, RNN): 0.4139233 --- L(Val, SINDy): 0.4148046 --- Time: 0.39s; --- Convergence: 2.15e-05; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.001 value_reward_chosen[t] + 0.08 contr_diff + 0.899 reward + 0.901 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 894/1000 --- L(Train): 0.4245997 --- L(Val, RNN): 0.4139273 --- L(Val, SINDy): 0.4148029 --- Time: 0.31s; --- Convergence: 1.28e-05; LR: 1.00e-02; Metric: 0.4138740; Bad epochs: 13/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + -0.002 value_reward_chosen[t] + 0.081 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 895/1000 --- L(Train): 0.4095590 --- L(Val, RNN): 0.4138681 --- L(Val, SINDy): 0.4147803 --- Time: 0.39s; --- Convergence: 3.60e-05; LR: 1.00e-02; Metric: 0.4138681; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + -0.005 value_reward_chosen[t] + 0.082 contr_diff + 0.896 reward + 0.897 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.486 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 896/1000 --- L(Train): 0.4059124 --- L(Val, RNN): 0.4138846 --- L(Val, SINDy): 0.4147410 --- Time: 0.32s; --- Convergence: 2.62e-05; LR: 1.00e-02; Metric: 0.4138681; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + -0.006 value_reward_chosen[t] + 0.084 contr_diff + 0.896 reward + 0.897 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.741 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 897/1000 --- L(Train): 0.4158385 --- L(Val, RNN): 0.4138608 --- L(Val, SINDy): 0.4147440 --- Time: 0.41s; --- Convergence: 2.50e-05; LR: 1.00e-02; Metric: 0.4138608; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + -0.007 value_reward_chosen[t] + 0.085 contr_diff + 0.895 reward + 0.896 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.484 1 + 0.739 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 898/1000 --- L(Train): 0.4146142 --- L(Val, RNN): 0.4138539 --- L(Val, SINDy): 0.4147694 --- Time: 0.37s; --- Convergence: 1.60e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.594 1 + -0.007 value_reward_chosen[t] + 0.085 contr_diff + 0.895 reward + 0.896 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.482 1 + 0.736 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 899/1000 --- L(Train): 0.4112377 --- L(Val, RNN): 0.4138834 --- L(Val, SINDy): 0.4147769 --- Time: 0.32s; --- Convergence: 2.27e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.594 1 + -0.006 value_reward_chosen[t] + 0.085 contr_diff + 0.896 reward + 0.897 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.734 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 900/1000 --- L(Train): 0.4190717 --- L(Val, RNN): 0.4139372 --- L(Val, SINDy): 0.4147792 --- Time: 0.27s; --- Convergence: 3.83e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + -0.004 value_reward_chosen[t] + 0.083 contr_diff + 0.896 reward + 0.897 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.734 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 901/1000 --- L(Train): 0.4161044 --- L(Val, RNN): 0.4139805 --- L(Val, SINDy): 0.4147836 --- Time: 0.33s; --- Convergence: 4.08e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + -0.001 value_reward_chosen[t] + 0.08 contr_diff + 0.897 reward + 0.898 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.737 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 902/1000 --- L(Train): 0.4155074 --- L(Val, RNN): 0.4139303 --- L(Val, SINDy): 0.4147711 --- Time: 0.30s; --- Convergence: 4.54e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.002 value_reward_chosen[t] + 0.077 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.74 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 903/1000 --- L(Train): 0.4119799 --- L(Val, RNN): 0.4139569 --- L(Val, SINDy): 0.4147707 --- Time: 0.30s; --- Convergence: 3.60e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + 0.005 value_reward_chosen[t] + 0.075 contr_diff + 0.9 reward + 0.901 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 904/1000 --- L(Train): 0.4147029 --- L(Val, RNN): 0.4138977 --- L(Val, SINDy): 0.4147831 --- Time: 0.37s; --- Convergence: 4.76e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + 0.007 value_reward_chosen[t] + 0.073 contr_diff + 0.902 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 905/1000 --- L(Train): 0.4153899 --- L(Val, RNN): 0.4139133 --- L(Val, SINDy): 0.4147724 --- Time: 0.34s; --- Convergence: 3.16e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + 0.008 value_reward_chosen[t] + 0.073 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 906/1000 --- L(Train): 0.4195091 --- L(Val, RNN): 0.4139620 --- L(Val, SINDy): 0.4147883 --- Time: 0.40s; --- Convergence: 4.01e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + 0.008 value_reward_chosen[t] + 0.075 contr_diff + 0.906 reward + 0.907 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.244 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 907/1000 --- L(Train): 0.4102746 --- L(Val, RNN): 0.4139781 --- L(Val, SINDy): 0.4148538 --- Time: 0.37s; --- Convergence: 2.81e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.007 value_reward_chosen[t] + 0.078 contr_diff + 0.907 reward + 0.908 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 908/1000 --- L(Train): 0.4048576 --- L(Val, RNN): 0.4139317 --- L(Val, SINDy): 0.4148834 --- Time: 0.30s; --- Convergence: 3.73e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + 0.003 value_reward_chosen[t] + 0.08 contr_diff + 0.907 reward + 0.908 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 909/1000 --- L(Train): 0.4141070 --- L(Val, RNN): 0.4138824 --- L(Val, SINDy): 0.4148743 --- Time: 0.30s; --- Convergence: 4.33e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + -0.0 value_reward_chosen[t] + 0.083 contr_diff + 0.906 reward + 0.907 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 910/1000 --- L(Train): 0.4087791 --- L(Val, RNN): 0.4139607 --- L(Val, SINDy): 0.4148715 --- Time: 0.31s; --- Convergence: 6.08e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + -0.004 value_reward_chosen[t] + 0.085 contr_diff + 0.905 reward + 0.906 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 911/1000 --- L(Train): 0.4251766 --- L(Val, RNN): 0.4139458 --- L(Val, SINDy): 0.4148548 --- Time: 0.36s; --- Convergence: 3.78e-05; LR: 1.00e-02; Metric: 0.4138539; Bad epochs: 13/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + -0.006 value_reward_chosen[t] + 0.085 contr_diff + 0.905 reward + 0.906 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 912/1000 --- L(Train): 0.4148531 --- L(Val, RNN): 0.4138267 --- L(Val, SINDy): 0.4148146 --- Time: 0.31s; --- Convergence: 7.85e-05; LR: 1.00e-02; Metric: 0.4138267; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + -0.007 value_reward_chosen[t] + 0.086 contr_diff + 0.905 reward + 0.906 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 913/1000 --- L(Train): 0.4265344 --- L(Val, RNN): 0.4138276 --- L(Val, SINDy): 0.4147893 --- Time: 0.40s; --- Convergence: 3.97e-05; LR: 1.00e-02; Metric: 0.4138267; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + -0.008 value_reward_chosen[t] + 0.086 contr_diff + 0.904 reward + 0.906 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 914/1000 --- L(Train): 0.4091654 --- L(Val, RNN): 0.4138319 --- L(Val, SINDy): 0.4148012 --- Time: 0.38s; --- Convergence: 2.20e-05; LR: 1.00e-02; Metric: 0.4138267; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + -0.008 value_reward_chosen[t] + 0.085 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.744 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 915/1000 --- L(Train): 0.4250948 --- L(Val, RNN): 0.4138594 --- L(Val, SINDy): 0.4148085 --- Time: 0.41s; --- Convergence: 2.47e-05; LR: 1.00e-02; Metric: 0.4138267; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + -0.007 value_reward_chosen[t] + 0.084 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.742 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 916/1000 --- L(Train): 0.4093429 --- L(Val, RNN): 0.4138780 --- L(Val, SINDy): 0.4147986 --- Time: 0.38s; --- Convergence: 2.17e-05; LR: 1.00e-02; Metric: 0.4138267; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + -0.005 value_reward_chosen[t] + 0.081 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.74 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 917/1000 --- L(Train): 0.4207579 --- L(Val, RNN): 0.4138145 --- L(Val, SINDy): 0.4147920 --- Time: 0.38s; --- Convergence: 4.26e-05; LR: 1.00e-02; Metric: 0.4138145; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.594 1 + -0.003 value_reward_chosen[t] + 0.077 contr_diff + 0.902 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.737 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 918/1000 --- L(Train): 0.4009525 --- L(Val, RNN): 0.4138536 --- L(Val, SINDy): 0.4147706 --- Time: 0.32s; --- Convergence: 4.09e-05; LR: 1.00e-02; Metric: 0.4138145; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + -0.002 value_reward_chosen[t] + 0.073 contr_diff + 0.9 reward + 0.901 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.735 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 919/1000 --- L(Train): 0.4175492 --- L(Val, RNN): 0.4138516 --- L(Val, SINDy): 0.4147272 --- Time: 0.35s; --- Convergence: 2.14e-05; LR: 1.00e-02; Metric: 0.4138145; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.0 value_reward_chosen[t] + 0.069 contr_diff + 0.899 reward + 0.9 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.733 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 920/1000 --- L(Train): 0.4089363 --- L(Val, RNN): 0.4138735 --- L(Val, SINDy): 0.4147294 --- Time: 0.41s; --- Convergence: 2.16e-05; LR: 1.00e-02; Metric: 0.4138145; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.003 value_reward_chosen[t] + 0.066 contr_diff + 0.899 reward + 0.9 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.732 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.243 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 921/1000 --- L(Train): 0.4130564 --- L(Val, RNN): 0.4138806 --- L(Val, SINDy): 0.4147519 --- Time: 0.51s; --- Convergence: 1.44e-05; LR: 1.00e-02; Metric: 0.4138145; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + 0.006 value_reward_chosen[t] + 0.063 contr_diff + 0.9 reward + 0.901 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.732 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.242 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 922/1000 --- L(Train): 0.4150663 --- L(Val, RNN): 0.4138758 --- L(Val, SINDy): 0.4147388 --- Time: 0.39s; --- Convergence: 9.56e-06; LR: 1.00e-02; Metric: 0.4138145; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + 0.009 value_reward_chosen[t] + 0.063 contr_diff + 0.902 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.733 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.242 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 923/1000 --- L(Train): 0.4064932 --- L(Val, RNN): 0.4138983 --- L(Val, SINDy): 0.4147192 --- Time: 0.35s; --- Convergence: 1.60e-05; LR: 1.00e-02; Metric: 0.4138145; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.009 value_reward_chosen[t] + 0.068 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.735 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.242 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 924/1000 --- L(Train): 0.4117568 --- L(Val, RNN): 0.4138635 --- L(Val, SINDy): 0.4147254 --- Time: 0.30s; --- Convergence: 2.54e-05; LR: 1.00e-02; Metric: 0.4138145; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + 0.005 value_reward_chosen[t] + 0.073 contr_diff + 0.901 reward + 0.902 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.738 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.242 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 925/1000 --- L(Train): 0.4119284 --- L(Val, RNN): 0.4138204 --- L(Val, SINDy): 0.4147130 --- Time: 0.32s; --- Convergence: 3.42e-05; LR: 1.00e-02; Metric: 0.4138145; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.594 1 + 0.001 value_reward_chosen[t] + 0.079 contr_diff + 0.899 reward + 0.9 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.74 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.242 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 926/1000 --- L(Train): 0.4206709 --- L(Val, RNN): 0.4138144 --- L(Val, SINDy): 0.4146968 --- Time: 0.36s; --- Convergence: 2.01e-05; LR: 1.00e-02; Metric: 0.4138144; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + -0.003 value_reward_chosen[t] + 0.085 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.486 1 + 0.74 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.242 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 927/1000 --- L(Train): 0.4134499 --- L(Val, RNN): 0.4137973 --- L(Val, SINDy): 0.4147103 --- Time: 0.37s; --- Convergence: 1.86e-05; LR: 1.00e-02; Metric: 0.4137973; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + -0.006 value_reward_chosen[t] + 0.088 contr_diff + 0.897 reward + 0.898 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.738 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.242 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 928/1000 --- L(Train): 0.4125082 --- L(Val, RNN): 0.4138380 --- L(Val, SINDy): 0.4147451 --- Time: 0.31s; --- Convergence: 2.96e-05; LR: 1.00e-02; Metric: 0.4137973; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.597 1 + -0.007 value_reward_chosen[t] + 0.088 contr_diff + 0.897 reward + 0.898 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.734 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.242 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 929/1000 --- L(Train): 0.4200020 --- L(Val, RNN): 0.4138314 --- L(Val, SINDy): 0.4147779 --- Time: 0.38s; --- Convergence: 1.81e-05; LR: 1.00e-02; Metric: 0.4137973; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.595 1 + -0.007 value_reward_chosen[t] + 0.085 contr_diff + 0.898 reward + 0.899 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.73 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.242 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 930/1000 --- L(Train): 0.4229541 --- L(Val, RNN): 0.4138240 --- L(Val, SINDy): 0.4148194 --- Time: 0.40s; --- Convergence: 1.28e-05; LR: 1.00e-02; Metric: 0.4137973; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.594 1 + -0.005 value_reward_chosen[t] + 0.082 contr_diff + 0.899 reward + 0.9 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.726 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.242 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 931/1000 --- L(Train): 0.4019639 --- L(Val, RNN): 0.4137933 --- L(Val, SINDy): 0.4148625 --- Time: 0.33s; --- Convergence: 2.17e-05; LR: 1.00e-02; Metric: 0.4137933; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + -0.004 value_reward_chosen[t] + 0.078 contr_diff + 0.901 reward + 0.902 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.723 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.242 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 932/1000 --- L(Train): 0.4071782 --- L(Val, RNN): 0.4137766 --- L(Val, SINDy): 0.4148758 --- Time: 0.31s; --- Convergence: 1.92e-05; LR: 1.00e-02; Metric: 0.4137766; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + -0.001 value_reward_chosen[t] + 0.075 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.477 1 + 0.721 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 933/1000 --- L(Train): 0.4188194 --- L(Val, RNN): 0.4138081 --- L(Val, SINDy): 0.4148642 --- Time: 0.34s; --- Convergence: 2.53e-05; LR: 1.00e-02; Metric: 0.4137766; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.002 value_reward_chosen[t] + 0.072 contr_diff + 0.905 reward + 0.906 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.721 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 934/1000 --- L(Train): 0.4147400 --- L(Val, RNN): 0.4137968 --- L(Val, SINDy): 0.4148803 --- Time: 0.33s; --- Convergence: 1.83e-05; LR: 1.00e-02; Metric: 0.4137766; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.003 value_reward_chosen[t] + 0.07 contr_diff + 0.906 reward + 0.907 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 935/1000 --- L(Train): 0.4119867 --- L(Val, RNN): 0.4137795 --- L(Val, SINDy): 0.4149106 --- Time: 0.40s; --- Convergence: 1.78e-05; LR: 1.00e-02; Metric: 0.4137766; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.003 value_reward_chosen[t] + 0.068 contr_diff + 0.904 reward + 0.906 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.488 1 + 0.724 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 936/1000 --- L(Train): 0.4173474 --- L(Val, RNN): 0.4137518 --- L(Val, SINDy): 0.4148985 --- Time: 0.29s; --- Convergence: 2.28e-05; LR: 1.00e-02; Metric: 0.4137518; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + 0.002 value_reward_chosen[t] + 0.067 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.493 1 + 0.727 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 937/1000 --- L(Train): 0.4182881 --- L(Val, RNN): 0.4137477 --- L(Val, SINDy): 0.4148881 --- Time: 0.31s; --- Convergence: 1.34e-05; LR: 1.00e-02; Metric: 0.4137477; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.594 1 + 0.001 value_reward_chosen[t] + 0.065 contr_diff + 0.901 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.497 1 + 0.73 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 938/1000 --- L(Train): 0.4169141 --- L(Val, RNN): 0.4137329 --- L(Val, SINDy): 0.4148830 --- Time: 0.29s; --- Convergence: 1.41e-05; LR: 1.00e-02; Metric: 0.4137329; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + 0.001 value_reward_chosen[t] + 0.063 contr_diff + 0.902 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.498 1 + 0.731 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 939/1000 --- L(Train): 0.4095250 --- L(Val, RNN): 0.4137413 --- L(Val, SINDy): 0.4148492 --- Time: 0.30s; --- Convergence: 1.13e-05; LR: 1.00e-02; Metric: 0.4137329; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.062 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.497 1 + 0.73 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 940/1000 --- L(Train): 0.4222491 --- L(Val, RNN): 0.4137544 --- L(Val, SINDy): 0.4148131 --- Time: 0.27s; --- Convergence: 1.22e-05; LR: 1.00e-02; Metric: 0.4137329; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.061 contr_diff + 0.906 reward + 0.907 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.496 1 + 0.73 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 941/1000 --- L(Train): 0.4174618 --- L(Val, RNN): 0.4137292 --- L(Val, SINDy): 0.4148183 --- Time: 0.28s; --- Convergence: 1.87e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.005 value_reward_chosen[t] + 0.062 contr_diff + 0.907 reward + 0.909 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.494 1 + 0.729 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 942/1000 --- L(Train): 0.4202985 --- L(Val, RNN): 0.4137685 --- L(Val, SINDy): 0.4148350 --- Time: 0.26s; --- Convergence: 2.90e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.586 1 + 0.005 value_reward_chosen[t] + 0.064 contr_diff + 0.908 reward + 0.909 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.492 1 + 0.727 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 943/1000 --- L(Train): 0.4270131 --- L(Val, RNN): 0.4138531 --- L(Val, SINDy): 0.4148432 --- Time: 0.37s; --- Convergence: 5.68e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + 0.004 value_reward_chosen[t] + 0.066 contr_diff + 0.908 reward + 0.909 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.489 1 + 0.725 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 944/1000 --- L(Train): 0.4166861 --- L(Val, RNN): 0.4138869 --- L(Val, SINDy): 0.4148688 --- Time: 0.29s; --- Convergence: 4.53e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + 0.001 value_reward_chosen[t] + 0.068 contr_diff + 0.908 reward + 0.909 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.487 1 + 0.724 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.241 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 945/1000 --- L(Train): 0.4179371 --- L(Val, RNN): 0.4138689 --- L(Val, SINDy): 0.4148873 --- Time: 0.27s; --- Convergence: 3.16e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.586 1 + -0.001 value_reward_chosen[t] + 0.069 contr_diff + 0.906 reward + 0.907 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.723 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 946/1000 --- L(Train): 0.4095840 --- L(Val, RNN): 0.4138344 --- L(Val, SINDy): 0.4148630 --- Time: 0.57s; --- Convergence: 3.31e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + -0.003 value_reward_chosen[t] + 0.07 contr_diff + 0.905 reward + 0.906 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.486 1 + 0.726 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 947/1000 --- L(Train): 0.4127215 --- L(Val, RNN): 0.4138176 --- L(Val, SINDy): 0.4148214 --- Time: 0.40s; --- Convergence: 2.49e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + -0.005 value_reward_chosen[t] + 0.069 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.487 1 + 0.729 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 948/1000 --- L(Train): 0.4112898 --- L(Val, RNN): 0.4138022 --- L(Val, SINDy): 0.4147882 --- Time: 0.31s; --- Convergence: 2.02e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + -0.006 value_reward_chosen[t] + 0.068 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.488 1 + 0.731 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 949/1000 --- L(Train): 0.4131863 --- L(Val, RNN): 0.4138002 --- L(Val, SINDy): 0.4147411 --- Time: 0.29s; --- Convergence: 1.11e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.586 1 + -0.004 value_reward_chosen[t] + 0.067 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.489 1 + 0.733 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 950/1000 --- L(Train): 0.4002511 --- L(Val, RNN): 0.4137762 --- L(Val, SINDy): 0.4147153 --- Time: 0.34s; --- Convergence: 1.75e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + -0.002 value_reward_chosen[t] + 0.065 contr_diff + 0.905 reward + 0.906 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.49 1 + 0.733 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 951/1000 --- L(Train): 0.4065274 --- L(Val, RNN): 0.4137493 --- L(Val, SINDy): 0.4147276 --- Time: 0.29s; --- Convergence: 2.22e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.001 value_reward_chosen[t] + 0.065 contr_diff + 0.905 reward + 0.906 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.49 1 + 0.732 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 952/1000 --- L(Train): 0.4190205 --- L(Val, RNN): 0.4137575 --- L(Val, SINDy): 0.4147401 --- Time: 0.34s; --- Convergence: 1.52e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + -0.001 value_reward_chosen[t] + 0.066 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.488 1 + 0.728 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 953/1000 --- L(Train): 0.4304756 --- L(Val, RNN): 0.4138153 --- L(Val, SINDy): 0.4147565 --- Time: 0.32s; --- Convergence: 3.65e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.586 1 + -0.0 value_reward_chosen[t] + 0.068 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.487 1 + 0.724 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 954/1000 --- L(Train): 0.4088508 --- L(Val, RNN): 0.4138005 --- L(Val, SINDy): 0.4147604 --- Time: 0.42s; --- Convergence: 2.56e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 13/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.586 1 + 0.0 value_reward_chosen[t] + 0.07 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.486 1 + 0.72 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 955/1000 --- L(Train): 0.4119459 --- L(Val, RNN): 0.4137760 --- L(Val, SINDy): 0.4147408 --- Time: 0.30s; --- Convergence: 2.51e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 14/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + 0.002 value_reward_chosen[t] + 0.073 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.717 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 956/1000 --- L(Train): 0.4076285 --- L(Val, RNN): 0.4137652 --- L(Val, SINDy): 0.4147236 --- Time: 0.38s; --- Convergence: 1.79e-05; LR: 1.00e-02; Metric: 0.4137292; Bad epochs: 15/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.582 1 + 0.005 value_reward_chosen[t] + 0.075 contr_diff + 0.906 reward + 0.907 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.487 1 + 0.715 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 957/1000 --- L(Train): 0.4090168 --- L(Val, RNN): 0.4137098 --- L(Val, SINDy): 0.4147108 --- Time: 0.31s; --- Convergence: 3.67e-05; LR: 1.00e-02; Metric: 0.4137098; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.581 1 + 0.006 value_reward_chosen[t] + 0.077 contr_diff + 0.907 reward + 0.908 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.489 1 + 0.716 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 958/1000 --- L(Train): 0.4251035 --- L(Val, RNN): 0.4137197 --- L(Val, SINDy): 0.4146942 --- Time: 0.33s; --- Convergence: 2.33e-05; LR: 1.00e-02; Metric: 0.4137098; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.581 1 + 0.004 value_reward_chosen[t] + 0.079 contr_diff + 0.907 reward + 0.908 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.493 1 + 0.719 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\u001b[H\u001b[2J\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 959/1000 --- L(Train): 0.4157271 --- L(Val, RNN): 0.4137328 --- L(Val, SINDy): 0.4146888 --- Time: 0.29s; --- Convergence: 1.82e-05; LR: 1.00e-02; Metric: 0.4137098; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.583 1 + 0.001 value_reward_chosen[t] + 0.081 contr_diff + 0.905 reward + 0.906 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.497 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.24 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 960/1000 --- L(Train): 0.4093434 --- L(Val, RNN): 0.4137293 --- L(Val, SINDy): 0.4146856 --- Time: 0.40s; --- Convergence: 1.09e-05; LR: 1.00e-02; Metric: 0.4137098; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.001 value_reward_chosen[t] + 0.08 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.5 1 + 0.724 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 961/1000 --- L(Train): 0.4232086 --- L(Val, RNN): 0.4137254 --- L(Val, SINDy): 0.4146812 --- Time: 0.28s; --- Convergence: 7.40e-06; LR: 1.00e-02; Metric: 0.4137098; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.003 value_reward_chosen[t] + 0.078 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.5 1 + 0.726 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 962/1000 --- L(Train): 0.4204794 --- L(Val, RNN): 0.4137674 --- L(Val, SINDy): 0.4146776 --- Time: 0.27s; --- Convergence: 2.47e-05; LR: 1.00e-02; Metric: 0.4137098; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.584 1 + -0.003 value_reward_chosen[t] + 0.075 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.497 1 + 0.725 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 963/1000 --- L(Train): 0.4113485 --- L(Val, RNN): 0.4137807 --- L(Val, SINDy): 0.4146936 --- Time: 0.28s; --- Convergence: 1.90e-05; LR: 1.00e-02; Metric: 0.4137098; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.585 1 + -0.003 value_reward_chosen[t] + 0.072 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.493 1 + 0.724 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 964/1000 --- L(Train): 0.4108832 --- L(Val, RNN): 0.4137428 --- L(Val, SINDy): 0.4146966 --- Time: 0.36s; --- Convergence: 2.85e-05; LR: 1.00e-02; Metric: 0.4137098; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.587 1 + -0.004 value_reward_chosen[t] + 0.07 contr_diff + 0.902 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.488 1 + 0.723 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 965/1000 --- L(Train): 0.4201871 --- L(Val, RNN): 0.4137702 --- L(Val, SINDy): 0.4147024 --- Time: 0.32s; --- Convergence: 2.80e-05; LR: 1.00e-02; Metric: 0.4137098; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.589 1 + -0.005 value_reward_chosen[t] + 0.069 contr_diff + 0.901 reward + 0.902 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.483 1 + 0.721 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 966/1000 --- L(Train): 0.4273663 --- L(Val, RNN): 0.4137754 --- L(Val, SINDy): 0.4147191 --- Time: 0.26s; --- Convergence: 1.66e-05; LR: 1.00e-02; Metric: 0.4137098; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + -0.004 value_reward_chosen[t] + 0.069 contr_diff + 0.9 reward + 0.901 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.721 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 967/1000 --- L(Train): 0.4162154 --- L(Val, RNN): 0.4137337 --- L(Val, SINDy): 0.4147457 --- Time: 0.33s; --- Convergence: 2.91e-05; LR: 1.00e-02; Metric: 0.4137098; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.591 1 + -0.001 value_reward_chosen[t] + 0.069 contr_diff + 0.902 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.721 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 968/1000 --- L(Train): 0.4131281 --- L(Val, RNN): 0.4137060 --- L(Val, SINDy): 0.4147643 --- Time: 0.32s; --- Convergence: 2.84e-05; LR: 1.00e-02; Metric: 0.4137060; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.592 1 + 0.001 value_reward_chosen[t] + 0.07 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.723 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 969/1000 --- L(Train): 0.4149888 --- L(Val, RNN): 0.4137343 --- L(Val, SINDy): 0.4147790 --- Time: 0.30s; --- Convergence: 2.84e-05; LR: 1.00e-02; Metric: 0.4137060; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.593 1 + 0.003 value_reward_chosen[t] + 0.072 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.724 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 970/1000 --- L(Train): 0.4256366 --- L(Val, RNN): 0.4137383 --- L(Val, SINDy): 0.4147751 --- Time: 0.28s; --- Convergence: 1.61e-05; LR: 1.00e-02; Metric: 0.4137060; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.596 1 + 0.002 value_reward_chosen[t] + 0.074 contr_diff + 0.902 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.727 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 971/1000 --- L(Train): 0.4164752 --- L(Val, RNN): 0.4137027 --- L(Val, SINDy): 0.4147583 --- Time: 0.41s; --- Convergence: 2.58e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.6 1 + 0.0 value_reward_chosen[t] + 0.076 contr_diff + 0.9 reward + 0.901 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.731 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 972/1000 --- L(Train): 0.4209559 --- L(Val, RNN): 0.4137524 --- L(Val, SINDy): 0.4147169 --- Time: 0.36s; --- Convergence: 3.78e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 1/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.602 1 + 0.0 value_reward_chosen[t] + 0.077 contr_diff + 0.9 reward + 0.901 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.734 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 973/1000 --- L(Train): 0.4045709 --- L(Val, RNN): 0.4138177 --- L(Val, SINDy): 0.4146806 --- Time: 0.33s; --- Convergence: 5.15e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 2/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.602 1 + 0.001 value_reward_chosen[t] + 0.078 contr_diff + 0.901 reward + 0.903 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.482 1 + 0.736 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.239 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 974/1000 --- L(Train): 0.4168692 --- L(Val, RNN): 0.4138004 --- L(Val, SINDy): 0.4146739 --- Time: 0.28s; --- Convergence: 3.44e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 3/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.602 1 + 0.002 value_reward_chosen[t] + 0.078 contr_diff + 0.903 reward + 0.904 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.736 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.238 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 975/1000 --- L(Train): 0.4182107 --- L(Val, RNN): 0.4137992 --- L(Val, SINDy): 0.4146864 --- Time: 0.40s; --- Convergence: 1.78e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 4/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.603 1 + 0.002 value_reward_chosen[t] + 0.079 contr_diff + 0.905 reward + 0.906 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.48 1 + 0.735 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.238 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 976/1000 --- L(Train): 0.4097838 --- L(Val, RNN): 0.4138274 --- L(Val, SINDy): 0.4147005 --- Time: 0.30s; --- Convergence: 2.30e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 5/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.605 1 + 0.002 value_reward_chosen[t] + 0.081 contr_diff + 0.905 reward + 0.906 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.733 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.238 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 977/1000 --- L(Train): 0.4166342 --- L(Val, RNN): 0.4138419 --- L(Val, SINDy): 0.4147188 --- Time: 0.31s; --- Convergence: 1.87e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 6/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.608 1 + -0.0 value_reward_chosen[t] + 0.082 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.476 1 + 0.73 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.238 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 978/1000 --- L(Train): 0.4188326 --- L(Val, RNN): 0.4138818 --- L(Val, SINDy): 0.4147218 --- Time: 0.29s; --- Convergence: 2.93e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 7/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.611 1 + -0.002 value_reward_chosen[t] + 0.082 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.474 1 + 0.727 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.238 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 979/1000 --- L(Train): 0.4095999 --- L(Val, RNN): 0.4138650 --- L(Val, SINDy): 0.4147153 --- Time: 0.33s; --- Convergence: 2.31e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 8/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.612 1 + -0.002 value_reward_chosen[t] + 0.082 contr_diff + 0.904 reward + 0.905 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.724 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.238 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 980/1000 --- L(Train): 0.4170466 --- L(Val, RNN): 0.4138317 --- L(Val, SINDy): 0.4147196 --- Time: 0.32s; --- Convergence: 2.82e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 9/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.613 1 + -0.0 value_reward_chosen[t] + 0.081 contr_diff + 0.906 reward + 0.907 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.473 1 + 0.724 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.238 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 981/1000 --- L(Train): 0.4179859 --- L(Val, RNN): 0.4137694 --- L(Val, SINDy): 0.4147255 --- Time: 0.29s; --- Convergence: 4.52e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 10/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.613 1 + 0.002 value_reward_chosen[t] + 0.081 contr_diff + 0.908 reward + 0.909 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.475 1 + 0.725 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.238 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 982/1000 --- L(Train): 0.4312326 --- L(Val, RNN): 0.4138126 --- L(Val, SINDy): 0.4147123 --- Time: 0.29s; --- Convergence: 4.42e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 11/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.613 1 + 0.004 value_reward_chosen[t] + 0.08 contr_diff + 0.91 reward + 0.911 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.478 1 + 0.728 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.238 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 983/1000 --- L(Train): 0.4221793 --- L(Val, RNN): 0.4137894 --- L(Val, SINDy): 0.4147072 --- Time: 0.38s; --- Convergence: 3.38e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 12/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.613 1 + 0.005 value_reward_chosen[t] + 0.079 contr_diff + 0.912 reward + 0.913 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.481 1 + 0.732 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.238 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 984/1000 --- L(Train): 0.4103387 --- L(Val, RNN): 0.4137554 --- L(Val, SINDy): 0.4147342 --- Time: 0.26s; --- Convergence: 3.39e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 13/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.613 1 + 0.005 value_reward_chosen[t] + 0.078 contr_diff + 0.913 reward + 0.914 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.738 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.238 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 985/1000 --- L(Train): 0.4137734 --- L(Val, RNN): 0.4137590 --- L(Val, SINDy): 0.4147429 --- Time: 0.32s; --- Convergence: 1.88e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 14/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.614 1 + 0.004 value_reward_chosen[t] + 0.076 contr_diff + 0.914 reward + 0.915 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.486 1 + 0.743 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.237 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 986/1000 --- L(Train): 0.4118783 --- L(Val, RNN): 0.4138293 --- L(Val, SINDy): 0.4147438 --- Time: 0.29s; --- Convergence: 4.45e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 15/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.614 1 + 0.003 value_reward_chosen[t] + 0.075 contr_diff + 0.914 reward + 0.915 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.485 1 + 0.748 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.237 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 987/1000 --- L(Train): 0.4271072 --- L(Val, RNN): 0.4137722 --- L(Val, SINDy): 0.4147620 --- Time: 0.38s; --- Convergence: 5.08e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 16/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.617 1 + 0.0 value_reward_chosen[t] + 0.074 contr_diff + 0.913 reward + 0.914 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.479 1 + 0.748 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.237 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 988/1000 --- L(Train): 0.4049601 --- L(Val, RNN): 0.4137794 --- L(Val, SINDy): 0.4147547 --- Time: 0.29s; --- Convergence: 2.90e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 17/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.618 1 + -0.002 value_reward_chosen[t] + 0.073 contr_diff + 0.912 reward + 0.913 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.472 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.237 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 989/1000 --- L(Train): 0.4174242 --- L(Val, RNN): 0.4137850 --- L(Val, SINDy): 0.4147334 --- Time: 0.29s; --- Convergence: 1.73e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 18/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.618 1 + -0.003 value_reward_chosen[t] + 0.072 contr_diff + 0.912 reward + 0.913 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.237 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 990/1000 --- L(Train): 0.4129134 --- L(Val, RNN): 0.4137384 --- L(Val, SINDy): 0.4147433 --- Time: 0.35s; --- Convergence: 3.20e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 19/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.619 1 + -0.004 value_reward_chosen[t] + 0.071 contr_diff + 0.912 reward + 0.913 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.745 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.237 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 991/1000 --- L(Train): 0.4101957 --- L(Val, RNN): 0.4137218 --- L(Val, SINDy): 0.4147831 --- Time: 0.34s; --- Convergence: 2.43e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 20/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.619 1 + -0.004 value_reward_chosen[t] + 0.071 contr_diff + 0.912 reward + 0.913 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.449 1 + 0.747 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.237 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 992/1000 --- L(Train): 0.4195373 --- L(Val, RNN): 0.4137261 --- L(Val, SINDy): 0.4148214 --- Time: 0.31s; --- Convergence: 1.43e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 21/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.619 1 + -0.004 value_reward_chosen[t] + 0.071 contr_diff + 0.912 reward + 0.913 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.447 1 + 0.751 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.237 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 993/1000 --- L(Train): 0.4159046 --- L(Val, RNN): 0.4137417 --- L(Val, SINDy): 0.4148863 --- Time: 0.29s; --- Convergence: 1.49e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 22/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.619 1 + -0.002 value_reward_chosen[t] + 0.072 contr_diff + 0.912 reward + 0.913 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.448 1 + 0.757 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.237 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 994/1000 --- L(Train): 0.4199006 --- L(Val, RNN): 0.4137246 --- L(Val, SINDy): 0.4149534 --- Time: 0.30s; --- Convergence: 1.60e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 23/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.618 1 + -0.001 value_reward_chosen[t] + 0.074 contr_diff + 0.913 reward + 0.914 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.764 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.237 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 995/1000 --- L(Train): 0.4068675 --- L(Val, RNN): 0.4138299 --- L(Val, SINDy): 0.4149476 --- Time: 0.40s; --- Convergence: 6.06e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 24/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.616 1 + 0.002 value_reward_chosen[t] + 0.076 contr_diff + 0.915 reward + 0.916 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.77 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.237 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 996/1000 --- L(Train): 0.4025191 --- L(Val, RNN): 0.4138237 --- L(Val, SINDy): 0.4148974 --- Time: 0.29s; --- Convergence: 3.34e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 25/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.615 1 + 0.003 value_reward_chosen[t] + 0.08 contr_diff + 0.916 reward + 0.917 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.454 1 + 0.775 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.237 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 997/1000 --- L(Train): 0.4168953 --- L(Val, RNN): 0.4137134 --- L(Val, SINDy): 0.4148519 --- Time: 0.35s; --- Convergence: 7.18e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 26/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.615 1 + 0.004 value_reward_chosen[t] + 0.083 contr_diff + 0.916 reward + 0.917 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.778 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.236 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 998/1000 --- L(Train): 0.4170371 --- L(Val, RNN): 0.4137111 --- L(Val, SINDy): 0.4148035 --- Time: 0.42s; --- Convergence: 3.71e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 27/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.615 1 + 0.003 value_reward_chosen[t] + 0.085 contr_diff + 0.915 reward + 0.916 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.777 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.236 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 999/1000 --- L(Train): 0.4175559 --- L(Val, RNN): 0.4137271 --- L(Val, SINDy): 0.4147588 --- Time: 0.35s; --- Convergence: 2.65e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 28/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.616 1 + 0.002 value_reward_chosen[t] + 0.086 contr_diff + 0.914 reward + 0.915 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.774 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.236 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 1000/1000 --- L(Train): 0.4178554 --- L(Val, RNN): 0.4137070 --- L(Val, SINDy): 0.4147162 --- Time: 0.30s; --- Convergence: 2.33e-05; LR: 1.00e-02; Metric: 0.4137027; Bad epochs: 29/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.616 1 + 0.001 value_reward_chosen[t] + 0.086 contr_diff + 0.913 reward + 0.914 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.446 1 + 0.769 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.053 value_choice^2 + 1.236 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, 0, 0, -, -, -, -\n",
            "================================================================================\n",
            "Maximum number of training epochs reached.\n",
            "Model did not converge yet.\n",
            "\n",
            "================================================================================\n",
            "Starting second stage SINDy fitting (threshold=0.05, single model)\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 1/1000 --- L(Train): 0.4132524 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.011 1 + 0.987 value_reward_chosen[t] + 0.009 contr_diff + 0.01 reward + -0.01 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + -0.01 value_reward_chosen*reward + 0.009 contr_diff^2 + 0.009 contr_diff*reward + 0.009 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.01 1 + 1.009 value_reward_not_chosen[t] + -0.01 contr_diff + -0.01 value_reward_not_chosen^2 + -0.01 value_reward_not_chosen*contr_diff + -0.011 contr_diff^2 \n",
            "value_choice[t+1] = 0.009 1 + 0.99 value_choice[t] + -0.009 contr_diff + 0.009 choice + 0.007 value_choice^2 + -0.01 value_choice*contr_diff + -0.008 value_choice*choice + 0.007 contr_diff^2 + 0.009 contr_diff*choice + -0.009 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 2/1000 --- L(Train): 0.3950320 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.021 1 + 0.977 value_reward_chosen[t] + 0.019 contr_diff + 0.02 reward + -0.02 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.02 value_reward_chosen*reward + 0.018 contr_diff^2 + 0.016 contr_diff*reward + 0.019 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.02 1 + 1.018 value_reward_not_chosen[t] + -0.008 contr_diff + -0.019 value_reward_not_chosen^2 + -0.02 value_reward_not_chosen*contr_diff + -0.021 contr_diff^2 \n",
            "value_choice[t+1] = 0.003 1 + 0.991 value_choice[t] + -0.008 contr_diff + 0.004 choice + 0.007 value_choice^2 + -0.009 value_choice*contr_diff + -0.008 value_choice*choice + 0.006 contr_diff^2 + 0.008 contr_diff*choice + -0.019 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 3/1000 --- L(Train): 0.3796528 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.03 1 + 0.967 value_reward_chosen[t] + 0.028 contr_diff + 0.03 reward + -0.03 value_reward_chosen^2 + 0.006 value_reward_chosen*contr_diff + -0.03 value_reward_chosen*reward + 0.026 contr_diff^2 + 0.022 contr_diff*reward + 0.029 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.03 1 + 1.024 value_reward_not_chosen[t] + -0.005 contr_diff + -0.028 value_reward_not_chosen^2 + -0.028 value_reward_not_chosen*contr_diff + -0.03 contr_diff^2 \n",
            "value_choice[t+1] = 0.004 1 + 0.995 value_choice[t] + -0.005 contr_diff + 0.004 choice + 0.003 value_choice^2 + -0.005 value_choice*contr_diff + -0.004 value_choice*choice + 0.002 contr_diff^2 + 0.004 contr_diff*choice + -0.016 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 4/1000 --- L(Train): 0.3656787 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.04 1 + 0.957 value_reward_chosen[t] + 0.038 contr_diff + 0.04 reward + -0.04 value_reward_chosen^2 + 0.013 value_reward_chosen*contr_diff + -0.04 value_reward_chosen*reward + 0.034 contr_diff^2 + 0.026 contr_diff*reward + 0.039 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.039 1 + 1.026 value_reward_not_chosen[t] + -0.001 contr_diff + -0.034 value_reward_not_chosen^2 + -0.031 value_reward_not_chosen*contr_diff + -0.04 contr_diff^2 \n",
            "value_choice[t+1] = 0.007 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.007 choice + -0.003 value_choice^2 + 0.001 value_choice*contr_diff + 0.002 value_choice*choice + -0.003 contr_diff^2 + -0.002 contr_diff*choice + -0.011 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 5/1000 --- L(Train): 0.3521926 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.05 1 + 0.947 value_reward_chosen[t] + 0.048 contr_diff + 0.05 reward + -0.049 value_reward_chosen^2 + 0.017 value_reward_chosen*contr_diff + -0.05 value_reward_chosen*reward + 0.042 contr_diff^2 + 0.028 contr_diff*reward + 0.049 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.048 1 + 1.024 value_reward_not_chosen[t] + 0.002 contr_diff + -0.037 value_reward_not_chosen^2 + -0.031 value_reward_not_chosen*contr_diff + -0.049 contr_diff^2 \n",
            "value_choice[t+1] = 0.008 1 + 1.002 value_choice[t] + 0.002 contr_diff + 0.007 choice + -0.005 value_choice^2 + 0.002 value_choice*contr_diff + 0.004 value_choice*choice + -0.005 contr_diff^2 + -0.004 contr_diff*choice + -0.006 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 6/1000 --- L(Train): 0.3391011 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.06 1 + 0.937 value_reward_chosen[t] + 0.058 contr_diff + 0.06 reward + -0.058 value_reward_chosen^2 + 0.02 value_reward_chosen*contr_diff + -0.06 value_reward_chosen*reward + 0.049 contr_diff^2 + 0.03 contr_diff*reward + 0.059 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.057 1 + 1.019 value_reward_not_chosen[t] + -0.0 contr_diff + -0.038 value_reward_not_chosen^2 + -0.027 value_reward_not_chosen*contr_diff + -0.057 contr_diff^2 \n",
            "value_choice[t+1] = 0.007 1 + 1.002 value_choice[t] + 0.001 contr_diff + 0.005 choice + -0.004 value_choice^2 + 0.002 value_choice*contr_diff + 0.003 value_choice*choice + -0.004 contr_diff^2 + -0.004 contr_diff*choice + -0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 7/1000 --- L(Train): 0.3264939 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.07 1 + 0.927 value_reward_chosen[t] + 0.067 contr_diff + 0.07 reward + -0.067 value_reward_chosen^2 + 0.022 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + 0.056 contr_diff^2 + 0.03 contr_diff*reward + 0.069 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.066 1 + 1.013 value_reward_not_chosen[t] + -0.002 contr_diff + -0.037 value_reward_not_chosen^2 + -0.022 value_reward_not_chosen*contr_diff + -0.065 contr_diff^2 \n",
            "value_choice[t+1] = 0.003 1 + 0.999 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.002 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 8/1000 --- L(Train): 0.3145371 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.08 1 + 0.918 value_reward_chosen[t] + 0.077 contr_diff + 0.079 reward + -0.075 value_reward_chosen^2 + 0.023 value_reward_chosen*contr_diff + -0.079 value_reward_chosen*reward + 0.062 contr_diff^2 + 0.03 contr_diff*reward + 0.079 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.074 1 + 1.006 value_reward_not_chosen[t] + -0.004 contr_diff + -0.035 value_reward_not_chosen^2 + -0.016 value_reward_not_chosen*contr_diff + -0.073 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.004 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + -0.003 value_choice*choice + 0.002 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 9/1000 --- L(Train): 0.3032407 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.09 1 + 0.908 value_reward_chosen[t] + 0.086 contr_diff + 0.089 reward + -0.083 value_reward_chosen^2 + 0.022 value_reward_chosen*contr_diff + -0.089 value_reward_chosen*reward + 0.068 contr_diff^2 + 0.028 contr_diff*reward + 0.089 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.082 1 + 0.999 value_reward_not_chosen[t] + -0.004 contr_diff + -0.031 value_reward_not_chosen^2 + -0.01 value_reward_not_chosen*contr_diff + -0.081 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.0 value_choice[t] + -0.0 contr_diff + -0.006 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + -0.004 value_choice*choice + 0.003 contr_diff^2 + 0.004 contr_diff*choice + 0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 10/1000 --- L(Train): 0.2924247 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.099 1 + 0.898 value_reward_chosen[t] + 0.095 contr_diff + 0.099 reward + -0.091 value_reward_chosen^2 + 0.021 value_reward_chosen*contr_diff + -0.098 value_reward_chosen*reward + 0.073 contr_diff^2 + 0.026 contr_diff*reward + 0.099 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.09 1 + 0.991 value_reward_not_chosen[t] + -0.004 contr_diff + -0.027 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.088 contr_diff^2 \n",
            "value_choice[t+1] = -0.003 1 + 1.0 value_choice[t] + 0.002 contr_diff + -0.007 choice + 0.003 value_choice^2 + 0.0 value_choice*contr_diff + -0.004 value_choice*choice + 0.003 contr_diff^2 + 0.003 contr_diff*choice + 0.006 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 11/1000 --- L(Train): 0.2819704 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.109 1 + 0.888 value_reward_chosen[t] + 0.104 contr_diff + 0.109 reward + -0.099 value_reward_chosen^2 + 0.019 value_reward_chosen*contr_diff + -0.108 value_reward_chosen*reward + 0.077 contr_diff^2 + 0.022 contr_diff*reward + 0.109 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.098 1 + 0.983 value_reward_not_chosen[t] + -0.002 contr_diff + -0.022 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.096 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.998 value_choice[t] + 0.003 contr_diff + -0.007 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + -0.003 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.009 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 12/1000 --- L(Train): 0.2718664 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.118 1 + 0.878 value_reward_chosen[t] + 0.114 contr_diff + 0.119 reward + -0.106 value_reward_chosen^2 + 0.016 value_reward_chosen*contr_diff + -0.117 value_reward_chosen*reward + 0.081 contr_diff^2 + 0.018 contr_diff*reward + 0.119 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.106 1 + 0.975 value_reward_not_chosen[t] + 0.001 contr_diff + -0.018 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.103 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.998 value_choice[t] + 0.001 contr_diff + -0.006 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.011 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 13/1000 --- L(Train): 0.2621328 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.128 1 + 0.869 value_reward_chosen[t] + 0.123 contr_diff + 0.129 reward + -0.113 value_reward_chosen^2 + 0.013 value_reward_chosen*contr_diff + -0.127 value_reward_chosen*reward + 0.083 contr_diff^2 + 0.012 contr_diff*reward + 0.128 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.115 1 + 0.967 value_reward_not_chosen[t] + 0.002 contr_diff + -0.014 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.11 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.006 choice + -0.002 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + -0.003 contr_diff^2 + -0.003 contr_diff*choice + 0.01 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 14/1000 --- L(Train): 0.2528000 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.137 1 + 0.859 value_reward_chosen[t] + 0.131 contr_diff + 0.138 reward + -0.12 value_reward_chosen^2 + 0.009 value_reward_chosen*contr_diff + -0.136 value_reward_chosen*reward + 0.084 contr_diff^2 + 0.006 contr_diff*reward + 0.138 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.123 1 + 0.96 value_reward_not_chosen[t] + 0.001 contr_diff + -0.011 value_reward_not_chosen^2 + -0.007 value_reward_not_chosen*contr_diff + -0.118 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.002 value_choice[t] + -0.002 contr_diff + -0.006 choice + -0.002 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.003 contr_diff^2 + -0.003 contr_diff*choice + 0.009 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 15/1000 --- L(Train): 0.2438422 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.146 1 + 0.85 value_reward_chosen[t] + 0.14 contr_diff + 0.148 reward + -0.128 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + -0.145 value_reward_chosen*reward + 0.085 contr_diff^2 + -0.0 contr_diff*reward + 0.148 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.132 1 + 0.953 value_reward_not_chosen[t] + -0.002 contr_diff + -0.008 value_reward_not_chosen^2 + -0.01 value_reward_not_chosen*contr_diff + -0.125 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.003 value_choice[t] + -0.001 contr_diff + -0.006 choice + -0.001 value_choice^2 + 0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.007 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 16/1000 --- L(Train): 0.2352131 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.154 1 + 0.84 value_reward_chosen[t] + 0.149 contr_diff + 0.158 reward + -0.135 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.154 value_reward_chosen*reward + 0.084 contr_diff^2 + -0.004 contr_diff*reward + 0.157 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.14 1 + 0.947 value_reward_not_chosen[t] + -0.004 contr_diff + -0.006 value_reward_not_chosen^2 + -0.013 value_reward_not_chosen*contr_diff + -0.132 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.002 value_choice[t] + 0.0 contr_diff + -0.005 choice + 0.002 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.004 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 17/1000 --- L(Train): 0.2268784 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.163 1 + 0.831 value_reward_chosen[t] + 0.157 contr_diff + 0.167 reward + -0.142 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.163 value_reward_chosen*reward + 0.082 contr_diff^2 + -0.006 contr_diff*reward + 0.167 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.149 1 + 0.942 value_reward_not_chosen[t] + -0.004 contr_diff + -0.005 value_reward_not_chosen^2 + -0.014 value_reward_not_chosen*contr_diff + -0.14 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + 0.001 contr_diff + -0.003 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.002 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 18/1000 --- L(Train): 0.2188246 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.171 1 + 0.821 value_reward_chosen[t] + 0.165 contr_diff + 0.177 reward + -0.149 value_reward_chosen^2 + -0.005 value_reward_chosen*contr_diff + -0.172 value_reward_chosen*reward + 0.078 contr_diff^2 + -0.007 contr_diff*reward + 0.176 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.158 1 + 0.937 value_reward_not_chosen[t] + -0.003 contr_diff + -0.005 value_reward_not_chosen^2 + -0.015 value_reward_not_chosen*contr_diff + -0.147 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.998 value_choice[t] + -0.0 contr_diff + -0.001 choice + 0.003 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 19/1000 --- L(Train): 0.2110668 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.18 1 + 0.812 value_reward_chosen[t] + 0.173 contr_diff + 0.186 reward + -0.157 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + -0.18 value_reward_chosen*reward + 0.074 contr_diff^2 + -0.006 contr_diff*reward + 0.186 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.167 1 + 0.932 value_reward_not_chosen[t] + -0.002 contr_diff + -0.005 value_reward_not_chosen^2 + -0.016 value_reward_not_chosen*contr_diff + -0.155 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 0.997 value_choice[t] + -0.0 contr_diff + 0.002 choice + 0.002 value_choice^2 + -0.003 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 20/1000 --- L(Train): 0.2036060 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.187 1 + 0.803 value_reward_chosen[t] + 0.181 contr_diff + 0.196 reward + -0.164 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.189 value_reward_chosen*reward + 0.069 contr_diff^2 + -0.004 contr_diff*reward + 0.195 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.176 1 + 0.928 value_reward_not_chosen[t] + -0.0 contr_diff + -0.006 value_reward_not_chosen^2 + -0.016 value_reward_not_chosen*contr_diff + -0.162 contr_diff^2 \n",
            "value_choice[t+1] = 0.003 1 + 0.997 value_choice[t] + 0.001 contr_diff + 0.004 choice + -0.001 value_choice^2 + -0.003 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 21/1000 --- L(Train): 0.1964303 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.195 1 + 0.794 value_reward_chosen[t] + 0.189 contr_diff + 0.205 reward + -0.171 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.197 value_reward_chosen*reward + 0.063 contr_diff^2 + -0.001 contr_diff*reward + 0.205 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.185 1 + 0.924 value_reward_not_chosen[t] + 0.002 contr_diff + -0.007 value_reward_not_chosen^2 + -0.015 value_reward_not_chosen*contr_diff + -0.169 contr_diff^2 \n",
            "value_choice[t+1] = 0.003 1 + 0.998 value_choice[t] + 0.001 contr_diff + 0.004 choice + -0.002 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + 0.0 contr_diff*choice + -0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 22/1000 --- L(Train): 0.1895328 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.203 1 + 0.784 value_reward_chosen[t] + 0.196 contr_diff + 0.214 reward + -0.178 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + -0.205 value_reward_chosen*reward + 0.056 contr_diff^2 + 0.002 contr_diff*reward + 0.214 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.194 1 + 0.921 value_reward_not_chosen[t] + 0.002 contr_diff + -0.009 value_reward_not_chosen^2 + -0.013 value_reward_not_chosen*contr_diff + -0.175 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.0 value_choice[t] + -0.0 contr_diff + 0.004 choice + -0.002 value_choice^2 + 0.0 value_choice*contr_diff + 0.002 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + -0.004 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 23/1000 --- L(Train): 0.1829035 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.21 1 + 0.775 value_reward_chosen[t] + 0.203 contr_diff + 0.223 reward + -0.184 value_reward_chosen^2 + 0.007 value_reward_chosen*contr_diff + -0.213 value_reward_chosen*reward + 0.048 contr_diff^2 + 0.003 contr_diff*reward + 0.223 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.203 1 + 0.917 value_reward_not_chosen[t] + 0.001 contr_diff + -0.01 value_reward_not_chosen^2 + -0.011 value_reward_not_chosen*contr_diff + -0.182 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + -0.0 contr_diff + 0.003 choice + -0.0 value_choice^2 + 0.001 value_choice*contr_diff + 0.003 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.004 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 24/1000 --- L(Train): 0.1765384 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.217 1 + 0.766 value_reward_chosen[t] + 0.21 contr_diff + 0.233 reward + -0.19 value_reward_chosen^2 + 0.008 value_reward_chosen*contr_diff + -0.22 value_reward_chosen*reward + 0.04 contr_diff^2 + 0.002 contr_diff*reward + 0.232 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.212 1 + 0.913 value_reward_not_chosen[t] + -0.002 contr_diff + -0.011 value_reward_not_chosen^2 + -0.008 value_reward_not_chosen*contr_diff + -0.187 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.002 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + 0.002 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 25/1000 --- L(Train): 0.1704196 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.223 1 + 0.758 value_reward_chosen[t] + 0.217 contr_diff + 0.242 reward + -0.196 value_reward_chosen^2 + 0.008 value_reward_chosen*contr_diff + -0.228 value_reward_chosen*reward + 0.031 contr_diff^2 + -0.001 contr_diff*reward + 0.241 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.22 1 + 0.909 value_reward_not_chosen[t] + -0.004 contr_diff + -0.012 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + -0.192 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.999 value_choice[t] + 0.0 contr_diff + 0.001 choice + 0.003 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 26/1000 --- L(Train): 0.1645399 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.229 1 + 0.749 value_reward_chosen[t] + 0.223 contr_diff + 0.251 reward + -0.201 value_reward_chosen^2 + 0.007 value_reward_chosen*contr_diff + -0.235 value_reward_chosen*reward + 0.022 contr_diff^2 + -0.003 contr_diff*reward + 0.25 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.228 1 + 0.904 value_reward_not_chosen[t] + -0.005 contr_diff + -0.012 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.197 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.003 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 27/1000 --- L(Train): 0.1589020 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.235 1 + 0.74 value_reward_chosen[t] + 0.229 contr_diff + 0.26 reward + -0.207 value_reward_chosen^2 + 0.006 value_reward_chosen*contr_diff + -0.242 value_reward_chosen*reward + 0.012 contr_diff^2 + -0.004 contr_diff*reward + 0.259 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.236 1 + 0.899 value_reward_not_chosen[t] + -0.004 contr_diff + -0.011 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.201 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.0 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.002 value_choice^2 + 0.0 value_choice*contr_diff + -0.003 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.004 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 28/1000 --- L(Train): 0.1534895 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.241 1 + 0.732 value_reward_chosen[t] + 0.235 contr_diff + 0.269 reward + -0.212 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + -0.249 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.005 contr_diff*reward + 0.268 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.244 1 + 0.894 value_reward_not_chosen[t] + -0.003 contr_diff + -0.01 value_reward_not_chosen^2 + 0.004 value_reward_not_chosen*contr_diff + -0.204 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.0 value_choice[t] + 0.0 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.0 value_choice*contr_diff + -0.002 value_choice*choice + -0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.005 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 29/1000 --- L(Train): 0.1482963 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.246 1 + 0.723 value_reward_chosen[t] + 0.24 contr_diff + 0.277 reward + -0.217 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.255 value_reward_chosen*reward + -0.009 contr_diff^2 + -0.005 contr_diff*reward + 0.277 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.251 1 + 0.889 value_reward_not_chosen[t] + -0.0 contr_diff + -0.009 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.206 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.001 value_choice[t] + -0.0 contr_diff + -0.002 choice + -0.002 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + 0.001 contr_diff*choice + 0.004 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 30/1000 --- L(Train): 0.1433149 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.251 1 + 0.715 value_reward_chosen[t] + 0.245 contr_diff + 0.286 reward + -0.221 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.262 value_reward_chosen*reward + -0.019 contr_diff^2 + -0.006 contr_diff*reward + 0.286 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.258 1 + 0.883 value_reward_not_chosen[t] + 0.003 contr_diff + -0.008 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.208 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.001 value_choice[t] + 0.001 contr_diff + -0.002 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 31/1000 --- L(Train): 0.1385303 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.255 1 + 0.707 value_reward_chosen[t] + 0.25 contr_diff + 0.295 reward + -0.226 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + -0.268 value_reward_chosen*reward + -0.028 contr_diff^2 + -0.006 contr_diff*reward + 0.294 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.266 1 + 0.878 value_reward_not_chosen[t] + 0.005 contr_diff + -0.006 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.21 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + 0.0 contr_diff + -0.001 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 32/1000 --- L(Train): 0.1339395 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.259 1 + 0.698 value_reward_chosen[t] + 0.254 contr_diff + 0.303 reward + -0.231 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.273 value_reward_chosen*reward + -0.037 contr_diff^2 + -0.005 contr_diff*reward + 0.303 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.273 1 + 0.872 value_reward_not_chosen[t] + 0.005 contr_diff + -0.004 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + -0.21 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.0 choice + 0.002 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 33/1000 --- L(Train): 0.1295505 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.263 1 + 0.69 value_reward_chosen[t] + 0.258 contr_diff + 0.312 reward + -0.235 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.279 value_reward_chosen*reward + -0.046 contr_diff^2 + -0.005 contr_diff*reward + 0.311 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.279 1 + 0.867 value_reward_not_chosen[t] + 0.003 contr_diff + -0.003 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + -0.211 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.002 choice + 0.003 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 34/1000 --- L(Train): 0.1253458 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.266 1 + 0.682 value_reward_chosen[t] + 0.262 contr_diff + 0.32 reward + -0.24 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.284 value_reward_chosen*reward + -0.054 contr_diff^2 + -0.005 contr_diff*reward + 0.32 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.286 1 + 0.862 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + -0.21 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.0 value_choice[t] + -0.001 contr_diff + 0.003 choice + 0.003 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 35/1000 --- L(Train): 0.1213167 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.269 1 + 0.674 value_reward_chosen[t] + 0.265 contr_diff + 0.328 reward + -0.244 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.289 value_reward_chosen*reward + -0.062 contr_diff^2 + -0.005 contr_diff*reward + 0.328 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.293 1 + 0.857 value_reward_not_chosen[t] + -0.003 contr_diff + -0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.21 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.003 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.0 contr_diff^2 + 0.001 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 36/1000 --- L(Train): 0.1174498 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.271 1 + 0.667 value_reward_chosen[t] + 0.268 contr_diff + 0.337 reward + -0.248 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + -0.294 value_reward_chosen*reward + -0.071 contr_diff^2 + -0.004 contr_diff*reward + 0.336 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.3 1 + 0.852 value_reward_not_chosen[t] + -0.005 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.209 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.002 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 37/1000 --- L(Train): 0.1137440 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.273 1 + 0.659 value_reward_chosen[t] + 0.27 contr_diff + 0.345 reward + -0.251 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + -0.298 value_reward_chosen*reward + -0.079 contr_diff^2 + -0.004 contr_diff*reward + 0.344 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.306 1 + 0.848 value_reward_not_chosen[t] + -0.006 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.207 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.001 value_choice^2 + -0.003 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 38/1000 --- L(Train): 0.1101857 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.274 1 + 0.651 value_reward_chosen[t] + 0.272 contr_diff + 0.353 reward + -0.254 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.303 value_reward_chosen*reward + -0.087 contr_diff^2 + -0.004 contr_diff*reward + 0.352 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.313 1 + 0.844 value_reward_not_chosen[t] + -0.005 contr_diff + 0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.205 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.001 value_choice^2 + -0.003 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 39/1000 --- L(Train): 0.1067771 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.275 1 + 0.644 value_reward_chosen[t] + 0.274 contr_diff + 0.361 reward + -0.257 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.307 value_reward_chosen*reward + -0.095 contr_diff^2 + -0.005 contr_diff*reward + 0.36 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.319 1 + 0.841 value_reward_not_chosen[t] + -0.004 contr_diff + -0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.203 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + -0.002 contr_diff + -0.002 choice + -0.0 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 40/1000 --- L(Train): 0.1035070 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.276 1 + 0.637 value_reward_chosen[t] + 0.276 contr_diff + 0.369 reward + -0.26 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.31 value_reward_chosen*reward + -0.103 contr_diff^2 + -0.005 contr_diff*reward + 0.368 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.325 1 + 0.837 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.2 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + -0.001 contr_diff + -0.002 choice + 0.002 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 41/1000 --- L(Train): 0.1003692 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.276 1 + 0.629 value_reward_chosen[t] + 0.277 contr_diff + 0.376 reward + -0.262 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.314 value_reward_chosen*reward + -0.111 contr_diff^2 + -0.006 contr_diff*reward + 0.376 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.332 1 + 0.834 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.197 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.002 value_choice[t] + 0.0 contr_diff + -0.002 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 42/1000 --- L(Train): 0.0973623 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.276 1 + 0.622 value_reward_chosen[t] + 0.277 contr_diff + 0.384 reward + -0.264 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.317 value_reward_chosen*reward + -0.119 contr_diff^2 + -0.006 contr_diff*reward + 0.383 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.338 1 + 0.831 value_reward_not_chosen[t] + 0.004 contr_diff + -0.002 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.193 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.002 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 43/1000 --- L(Train): 0.0944746 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.275 1 + 0.615 value_reward_chosen[t] + 0.278 contr_diff + 0.392 reward + -0.266 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.319 value_reward_chosen*reward + -0.128 contr_diff^2 + -0.007 contr_diff*reward + 0.391 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.344 1 + 0.828 value_reward_not_chosen[t] + 0.005 contr_diff + -0.003 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + -0.188 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.001 value_choice[t] + -0.0 contr_diff + -0.0 choice + 0.002 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 44/1000 --- L(Train): 0.0917022 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.274 1 + 0.608 value_reward_chosen[t] + 0.278 contr_diff + 0.399 reward + -0.268 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.322 value_reward_chosen*reward + -0.137 contr_diff^2 + -0.009 contr_diff*reward + 0.398 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.349 1 + 0.824 value_reward_not_chosen[t] + 0.004 contr_diff + -0.003 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.183 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.998 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.002 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 45/1000 --- L(Train): 0.0890446 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.273 1 + 0.601 value_reward_chosen[t] + 0.278 contr_diff + 0.406 reward + -0.27 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.324 value_reward_chosen*reward + -0.145 contr_diff^2 + -0.01 contr_diff*reward + 0.406 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.355 1 + 0.821 value_reward_not_chosen[t] + 0.002 contr_diff + -0.003 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.178 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.997 value_choice[t] + -0.001 contr_diff + 0.002 choice + -0.001 value_choice^2 + 0.0 value_choice*contr_diff + 0.002 value_choice*choice + -0.003 contr_diff^2 + -0.002 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 46/1000 --- L(Train): 0.0864911 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.271 1 + 0.595 value_reward_chosen[t] + 0.277 contr_diff + 0.414 reward + -0.271 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.326 value_reward_chosen*reward + -0.154 contr_diff^2 + -0.012 contr_diff*reward + 0.413 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.36 1 + 0.817 value_reward_not_chosen[t] + -0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.172 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.997 value_choice[t] + -0.0 contr_diff + 0.002 choice + -0.001 value_choice^2 + 0.0 value_choice*contr_diff + 0.002 value_choice*choice + -0.003 contr_diff^2 + -0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 47/1000 --- L(Train): 0.0840415 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.268 1 + 0.588 value_reward_chosen[t] + 0.277 contr_diff + 0.421 reward + -0.272 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.328 value_reward_chosen*reward + -0.163 contr_diff^2 + -0.013 contr_diff*reward + 0.42 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.365 1 + 0.814 value_reward_not_chosen[t] + -0.002 contr_diff + -0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.166 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.998 value_choice[t] + 0.001 contr_diff + 0.002 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.002 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 48/1000 --- L(Train): 0.0816893 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.265 1 + 0.581 value_reward_chosen[t] + 0.276 contr_diff + 0.428 reward + -0.273 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.329 value_reward_chosen*reward + -0.172 contr_diff^2 + -0.015 contr_diff*reward + 0.427 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.37 1 + 0.81 value_reward_not_chosen[t] + -0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.159 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.001 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.0 contr_diff^2 + 0.003 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 49/1000 --- L(Train): 0.0794344 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.262 1 + 0.575 value_reward_chosen[t] + 0.275 contr_diff + 0.435 reward + -0.274 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.331 value_reward_chosen*reward + -0.182 contr_diff^2 + -0.017 contr_diff*reward + 0.434 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.374 1 + 0.807 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.152 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + -0.0 contr_diff + -0.001 choice + 0.003 value_choice^2 + -0.0 value_choice*contr_diff + -0.003 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 50/1000 --- L(Train): 0.0772654 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.259 1 + 0.568 value_reward_chosen[t] + 0.273 contr_diff + 0.442 reward + -0.274 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.332 value_reward_chosen*reward + -0.191 contr_diff^2 + -0.019 contr_diff*reward + 0.441 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.379 1 + 0.804 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.144 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + -0.0 contr_diff + -0.002 choice + 0.003 value_choice^2 + 0.002 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 51/1000 --- L(Train): 0.0751790 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.255 1 + 0.562 value_reward_chosen[t] + 0.272 contr_diff + 0.449 reward + -0.275 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.332 value_reward_chosen*reward + -0.2 contr_diff^2 + -0.021 contr_diff*reward + 0.448 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.383 1 + 0.8 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.137 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + 0.001 contr_diff + -0.002 choice + 0.002 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 52/1000 --- L(Train): 0.0731683 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.251 1 + 0.556 value_reward_chosen[t] + 0.27 contr_diff + 0.456 reward + -0.275 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.333 value_reward_chosen*reward + -0.21 contr_diff^2 + -0.023 contr_diff*reward + 0.455 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.387 1 + 0.798 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.129 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.999 value_choice[t] + 0.0 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 53/1000 --- L(Train): 0.0712384 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.247 1 + 0.55 value_reward_chosen[t] + 0.268 contr_diff + 0.462 reward + -0.275 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.333 value_reward_chosen*reward + -0.22 contr_diff^2 + -0.026 contr_diff*reward + 0.462 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.392 1 + 0.795 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.121 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.0 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 54/1000 --- L(Train): 0.0693788 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.242 1 + 0.544 value_reward_chosen[t] + 0.266 contr_diff + 0.469 reward + -0.274 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.333 value_reward_chosen*reward + -0.23 contr_diff^2 + -0.028 contr_diff*reward + 0.468 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.396 1 + 0.792 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.112 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.0 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 55/1000 --- L(Train): 0.0675911 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.237 1 + 0.538 value_reward_chosen[t] + 0.264 contr_diff + 0.476 reward + -0.274 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.333 value_reward_chosen*reward + -0.239 contr_diff^2 + -0.03 contr_diff*reward + 0.475 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.4 1 + 0.79 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.104 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.999 value_choice[t] + -0.0 contr_diff + -0.0 choice + 0.0 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 56/1000 --- L(Train): 0.0658658 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.231 1 + 0.532 value_reward_chosen[t] + 0.261 contr_diff + 0.482 reward + -0.273 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.333 value_reward_chosen*reward + -0.249 contr_diff^2 + -0.032 contr_diff*reward + 0.481 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.404 1 + 0.788 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.095 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.999 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.0 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 57/1000 --- L(Train): 0.0642045 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.226 1 + 0.526 value_reward_chosen[t] + 0.259 contr_diff + 0.488 reward + -0.272 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.332 value_reward_chosen*reward + -0.259 contr_diff^2 + -0.034 contr_diff*reward + 0.487 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.407 1 + 0.786 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.086 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + 0.002 contr_diff + -0.0 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 58/1000 --- L(Train): 0.0626042 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.22 1 + 0.521 value_reward_chosen[t] + 0.256 contr_diff + 0.495 reward + -0.272 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.332 value_reward_chosen*reward + -0.269 contr_diff^2 + -0.036 contr_diff*reward + 0.494 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.411 1 + 0.784 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.077 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 59/1000 --- L(Train): 0.0610600 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.214 1 + 0.515 value_reward_chosen[t] + 0.254 contr_diff + 0.501 reward + -0.271 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.331 value_reward_chosen*reward + -0.279 contr_diff^2 + -0.038 contr_diff*reward + 0.5 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.414 1 + 0.782 value_reward_not_chosen[t] + -0.002 contr_diff + -0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.068 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + -0.001 contr_diff + 0.001 choice + 0.0 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 60/1000 --- L(Train): 0.0595711 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.208 1 + 0.509 value_reward_chosen[t] + 0.251 contr_diff + 0.507 reward + -0.269 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.33 value_reward_chosen*reward + -0.289 contr_diff^2 + -0.04 contr_diff*reward + 0.506 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.418 1 + 0.78 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.058 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.002 value_choice[t] + -0.001 contr_diff + 0.0 choice + 0.0 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 61/1000 --- L(Train): 0.0581355 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.201 1 + 0.504 value_reward_chosen[t] + 0.249 contr_diff + 0.513 reward + -0.268 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.329 value_reward_chosen*reward + -0.299 contr_diff^2 + -0.041 contr_diff*reward + 0.512 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.421 1 + 0.778 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.049 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.0 value_choice[t] + -0.0 contr_diff + -0.001 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 62/1000 --- L(Train): 0.0567484 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.194 1 + 0.498 value_reward_chosen[t] + 0.246 contr_diff + 0.519 reward + -0.267 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.327 value_reward_chosen*reward + -0.309 contr_diff^2 + -0.042 contr_diff*reward + 0.518 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.424 1 + 0.776 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.039 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.998 value_choice[t] + 0.001 contr_diff + -0.001 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.002 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 63/1000 --- L(Train): 0.0554079 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.188 1 + 0.493 value_reward_chosen[t] + 0.243 contr_diff + 0.525 reward + -0.265 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.326 value_reward_chosen*reward + -0.319 contr_diff^2 + -0.043 contr_diff*reward + 0.523 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.427 1 + 0.774 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.029 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.997 value_choice[t] + 0.001 contr_diff + 0.0 choice + 0.001 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 64/1000 --- L(Train): 0.0541101 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.181 1 + 0.488 value_reward_chosen[t] + 0.241 contr_diff + 0.53 reward + -0.264 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.324 value_reward_chosen*reward + -0.329 contr_diff^2 + -0.044 contr_diff*reward + 0.529 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.429 1 + 0.772 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.019 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.997 value_choice[t] + 0.0 contr_diff + 0.001 choice + 0.001 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 65/1000 --- L(Train): 0.0528601 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.173 1 + 0.483 value_reward_chosen[t] + 0.238 contr_diff + 0.536 reward + -0.262 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.323 value_reward_chosen*reward + -0.339 contr_diff^2 + -0.045 contr_diff*reward + 0.535 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.432 1 + 0.77 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.009 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.998 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 66/1000 --- L(Train): 0.0516491 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.166 1 + 0.477 value_reward_chosen[t] + 0.235 contr_diff + 0.542 reward + -0.26 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.321 value_reward_chosen*reward + -0.349 contr_diff^2 + -0.045 contr_diff*reward + 0.54 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.435 1 + 0.769 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + -0.002 contr_diff + -0.001 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + 0.002 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 67/1000 --- L(Train): 0.0504756 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.159 1 + 0.472 value_reward_chosen[t] + 0.233 contr_diff + 0.547 reward + -0.258 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.319 value_reward_chosen*reward + -0.358 contr_diff^2 + -0.045 contr_diff*reward + 0.546 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.437 1 + 0.768 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.008 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + -0.002 contr_diff + -0.0 choice + 0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.002 value_choice*choice + -0.003 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 68/1000 --- L(Train): 0.0493413 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.151 1 + 0.467 value_reward_chosen[t] + 0.23 contr_diff + 0.553 reward + -0.256 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.317 value_reward_chosen*reward + -0.368 contr_diff^2 + -0.045 contr_diff*reward + 0.551 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.439 1 + 0.766 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.001 value_choice[t] + -0.0 contr_diff + 0.001 choice + 0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.002 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 69/1000 --- L(Train): 0.0482467 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.144 1 + 0.462 value_reward_chosen[t] + 0.227 contr_diff + 0.558 reward + -0.254 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.315 value_reward_chosen*reward + -0.377 contr_diff^2 + -0.045 contr_diff*reward + 0.557 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.442 1 + 0.765 value_reward_not_chosen[t] + -0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.015 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.0 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 70/1000 --- L(Train): 0.0471851 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.136 1 + 0.457 value_reward_chosen[t] + 0.225 contr_diff + 0.563 reward + -0.252 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.313 value_reward_chosen*reward + -0.386 contr_diff^2 + -0.044 contr_diff*reward + 0.562 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.444 1 + 0.764 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.016 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.999 value_choice[t] + 0.003 contr_diff + -0.0 choice + 0.0 value_choice^2 + 0.002 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 71/1000 --- L(Train): 0.0461541 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.128 1 + 0.452 value_reward_chosen[t] + 0.222 contr_diff + 0.569 reward + -0.25 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.311 value_reward_chosen*reward + -0.396 contr_diff^2 + -0.044 contr_diff*reward + 0.567 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.446 1 + 0.762 value_reward_not_chosen[t] + -0.0 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.015 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.0 value_choice[t] + 0.002 contr_diff + -0.0 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.003 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 72/1000 --- L(Train): 0.0451527 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.12 1 + 0.447 value_reward_chosen[t] + 0.22 contr_diff + 0.574 reward + -0.247 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.308 value_reward_chosen*reward + -0.405 contr_diff^2 + -0.043 contr_diff*reward + 0.572 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.448 1 + 0.761 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.013 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 73/1000 --- L(Train): 0.0441861 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.112 1 + 0.443 value_reward_chosen[t] + 0.217 contr_diff + 0.579 reward + -0.245 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.306 value_reward_chosen*reward + -0.413 contr_diff^2 + -0.041 contr_diff*reward + 0.577 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.45 1 + 0.76 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.009 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + -0.001 contr_diff + 0.001 choice + 0.0 value_choice^2 + -0.003 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 74/1000 --- L(Train): 0.0432498 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.104 1 + 0.438 value_reward_chosen[t] + 0.215 contr_diff + 0.584 reward + -0.243 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.304 value_reward_chosen*reward + -0.422 contr_diff^2 + -0.04 contr_diff*reward + 0.582 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.452 1 + 0.759 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.001 choice + 0.0 value_choice^2 + -0.003 value_choice*contr_diff + 0.001 value_choice*choice + -0.0 contr_diff^2 + -0.004 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 75/1000 --- L(Train): 0.0423400 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.096 1 + 0.433 value_reward_chosen[t] + 0.212 contr_diff + 0.589 reward + -0.24 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.301 value_reward_chosen*reward + -0.431 contr_diff^2 + -0.038 contr_diff*reward + 0.587 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.453 1 + 0.758 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + -0.002 contr_diff + -0.001 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + -0.004 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 76/1000 --- L(Train): 0.0414547 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.088 1 + 0.428 value_reward_chosen[t] + 0.209 contr_diff + 0.594 reward + -0.238 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.299 value_reward_chosen*reward + -0.439 contr_diff^2 + -0.036 contr_diff*reward + 0.592 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.455 1 + 0.757 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.005 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.0 value_choice^2 + 0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.0 contr_diff^2 + -0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 77/1000 --- L(Train): 0.0405983 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.08 1 + 0.424 value_reward_chosen[t] + 0.207 contr_diff + 0.599 reward + -0.235 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.296 value_reward_chosen*reward + -0.447 contr_diff^2 + -0.034 contr_diff*reward + 0.597 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.456 1 + 0.756 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.007 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.001 contr_diff + -0.0 choice + 0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 78/1000 --- L(Train): 0.0397697 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.072 1 + 0.419 value_reward_chosen[t] + 0.204 contr_diff + 0.603 reward + -0.233 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.294 value_reward_chosen*reward + -0.455 contr_diff^2 + -0.032 contr_diff*reward + 0.602 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.458 1 + 0.755 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.008 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.0 value_choice[t] + 0.002 contr_diff + 0.001 choice + 0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 79/1000 --- L(Train): 0.0389612 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.064 1 + 0.414 value_reward_chosen[t] + 0.202 contr_diff + 0.608 reward + -0.23 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.291 value_reward_chosen*reward + -0.463 contr_diff^2 + -0.029 contr_diff*reward + 0.606 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.459 1 + 0.754 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.008 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.0 contr_diff^2 + 0.003 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 80/1000 --- L(Train): 0.0381739 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.056 1 + 0.41 value_reward_chosen[t] + 0.2 contr_diff + 0.613 reward + -0.227 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.289 value_reward_chosen*reward + -0.47 contr_diff^2 + -0.027 contr_diff*reward + 0.611 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.46 1 + 0.753 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.006 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + -0.0 contr_diff + -0.001 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + 0.004 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 81/1000 --- L(Train): 0.0374134 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.048 1 + 0.405 value_reward_chosen[t] + 0.197 contr_diff + 0.617 reward + -0.225 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.286 value_reward_chosen*reward + -0.477 contr_diff^2 + -0.024 contr_diff*reward + 0.615 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.461 1 + 0.752 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.002 choice + 0.001 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 82/1000 --- L(Train): 0.0366724 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.04 1 + 0.401 value_reward_chosen[t] + 0.195 contr_diff + 0.622 reward + -0.222 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.284 value_reward_chosen*reward + -0.485 contr_diff^2 + -0.021 contr_diff*reward + 0.62 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.462 1 + 0.751 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + -0.0 contr_diff + -0.0 choice + 0.001 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 83/1000 --- L(Train): 0.0359534 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.032 1 + 0.396 value_reward_chosen[t] + 0.192 contr_diff + 0.626 reward + -0.219 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.281 value_reward_chosen*reward + -0.491 contr_diff^2 + -0.018 contr_diff*reward + 0.624 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.463 1 + 0.751 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.002 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 84/1000 --- L(Train): 0.0352565 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.024 1 + 0.392 value_reward_chosen[t] + 0.189 contr_diff + 0.631 reward + -0.216 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.278 value_reward_chosen*reward + -0.498 contr_diff^2 + -0.015 contr_diff*reward + 0.628 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.464 1 + 0.75 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + 0.001 contr_diff + 0.002 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 85/1000 --- L(Train): 0.0345777 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.016 1 + 0.387 value_reward_chosen[t] + 0.187 contr_diff + 0.635 reward + -0.214 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.276 value_reward_chosen*reward + -0.504 contr_diff^2 + -0.012 contr_diff*reward + 0.633 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.465 1 + 0.749 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.006 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.002 value_choice[t] + 0.001 contr_diff + 0.001 choice + 0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.0 contr_diff^2 + -0.004 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 86/1000 --- L(Train): 0.0339184 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.008 1 + 0.383 value_reward_chosen[t] + 0.184 contr_diff + 0.639 reward + -0.211 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.273 value_reward_chosen*reward + -0.51 contr_diff^2 + -0.008 contr_diff*reward + 0.637 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.466 1 + 0.749 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + -0.001 contr_diff + 0.0 choice + 0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.0 contr_diff^2 + -0.003 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 87/1000 --- L(Train): 0.0332779 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.0 1 + 0.379 value_reward_chosen[t] + 0.182 contr_diff + 0.643 reward + -0.208 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.271 value_reward_chosen*reward + -0.516 contr_diff^2 + -0.005 contr_diff*reward + 0.641 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.466 1 + 0.748 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.0 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 88/1000 --- L(Train): 0.0326509 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.008 1 + 0.374 value_reward_chosen[t] + 0.179 contr_diff + 0.648 reward + -0.205 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.268 value_reward_chosen*reward + -0.522 contr_diff^2 + -0.002 contr_diff*reward + 0.645 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.998 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.002 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 89/1000 --- L(Train): 0.0320456 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.016 1 + 0.37 value_reward_chosen[t] + 0.176 contr_diff + 0.652 reward + -0.203 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.266 value_reward_chosen*reward + -0.527 contr_diff^2 + 0.002 contr_diff*reward + 0.649 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.997 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.003 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 90/1000 --- L(Train): 0.0314545 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.023 1 + 0.366 value_reward_chosen[t] + 0.174 contr_diff + 0.656 reward + -0.2 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.263 value_reward_chosen*reward + -0.532 contr_diff^2 + 0.001 contr_diff*reward + 0.653 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.006 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.997 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.002 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 91/1000 --- L(Train): 0.0308805 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.031 1 + 0.362 value_reward_chosen[t] + 0.171 contr_diff + 0.66 reward + -0.197 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.261 value_reward_chosen*reward + -0.537 contr_diff^2 + -0.003 contr_diff*reward + 0.657 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.007 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.998 value_choice[t] + -0.0 contr_diff + -0.001 choice + -0.001 value_choice^2 + -0.003 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 92/1000 --- L(Train): 0.0303242 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.039 1 + 0.357 value_reward_chosen[t] + 0.168 contr_diff + 0.664 reward + -0.194 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.258 value_reward_chosen*reward + -0.541 contr_diff^2 + -0.005 contr_diff*reward + 0.661 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.745 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.006 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + -0.0 contr_diff + -0.0 choice + 0.001 value_choice^2 + -0.003 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 93/1000 --- L(Train): 0.0297832 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.046 1 + 0.353 value_reward_chosen[t] + 0.166 contr_diff + 0.667 reward + -0.192 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.256 value_reward_chosen*reward + -0.545 contr_diff^2 + -0.008 contr_diff*reward + 0.665 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.745 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.005 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.001 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 94/1000 --- L(Train): 0.0292559 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.054 1 + 0.349 value_reward_chosen[t] + 0.163 contr_diff + 0.671 reward + -0.189 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.253 value_reward_chosen*reward + -0.549 contr_diff^2 + -0.009 contr_diff*reward + 0.669 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.744 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.001 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + 0.002 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 95/1000 --- L(Train): 0.0287416 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.061 1 + 0.345 value_reward_chosen[t] + 0.16 contr_diff + 0.675 reward + -0.186 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.251 value_reward_chosen*reward + -0.553 contr_diff^2 + -0.01 contr_diff*reward + 0.672 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.744 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + -0.0 contr_diff + 0.001 choice + 0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 96/1000 --- L(Train): 0.0282444 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.069 1 + 0.341 value_reward_chosen[t] + 0.158 contr_diff + 0.679 reward + -0.184 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.249 value_reward_chosen*reward + -0.556 contr_diff^2 + -0.01 contr_diff*reward + 0.676 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.744 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + -0.0 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 97/1000 --- L(Train): 0.0277606 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.076 1 + 0.337 value_reward_chosen[t] + 0.155 contr_diff + 0.682 reward + -0.181 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.246 value_reward_chosen*reward + -0.559 contr_diff^2 + -0.009 contr_diff*reward + 0.68 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.744 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + 0.001 contr_diff + -0.001 choice + -0.002 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 98/1000 --- L(Train): 0.0272880 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.083 1 + 0.333 value_reward_chosen[t] + 0.153 contr_diff + 0.686 reward + -0.178 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.244 value_reward_chosen*reward + -0.562 contr_diff^2 + -0.008 contr_diff*reward + 0.683 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.744 value_reward_not_chosen[t] + -0.0 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.0 contr_diff + -0.0 choice + -0.002 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 99/1000 --- L(Train): 0.0268273 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.09 1 + 0.329 value_reward_chosen[t] + 0.151 contr_diff + 0.69 reward + -0.176 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.241 value_reward_chosen*reward + -0.564 contr_diff^2 + -0.006 contr_diff*reward + 0.687 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.744 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.001 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.0 value_choice^2 + 0.0 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 100/1000 --- L(Train): 0.0263806 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.097 1 + 0.325 value_reward_chosen[t] + 0.148 contr_diff + 0.693 reward + -0.173 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.239 value_reward_chosen*reward + -0.567 contr_diff^2 + -0.004 contr_diff*reward + 0.69 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.744 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.001 value_choice[t] + -0.001 contr_diff + 0.001 choice + 0.002 value_choice^2 + 0.0 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 101/1000 --- L(Train): 0.0259480 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.105 1 + 0.321 value_reward_chosen[t] + 0.146 contr_diff + 0.697 reward + -0.171 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.237 value_reward_chosen*reward + -0.569 contr_diff^2 + -0.001 contr_diff*reward + 0.694 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.744 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.999 value_choice[t] + -0.0 contr_diff + -0.001 choice + 0.003 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 102/1000 --- L(Train): 0.0255255 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.111 1 + 0.317 value_reward_chosen[t] + 0.144 contr_diff + 0.7 reward + -0.168 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.235 value_reward_chosen*reward + -0.57 contr_diff^2 + 0.002 contr_diff*reward + 0.697 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.744 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 103/1000 --- L(Train): 0.0251145 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.118 1 + 0.313 value_reward_chosen[t] + 0.142 contr_diff + 0.704 reward + -0.165 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.232 value_reward_chosen*reward + -0.572 contr_diff^2 + 0.002 contr_diff*reward + 0.7 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.744 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + 0.002 contr_diff + -0.001 choice + 0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + -0.003 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 104/1000 --- L(Train): 0.0247148 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.125 1 + 0.309 value_reward_chosen[t] + 0.14 contr_diff + 0.707 reward + -0.163 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.23 value_reward_chosen*reward + -0.573 contr_diff^2 + -0.001 contr_diff*reward + 0.704 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.744 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.003 contr_diff^2 + 0.001 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 105/1000 --- L(Train): 0.0243292 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.132 1 + 0.305 value_reward_chosen[t] + 0.138 contr_diff + 0.71 reward + -0.16 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.228 value_reward_chosen*reward + -0.574 contr_diff^2 + -0.003 contr_diff*reward + 0.707 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.744 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.002 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 106/1000 --- L(Train): 0.0239510 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.139 1 + 0.301 value_reward_chosen[t] + 0.135 contr_diff + 0.713 reward + -0.158 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.226 value_reward_chosen*reward + -0.574 contr_diff^2 + -0.004 contr_diff*reward + 0.71 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.744 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 107/1000 --- L(Train): 0.0235831 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.145 1 + 0.298 value_reward_chosen[t] + 0.134 contr_diff + 0.717 reward + -0.155 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.224 value_reward_chosen*reward + -0.575 contr_diff^2 + -0.004 contr_diff*reward + 0.713 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.745 value_reward_not_chosen[t] + 0.003 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.0 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 108/1000 --- L(Train): 0.0232239 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.152 1 + 0.294 value_reward_chosen[t] + 0.132 contr_diff + 0.72 reward + -0.153 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.222 value_reward_chosen*reward + -0.575 contr_diff^2 + -0.003 contr_diff*reward + 0.716 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.745 value_reward_not_chosen[t] + 0.003 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.999 value_choice[t] + 0.001 contr_diff + 0.001 choice + 0.002 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 109/1000 --- L(Train): 0.0228796 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.29 value_reward_chosen[t] + 0.13 contr_diff + 0.723 reward + -0.151 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.219 value_reward_chosen*reward + -0.575 contr_diff^2 + -0.002 contr_diff*reward + 0.719 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.745 value_reward_not_chosen[t] + 0.003 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.001 choice + 0.003 value_choice^2 + -0.003 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 110/1000 --- L(Train): 0.0225413 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.286 value_reward_chosen[t] + 0.128 contr_diff + 0.726 reward + -0.148 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.217 value_reward_chosen*reward + -0.574 contr_diff^2 + 0.0 contr_diff*reward + 0.722 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.745 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.002 value_choice[t] + 0.0 contr_diff + 0.001 choice + 0.003 value_choice^2 + -0.003 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 111/1000 --- L(Train): 0.0222107 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.171 1 + 0.283 value_reward_chosen[t] + 0.126 contr_diff + 0.729 reward + -0.146 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.215 value_reward_chosen*reward + -0.574 contr_diff^2 + -0.001 contr_diff*reward + 0.725 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.745 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + -0.002 contr_diff + -0.001 choice + 0.001 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 112/1000 --- L(Train): 0.0218859 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.177 1 + 0.279 value_reward_chosen[t] + 0.125 contr_diff + 0.732 reward + -0.144 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.213 value_reward_chosen*reward + -0.573 contr_diff^2 + -0.0 contr_diff*reward + 0.728 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.471 1 + 0.745 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.002 value_choice[t] + -0.002 contr_diff + -0.002 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 113/1000 --- L(Train): 0.0215743 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.183 1 + 0.275 value_reward_chosen[t] + 0.123 contr_diff + 0.735 reward + -0.141 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.211 value_reward_chosen*reward + -0.572 contr_diff^2 + 0.001 contr_diff*reward + 0.731 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.745 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + -0.002 contr_diff + -0.002 choice + -0.002 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.002 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 114/1000 --- L(Train): 0.0212733 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.19 1 + 0.272 value_reward_chosen[t] + 0.121 contr_diff + 0.738 reward + -0.139 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.209 value_reward_chosen*reward + -0.57 contr_diff^2 + -0.001 contr_diff*reward + 0.734 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.745 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.998 value_choice[t] + -0.0 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.002 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 115/1000 --- L(Train): 0.0209756 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.196 1 + 0.268 value_reward_chosen[t] + 0.12 contr_diff + 0.741 reward + -0.137 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.207 value_reward_chosen*reward + -0.569 contr_diff^2 + -0.002 contr_diff*reward + 0.737 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.745 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 116/1000 --- L(Train): 0.0206856 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.265 value_reward_chosen[t] + 0.118 contr_diff + 0.744 reward + -0.134 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.205 value_reward_chosen*reward + -0.567 contr_diff^2 + -0.001 contr_diff*reward + 0.74 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.744 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.001 choice + 0.002 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 117/1000 --- L(Train): 0.0204085 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.208 1 + 0.261 value_reward_chosen[t] + 0.117 contr_diff + 0.746 reward + -0.132 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.203 value_reward_chosen*reward + -0.565 contr_diff^2 + 0.0 contr_diff*reward + 0.742 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.744 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.998 value_choice[t] + 0.002 contr_diff + 0.001 choice + 0.003 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.003 contr_diff^2 + -0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 118/1000 --- L(Train): 0.0201363 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.213 1 + 0.258 value_reward_chosen[t] + 0.115 contr_diff + 0.749 reward + -0.13 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.201 value_reward_chosen*reward + -0.563 contr_diff^2 + -0.001 contr_diff*reward + 0.745 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.47 1 + 0.744 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.001 choice + 0.003 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.003 contr_diff^2 + -0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 119/1000 --- L(Train): 0.0198695 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.219 1 + 0.255 value_reward_chosen[t] + 0.114 contr_diff + 0.752 reward + -0.128 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.199 value_reward_chosen*reward + -0.56 contr_diff^2 + -0.001 contr_diff*reward + 0.748 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.744 value_reward_not_chosen[t] + -0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + -0.001 contr_diff + -0.0 choice + 0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 120/1000 --- L(Train): 0.0196106 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.225 1 + 0.251 value_reward_chosen[t] + 0.113 contr_diff + 0.755 reward + -0.126 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.197 value_reward_chosen*reward + -0.558 contr_diff^2 + -0.001 contr_diff*reward + 0.75 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.744 value_reward_not_chosen[t] + -0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + -0.002 contr_diff + -0.001 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 121/1000 --- L(Train): 0.0193591 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.231 1 + 0.248 value_reward_chosen[t] + 0.111 contr_diff + 0.757 reward + -0.124 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.195 value_reward_chosen*reward + -0.555 contr_diff^2 + 0.001 contr_diff*reward + 0.753 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.744 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + -0.002 contr_diff + -0.001 choice + -0.001 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 122/1000 --- L(Train): 0.0191146 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.236 1 + 0.244 value_reward_chosen[t] + 0.11 contr_diff + 0.76 reward + -0.122 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.193 value_reward_chosen*reward + -0.552 contr_diff^2 + -0.0 contr_diff*reward + 0.755 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.744 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.002 value_choice*choice + 0.0 contr_diff^2 + 0.004 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 123/1000 --- L(Train): 0.0188795 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.242 1 + 0.241 value_reward_chosen[t] + 0.109 contr_diff + 0.762 reward + -0.119 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.191 value_reward_chosen*reward + -0.549 contr_diff^2 + 0.0 contr_diff*reward + 0.758 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.745 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.0 value_choice^2 + 0.002 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + 0.004 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 124/1000 --- L(Train): 0.0186486 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.247 1 + 0.238 value_reward_chosen[t] + 0.108 contr_diff + 0.765 reward + -0.117 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.189 value_reward_chosen*reward + -0.545 contr_diff^2 + -0.002 contr_diff*reward + 0.76 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.745 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.002 contr_diff + 0.001 choice + 0.002 value_choice^2 + 0.002 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 125/1000 --- L(Train): 0.0184249 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.253 1 + 0.235 value_reward_chosen[t] + 0.106 contr_diff + 0.767 reward + -0.115 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.187 value_reward_chosen*reward + -0.542 contr_diff^2 + -0.003 contr_diff*reward + 0.763 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.745 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.001 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 126/1000 --- L(Train): 0.0182047 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.258 1 + 0.232 value_reward_chosen[t] + 0.105 contr_diff + 0.77 reward + -0.113 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.185 value_reward_chosen*reward + -0.538 contr_diff^2 + -0.003 contr_diff*reward + 0.765 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.745 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.003 value_choice^2 + -0.002 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 127/1000 --- L(Train): 0.0179925 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.263 1 + 0.228 value_reward_chosen[t] + 0.104 contr_diff + 0.772 reward + -0.111 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.183 value_reward_chosen*reward + -0.534 contr_diff^2 + -0.002 contr_diff*reward + 0.767 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.469 1 + 0.745 value_reward_not_chosen[t] + -0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.002 value_choice^2 + -0.003 value_choice*contr_diff + -0.003 value_choice*choice + 0.002 contr_diff^2 + -0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 128/1000 --- L(Train): 0.0177840 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.269 1 + 0.225 value_reward_chosen[t] + 0.103 contr_diff + 0.775 reward + -0.11 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.181 value_reward_chosen*reward + -0.53 contr_diff^2 + 0.001 contr_diff*reward + 0.77 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.745 value_reward_not_chosen[t] + -0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.001 value_choice^2 + -0.003 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + -0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 129/1000 --- L(Train): 0.0175851 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.274 1 + 0.222 value_reward_chosen[t] + 0.102 contr_diff + 0.777 reward + -0.108 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.179 value_reward_chosen*reward + -0.525 contr_diff^2 + 0.0 contr_diff*reward + 0.772 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.745 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 130/1000 --- L(Train): 0.0173899 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.279 1 + 0.219 value_reward_chosen[t] + 0.101 contr_diff + 0.779 reward + -0.106 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.178 value_reward_chosen*reward + -0.521 contr_diff^2 + -0.002 contr_diff*reward + 0.774 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 131/1000 --- L(Train): 0.0171967 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.284 1 + 0.216 value_reward_chosen[t] + 0.1 contr_diff + 0.782 reward + -0.104 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.176 value_reward_chosen*reward + -0.516 contr_diff^2 + -0.003 contr_diff*reward + 0.776 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.0 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.003 contr_diff^2 + 0.002 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 132/1000 --- L(Train): 0.0170098 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.289 1 + 0.213 value_reward_chosen[t] + 0.099 contr_diff + 0.784 reward + -0.102 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.174 value_reward_chosen*reward + -0.512 contr_diff^2 + -0.003 contr_diff*reward + 0.779 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + -0.001 contr_diff + -0.0 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.003 contr_diff^2 + 0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 133/1000 --- L(Train): 0.0168297 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.294 1 + 0.21 value_reward_chosen[t] + 0.098 contr_diff + 0.786 reward + -0.1 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.172 value_reward_chosen*reward + -0.507 contr_diff^2 + -0.002 contr_diff*reward + 0.781 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + -0.002 contr_diff + -0.0 choice + 0.003 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 134/1000 --- L(Train): 0.0166534 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.299 1 + 0.207 value_reward_chosen[t] + 0.097 contr_diff + 0.788 reward + -0.099 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.17 value_reward_chosen*reward + -0.502 contr_diff^2 + -0.0 contr_diff*reward + 0.783 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.998 value_choice[t] + -0.001 contr_diff + 0.0 choice + 0.003 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 135/1000 --- L(Train): 0.0164828 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.304 1 + 0.204 value_reward_chosen[t] + 0.096 contr_diff + 0.791 reward + -0.097 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.168 value_reward_chosen*reward + -0.497 contr_diff^2 + 0.003 contr_diff*reward + 0.785 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.997 value_choice[t] + -0.0 contr_diff + 0.0 choice + 0.002 value_choice^2 + 0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.003 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 136/1000 --- L(Train): 0.0163165 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.309 1 + 0.202 value_reward_chosen[t] + 0.096 contr_diff + 0.793 reward + -0.095 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.166 value_reward_chosen*reward + -0.491 contr_diff^2 + 0.003 contr_diff*reward + 0.787 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.997 value_choice[t] + 0.002 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.004 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 137/1000 --- L(Train): 0.0161567 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.314 1 + 0.199 value_reward_chosen[t] + 0.095 contr_diff + 0.795 reward + -0.093 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.164 value_reward_chosen*reward + -0.486 contr_diff^2 + 0.002 contr_diff*reward + 0.789 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.998 value_choice[t] + 0.003 contr_diff + -0.0 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.005 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 138/1000 --- L(Train): 0.0160006 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.318 1 + 0.196 value_reward_chosen[t] + 0.094 contr_diff + 0.797 reward + -0.092 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.162 value_reward_chosen*reward + -0.48 contr_diff^2 + -0.002 contr_diff*reward + 0.791 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.004 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 139/1000 --- L(Train): 0.0158440 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.323 1 + 0.193 value_reward_chosen[t] + 0.093 contr_diff + 0.799 reward + -0.09 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.16 value_reward_chosen*reward + -0.474 contr_diff^2 + -0.004 contr_diff*reward + 0.793 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 140/1000 --- L(Train): 0.0156922 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.327 1 + 0.191 value_reward_chosen[t] + 0.092 contr_diff + 0.801 reward + -0.089 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.159 value_reward_chosen*reward + -0.469 contr_diff^2 + -0.004 contr_diff*reward + 0.795 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.002 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 141/1000 --- L(Train): 0.0155516 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.332 1 + 0.188 value_reward_chosen[t] + 0.091 contr_diff + 0.803 reward + -0.087 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.157 value_reward_chosen*reward + -0.463 contr_diff^2 + -0.004 contr_diff*reward + 0.797 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.999 value_choice[t] + -0.002 contr_diff + -0.001 choice + 0.003 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 142/1000 --- L(Train): 0.0154136 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.336 1 + 0.185 value_reward_chosen[t] + 0.09 contr_diff + 0.805 reward + -0.085 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.155 value_reward_chosen*reward + -0.457 contr_diff^2 + -0.002 contr_diff*reward + 0.799 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + -0.002 contr_diff + -0.0 choice + 0.003 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 143/1000 --- L(Train): 0.0152768 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.341 1 + 0.183 value_reward_chosen[t] + 0.09 contr_diff + 0.807 reward + -0.084 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.153 value_reward_chosen*reward + -0.45 contr_diff^2 + 0.0 contr_diff*reward + 0.8 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.002 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 144/1000 --- L(Train): 0.0151388 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.345 1 + 0.18 value_reward_chosen[t] + 0.089 contr_diff + 0.809 reward + -0.082 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.151 value_reward_chosen*reward + -0.444 contr_diff^2 + 0.001 contr_diff*reward + 0.802 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.0 contr_diff + 0.002 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 145/1000 --- L(Train): 0.0150091 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.35 1 + 0.177 value_reward_chosen[t] + 0.088 contr_diff + 0.811 reward + -0.081 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.149 value_reward_chosen*reward + -0.438 contr_diff^2 + -0.001 contr_diff*reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.005 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.002 choice + -0.001 value_choice^2 + -0.003 value_choice*contr_diff + 0.0 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 146/1000 --- L(Train): 0.0148838 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.354 1 + 0.175 value_reward_chosen[t] + 0.087 contr_diff + 0.812 reward + -0.079 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.147 value_reward_chosen*reward + -0.431 contr_diff^2 + -0.001 contr_diff*reward + 0.806 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.001 value_choice^2 + -0.003 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 147/1000 --- L(Train): 0.0147606 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.358 1 + 0.172 value_reward_chosen[t] + 0.086 contr_diff + 0.814 reward + -0.078 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.145 value_reward_chosen*reward + -0.425 contr_diff^2 + -0.0 contr_diff*reward + 0.808 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.0 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 148/1000 --- L(Train): 0.0146375 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.362 1 + 0.17 value_reward_chosen[t] + 0.086 contr_diff + 0.816 reward + -0.076 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.144 value_reward_chosen*reward + -0.418 contr_diff^2 + 0.002 contr_diff*reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.002 choice + 0.0 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 149/1000 --- L(Train): 0.0145202 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.366 1 + 0.167 value_reward_chosen[t] + 0.085 contr_diff + 0.818 reward + -0.075 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.142 value_reward_chosen*reward + -0.411 contr_diff^2 + 0.001 contr_diff*reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 150/1000 --- L(Train): 0.0144062 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.37 1 + 0.165 value_reward_chosen[t] + 0.084 contr_diff + 0.819 reward + -0.074 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.14 value_reward_chosen*reward + -0.404 contr_diff^2 + -0.0 contr_diff*reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.002 value_choice[t] + 0.001 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.002 value_choice*choice + -0.0 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 151/1000 --- L(Train): 0.0142950 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.374 1 + 0.163 value_reward_chosen[t] + 0.083 contr_diff + 0.821 reward + -0.072 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.138 value_reward_chosen*reward + -0.398 contr_diff^2 + -0.001 contr_diff*reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.002 value_choice[t] + 0.001 contr_diff + 0.0 choice + 0.0 value_choice^2 + -0.0 value_choice*contr_diff + 0.002 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 152/1000 --- L(Train): 0.0141883 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.378 1 + 0.16 value_reward_chosen[t] + 0.083 contr_diff + 0.823 reward + -0.071 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.136 value_reward_chosen*reward + -0.391 contr_diff^2 + -0.0 contr_diff*reward + 0.816 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + 0.0 contr_diff + 0.0 choice + 0.0 value_choice^2 + -0.0 value_choice*contr_diff + 0.002 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 153/1000 --- L(Train): 0.0140833 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.382 1 + 0.158 value_reward_chosen[t] + 0.082 contr_diff + 0.824 reward + -0.07 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.135 value_reward_chosen*reward + -0.383 contr_diff^2 + 0.001 contr_diff*reward + 0.817 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.001 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.0 value_choice^2 + 0.0 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 154/1000 --- L(Train): 0.0139783 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.386 1 + 0.156 value_reward_chosen[t] + 0.081 contr_diff + 0.826 reward + -0.068 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.133 value_reward_chosen*reward + -0.376 contr_diff^2 + 0.001 contr_diff*reward + 0.819 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.998 value_choice[t] + -0.002 contr_diff + -0.001 choice + -0.0 value_choice^2 + 0.0 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 155/1000 --- L(Train): 0.0138775 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.39 1 + 0.153 value_reward_chosen[t] + 0.08 contr_diff + 0.828 reward + -0.067 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.131 value_reward_chosen*reward + -0.369 contr_diff^2 + -0.001 contr_diff*reward + 0.82 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.997 value_choice[t] + -0.001 contr_diff + -0.0 choice + 0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.003 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 156/1000 --- L(Train): 0.0137790 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.394 1 + 0.151 value_reward_chosen[t] + 0.08 contr_diff + 0.829 reward + -0.066 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.129 value_reward_chosen*reward + -0.362 contr_diff^2 + -0.001 contr_diff*reward + 0.822 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.997 value_choice[t] + 0.0 contr_diff + 0.001 choice + 0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 157/1000 --- L(Train): 0.0136820 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.397 1 + 0.149 value_reward_chosen[t] + 0.079 contr_diff + 0.831 reward + -0.065 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.127 value_reward_chosen*reward + -0.354 contr_diff^2 + -0.001 contr_diff*reward + 0.823 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.998 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 158/1000 --- L(Train): 0.0135893 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.401 1 + 0.147 value_reward_chosen[t] + 0.078 contr_diff + 0.832 reward + -0.063 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.126 value_reward_chosen*reward + -0.347 contr_diff^2 + 0.001 contr_diff*reward + 0.825 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + -0.0 contr_diff + -0.0 choice + -0.0 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 159/1000 --- L(Train): 0.0135006 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.405 1 + 0.145 value_reward_chosen[t] + 0.077 contr_diff + 0.834 reward + -0.062 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.124 value_reward_chosen*reward + -0.34 contr_diff^2 + 0.001 contr_diff*reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + -0.0 contr_diff + 0.001 choice + 0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 160/1000 --- L(Train): 0.0134122 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.408 1 + 0.143 value_reward_chosen[t] + 0.077 contr_diff + 0.835 reward + -0.061 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.122 value_reward_chosen*reward + -0.332 contr_diff^2 + -0.001 contr_diff*reward + 0.827 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.001 choice + 0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 161/1000 --- L(Train): 0.0133262 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.412 1 + 0.14 value_reward_chosen[t] + 0.076 contr_diff + 0.837 reward + -0.06 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.12 value_reward_chosen*reward + -0.325 contr_diff^2 + -0.001 contr_diff*reward + 0.829 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 0.999 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.0 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 162/1000 --- L(Train): 0.0132396 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.415 1 + 0.138 value_reward_chosen[t] + 0.076 contr_diff + 0.838 reward + -0.059 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.119 value_reward_chosen*reward + -0.317 contr_diff^2 + -0.0 contr_diff*reward + 0.83 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.999 value_choice[t] + -0.0 contr_diff + -0.001 choice + 0.0 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 163/1000 --- L(Train): 0.0131581 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.419 1 + 0.136 value_reward_chosen[t] + 0.075 contr_diff + 0.84 reward + -0.058 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.117 value_reward_chosen*reward + -0.309 contr_diff^2 + 0.002 contr_diff*reward + 0.831 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + -0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + -0.0 contr_diff + -0.002 choice + -0.001 value_choice^2 + -0.003 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 164/1000 --- L(Train): 0.0130798 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.422 1 + 0.134 value_reward_chosen[t] + 0.074 contr_diff + 0.841 reward + -0.057 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.115 value_reward_chosen*reward + -0.302 contr_diff^2 + 0.002 contr_diff*reward + 0.833 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + -0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + 0.001 contr_diff + -0.003 choice + -0.001 value_choice^2 + -0.003 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 165/1000 --- L(Train): 0.0130019 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.426 1 + 0.132 value_reward_chosen[t] + 0.074 contr_diff + 0.842 reward + -0.056 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.113 value_reward_chosen*reward + -0.294 contr_diff^2 + 0.0 contr_diff*reward + 0.834 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.001 value_choice[t] + 0.0 contr_diff + -0.002 choice + 0.0 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 166/1000 --- L(Train): 0.0129237 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.429 1 + 0.13 value_reward_chosen[t] + 0.073 contr_diff + 0.844 reward + -0.054 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.112 value_reward_chosen*reward + -0.286 contr_diff^2 + -0.003 contr_diff*reward + 0.835 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + -0.001 contr_diff + 0.0 choice + 0.0 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.004 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 167/1000 --- L(Train): 0.0128488 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.432 1 + 0.128 value_reward_chosen[t] + 0.072 contr_diff + 0.845 reward + -0.053 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.11 value_reward_chosen*reward + -0.278 contr_diff^2 + -0.004 contr_diff*reward + 0.836 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 0.999 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.004 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 168/1000 --- L(Train): 0.0127749 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.435 1 + 0.126 value_reward_chosen[t] + 0.072 contr_diff + 0.846 reward + -0.052 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.108 value_reward_chosen*reward + -0.27 contr_diff^2 + -0.004 contr_diff*reward + 0.837 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 0.999 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.0 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 169/1000 --- L(Train): 0.0127042 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.438 1 + 0.125 value_reward_chosen[t] + 0.071 contr_diff + 0.848 reward + -0.051 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.107 value_reward_chosen*reward + -0.262 contr_diff^2 + -0.003 contr_diff*reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + -0.0 contr_diff + -0.0 choice + 0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 170/1000 --- L(Train): 0.0126360 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.442 1 + 0.123 value_reward_chosen[t] + 0.071 contr_diff + 0.849 reward + -0.05 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.105 value_reward_chosen*reward + -0.255 contr_diff^2 + -0.0 contr_diff*reward + 0.84 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 171/1000 --- L(Train): 0.0125686 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.445 1 + 0.121 value_reward_chosen[t] + 0.07 contr_diff + 0.85 reward + -0.049 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.104 value_reward_chosen*reward + -0.247 contr_diff^2 + 0.003 contr_diff*reward + 0.841 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + 0.001 contr_diff + -0.001 choice + -0.0 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 172/1000 --- L(Train): 0.0125003 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.448 1 + 0.119 value_reward_chosen[t] + 0.07 contr_diff + 0.851 reward + -0.048 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.102 value_reward_chosen*reward + -0.239 contr_diff^2 + 0.004 contr_diff*reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.0 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 173/1000 --- L(Train): 0.0124369 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.451 1 + 0.117 value_reward_chosen[t] + 0.069 contr_diff + 0.852 reward + -0.047 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.1 value_reward_chosen*reward + -0.231 contr_diff^2 + 0.004 contr_diff*reward + 0.843 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + -0.001 contr_diff + 0.001 choice + 0.001 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 174/1000 --- L(Train): 0.0123761 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.454 1 + 0.115 value_reward_chosen[t] + 0.069 contr_diff + 0.854 reward + -0.046 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.099 value_reward_chosen*reward + -0.223 contr_diff^2 + 0.002 contr_diff*reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.998 value_choice[t] + -0.001 contr_diff + 0.0 choice + 0.001 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 175/1000 --- L(Train): 0.0123150 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.457 1 + 0.114 value_reward_chosen[t] + 0.068 contr_diff + 0.855 reward + -0.046 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.097 value_reward_chosen*reward + -0.215 contr_diff^2 + -0.001 contr_diff*reward + 0.845 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.997 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 176/1000 --- L(Train): 0.0122528 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.46 1 + 0.112 value_reward_chosen[t] + 0.068 contr_diff + 0.856 reward + -0.045 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.096 value_reward_chosen*reward + -0.207 contr_diff^2 + -0.002 contr_diff*reward + 0.846 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 0.997 value_choice[t] + 0.001 contr_diff + -0.001 choice + -0.0 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 177/1000 --- L(Train): 0.0121982 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.462 1 + 0.11 value_reward_chosen[t] + 0.067 contr_diff + 0.857 reward + -0.044 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.094 value_reward_chosen*reward + -0.198 contr_diff^2 + -0.002 contr_diff*reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.003 1 + 0.998 value_choice[t] + 0.0 contr_diff + -0.001 choice + 0.001 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.004 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 178/1000 --- L(Train): 0.0121404 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.465 1 + 0.108 value_reward_chosen[t] + 0.067 contr_diff + 0.858 reward + -0.043 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.092 value_reward_chosen*reward + -0.19 contr_diff^2 + -0.001 contr_diff*reward + 0.848 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.0 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 179/1000 --- L(Train): 0.0120828 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.468 1 + 0.107 value_reward_chosen[t] + 0.067 contr_diff + 0.859 reward + -0.042 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.091 value_reward_chosen*reward + -0.182 contr_diff^2 + 0.001 contr_diff*reward + 0.849 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.747 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.001 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.0 value_choice^2 + 0.001 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 180/1000 --- L(Train): 0.0120258 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.471 1 + 0.105 value_reward_chosen[t] + 0.066 contr_diff + 0.86 reward + -0.041 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.089 value_reward_chosen*reward + -0.174 contr_diff^2 + 0.002 contr_diff*reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.001 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.002 value_choice^2 + -0.002 value_choice*contr_diff + 0.002 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 181/1000 --- L(Train): 0.0119740 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.473 1 + 0.104 value_reward_chosen[t] + 0.066 contr_diff + 0.861 reward + -0.04 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.088 value_reward_chosen*reward + -0.166 contr_diff^2 + 0.001 contr_diff*reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + -0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.999 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.003 value_choice^2 + -0.003 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 182/1000 --- L(Train): 0.0119225 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.476 1 + 0.102 value_reward_chosen[t] + 0.065 contr_diff + 0.862 reward + -0.04 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.086 value_reward_chosen*reward + -0.158 contr_diff^2 + -0.001 contr_diff*reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.999 value_choice[t] + 0.001 contr_diff + 0.002 choice + -0.002 value_choice^2 + -0.003 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 183/1000 --- L(Train): 0.0118717 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.479 1 + 0.1 value_reward_chosen[t] + 0.065 contr_diff + 0.863 reward + -0.039 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.085 value_reward_chosen*reward + -0.15 contr_diff^2 + -0.002 contr_diff*reward + 0.853 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + 0.0 contr_diff + 0.004 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.003 value_choice*choice + -0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 184/1000 --- L(Train): 0.0118209 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.481 1 + 0.099 value_reward_chosen[t] + 0.064 contr_diff + 0.864 reward + -0.038 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.083 value_reward_chosen*reward + -0.142 contr_diff^2 + -0.001 contr_diff*reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.004 choice + 0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 185/1000 --- L(Train): 0.0117721 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.484 1 + 0.097 value_reward_chosen[t] + 0.064 contr_diff + 0.865 reward + -0.037 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.082 value_reward_chosen*reward + -0.134 contr_diff^2 + 0.0 contr_diff*reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.001 value_choice[t] + -0.002 contr_diff + 0.002 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 186/1000 --- L(Train): 0.0117238 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.486 1 + 0.096 value_reward_chosen[t] + 0.064 contr_diff + 0.866 reward + -0.037 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.08 value_reward_chosen*reward + -0.126 contr_diff^2 + 0.001 contr_diff*reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.001 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 187/1000 --- L(Train): 0.0116756 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.489 1 + 0.094 value_reward_chosen[t] + 0.063 contr_diff + 0.867 reward + -0.036 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.079 value_reward_chosen*reward + -0.118 contr_diff^2 + -0.0 contr_diff*reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.0 choice + 0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 188/1000 --- L(Train): 0.0116287 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.491 1 + 0.093 value_reward_chosen[t] + 0.063 contr_diff + 0.868 reward + -0.035 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.078 value_reward_chosen*reward + -0.109 contr_diff^2 + -0.0 contr_diff*reward + 0.857 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 0.999 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 189/1000 --- L(Train): 0.0115865 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.494 1 + 0.091 value_reward_chosen[t] + 0.063 contr_diff + 0.869 reward + -0.034 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.076 value_reward_chosen*reward + -0.101 contr_diff^2 + 0.001 contr_diff*reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.003 1 + 1.0 value_choice[t] + 0.002 contr_diff + -0.001 choice + -0.002 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 190/1000 --- L(Train): 0.0115430 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.496 1 + 0.09 value_reward_chosen[t] + 0.062 contr_diff + 0.87 reward + -0.034 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.075 value_reward_chosen*reward + -0.093 contr_diff^2 + 0.001 contr_diff*reward + 0.859 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.003 1 + 1.002 value_choice[t] + 0.002 contr_diff + -0.002 choice + -0.002 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 191/1000 --- L(Train): 0.0114996 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.499 1 + 0.088 value_reward_chosen[t] + 0.062 contr_diff + 0.871 reward + -0.033 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.073 value_reward_chosen*reward + -0.085 contr_diff^2 + -0.0 contr_diff*reward + 0.859 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.003 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.002 value_choice[t] + 0.001 contr_diff + -0.003 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 192/1000 --- L(Train): 0.0114579 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.501 1 + 0.087 value_reward_chosen[t] + 0.062 contr_diff + 0.872 reward + -0.032 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.072 value_reward_chosen*reward + -0.077 contr_diff^2 + -0.0 contr_diff*reward + 0.86 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + -0.001 contr_diff + -0.002 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 193/1000 --- L(Train): 0.0114166 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.503 1 + 0.086 value_reward_chosen[t] + 0.061 contr_diff + 0.872 reward + -0.032 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + -0.069 contr_diff^2 + 0.001 contr_diff*reward + 0.861 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + -0.002 contr_diff + -0.001 choice + 0.003 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 194/1000 --- L(Train): 0.0113765 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.505 1 + 0.084 value_reward_chosen[t] + 0.061 contr_diff + 0.873 reward + -0.031 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + -0.061 contr_diff^2 + 0.001 contr_diff*reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.998 value_choice[t] + -0.002 contr_diff + 0.0 choice + 0.002 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 195/1000 --- L(Train): 0.0113394 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.508 1 + 0.083 value_reward_chosen[t] + 0.061 contr_diff + 0.874 reward + -0.03 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.053 contr_diff^2 + -0.0 contr_diff*reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.005 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.997 value_choice[t] + -0.001 contr_diff + 0.001 choice + 0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 196/1000 --- L(Train): 0.0112985 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.51 1 + 0.082 value_reward_chosen[t] + 0.061 contr_diff + 0.875 reward + -0.03 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.045 contr_diff^2 + -0.0 contr_diff*reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.005 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.997 value_choice[t] + 0.001 contr_diff + 0.002 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 197/1000 --- L(Train): 0.0112627 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.512 1 + 0.08 value_reward_chosen[t] + 0.06 contr_diff + 0.876 reward + -0.029 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.065 value_reward_chosen*reward + -0.037 contr_diff^2 + 0.001 contr_diff*reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.998 value_choice[t] + 0.002 contr_diff + 0.002 choice + -0.002 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 198/1000 --- L(Train): 0.0112260 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.514 1 + 0.079 value_reward_chosen[t] + 0.06 contr_diff + 0.877 reward + -0.028 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.064 value_reward_chosen*reward + -0.029 contr_diff^2 + 0.001 contr_diff*reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 199/1000 --- L(Train): 0.0111903 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.516 1 + 0.078 value_reward_chosen[t] + 0.06 contr_diff + 0.877 reward + -0.028 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.063 value_reward_chosen*reward + -0.021 contr_diff^2 + -0.001 contr_diff*reward + 0.865 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + -0.0 contr_diff + -0.001 choice + -0.0 value_choice^2 + -0.003 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 200/1000 --- L(Train): 0.0111525 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.518 1 + 0.076 value_reward_chosen[t] + 0.059 contr_diff + 0.878 reward + -0.027 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.061 value_reward_chosen*reward + -0.013 contr_diff^2 + -0.001 contr_diff*reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + -0.001 contr_diff + -0.002 choice + 0.002 value_choice^2 + -0.003 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 201/1000 --- L(Train): 0.0111188 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.52 1 + 0.075 value_reward_chosen[t] + 0.059 contr_diff + 0.879 reward + -0.027 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.06 value_reward_chosen*reward + -0.005 contr_diff^2 + 0.001 contr_diff*reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.999 value_choice[t] + -0.0 contr_diff + -0.002 choice + 0.003 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 202/1000 --- L(Train): 0.0110860 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.522 1 + 0.074 value_reward_chosen[t] + 0.059 contr_diff + 0.88 reward + -0.026 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.059 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.999 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 203/1000 --- L(Train): 0.0110510 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.524 1 + 0.073 value_reward_chosen[t] + 0.059 contr_diff + 0.88 reward + -0.026 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.058 value_reward_chosen*reward + 0.008 contr_diff^2 + -0.001 contr_diff*reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.002 contr_diff + 0.001 choice + 0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 204/1000 --- L(Train): 0.0110179 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.526 1 + 0.072 value_reward_chosen[t] + 0.058 contr_diff + 0.881 reward + -0.025 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.056 value_reward_chosen*reward + 0.011 contr_diff^2 + -0.001 contr_diff*reward + 0.868 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 205/1000 --- L(Train): 0.0109868 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.528 1 + 0.07 value_reward_chosen[t] + 0.058 contr_diff + 0.882 reward + -0.024 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.055 value_reward_chosen*reward + 0.013 contr_diff^2 + 0.001 contr_diff*reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.747 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.002 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 206/1000 --- L(Train): 0.0109562 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.53 1 + 0.069 value_reward_chosen[t] + 0.058 contr_diff + 0.882 reward + -0.024 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.054 value_reward_chosen*reward + 0.013 contr_diff^2 + 0.001 contr_diff*reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.002 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 207/1000 --- L(Train): 0.0109270 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.532 1 + 0.068 value_reward_chosen[t] + 0.058 contr_diff + 0.883 reward + -0.023 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.053 value_reward_chosen*reward + 0.011 contr_diff^2 + -0.001 contr_diff*reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.0 value_choice^2 + 0.0 value_choice*contr_diff + 0.002 value_choice*choice + 0.002 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 208/1000 --- L(Train): 0.0108936 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.534 1 + 0.067 value_reward_chosen[t] + 0.057 contr_diff + 0.884 reward + -0.022 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.052 value_reward_chosen*reward + 0.008 contr_diff^2 + -0.0 contr_diff*reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + 0.0 contr_diff + -0.001 choice + 0.002 value_choice^2 + 0.0 value_choice*contr_diff + 0.002 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 209/1000 --- L(Train): 0.0108630 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.535 1 + 0.066 value_reward_chosen[t] + 0.057 contr_diff + 0.884 reward + -0.021 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.05 value_reward_chosen*reward + 0.004 contr_diff^2 + 0.001 contr_diff*reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + 0.0 contr_diff + -0.0 choice + 0.003 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 210/1000 --- L(Train): 0.0108372 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.537 1 + 0.065 value_reward_chosen[t] + 0.057 contr_diff + 0.885 reward + -0.02 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.049 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + -0.0 contr_diff + 0.001 choice + 0.003 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 211/1000 --- L(Train): 0.0108101 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.539 1 + 0.064 value_reward_chosen[t] + 0.057 contr_diff + 0.886 reward + -0.019 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.048 value_reward_chosen*reward + -0.004 contr_diff^2 + 0.0 contr_diff*reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + -0.0 contr_diff + 0.001 choice + 0.001 value_choice^2 + -0.0 value_choice*contr_diff + -0.003 value_choice*choice + 0.0 contr_diff^2 + 0.001 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 212/1000 --- L(Train): 0.0107794 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.54 1 + 0.063 value_reward_chosen[t] + 0.056 contr_diff + 0.886 reward + -0.018 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.047 value_reward_chosen*reward + -0.007 contr_diff^2 + -0.002 contr_diff*reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + -0.002 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 213/1000 --- L(Train): 0.0107555 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.542 1 + 0.061 value_reward_chosen[t] + 0.056 contr_diff + 0.887 reward + -0.018 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.046 value_reward_chosen*reward + -0.008 contr_diff^2 + -0.003 contr_diff*reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 214/1000 --- L(Train): 0.0107294 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.544 1 + 0.06 value_reward_chosen[t] + 0.056 contr_diff + 0.887 reward + -0.017 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.045 value_reward_chosen*reward + -0.009 contr_diff^2 + -0.002 contr_diff*reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.998 value_choice[t] + 0.0 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 215/1000 --- L(Train): 0.0107052 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.545 1 + 0.059 value_reward_chosen[t] + 0.056 contr_diff + 0.888 reward + -0.017 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.044 value_reward_chosen*reward + -0.008 contr_diff^2 + -0.0 contr_diff*reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.997 value_choice[t] + -0.002 contr_diff + -0.001 choice + -0.0 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 216/1000 --- L(Train): 0.0106770 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.546 1 + 0.058 value_reward_chosen[t] + 0.055 contr_diff + 0.889 reward + -0.016 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.043 value_reward_chosen*reward + -0.007 contr_diff^2 + 0.003 contr_diff*reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.997 value_choice[t] + -0.003 contr_diff + -0.001 choice + 0.002 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 217/1000 --- L(Train): 0.0106549 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.548 1 + 0.057 value_reward_chosen[t] + 0.055 contr_diff + 0.889 reward + -0.016 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.042 value_reward_chosen*reward + -0.005 contr_diff^2 + 0.004 contr_diff*reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.998 value_choice[t] + -0.002 contr_diff + -0.001 choice + 0.003 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 218/1000 --- L(Train): 0.0106294 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.549 1 + 0.056 value_reward_chosen[t] + 0.055 contr_diff + 0.89 reward + -0.015 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.041 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.004 contr_diff*reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + -0.001 contr_diff + 0.0 choice + 0.003 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + 0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 219/1000 --- L(Train): 0.0106059 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.551 1 + 0.055 value_reward_chosen[t] + 0.054 contr_diff + 0.89 reward + -0.014 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.04 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.001 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 220/1000 --- L(Train): 0.0105827 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.552 1 + 0.054 value_reward_chosen[t] + 0.054 contr_diff + 0.891 reward + -0.014 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.04 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.001 contr_diff*reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 221/1000 --- L(Train): 0.0105604 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.553 1 + 0.053 value_reward_chosen[t] + 0.054 contr_diff + 0.891 reward + -0.014 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.039 value_reward_chosen*reward + 0.003 contr_diff^2 + -0.002 contr_diff*reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.999 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 222/1000 --- L(Train): 0.0105370 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.554 1 + 0.052 value_reward_chosen[t] + 0.053 contr_diff + 0.892 reward + -0.013 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.038 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.003 contr_diff*reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + -0.0 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 223/1000 --- L(Train): 0.0105127 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.556 1 + 0.051 value_reward_chosen[t] + 0.053 contr_diff + 0.892 reward + -0.013 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.037 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.003 contr_diff*reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + -0.0 contr_diff + -0.001 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 224/1000 --- L(Train): 0.0104903 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.557 1 + 0.05 value_reward_chosen[t] + 0.053 contr_diff + 0.893 reward + -0.012 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.036 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.002 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 225/1000 --- L(Train): 0.0104712 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.558 1 + 0.049 value_reward_chosen[t] + 0.052 contr_diff + 0.893 reward + -0.012 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.036 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.003 value_choice^2 + 0.0 value_choice*contr_diff + 0.001 value_choice*choice + -0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 226/1000 --- L(Train): 0.0104505 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.559 1 + 0.049 value_reward_chosen[t] + 0.052 contr_diff + 0.894 reward + -0.011 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.035 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + -0.001 contr_diff + 0.001 choice + 0.003 value_choice^2 + 0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 227/1000 --- L(Train): 0.0104285 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.56 1 + 0.048 value_reward_chosen[t] + 0.052 contr_diff + 0.894 reward + -0.01 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.034 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.999 value_choice[t] + -0.001 contr_diff + 0.001 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 228/1000 --- L(Train): 0.0104067 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.561 1 + 0.047 value_reward_chosen[t] + 0.052 contr_diff + 0.895 reward + -0.01 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.033 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.999 value_choice[t] + -0.0 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 229/1000 --- L(Train): 0.0103868 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.562 1 + 0.046 value_reward_chosen[t] + 0.051 contr_diff + 0.895 reward + -0.01 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.033 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 230/1000 --- L(Train): 0.0103672 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.563 1 + 0.045 value_reward_chosen[t] + 0.051 contr_diff + 0.896 reward + -0.01 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.032 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.002 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 231/1000 --- L(Train): 0.0103503 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.564 1 + 0.044 value_reward_chosen[t] + 0.051 contr_diff + 0.896 reward + -0.009 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.031 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.003 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.0 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.003 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 232/1000 --- L(Train): 0.0103302 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.565 1 + 0.043 value_reward_chosen[t] + 0.05 contr_diff + 0.897 reward + -0.009 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.031 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.003 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.002 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.002 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.004 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 233/1000 --- L(Train): 0.0103114 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.565 1 + 0.042 value_reward_chosen[t] + 0.05 contr_diff + 0.897 reward + -0.008 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.03 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.003 contr_diff*reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.0 value_choice[t] + -0.002 contr_diff + -0.001 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 234/1000 --- L(Train): 0.0102910 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.566 1 + 0.042 value_reward_chosen[t] + 0.05 contr_diff + 0.898 reward + -0.008 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.03 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.004 contr_diff*reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.998 value_choice[t] + -0.001 contr_diff + 0.001 choice + 0.003 value_choice^2 + -0.002 value_choice*contr_diff + 0.002 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 235/1000 --- L(Train): 0.0102747 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.567 1 + 0.041 value_reward_chosen[t] + 0.05 contr_diff + 0.898 reward + -0.007 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.029 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.004 contr_diff*reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.997 value_choice[t] + 0.001 contr_diff + 0.001 choice + 0.002 value_choice^2 + -0.003 value_choice*contr_diff + 0.002 value_choice*choice + 0.0 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 236/1000 --- L(Train): 0.0102592 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.568 1 + 0.04 value_reward_chosen[t] + 0.049 contr_diff + 0.898 reward + -0.007 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.028 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.003 contr_diff*reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.997 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + 0.002 value_choice*choice + -0.0 contr_diff^2 + 0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 237/1000 --- L(Train): 0.0102424 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.569 1 + 0.039 value_reward_chosen[t] + 0.049 contr_diff + 0.899 reward + -0.007 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.028 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.998 value_choice[t] + 0.0 contr_diff + -0.001 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 238/1000 --- L(Train): 0.0102222 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.569 1 + 0.038 value_reward_chosen[t] + 0.049 contr_diff + 0.899 reward + -0.007 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.027 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + -0.002 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 239/1000 --- L(Train): 0.0102053 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.57 1 + 0.038 value_reward_chosen[t] + 0.048 contr_diff + 0.9 reward + -0.007 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.026 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.001 value_choice[t] + -0.002 contr_diff + -0.001 choice + -0.0 value_choice^2 + 0.001 value_choice*contr_diff + -0.003 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 240/1000 --- L(Train): 0.0101884 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.571 1 + 0.037 value_reward_chosen[t] + 0.048 contr_diff + 0.9 reward + -0.006 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.026 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.001 value_choice[t] + -0.002 contr_diff + -0.0 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 241/1000 --- L(Train): 0.0101726 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.571 1 + 0.036 value_reward_chosen[t] + 0.048 contr_diff + 0.9 reward + -0.006 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.025 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.999 value_choice[t] + -0.001 contr_diff + 0.001 choice + 0.003 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 242/1000 --- L(Train): 0.0101562 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.572 1 + 0.035 value_reward_chosen[t] + 0.048 contr_diff + 0.901 reward + -0.006 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.025 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + 0.002 contr_diff + 0.001 choice + 0.003 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 243/1000 --- L(Train): 0.0101410 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.573 1 + 0.035 value_reward_chosen[t] + 0.047 contr_diff + 0.901 reward + -0.005 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.024 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + 0.002 contr_diff + 0.001 choice + 0.002 value_choice^2 + 0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 244/1000 --- L(Train): 0.0101227 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.573 1 + 0.034 value_reward_chosen[t] + 0.047 contr_diff + 0.901 reward + -0.005 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.024 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + 0.002 contr_diff + -0.0 choice + -0.001 value_choice^2 + 0.0 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 245/1000 --- L(Train): 0.0101079 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.574 1 + 0.033 value_reward_chosen[t] + 0.047 contr_diff + 0.902 reward + -0.005 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.023 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + 0.001 contr_diff + -0.001 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 246/1000 --- L(Train): 0.0100937 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.574 1 + 0.033 value_reward_chosen[t] + 0.047 contr_diff + 0.902 reward + -0.005 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.023 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 247/1000 --- L(Train): 0.0100799 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.575 1 + 0.032 value_reward_chosen[t] + 0.046 contr_diff + 0.902 reward + -0.004 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.022 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 248/1000 --- L(Train): 0.0100652 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.576 1 + 0.031 value_reward_chosen[t] + 0.046 contr_diff + 0.903 reward + -0.004 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.021 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.004 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.999 value_choice[t] + -0.002 contr_diff + 0.001 choice + 0.002 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 249/1000 --- L(Train): 0.0100537 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.576 1 + 0.031 value_reward_chosen[t] + 0.046 contr_diff + 0.903 reward + -0.004 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.021 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.004 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + -0.001 contr_diff + -0.0 choice + 0.003 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 250/1000 --- L(Train): 0.0100408 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.577 1 + 0.03 value_reward_chosen[t] + 0.046 contr_diff + 0.903 reward + -0.004 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.02 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.002 value_choice[t] + 0.001 contr_diff + 0.0 choice + 0.003 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 251/1000 --- L(Train): 0.0100265 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.577 1 + 0.03 value_reward_chosen[t] + 0.046 contr_diff + 0.904 reward + -0.004 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.02 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.002 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 1, 0, 1, 1, 1, 1, 1, 0\n",
            "value_reward_not_chosen: 0, 0, 1, 1, 1, 1\n",
            "value_choice: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 252/1000 --- L(Train): 0.0100094 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.578 1 + 0.029 value_reward_chosen[t] + 0.046 contr_diff + 0.904 reward + -0.003 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.019 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.002 value_choice[t] + 0.001 contr_diff + -0.001 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 2, 0, 2, 2, 2, 2, 2, 0\n",
            "value_reward_not_chosen: 0, 0, 2, 2, 2, 2\n",
            "value_choice: 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 253/1000 --- L(Train): 0.0100006 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.578 1 + 0.028 value_reward_chosen[t] + 0.046 contr_diff + 0.904 reward + -0.003 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.019 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.001 value_choice^2 + -0.003 value_choice*contr_diff + 0.001 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 3, 0, 3, 3, 3, 3, 3, 0\n",
            "value_reward_not_chosen: 0, 0, 3, 3, 3, 3\n",
            "value_choice: 3, 3, 3, 3, 3, 3, 3, 3, 3, 3\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 254/1000 --- L(Train): 0.0099881 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.579 1 + 0.028 value_reward_chosen[t] + 0.045 contr_diff + 0.904 reward + -0.003 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.018 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.998 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 4, 0, 4, 4, 4, 4, 4, 0\n",
            "value_reward_not_chosen: 0, 0, 4, 4, 4, 4\n",
            "value_choice: 4, 4, 4, 4, 4, 4, 4, 4, 4, 4\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 255/1000 --- L(Train): 0.0099747 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.579 1 + 0.027 value_reward_chosen[t] + 0.045 contr_diff + 0.905 reward + -0.003 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.018 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.997 value_choice[t] + -0.0 contr_diff + -0.0 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 5, 0, 5, 5, 5, 5, 5, 0\n",
            "value_reward_not_chosen: 0, 0, 5, 5, 5, 5\n",
            "value_choice: 5, 5, 5, 5, 5, 5, 5, 5, 5, 5\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 256/1000 --- L(Train): 0.0099604 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.579 1 + 0.027 value_reward_chosen[t] + 0.045 contr_diff + 0.905 reward + -0.003 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.017 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.997 value_choice[t] + 0.001 contr_diff + -0.0 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 6, 0, 6, 6, 6, 6, 6, 0\n",
            "value_reward_not_chosen: 0, 0, 6, 6, 6, 6\n",
            "value_choice: 6, 6, 6, 6, 6, 6, 6, 6, 6, 6\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 257/1000 --- L(Train): 0.0099474 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.58 1 + 0.026 value_reward_chosen[t] + 0.045 contr_diff + 0.905 reward + -0.003 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.017 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.998 value_choice[t] + 0.002 contr_diff + 0.0 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 7, 0, 7, 7, 7, 7, 7, 0\n",
            "value_reward_not_chosen: 0, 0, 7, 7, 7, 7\n",
            "value_choice: 7, 7, 7, 7, 7, 7, 7, 7, 7, 7\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 258/1000 --- L(Train): 0.0099352 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.58 1 + 0.026 value_reward_chosen[t] + 0.045 contr_diff + 0.906 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.016 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.0 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 8, 0, 8, 8, 8, 8, 8, 0\n",
            "value_reward_not_chosen: 0, 0, 8, 8, 8, 8\n",
            "value_choice: 8, 8, 8, 8, 8, 8, 8, 8, 8, 8\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 259/1000 --- L(Train): 0.0099236 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.581 1 + 0.025 value_reward_chosen[t] + 0.045 contr_diff + 0.906 reward + -0.002 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.016 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + -0.001 contr_diff + -0.0 choice + 0.002 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 9, 0, 9, 9, 9, 9, 9, 0\n",
            "value_reward_not_chosen: 0, 0, 9, 9, 9, 9\n",
            "value_choice: 9, 9, 9, 9, 9, 9, 9, 9, 9, 9\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 260/1000 --- L(Train): 0.0099097 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.581 1 + 0.025 value_reward_chosen[t] + 0.045 contr_diff + 0.906 reward + -0.002 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.015 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + -0.002 contr_diff + -0.0 choice + -0.001 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 10, 0, 10, 10, 10, 10, 10, 0\n",
            "value_reward_not_chosen: 0, 0, 10, 10, 10, 10\n",
            "value_choice: 10, 10, 10, 10, 10, 10, 10, 10, 10, 10\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 261/1000 --- L(Train): 0.0098981 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.581 1 + 0.024 value_reward_chosen[t] + 0.045 contr_diff + 0.906 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.015 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.001 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 11, 0, 11, 11, 11, 11, 11, 0\n",
            "value_reward_not_chosen: 0, 0, 11, 11, 11, 11\n",
            "value_choice: 11, 11, 11, 11, 11, 11, 11, 11, 11, 11\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 262/1000 --- L(Train): 0.0098867 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.582 1 + 0.023 value_reward_chosen[t] + 0.044 contr_diff + 0.907 reward + -0.002 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.014 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + 0.0 contr_diff + -0.0 choice + -0.001 value_choice^2 + 0.0 value_choice*contr_diff + 0.002 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 12, 0, 12, 12, 12, 12, 12, 0\n",
            "value_reward_not_chosen: 0, 0, 12, 12, 12, 12\n",
            "value_choice: 12, 12, 12, 12, 12, 12, 12, 12, 12, 12\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 263/1000 --- L(Train): 0.0098761 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.582 1 + 0.023 value_reward_chosen[t] + 0.044 contr_diff + 0.907 reward + -0.002 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.014 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 13, 0, 13, 13, 13, 13, 13, 0\n",
            "value_reward_not_chosen: 0, 0, 13, 13, 13, 13\n",
            "value_choice: 13, 13, 13, 13, 13, 13, 13, 13, 13, 13\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 264/1000 --- L(Train): 0.0098629 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.582 1 + 0.023 value_reward_chosen[t] + 0.044 contr_diff + 0.907 reward + -0.001 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.014 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + -0.0 contr_diff + 0.0 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 14, 0, 14, 14, 14, 14, 14, 0\n",
            "value_reward_not_chosen: 0, 0, 14, 14, 14, 14\n",
            "value_choice: 14, 14, 14, 14, 14, 14, 14, 14, 14, 14\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 265/1000 --- L(Train): 0.0098521 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.583 1 + 0.022 value_reward_chosen[t] + 0.044 contr_diff + 0.907 reward + -0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.013 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.001 value_choice[t] + -0.0 contr_diff + -0.001 choice + 0.003 value_choice^2 + -0.0 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 15, 0, 15, 15, 15, 15, 15, 0\n",
            "value_reward_not_chosen: 0, 0, 15, 15, 15, 15\n",
            "value_choice: 15, 15, 15, 15, 15, 15, 15, 15, 15, 15\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 266/1000 --- L(Train): 0.0098419 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.583 1 + 0.022 value_reward_chosen[t] + 0.044 contr_diff + 0.907 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.013 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.003 value_choice^2 + 0.002 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 16, 0, 16, 16, 16, 16, 16, 0\n",
            "value_reward_not_chosen: 0, 0, 16, 16, 16, 16\n",
            "value_choice: 16, 16, 16, 16, 16, 16, 16, 16, 16, 16\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 267/1000 --- L(Train): 0.0098330 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.583 1 + 0.021 value_reward_chosen[t] + 0.044 contr_diff + 0.908 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.012 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + 0.001 contr_diff + -0.0 choice + 0.002 value_choice^2 + 0.002 value_choice*contr_diff + -0.003 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 17, 0, 17, 17, 17, 17, 17, 0\n",
            "value_reward_not_chosen: 0, 0, 17, 17, 17, 17\n",
            "value_choice: 17, 17, 17, 17, 17, 17, 17, 17, 17, 17\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 268/1000 --- L(Train): 0.0098201 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.584 1 + 0.021 value_reward_chosen[t] + 0.044 contr_diff + 0.908 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.012 value_reward_chosen*reward + -0.003 contr_diff^2 + -0.0 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.999 value_choice[t] + -0.0 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 18, 0, 18, 18, 18, 18, 18, 0\n",
            "value_reward_not_chosen: 0, 0, 18, 18, 18, 18\n",
            "value_choice: 18, 18, 18, 18, 18, 18, 18, 18, 18, 18\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 269/1000 --- L(Train): 0.0098102 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.584 1 + 0.02 value_reward_chosen[t] + 0.044 contr_diff + 0.908 reward + -0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.011 value_reward_chosen*reward + -0.004 contr_diff^2 + -0.0 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + -0.0 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 19, 0, 19, 19, 19, 19, 19, 0\n",
            "value_reward_not_chosen: 0, 0, 19, 19, 19, 19\n",
            "value_choice: 19, 19, 19, 19, 19, 19, 19, 19, 19, 19\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 270/1000 --- L(Train): 0.0098003 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.584 1 + 0.02 value_reward_chosen[t] + 0.043 contr_diff + 0.908 reward + -0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.011 value_reward_chosen*reward + -0.004 contr_diff^2 + 0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 20, 0, 20, 20, 20, 20, 20, 0\n",
            "value_reward_not_chosen: 0, 0, 20, 20, 20, 20\n",
            "value_choice: 20, 20, 20, 20, 20, 20, 20, 20, 20, 20\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 271/1000 --- L(Train): 0.0097907 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.585 1 + 0.019 value_reward_chosen[t] + 0.043 contr_diff + 0.908 reward + -0.0 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.011 value_reward_chosen*reward + -0.003 contr_diff^2 + 0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + 0.0 contr_diff + -0.0 choice + -0.0 value_choice^2 + -0.003 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 21, 0, 21, 21, 21, 21, 21, 0\n",
            "value_reward_not_chosen: 0, 0, 21, 21, 21, 21\n",
            "value_choice: 21, 21, 21, 21, 21, 21, 21, 21, 21, 21\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 272/1000 --- L(Train): 0.0097800 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.585 1 + 0.019 value_reward_chosen[t] + 0.043 contr_diff + 0.909 reward + -0.0 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.01 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + -0.001 contr_diff + 0.0 choice + 0.002 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 22, 0, 22, 22, 22, 22, 22, 0\n",
            "value_reward_not_chosen: 0, 0, 22, 22, 22, 22\n",
            "value_choice: 22, 22, 22, 22, 22, 22, 22, 22, 22, 22\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 273/1000 --- L(Train): 0.0097705 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.585 1 + 0.019 value_reward_chosen[t] + 0.043 contr_diff + 0.909 reward + -0.0 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.01 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + -0.001 contr_diff + -0.0 choice + 0.003 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 23, 0, 23, 23, 23, 23, 23, 0\n",
            "value_reward_not_chosen: 0, 0, 23, 23, 23, 23\n",
            "value_choice: 23, 23, 23, 23, 23, 23, 23, 23, 23, 23\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 274/1000 --- L(Train): 0.0097594 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.585 1 + 0.018 value_reward_chosen[t] + 0.043 contr_diff + 0.909 reward + -0.0 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.009 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.998 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 24, 0, 24, 24, 24, 24, 24, 0\n",
            "value_reward_not_chosen: 0, 0, 24, 24, 24, 24\n",
            "value_choice: 24, 24, 24, 24, 24, 24, 24, 24, 24, 24\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 275/1000 --- L(Train): 0.0097488 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.586 1 + 0.018 value_reward_chosen[t] + 0.043 contr_diff + 0.909 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.009 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.997 value_choice[t] + 0.001 contr_diff + -0.0 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 25, 0, 25, 25, 25, 25, 25, 0\n",
            "value_reward_not_chosen: 0, 0, 25, 25, 25, 25\n",
            "value_choice: 25, 25, 25, 25, 25, 25, 25, 25, 25, 25\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 276/1000 --- L(Train): 0.0097373 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.586 1 + 0.017 value_reward_chosen[t] + 0.043 contr_diff + 0.909 reward + -0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.009 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 26, 0, 26, 26, 26, 26, 26, 0\n",
            "value_reward_not_chosen: 0, 0, 26, 26, 26, 26\n",
            "value_choice: 26, 26, 26, 26, 26, 26, 26, 26, 26, 26\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 277/1000 --- L(Train): 0.0097273 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.586 1 + 0.017 value_reward_chosen[t] + 0.043 contr_diff + 0.91 reward + -0.001 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.008 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.003 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.998 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 27, 0, 27, 27, 27, 27, 27, 0\n",
            "value_reward_not_chosen: 0, 0, 27, 27, 27, 27\n",
            "value_choice: 27, 27, 27, 27, 27, 27, 27, 27, 27, 27\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 278/1000 --- L(Train): 0.0097192 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.586 1 + 0.017 value_reward_chosen[t] + 0.043 contr_diff + 0.91 reward + -0.0 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.008 value_reward_chosen*reward + -0.003 contr_diff^2 + 0.003 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 28, 0, 28, 28, 28, 28, 28, 0\n",
            "value_reward_not_chosen: 0, 0, 28, 28, 28, 28\n",
            "value_choice: 28, 28, 28, 28, 28, 28, 28, 28, 28, 28\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 279/1000 --- L(Train): 0.0097105 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.016 value_reward_chosen[t] + 0.042 contr_diff + 0.91 reward + 0.0 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.007 value_reward_chosen*reward + -0.003 contr_diff^2 + 0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.0 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 29, 0, 29, 29, 29, 29, 29, 0\n",
            "value_reward_not_chosen: 0, 0, 29, 29, 29, 29\n",
            "value_choice: 29, 29, 29, 29, 29, 29, 29, 29, 29, 29\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 280/1000 --- L(Train): 0.0096988 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.016 value_reward_chosen[t] + 0.042 contr_diff + 0.91 reward + -0.002 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.007 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.001 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.002 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 30, 0, 30, 30, 30, 30, 30, 0\n",
            "value_reward_not_chosen: 0, 0, 30, 30, 30, 30\n",
            "value_choice: 30, 30, 30, 30, 30, 30, 30, 30, 30, 30\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 281/1000 --- L(Train): 0.0096912 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.016 value_reward_chosen[t] + 0.042 contr_diff + 0.91 reward + -0.003 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.007 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.999 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.003 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 31, 0, 31, 31, 31, 31, 31, 0\n",
            "value_reward_not_chosen: 0, 0, 31, 31, 31, 31\n",
            "value_choice: 31, 31, 31, 31, 31, 31, 31, 31, 31, 31\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 282/1000 --- L(Train): 0.0096828 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.015 value_reward_chosen[t] + 0.042 contr_diff + 0.91 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.006 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.999 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.003 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 32, 0, 32, 32, 32, 32, 32, 0\n",
            "value_reward_not_chosen: 0, 0, 32, 32, 32, 32\n",
            "value_choice: 32, 32, 32, 32, 32, 32, 32, 32, 32, 32\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 283/1000 --- L(Train): 0.0096752 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.015 value_reward_chosen[t] + 0.042 contr_diff + 0.911 reward + -0.0 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.006 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.0 contr_diff + -0.0 choice + 0.002 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 33, 0, 33, 33, 33, 33, 33, 0\n",
            "value_reward_not_chosen: 0, 0, 33, 33, 33, 33\n",
            "value_choice: 33, 33, 33, 33, 33, 33, 33, 33, 33, 33\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 284/1000 --- L(Train): 0.0096642 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.015 value_reward_chosen[t] + 0.042 contr_diff + 0.911 reward + 0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.005 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 34, 0, 34, 34, 34, 34, 34, 0\n",
            "value_reward_not_chosen: 0, 0, 34, 34, 34, 34\n",
            "value_choice: 34, 34, 34, 34, 34, 34, 34, 34, 34, 34\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 285/1000 --- L(Train): 0.0096587 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.014 value_reward_chosen[t] + 0.042 contr_diff + 0.911 reward + -0.0 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.005 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 35, 0, 35, 35, 35, 35, 35, 0\n",
            "value_reward_not_chosen: 0, 0, 35, 35, 35, 35\n",
            "value_choice: 35, 35, 35, 35, 35, 35, 35, 35, 35, 35\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 286/1000 --- L(Train): 0.0096506 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.014 value_reward_chosen[t] + 0.042 contr_diff + 0.911 reward + -0.002 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.005 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 36, 0, 36, 36, 36, 36, 36, 0\n",
            "value_reward_not_chosen: 0, 0, 36, 36, 36, 36\n",
            "value_choice: 36, 36, 36, 36, 36, 36, 36, 36, 36, 36\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 287/1000 --- L(Train): 0.0096394 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.014 value_reward_chosen[t] + 0.042 contr_diff + 0.911 reward + -0.002 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.004 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + -0.0 contr_diff + -0.0 choice + -0.0 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 37, 0, 37, 37, 37, 37, 37, 0\n",
            "value_reward_not_chosen: 0, 0, 37, 37, 37, 37\n",
            "value_choice: 37, 37, 37, 37, 37, 37, 37, 37, 37, 37\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 288/1000 --- L(Train): 0.0096293 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.013 value_reward_chosen[t] + 0.041 contr_diff + 0.911 reward + -0.002 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.004 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.999 value_choice[t] + 0.002 contr_diff + 0.0 choice + 0.002 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 38, 0, 38, 38, 38, 38, 38, 0\n",
            "value_reward_not_chosen: 0, 0, 38, 38, 38, 38\n",
            "value_choice: 38, 38, 38, 38, 38, 38, 38, 38, 38, 38\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 289/1000 --- L(Train): 0.0096238 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.013 value_reward_chosen[t] + 0.041 contr_diff + 0.912 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.004 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.003 contr_diff + -0.0 choice + 0.003 value_choice^2 + -0.003 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 39, 0, 39, 39, 39, 39, 39, 0\n",
            "value_reward_not_chosen: 0, 0, 39, 39, 39, 39\n",
            "value_choice: 39, 39, 39, 39, 39, 39, 39, 39, 39, 39\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 290/1000 --- L(Train): 0.0096171 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.013 value_reward_chosen[t] + 0.041 contr_diff + 0.912 reward + 0.0 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.003 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + 0.002 contr_diff + -0.001 choice + 0.003 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 40, 0, 40, 40, 40, 40, 40, 0\n",
            "value_reward_not_chosen: 0, 0, 40, 40, 40, 40\n",
            "value_choice: 40, 40, 40, 40, 40, 40, 40, 40, 40, 40\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 291/1000 --- L(Train): 0.0096118 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.012 value_reward_chosen[t] + 0.041 contr_diff + 0.912 reward + -0.002 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.003 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.002 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 41, 0, 41, 41, 41, 41, 41, 0\n",
            "value_reward_not_chosen: 0, 0, 41, 41, 41, 41\n",
            "value_choice: 41, 41, 41, 41, 41, 41, 41, 41, 41, 41\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 292/1000 --- L(Train): 0.0096005 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.012 value_reward_chosen[t] + 0.041 contr_diff + 0.912 reward + -0.003 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.003 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.002 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 42, 0, 42, 42, 42, 42, 42, 0\n",
            "value_reward_not_chosen: 0, 0, 42, 42, 42, 42\n",
            "value_choice: 42, 42, 42, 42, 42, 42, 42, 42, 42, 42\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 293/1000 --- L(Train): 0.0095892 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.012 value_reward_chosen[t] + 0.041 contr_diff + 0.912 reward + -0.003 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.002 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 43, 0, 43, 43, 43, 43, 43, 0\n",
            "value_reward_not_chosen: 0, 0, 43, 43, 43, 43\n",
            "value_choice: 43, 43, 43, 43, 43, 43, 43, 43, 43, 43\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 294/1000 --- L(Train): 0.0095837 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.012 value_reward_chosen[t] + 0.041 contr_diff + 0.912 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.002 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.998 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 44, 0, 44, 44, 44, 44, 44, 0\n",
            "value_reward_not_chosen: 0, 0, 44, 44, 44, 44\n",
            "value_choice: 44, 44, 44, 44, 44, 44, 44, 44, 44, 44\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 295/1000 --- L(Train): 0.0095776 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.011 value_reward_chosen[t] + 0.041 contr_diff + 0.912 reward + -0.0 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.002 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.997 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 45, 0, 45, 45, 45, 45, 45, 0\n",
            "value_reward_not_chosen: 0, 0, 45, 45, 45, 45\n",
            "value_choice: 45, 45, 45, 45, 45, 45, 45, 45, 45, 45\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 296/1000 --- L(Train): 0.0095692 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.011 value_reward_chosen[t] + 0.041 contr_diff + 0.913 reward + 0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.001 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 0.997 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.002 value_choice^2 + -0.0 value_choice*contr_diff + 0.002 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 46, 0, 46, 46, 46, 46, 46, 0\n",
            "value_reward_not_chosen: 0, 0, 46, 46, 46, 46\n",
            "value_choice: 46, 46, 46, 46, 46, 46, 46, 46, 46, 46\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 297/1000 --- L(Train): 0.0095620 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.011 value_reward_chosen[t] + 0.041 contr_diff + 0.913 reward + -0.002 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.001 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 0.998 value_choice[t] + 0.001 contr_diff + -0.0 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + 0.002 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 47, 0, 47, 47, 47, 47, 47, 0\n",
            "value_reward_not_chosen: 0, 0, 47, 47, 47, 47\n",
            "value_choice: 47, 47, 47, 47, 47, 47, 47, 47, 47, 47\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 298/1000 --- L(Train): 0.0095530 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.011 value_reward_chosen[t] + 0.041 contr_diff + 0.913 reward + -0.003 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.001 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + 0.001 contr_diff + -0.0 choice + 0.003 value_choice^2 + 0.0 value_choice*contr_diff + 0.002 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 48, 0, 48, 48, 48, 48, 48, 0\n",
            "value_reward_not_chosen: 0, 0, 48, 48, 48, 48\n",
            "value_choice: 48, 48, 48, 48, 48, 48, 48, 48, 48, 48\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 299/1000 --- L(Train): 0.0095489 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.01 value_reward_chosen[t] + 0.041 contr_diff + 0.913 reward + -0.004 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.001 value_choice[t] + -0.0 contr_diff + 0.0 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 49, 0, 49, 49, 49, 49, 49, 0\n",
            "value_reward_not_chosen: 0, 0, 49, 49, 49, 49\n",
            "value_choice: 49, 49, 49, 49, 49, 49, 49, 49, 49, 49\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 300/1000 --- L(Train): 0.0095417 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.01 value_reward_chosen[t] + 0.041 contr_diff + 0.913 reward + -0.003 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.003 1 + 1.001 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 50, 0, 50, 50, 50, 50, 50, 0\n",
            "value_reward_not_chosen: 0, 0, 50, 50, 50, 50\n",
            "value_choice: 50, 50, 50, 50, 50, 50, 50, 50, 50, 50\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 301/1000 --- L(Train): 0.0095334 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.01 value_reward_chosen[t] + 0.041 contr_diff + 0.913 reward + -0.001 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.999 value_choice[t] + -0.0 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.0 value_choice*contr_diff + -0.003 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 51, 0, 51, 51, 51, 51, 51, 0\n",
            "value_reward_not_chosen: 0, 0, 51, 51, 51, 51\n",
            "value_choice: 51, 51, 51, 51, 51, 51, 51, 51, 51, 51\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 302/1000 --- L(Train): 0.0095265 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.01 value_reward_chosen[t] + 0.041 contr_diff + 0.913 reward + 0.0 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + 0.001 contr_diff + 0.002 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 52, 0, 52, 52, 52, 52, 52, 0\n",
            "value_reward_not_chosen: 0, 0, 52, 52, 52, 52\n",
            "value_choice: 52, 52, 52, 52, 52, 52, 52, 52, 52, 52\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 303/1000 --- L(Train): 0.0095231 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.009 value_reward_chosen[t] + 0.041 contr_diff + 0.913 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.887 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.0 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 53, 0, 53, 53, 53, 53, 53, 0\n",
            "value_reward_not_chosen: 0, 0, 53, 53, 53, 53\n",
            "value_choice: 53, 53, 53, 53, 53, 53, 53, 53, 53, 53\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 304/1000 --- L(Train): 0.0095157 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.009 value_reward_chosen[t] + 0.041 contr_diff + 0.913 reward + -0.003 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + 0.001 contr_diff + -0.001 choice + 0.002 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 54, 0, 54, 54, 54, 54, 54, 0\n",
            "value_reward_not_chosen: 0, 0, 54, 54, 54, 54\n",
            "value_choice: 54, 54, 54, 54, 54, 54, 54, 54, 54, 54\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 305/1000 --- L(Train): 0.0095108 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.009 value_reward_chosen[t] + 0.041 contr_diff + 0.914 reward + -0.003 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.002 value_choice[t] + -0.001 contr_diff + -0.002 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 55, 0, 55, 55, 55, 55, 55, 0\n",
            "value_reward_not_chosen: 0, 0, 55, 55, 55, 55\n",
            "value_choice: 55, 55, 55, 55, 55, 55, 55, 55, 55, 55\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 306/1000 --- L(Train): 0.0095011 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.009 value_reward_chosen[t] + 0.041 contr_diff + 0.914 reward + -0.002 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + -0.001 contr_diff + -0.002 choice + 0.003 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 56, 0, 56, 56, 56, 56, 56, 0\n",
            "value_reward_not_chosen: 0, 0, 56, 56, 56, 56\n",
            "value_choice: 56, 56, 56, 56, 56, 56, 56, 56, 56, 56\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 307/1000 --- L(Train): 0.0094966 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.008 value_reward_chosen[t] + 0.041 contr_diff + 0.914 reward + -0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + -0.001 contr_diff + -0.0 choice + 0.002 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 57, 0, 57, 57, 57, 57, 57, 0\n",
            "value_reward_not_chosen: 0, 0, 57, 57, 57, 57\n",
            "value_choice: 57, 57, 57, 57, 57, 57, 57, 57, 57, 57\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 308/1000 --- L(Train): 0.0094896 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.008 value_reward_chosen[t] + 0.041 contr_diff + 0.914 reward + 0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.998 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 58, 0, 58, 58, 58, 58, 58, 0\n",
            "value_reward_not_chosen: 0, 0, 58, 58, 58, 58\n",
            "value_choice: 58, 58, 58, 58, 58, 58, 58, 58, 58, 58\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 309/1000 --- L(Train): 0.0094861 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.008 value_reward_chosen[t] + 0.041 contr_diff + 0.914 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.997 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 59, 0, 59, 59, 59, 59, 59, 0\n",
            "value_reward_not_chosen: 0, 0, 59, 59, 59, 59\n",
            "value_choice: 59, 59, 59, 59, 59, 59, 59, 59, 59, 59\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 310/1000 --- L(Train): 0.0094812 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.008 value_reward_chosen[t] + 0.041 contr_diff + 0.914 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.997 value_choice[t] + -0.0 contr_diff + 0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 60, 0, 60, 60, 60, 60, 60, 0\n",
            "value_reward_not_chosen: 0, 0, 60, 60, 60, 60\n",
            "value_choice: 60, 60, 60, 60, 60, 60, 60, 60, 60, 60\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 311/1000 --- L(Train): 0.0094728 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.008 value_reward_chosen[t] + 0.041 contr_diff + 0.914 reward + -0.002 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 0.998 value_choice[t] + -0.0 contr_diff + -0.0 choice + -0.0 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 61, 0, 61, 61, 61, 61, 61, 0\n",
            "value_reward_not_chosen: 0, 0, 61, 61, 61, 61\n",
            "value_choice: 61, 61, 61, 61, 61, 61, 61, 61, 61, 61\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 312/1000 --- L(Train): 0.0094661 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.008 value_reward_chosen[t] + 0.041 contr_diff + 0.914 reward + -0.002 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.0 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 62, 0, 62, 62, 62, 62, 62, 0\n",
            "value_reward_not_chosen: 0, 0, 62, 62, 62, 62\n",
            "value_choice: 62, 62, 62, 62, 62, 62, 62, 62, 62, 62\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 313/1000 --- L(Train): 0.0094593 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.007 value_reward_chosen[t] + 0.041 contr_diff + 0.914 reward + -0.0 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.001 choice + 0.003 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 63, 0, 63, 63, 63, 63, 63, 0\n",
            "value_reward_not_chosen: 0, 0, 63, 63, 63, 63\n",
            "value_choice: 63, 63, 63, 63, 63, 63, 63, 63, 63, 63\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 314/1000 --- L(Train): 0.0094538 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.007 value_reward_chosen[t] + 0.041 contr_diff + 0.914 reward + 0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.001 value_choice[t] + -0.0 contr_diff + -0.0 choice + 0.003 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 64, 0, 64, 64, 64, 64, 64, 0\n",
            "value_reward_not_chosen: 0, 0, 64, 64, 64, 64\n",
            "value_choice: 64, 64, 64, 64, 64, 64, 64, 64, 64, 64\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 315/1000 --- L(Train): 0.0094493 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.007 value_reward_chosen[t] + 0.041 contr_diff + 0.915 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.999 value_choice[t] + -0.001 contr_diff + -0.001 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 65, 0, 65, 65, 65, 65, 65, 0\n",
            "value_reward_not_chosen: 0, 0, 65, 65, 65, 65\n",
            "value_choice: 65, 65, 65, 65, 65, 65, 65, 65, 65, 65\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 316/1000 --- L(Train): 0.0094395 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.007 value_reward_chosen[t] + 0.041 contr_diff + 0.915 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.999 value_choice[t] + 0.0 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 66, 0, 66, 66, 66, 66, 66, 0\n",
            "value_reward_not_chosen: 0, 0, 66, 66, 66, 66\n",
            "value_choice: 66, 66, 66, 66, 66, 66, 66, 66, 66, 66\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 317/1000 --- L(Train): 0.0094338 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.007 value_reward_chosen[t] + 0.041 contr_diff + 0.915 reward + -0.002 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.004 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 67, 0, 67, 67, 67, 67, 67, 0\n",
            "value_reward_not_chosen: 0, 0, 67, 67, 67, 67\n",
            "value_choice: 67, 67, 67, 67, 67, 67, 67, 67, 67, 67\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 318/1000 --- L(Train): 0.0094328 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.007 value_reward_chosen[t] + 0.04 contr_diff + 0.915 reward + -0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.004 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 68, 0, 68, 68, 68, 68, 68, 0\n",
            "value_reward_not_chosen: 0, 0, 68, 68, 68, 68\n",
            "value_choice: 68, 68, 68, 68, 68, 68, 68, 68, 68, 68\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 319/1000 --- L(Train): 0.0094273 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.007 value_reward_chosen[t] + 0.04 contr_diff + 0.915 reward + 0.0 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.886 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.0 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 69, 0, 69, 69, 69, 69, 69, 0\n",
            "value_reward_not_chosen: 0, 0, 69, 69, 69, 69\n",
            "value_choice: 69, 69, 69, 69, 69, 69, 69, 69, 69, 69\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 320/1000 --- L(Train): 0.0094206 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.915 reward + -0.001 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.003 contr_diff^2 + -0.002 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.0 value_choice[t] + -0.0 contr_diff + -0.001 choice + 0.002 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 70, 0, 70, 70, 70, 70, 70, 0\n",
            "value_reward_not_chosen: 0, 0, 70, 70, 70, 70\n",
            "value_choice: 70, 70, 70, 70, 70, 70, 70, 70, 70, 70\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 321/1000 --- L(Train): 0.0094185 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.915 reward + -0.001 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.999 value_choice[t] + 0.002 contr_diff + -0.001 choice + 0.003 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 71, 0, 71, 71, 71, 71, 71, 0\n",
            "value_reward_not_chosen: 0, 0, 71, 71, 71, 71\n",
            "value_choice: 71, 71, 71, 71, 71, 71, 71, 71, 71, 71\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 322/1000 --- L(Train): 0.0094142 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.915 reward + -0.001 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 0.999 value_choice[t] + 0.002 contr_diff + -0.0 choice + 0.003 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 72, 0, 72, 72, 72, 72, 72, 0\n",
            "value_reward_not_chosen: 0, 0, 72, 72, 72, 72\n",
            "value_choice: 72, 72, 72, 72, 72, 72, 72, 72, 72, 72\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 323/1000 --- L(Train): 0.0094105 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.915 reward + -0.0 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.003 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.0 value_choice[t] + 0.002 contr_diff + 0.001 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 73, 0, 73, 73, 73, 73, 73, 0\n",
            "value_reward_not_chosen: 0, 0, 73, 73, 73, 73\n",
            "value_choice: 73, 73, 73, 73, 73, 73, 73, 73, 73, 73\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 324/1000 --- L(Train): 0.0094013 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.915 reward + 0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.003 contr_diff^2 + 0.003 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 74, 0, 74, 74, 74, 74, 74, 0\n",
            "value_reward_not_chosen: 0, 0, 74, 74, 74, 74\n",
            "value_choice: 74, 74, 74, 74, 74, 74, 74, 74, 74, 74\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 325/1000 --- L(Train): 0.0093960 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.915 reward + -0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.003 contr_diff^2 + 0.002 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + -0.003 contr_diff + -0.001 choice + -0.001 value_choice^2 + -0.003 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 75, 0, 75, 75, 75, 75, 75, 0\n",
            "value_reward_not_chosen: 0, 0, 75, 75, 75, 75\n",
            "value_choice: 75, 75, 75, 75, 75, 75, 75, 75, 75, 75\n",
            "================================================================================\u001b[H\u001b[2J\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 326/1000 --- L(Train): 0.0093931 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.916 reward + -0.002 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.002 value_choice[t] + -0.004 contr_diff + -0.001 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 76, 0, 76, 76, 76, 76, 76, 0\n",
            "value_reward_not_chosen: 0, 0, 76, 76, 76, 76\n",
            "value_choice: 76, 76, 76, 76, 76, 76, 76, 76, 76, 76\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 327/1000 --- L(Train): 0.0093892 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.916 reward + -0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.0 value_choice[t] + -0.004 contr_diff + 0.0 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 77, 0, 77, 77, 77, 77, 77, 0\n",
            "value_reward_not_chosen: 0, 0, 77, 77, 77, 77\n",
            "value_choice: 77, 77, 77, 77, 77, 77, 77, 77, 77, 77\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 328/1000 --- L(Train): 0.0093840 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.916 reward + -0.0 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.0 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.998 value_choice[t] + -0.003 contr_diff + 0.001 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.004 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 78, 0, 78, 78, 78, 78, 78, 0\n",
            "value_reward_not_chosen: 0, 0, 78, 78, 78, 78\n",
            "value_choice: 78, 78, 78, 78, 78, 78, 78, 78, 78, 78\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 329/1000 --- L(Train): 0.0093783 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.916 reward + 0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.004 contr_diff^2 + -0.0 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.997 value_choice[t] + -0.001 contr_diff + 0.001 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 79, 0, 79, 79, 79, 79, 79, 0\n",
            "value_reward_not_chosen: 0, 0, 79, 79, 79, 79\n",
            "value_choice: 79, 79, 79, 79, 79, 79, 79, 79, 79, 79\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 330/1000 --- L(Train): 0.0093747 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.587 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.916 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.004 contr_diff^2 + 0.001 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.997 value_choice[t] + 0.002 contr_diff + -0.0 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 80, 0, 80, 80, 80, 80, 80, 0\n",
            "value_reward_not_chosen: 0, 0, 80, 80, 80, 80\n",
            "value_choice: 80, 80, 80, 80, 80, 80, 80, 80, 80, 80\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 331/1000 --- L(Train): 0.0093706 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.916 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.001 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 0.998 value_choice[t] + 0.003 contr_diff + -0.0 choice + 0.002 value_choice^2 + -0.0 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 81, 0, 81, 81, 81, 81, 81, 0\n",
            "value_reward_not_chosen: 0, 0, 81, 81, 81, 81\n",
            "value_choice: 81, 81, 81, 81, 81, 81, 81, 81, 81, 81\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 332/1000 --- L(Train): 0.0093633 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.006 value_reward_chosen[t] + 0.04 contr_diff + 0.916 reward + -0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.003 contr_diff + 0.0 choice + -0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.002 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 82, 0, 82, 82, 82, 82, 82, 0\n",
            "value_reward_not_chosen: 0, 0, 82, 82, 82, 82\n",
            "value_choice: 82, 82, 82, 82, 82, 82, 82, 82, 82, 82\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 333/1000 --- L(Train): 0.0093590 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.916 reward + -0.0 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.001 value_choice[t] + 0.002 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 83, 0, 83, 83, 83, 83, 83, 0\n",
            "value_reward_not_chosen: 0, 0, 83, 83, 83, 83\n",
            "value_choice: 83, 83, 83, 83, 83, 83, 83, 83, 83, 83\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 334/1000 --- L(Train): 0.0093546 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.916 reward + 0.001 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.004 contr_diff^2 + -0.002 contr_diff*reward + 0.885 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.001 value_choice[t] + 0.0 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.0 value_choice*contr_diff + -0.002 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 84, 0, 84, 84, 84, 84, 84, 0\n",
            "value_reward_not_chosen: 0, 0, 84, 84, 84, 84\n",
            "value_choice: 84, 84, 84, 84, 84, 84, 84, 84, 84, 84\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 335/1000 --- L(Train): 0.0093508 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.917 reward + -0.0 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.005 contr_diff^2 + -0.001 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 0.999 value_choice[t] + -0.003 contr_diff + 0.001 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + -0.003 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 85, 0, 85, 85, 85, 85, 85, 0\n",
            "value_reward_not_chosen: 0, 0, 85, 85, 85, 85\n",
            "value_choice: 85, 85, 85, 85, 85, 85, 85, 85, 85, 85\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 336/1000 --- L(Train): 0.0093464 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.917 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.004 contr_diff^2 + 0.001 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.999 value_choice[t] + -0.004 contr_diff + 0.001 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + -0.003 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 86, 0, 86, 86, 86, 86, 86, 0\n",
            "value_reward_not_chosen: 0, 0, 86, 86, 86, 86\n",
            "value_choice: 86, 86, 86, 86, 86, 86, 86, 86, 86, 86\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 337/1000 --- L(Train): 0.0093418 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.917 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.003 contr_diff^2 + 0.002 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + -0.005 contr_diff + 0.0 choice + 0.003 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 87, 0, 87, 87, 87, 87, 87, 0\n",
            "value_reward_not_chosen: 0, 0, 87, 87, 87, 87\n",
            "value_choice: 87, 87, 87, 87, 87, 87, 87, 87, 87, 87\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 338/1000 --- L(Train): 0.0093378 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.917 reward + -0.0 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + -0.004 contr_diff + -0.001 choice + 0.003 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 88, 0, 88, 88, 88, 88, 88, 0\n",
            "value_reward_not_chosen: 0, 0, 88, 88, 88, 88\n",
            "value_choice: 88, 88, 88, 88, 88, 88, 88, 88, 88, 88\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 339/1000 --- L(Train): 0.0093371 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.917 reward + 0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.001 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + -0.002 contr_diff + -0.001 choice + 0.002 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 89, 0, 89, 89, 89, 89, 89, 0\n",
            "value_reward_not_chosen: 0, 0, 89, 89, 89, 89\n",
            "value_choice: 89, 89, 89, 89, 89, 89, 89, 89, 89, 89\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 340/1000 --- L(Train): 0.0093324 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.917 reward + -0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.005 contr_diff^2 + -0.001 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.002 value_choice[t] + 0.0 contr_diff + -0.001 choice + -0.001 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 90, 0, 90, 90, 90, 90, 90, 0\n",
            "value_reward_not_chosen: 0, 0, 90, 90, 90, 90\n",
            "value_choice: 90, 90, 90, 90, 90, 90, 90, 90, 90, 90\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 341/1000 --- L(Train): 0.0093298 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.917 reward + -0.001 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.005 contr_diff^2 + -0.001 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 91, 0, 91, 91, 91, 91, 91, 0\n",
            "value_reward_not_chosen: 0, 0, 91, 91, 91, 91\n",
            "value_choice: 91, 91, 91, 91, 91, 91, 91, 91, 91, 91\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 342/1000 --- L(Train): 0.0093230 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.917 reward + -0.001 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.004 contr_diff^2 + -0.001 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.998 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 92, 0, 92, 92, 92, 92, 92, 0\n",
            "value_reward_not_chosen: 0, 0, 92, 92, 92, 92\n",
            "value_choice: 92, 92, 92, 92, 92, 92, 92, 92, 92, 92\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 343/1000 --- L(Train): 0.0093210 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.917 reward + -0.0 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.997 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.0 value_choice^2 + -0.003 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 93, 0, 93, 93, 93, 93, 93, 0\n",
            "value_reward_not_chosen: 0, 0, 93, 93, 93, 93\n",
            "value_choice: 93, 93, 93, 93, 93, 93, 93, 93, 93, 93\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 344/1000 --- L(Train): 0.0093180 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.917 reward + 0.001 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.997 value_choice[t] + -0.002 contr_diff + 0.0 choice + 0.002 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 94, 0, 94, 94, 94, 94, 94, 0\n",
            "value_reward_not_chosen: 0, 0, 94, 94, 94, 94\n",
            "value_choice: 94, 94, 94, 94, 94, 94, 94, 94, 94, 94\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 345/1000 --- L(Train): 0.0093147 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.918 reward + -0.0 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.998 value_choice[t] + -0.002 contr_diff + -0.001 choice + 0.003 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 95, 0, 95, 95, 95, 95, 95, 0\n",
            "value_reward_not_chosen: 0, 0, 95, 95, 95, 95\n",
            "value_choice: 95, 95, 95, 95, 95, 95, 95, 95, 95, 95\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 346/1000 --- L(Train): 0.0093108 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.918 reward + -0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 1.0 value_choice[t] + -0.002 contr_diff + -0.002 choice + 0.003 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 96, 0, 96, 96, 96, 96, 96, 0\n",
            "value_reward_not_chosen: 0, 0, 96, 96, 96, 96\n",
            "value_choice: 96, 96, 96, 96, 96, 96, 96, 96, 96, 96\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 347/1000 --- L(Train): 0.0093069 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.918 reward + -0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + 0.0 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.001 value_choice[t] + -0.001 contr_diff + -0.002 choice + 0.002 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 97, 0, 97, 97, 97, 97, 97, 0\n",
            "value_reward_not_chosen: 0, 0, 97, 97, 97, 97\n",
            "value_choice: 97, 97, 97, 97, 97, 97, 97, 97, 97, 97\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 348/1000 --- L(Train): 0.0093011 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.918 reward + -0.0 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 1.001 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 98, 0, 98, 98, 98, 98, 98, 0\n",
            "value_reward_not_chosen: 0, 0, 98, 98, 98, 98\n",
            "value_choice: 98, 98, 98, 98, 98, 98, 98, 98, 98, 98\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 349/1000 --- L(Train): 0.0092960 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.918 reward + 0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.884 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.002 1 + 0.999 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.002 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 99, 0, 99, 99, 99, 99, 99, 0\n",
            "value_reward_not_chosen: 0, 0, 99, 99, 99, 99\n",
            "value_choice: 99, 99, 99, 99, 99, 99, 99, 99, 99, 99\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 350/1000 --- L(Train): 0.0092935 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 25):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.918 reward + -0.0 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.001 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.0 1 + 0.999 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.001 value_choice^2 + -0.0 value_choice*contr_diff + 0.002 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 100, 0, 100, 100, 100, 100, 100, 0\n",
            "value_reward_not_chosen: 0, 0, -, 100, 100, 100\n",
            "value_choice: 100, 100, 100, 100, 100, 100, 100, 100, 100, 100\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 351/1000 --- L(Train): 0.0092924 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 24):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.918 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.002 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.0 value_choice[t] + -0.0 contr_diff + 0.0 choice + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 101, 0, 101, 101, 101, 101, 101, 0\n",
            "value_reward_not_chosen: 0, 0, -, 101, 101, 101\n",
            "value_choice: 101, 101, 101, 101, -, 101, 101, 101, 101, 101\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 352/1000 --- L(Train): 0.0092871 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 23):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.918 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.0 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.002 1 + 1.002 value_choice[t] + -0.0 contr_diff + -0.001 choice + 0.0 value_choice*contr_diff + -0.002 value_choice*choice + 0.0 contr_diff^2 + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 102, 0, 102, 102, 102, 102, 102, 0\n",
            "value_reward_not_chosen: 0, 0, -, 102, 102, 102\n",
            "value_choice: 102, 102, 102, 102, -, 102, 102, 102, -, 102\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 353/1000 --- L(Train): 0.0092821 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 22):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.918 reward + -0.0 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.001 1 + 1.002 value_choice[t] + 0.0 contr_diff + -0.0 choice + -0.001 value_choice*contr_diff + -0.003 value_choice*choice + -0.001 contr_diff^2 + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 103, 0, 103, 103, -, 103, 103, 0\n",
            "value_reward_not_chosen: 0, 0, -, 103, 103, 103\n",
            "value_choice: 103, 103, 103, 103, -, 103, 103, 103, -, 103\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 354/1000 --- L(Train): 0.0092790 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 21):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.918 reward + 0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.002 value_choice[t] + 0.001 choice + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.002 contr_diff^2 + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 104, 0, 104, 104, -, 104, 104, 0\n",
            "value_reward_not_chosen: 0, 0, -, 104, 104, 104\n",
            "value_choice: 104, 104, -, 104, -, 104, 104, 104, -, 104\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 355/1000 --- L(Train): 0.0092772 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 20):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.919 reward + -0.0 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.001 1 + 1.0 value_choice[t] + 0.0 choice + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 105, 0, 105, 105, -, 105, 105, 0\n",
            "value_reward_not_chosen: 0, 0, -, 105, 105, 105\n",
            "value_choice: 105, 105, -, 105, -, -, 105, 105, -, 105\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 356/1000 --- L(Train): 0.0092714 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 19):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.919 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.0 1 + 0.998 value_choice[t] + -0.001 choice + 0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 106, 0, 106, 106, -, 106, 106, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, 106, 106\n",
            "value_choice: 106, 106, -, 106, -, -, 106, 106, -, 106\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 357/1000 --- L(Train): 0.0092688 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 18):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.919 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.997 value_choice[t] + -0.001 choice + 0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 107, 0, 107, 107, -, 107, 107, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, 107, 107\n",
            "value_choice: -, 107, -, 107, -, -, 107, 107, -, 107\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 358/1000 --- L(Train): 0.0092660 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 17):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.919 reward + -0.0 value_reward_chosen*contr_diff + 0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.997 value_choice[t] + 0.0 choice + 0.001 value_choice*choice + 0.0 contr_diff^2 + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 108, 0, -, 108, -, 108, 108, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, 108, 108\n",
            "value_choice: -, 108, -, 108, -, -, 108, 108, -, 108\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 359/1000 --- L(Train): 0.0092628 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.919 reward + 0.001 value_reward_chosen*contr_diff + 0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.998 value_choice[t] + 0.0 choice + -0.001 contr_diff^2 + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 109, 0, -, 109, -, 109, 109, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, 109, 109\n",
            "value_choice: -, 109, -, 109, -, -, -, 109, -, 109\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 360/1000 --- L(Train): 0.0092593 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.919 reward + 0.002 value_reward_chosen*contr_diff + 0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.0 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.001 choice + -0.002 contr_diff^2 + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 110, 0, -, 110, -, 110, 110, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, 110, 110\n",
            "value_choice: -, -, -, 110, -, -, -, 110, -, 110\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 361/1000 --- L(Train): 0.0092565 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.919 reward + 0.002 value_reward_chosen*contr_diff + -0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + -0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.001 choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 111, 0, -, 111, -, 111, 111, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, 111, 111\n",
            "value_choice: -, -, -, 111, -, -, -, -, -, 111\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 362/1000 --- L(Train): 0.0092605 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.919 reward + -0.004 contr_diff^2 + -0.002 contr_diff*reward + 0.883 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.003 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + -0.0 choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 112, 0, -, -, -, 112, 112, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, 112, 112\n",
            "value_choice: -, -, -, 112, -, -, -, -, -, 112\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 363/1000 --- L(Train): 0.0092825 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 12):\n",
            "value_reward_chosen[t+1] = -0.588 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.919 reward + -0.004 contr_diff^2 + -0.001 contr_diff*reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.001 choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 113, 0, -, -, -, 113, 113, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, 113, 113\n",
            "value_choice: -, -, -, 113, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 364/1000 --- L(Train): 0.0093087 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 11):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.919 reward + -0.003 contr_diff^2 + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.002 choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 114, 0, -, -, -, 114, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, 114, 114\n",
            "value_choice: -, -, -, 114, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 365/1000 --- L(Train): 0.0093383 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 10):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.92 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + -0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] + 0.001 choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 115, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, 115, 115\n",
            "value_choice: -, -, -, 115, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 366/1000 --- L(Train): 0.0094301 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 10):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.92 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.467 1 + 0.746 value_reward_not_chosen[t] + 0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 116, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, 116, 116\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 367/1000 --- L(Train): 0.0094347 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 9):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.92 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] + 0.002 value_reward_not_chosen*contr_diff \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 117, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, 117, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 368/1000 --- L(Train): 0.0094890 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 9):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.04 contr_diff + 0.92 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 118, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 369/1000 --- L(Train): 0.0095888 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.92 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 370/1000 --- L(Train): 0.0096068 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.92 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 371/1000 --- L(Train): 0.0095606 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.92 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 372/1000 --- L(Train): 0.0095163 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.92 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 373/1000 --- L(Train): 0.0095044 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.92 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 374/1000 --- L(Train): 0.0095098 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.92 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 375/1000 --- L(Train): 0.0095106 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.92 reward + 0.882 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 376/1000 --- L(Train): 0.0094974 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.921 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 377/1000 --- L(Train): 0.0094718 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.921 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 378/1000 --- L(Train): 0.0094516 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.921 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 379/1000 --- L(Train): 0.0094386 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.921 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 380/1000 --- L(Train): 0.0094387 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.921 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 381/1000 --- L(Train): 0.0094405 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.921 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 382/1000 --- L(Train): 0.0094375 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.921 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 383/1000 --- L(Train): 0.0094320 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.921 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 384/1000 --- L(Train): 0.0094345 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.921 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 385/1000 --- L(Train): 0.0094256 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.922 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 386/1000 --- L(Train): 0.0094167 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.922 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 387/1000 --- L(Train): 0.0094074 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.922 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 388/1000 --- L(Train): 0.0094000 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.922 reward + 0.881 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 389/1000 --- L(Train): 0.0093982 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.922 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 390/1000 --- L(Train): 0.0093940 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.922 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 391/1000 --- L(Train): 0.0093877 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.922 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 392/1000 --- L(Train): 0.0093836 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.922 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 393/1000 --- L(Train): 0.0093808 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.922 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 394/1000 --- L(Train): 0.0093771 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.923 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 395/1000 --- L(Train): 0.0093728 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.923 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 396/1000 --- L(Train): 0.0093694 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.923 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 397/1000 --- L(Train): 0.0093680 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.923 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 398/1000 --- L(Train): 0.0093677 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.923 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 399/1000 --- L(Train): 0.0093642 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.005 value_reward_chosen[t] + 0.923 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 400/1000 --- L(Train): 0.0093600 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.004 value_reward_chosen[t] + 0.923 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 401/1000 --- L(Train): 0.0093710 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.004 value_reward_chosen[t] + 0.923 reward + 0.88 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 402/1000 --- L(Train): 0.0093701 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.004 value_reward_chosen[t] + 0.923 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 403/1000 --- L(Train): 0.0093668 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.004 value_reward_chosen[t] + 0.924 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 404/1000 --- L(Train): 0.0093641 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.004 value_reward_chosen[t] + 0.924 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 405/1000 --- L(Train): 0.0093594 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.004 value_reward_chosen[t] + 0.924 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 406/1000 --- L(Train): 0.0093552 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.004 value_reward_chosen[t] + 0.924 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 407/1000 --- L(Train): 0.0093511 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.589 1 + 0.004 value_reward_chosen[t] + 0.924 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 408/1000 --- L(Train): 0.0093488 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.924 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 409/1000 --- L(Train): 0.0093459 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.924 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 410/1000 --- L(Train): 0.0093414 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.924 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 411/1000 --- L(Train): 0.0093355 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.924 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 412/1000 --- L(Train): 0.0093358 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.924 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 413/1000 --- L(Train): 0.0093350 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.925 reward + 0.879 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 414/1000 --- L(Train): 0.0093322 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.925 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 415/1000 --- L(Train): 0.0093288 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.925 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 416/1000 --- L(Train): 0.0093285 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.925 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 417/1000 --- L(Train): 0.0093285 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.925 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 418/1000 --- L(Train): 0.0093288 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.925 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 419/1000 --- L(Train): 0.0093269 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.925 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 420/1000 --- L(Train): 0.0093263 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.925 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 421/1000 --- L(Train): 0.0093250 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.925 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 422/1000 --- L(Train): 0.0093222 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.925 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 423/1000 --- L(Train): 0.0093196 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.926 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 424/1000 --- L(Train): 0.0093204 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.926 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 425/1000 --- L(Train): 0.0093205 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.926 reward + 0.878 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 426/1000 --- L(Train): 0.0093187 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.926 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 427/1000 --- L(Train): 0.0093167 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.926 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 428/1000 --- L(Train): 0.0093138 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.926 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 429/1000 --- L(Train): 0.0093124 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.926 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 430/1000 --- L(Train): 0.0093137 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.926 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 431/1000 --- L(Train): 0.0093124 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.926 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 432/1000 --- L(Train): 0.0093109 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.927 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 433/1000 --- L(Train): 0.0093091 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.927 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 434/1000 --- L(Train): 0.0093066 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.927 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 435/1000 --- L(Train): 0.0093048 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.927 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 436/1000 --- L(Train): 0.0093050 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.927 reward + 0.877 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 437/1000 --- L(Train): 0.0093040 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.927 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 438/1000 --- L(Train): 0.0093021 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.927 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 439/1000 --- L(Train): 0.0092993 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.927 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 440/1000 --- L(Train): 0.0093024 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.927 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 441/1000 --- L(Train): 0.0093003 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.927 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 442/1000 --- L(Train): 0.0093007 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.928 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 443/1000 --- L(Train): 0.0092996 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.928 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 444/1000 --- L(Train): 0.0092987 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.928 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 445/1000 --- L(Train): 0.0092960 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.928 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 446/1000 --- L(Train): 0.0092942 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.928 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 447/1000 --- L(Train): 0.0092919 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.928 reward + 0.876 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 448/1000 --- L(Train): 0.0092928 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.928 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 449/1000 --- L(Train): 0.0092917 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.928 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 450/1000 --- L(Train): 0.0092914 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.928 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 451/1000 --- L(Train): 0.0092870 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.929 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 452/1000 --- L(Train): 0.0092869 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.929 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 453/1000 --- L(Train): 0.0092874 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.929 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 454/1000 --- L(Train): 0.0092876 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.929 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 455/1000 --- L(Train): 0.0092866 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.929 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 456/1000 --- L(Train): 0.0092834 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.929 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 457/1000 --- L(Train): 0.0092834 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.929 reward + 0.875 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 458/1000 --- L(Train): 0.0092840 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.929 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 459/1000 --- L(Train): 0.0092822 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.929 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 460/1000 --- L(Train): 0.0092803 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.929 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 461/1000 --- L(Train): 0.0092799 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.93 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 462/1000 --- L(Train): 0.0092796 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.93 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 463/1000 --- L(Train): 0.0092768 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.93 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 464/1000 --- L(Train): 0.0092755 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.93 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 465/1000 --- L(Train): 0.0092747 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.93 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 466/1000 --- L(Train): 0.0092756 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.93 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 467/1000 --- L(Train): 0.0092740 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.93 reward + 0.874 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 468/1000 --- L(Train): 0.0092737 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.93 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 469/1000 --- L(Train): 0.0092705 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.93 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 470/1000 --- L(Train): 0.0092703 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.931 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 471/1000 --- L(Train): 0.0092707 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.931 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 472/1000 --- L(Train): 0.0092714 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.931 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 473/1000 --- L(Train): 0.0092700 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.931 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 474/1000 --- L(Train): 0.0092689 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.931 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 475/1000 --- L(Train): 0.0092669 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.931 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 476/1000 --- L(Train): 0.0092669 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.931 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 477/1000 --- L(Train): 0.0092665 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.931 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 478/1000 --- L(Train): 0.0092667 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.931 reward + 0.873 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 479/1000 --- L(Train): 0.0092639 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.931 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 480/1000 --- L(Train): 0.0092634 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.932 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 481/1000 --- L(Train): 0.0092645 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.932 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 482/1000 --- L(Train): 0.0092650 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.932 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 483/1000 --- L(Train): 0.0092628 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.932 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 484/1000 --- L(Train): 0.0092632 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.932 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 485/1000 --- L(Train): 0.0092628 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.932 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 486/1000 --- L(Train): 0.0092643 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.932 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 487/1000 --- L(Train): 0.0092611 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.932 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 488/1000 --- L(Train): 0.0092598 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.932 reward + 0.872 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 489/1000 --- L(Train): 0.0092590 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.933 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 490/1000 --- L(Train): 0.0092591 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.933 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 491/1000 --- L(Train): 0.0092583 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.933 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 492/1000 --- L(Train): 0.0092574 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.933 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 493/1000 --- L(Train): 0.0092550 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.933 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 494/1000 --- L(Train): 0.0092508 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.933 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 495/1000 --- L(Train): 0.0092486 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.933 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 496/1000 --- L(Train): 0.0092503 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.933 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 497/1000 --- L(Train): 0.0092503 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.933 reward + 0.871 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 498/1000 --- L(Train): 0.0092504 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.934 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 499/1000 --- L(Train): 0.0092481 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.934 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 500/1000 --- L(Train): 0.0092473 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.934 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 501/1000 --- L(Train): 0.0092474 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.934 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 502/1000 --- L(Train): 0.0092479 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.934 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 503/1000 --- L(Train): 0.0092480 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.934 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 504/1000 --- L(Train): 0.0092482 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.934 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 505/1000 --- L(Train): 0.0092467 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.934 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 506/1000 --- L(Train): 0.0092462 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.934 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 507/1000 --- L(Train): 0.0092454 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.934 reward + 0.87 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 508/1000 --- L(Train): 0.0092471 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.935 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 509/1000 --- L(Train): 0.0092482 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.935 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 510/1000 --- L(Train): 0.0092478 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.935 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 511/1000 --- L(Train): 0.0092461 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.935 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 512/1000 --- L(Train): 0.0092441 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.935 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 513/1000 --- L(Train): 0.0092427 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.935 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 514/1000 --- L(Train): 0.0092436 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.935 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 515/1000 --- L(Train): 0.0092430 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.935 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 516/1000 --- L(Train): 0.0092435 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.935 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 517/1000 --- L(Train): 0.0092406 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.936 reward + 0.869 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 518/1000 --- L(Train): 0.0092386 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.936 reward + 0.868 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 519/1000 --- L(Train): 0.0092360 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.936 reward + 0.868 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\u001b[H\u001b[2J\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 520/1000 --- L(Train): 0.0092376 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.936 reward + 0.868 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 521/1000 --- L(Train): 0.0092389 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.936 reward + 0.868 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 522/1000 --- L(Train): 0.0092365 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.936 reward + 0.868 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 523/1000 --- L(Train): 0.0092329 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.936 reward + 0.868 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 524/1000 --- L(Train): 0.0092336 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.936 reward + 0.868 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 525/1000 --- L(Train): 0.0092334 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.936 reward + 0.868 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 526/1000 --- L(Train): 0.0092364 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.937 reward + 0.868 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 527/1000 --- L(Train): 0.0092353 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.937 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 528/1000 --- L(Train): 0.0092345 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.937 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 529/1000 --- L(Train): 0.0092331 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.937 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 530/1000 --- L(Train): 0.0092317 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.937 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 531/1000 --- L(Train): 0.0092289 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.937 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 532/1000 --- L(Train): 0.0092292 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.937 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 533/1000 --- L(Train): 0.0092299 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.937 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 534/1000 --- L(Train): 0.0092297 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.938 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 535/1000 --- L(Train): 0.0092275 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.938 reward + 0.867 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 536/1000 --- L(Train): 0.0092258 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.938 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 537/1000 --- L(Train): 0.0092273 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.938 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 538/1000 --- L(Train): 0.0092303 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.938 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 539/1000 --- L(Train): 0.0092275 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.938 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 540/1000 --- L(Train): 0.0092246 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.938 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 541/1000 --- L(Train): 0.0092247 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.938 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 542/1000 --- L(Train): 0.0092267 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.938 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 543/1000 --- L(Train): 0.0092261 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.939 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 544/1000 --- L(Train): 0.0092269 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.939 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 545/1000 --- L(Train): 0.0092267 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.939 reward + 0.866 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 546/1000 --- L(Train): 0.0092267 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.939 reward + 0.865 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 547/1000 --- L(Train): 0.0092255 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.939 reward + 0.865 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 548/1000 --- L(Train): 0.0092239 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.939 reward + 0.865 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 549/1000 --- L(Train): 0.0092219 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.939 reward + 0.865 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 550/1000 --- L(Train): 0.0092209 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.939 reward + 0.865 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 551/1000 --- L(Train): 0.0092189 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.939 reward + 0.865 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 552/1000 --- L(Train): 0.0092185 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.94 reward + 0.865 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 553/1000 --- L(Train): 0.0092162 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.94 reward + 0.865 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 554/1000 --- L(Train): 0.0092144 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.94 reward + 0.865 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 555/1000 --- L(Train): 0.0092147 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.94 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 556/1000 --- L(Train): 0.0092163 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.94 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 557/1000 --- L(Train): 0.0092151 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.94 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 558/1000 --- L(Train): 0.0092151 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.94 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 559/1000 --- L(Train): 0.0092133 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.94 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 560/1000 --- L(Train): 0.0092132 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.94 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 561/1000 --- L(Train): 0.0092136 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.941 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 562/1000 --- L(Train): 0.0092149 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.941 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 563/1000 --- L(Train): 0.0092153 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.941 reward + 0.864 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 564/1000 --- L(Train): 0.0092153 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.941 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 565/1000 --- L(Train): 0.0092204 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.941 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 566/1000 --- L(Train): 0.0092195 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.941 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 567/1000 --- L(Train): 0.0092163 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.941 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 568/1000 --- L(Train): 0.0092145 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.941 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 569/1000 --- L(Train): 0.0092147 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.942 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 570/1000 --- L(Train): 0.0092153 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.942 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 571/1000 --- L(Train): 0.0092113 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.942 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 572/1000 --- L(Train): 0.0092109 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.942 reward + 0.863 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 573/1000 --- L(Train): 0.0092082 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.942 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 574/1000 --- L(Train): 0.0092075 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.942 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 575/1000 --- L(Train): 0.0092071 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.942 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 576/1000 --- L(Train): 0.0092055 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.942 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 577/1000 --- L(Train): 0.0092045 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.942 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 578/1000 --- L(Train): 0.0092037 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.943 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 579/1000 --- L(Train): 0.0092010 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.943 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 580/1000 --- L(Train): 0.0092022 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.943 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 581/1000 --- L(Train): 0.0092027 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.943 reward + 0.862 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 582/1000 --- L(Train): 0.0092026 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.943 reward + 0.861 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 583/1000 --- L(Train): 0.0092018 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.943 reward + 0.861 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 584/1000 --- L(Train): 0.0092028 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.943 reward + 0.861 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 585/1000 --- L(Train): 0.0092017 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.943 reward + 0.861 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 586/1000 --- L(Train): 0.0092016 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.944 reward + 0.861 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 587/1000 --- L(Train): 0.0092014 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.944 reward + 0.861 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 588/1000 --- L(Train): 0.0092027 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.944 reward + 0.861 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 589/1000 --- L(Train): 0.0092015 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.944 reward + 0.861 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 590/1000 --- L(Train): 0.0091996 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.944 reward + 0.86 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 591/1000 --- L(Train): 0.0091973 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.944 reward + 0.86 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 592/1000 --- L(Train): 0.0091981 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.944 reward + 0.86 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 593/1000 --- L(Train): 0.0091984 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.944 reward + 0.86 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 594/1000 --- L(Train): 0.0091969 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.945 reward + 0.86 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 595/1000 --- L(Train): 0.0091947 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.945 reward + 0.86 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 596/1000 --- L(Train): 0.0091937 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.945 reward + 0.86 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 597/1000 --- L(Train): 0.0091945 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.945 reward + 0.86 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 598/1000 --- L(Train): 0.0091968 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.945 reward + 0.86 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 599/1000 --- L(Train): 0.0091950 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.945 reward + 0.859 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 600/1000 --- L(Train): 0.0091936 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.945 reward + 0.859 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 601/1000 --- L(Train): 0.0091924 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.945 reward + 0.859 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 602/1000 --- L(Train): 0.0091914 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.945 reward + 0.859 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 603/1000 --- L(Train): 0.0091905 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.946 reward + 0.859 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 604/1000 --- L(Train): 0.0091912 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.946 reward + 0.859 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 605/1000 --- L(Train): 0.0091907 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.946 reward + 0.859 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 606/1000 --- L(Train): 0.0091897 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.946 reward + 0.859 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 607/1000 --- L(Train): 0.0091868 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.946 reward + 0.859 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 608/1000 --- L(Train): 0.0091856 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.946 reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 609/1000 --- L(Train): 0.0091860 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.946 reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 610/1000 --- L(Train): 0.0091866 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.946 reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 611/1000 --- L(Train): 0.0091853 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.947 reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 612/1000 --- L(Train): 0.0091863 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.947 reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 613/1000 --- L(Train): 0.0091848 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.947 reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 614/1000 --- L(Train): 0.0091838 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.947 reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 615/1000 --- L(Train): 0.0091830 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.947 reward + 0.858 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 616/1000 --- L(Train): 0.0091841 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.947 reward + 0.857 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 617/1000 --- L(Train): 0.0091849 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.947 reward + 0.857 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 618/1000 --- L(Train): 0.0091870 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.947 reward + 0.857 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 619/1000 --- L(Train): 0.0091829 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.948 reward + 0.857 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 620/1000 --- L(Train): 0.0091810 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.948 reward + 0.857 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 621/1000 --- L(Train): 0.0091819 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.948 reward + 0.857 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 622/1000 --- L(Train): 0.0091880 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.948 reward + 0.857 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 623/1000 --- L(Train): 0.0091857 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.948 reward + 0.857 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 624/1000 --- L(Train): 0.0091826 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.948 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 625/1000 --- L(Train): 0.0091804 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.948 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 626/1000 --- L(Train): 0.0091811 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.948 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 627/1000 --- L(Train): 0.0091804 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.949 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 628/1000 --- L(Train): 0.0091817 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.949 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 629/1000 --- L(Train): 0.0091807 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.949 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 630/1000 --- L(Train): 0.0091802 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.949 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 631/1000 --- L(Train): 0.0091774 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.949 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 632/1000 --- L(Train): 0.0091760 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.949 reward + 0.856 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 633/1000 --- L(Train): 0.0091752 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.949 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 634/1000 --- L(Train): 0.0091742 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.949 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 635/1000 --- L(Train): 0.0091749 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.95 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 636/1000 --- L(Train): 0.0091743 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.95 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 637/1000 --- L(Train): 0.0091740 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.95 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 638/1000 --- L(Train): 0.0091726 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.95 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 639/1000 --- L(Train): 0.0091695 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.95 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 640/1000 --- L(Train): 0.0091689 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.95 reward + 0.855 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 641/1000 --- L(Train): 0.0091700 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.95 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 642/1000 --- L(Train): 0.0091704 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.95 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 643/1000 --- L(Train): 0.0091704 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.951 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 644/1000 --- L(Train): 0.0091709 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.951 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 645/1000 --- L(Train): 0.0091690 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.951 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 646/1000 --- L(Train): 0.0091700 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.951 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 647/1000 --- L(Train): 0.0091687 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.951 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 648/1000 --- L(Train): 0.0091686 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.951 reward + 0.854 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 649/1000 --- L(Train): 0.0091685 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.951 reward + 0.853 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 650/1000 --- L(Train): 0.0091691 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.951 reward + 0.853 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 651/1000 --- L(Train): 0.0091668 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.952 reward + 0.853 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 652/1000 --- L(Train): 0.0091669 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.952 reward + 0.853 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 653/1000 --- L(Train): 0.0091674 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.952 reward + 0.853 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 654/1000 --- L(Train): 0.0091664 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.952 reward + 0.853 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 655/1000 --- L(Train): 0.0091637 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.952 reward + 0.853 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 656/1000 --- L(Train): 0.0091633 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.952 reward + 0.853 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 657/1000 --- L(Train): 0.0091614 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.952 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 658/1000 --- L(Train): 0.0091621 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.952 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 659/1000 --- L(Train): 0.0091615 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.953 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 660/1000 --- L(Train): 0.0091601 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.953 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 661/1000 --- L(Train): 0.0091575 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.953 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 662/1000 --- L(Train): 0.0091555 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.953 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 663/1000 --- L(Train): 0.0091545 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.953 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 664/1000 --- L(Train): 0.0091563 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.953 reward + 0.852 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 665/1000 --- L(Train): 0.0091561 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.953 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 666/1000 --- L(Train): 0.0091562 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.954 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 667/1000 --- L(Train): 0.0091550 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.954 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 668/1000 --- L(Train): 0.0091556 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.954 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 669/1000 --- L(Train): 0.0091548 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.954 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 670/1000 --- L(Train): 0.0091554 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.954 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 671/1000 --- L(Train): 0.0091556 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.954 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 672/1000 --- L(Train): 0.0091556 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.954 reward + 0.851 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 673/1000 --- L(Train): 0.0091547 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.954 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 674/1000 --- L(Train): 0.0091529 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.955 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 675/1000 --- L(Train): 0.0091519 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.59 1 + 0.004 value_reward_chosen[t] + 0.955 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 676/1000 --- L(Train): 0.0091523 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.955 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 677/1000 --- L(Train): 0.0091539 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.955 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 678/1000 --- L(Train): 0.0091542 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.955 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 679/1000 --- L(Train): 0.0091515 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.955 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 680/1000 --- L(Train): 0.0091488 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.955 reward + 0.85 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 681/1000 --- L(Train): 0.0091495 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.955 reward + 0.849 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 682/1000 --- L(Train): 0.0091523 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.956 reward + 0.849 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 683/1000 --- L(Train): 0.0091509 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.956 reward + 0.849 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 684/1000 --- L(Train): 0.0091508 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.956 reward + 0.849 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 685/1000 --- L(Train): 0.0091597 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.956 reward + 0.849 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 686/1000 --- L(Train): 0.0091555 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.956 reward + 0.849 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 687/1000 --- L(Train): 0.0091498 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.956 reward + 0.849 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 688/1000 --- L(Train): 0.0091471 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.956 reward + 0.849 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 689/1000 --- L(Train): 0.0091472 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.957 reward + 0.848 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 690/1000 --- L(Train): 0.0091476 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.957 reward + 0.848 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 691/1000 --- L(Train): 0.0091464 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.957 reward + 0.848 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 692/1000 --- L(Train): 0.0091472 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.957 reward + 0.848 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 693/1000 --- L(Train): 0.0091450 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.957 reward + 0.848 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 694/1000 --- L(Train): 0.0091424 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.957 reward + 0.848 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 695/1000 --- L(Train): 0.0091402 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.957 reward + 0.848 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 696/1000 --- L(Train): 0.0091397 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.957 reward + 0.848 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 697/1000 --- L(Train): 0.0091396 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.958 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 698/1000 --- L(Train): 0.0091405 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.958 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 699/1000 --- L(Train): 0.0091396 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.958 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 700/1000 --- L(Train): 0.0091413 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.958 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 701/1000 --- L(Train): 0.0091418 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.958 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 702/1000 --- L(Train): 0.0091413 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.958 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 703/1000 --- L(Train): 0.0091379 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.958 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 704/1000 --- L(Train): 0.0091378 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.958 reward + 0.847 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 705/1000 --- L(Train): 0.0091405 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.959 reward + 0.846 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 706/1000 --- L(Train): 0.0091427 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.959 reward + 0.846 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 707/1000 --- L(Train): 0.0091410 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.959 reward + 0.846 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 708/1000 --- L(Train): 0.0091394 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.959 reward + 0.846 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 709/1000 --- L(Train): 0.0091388 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.959 reward + 0.846 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 710/1000 --- L(Train): 0.0091380 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.959 reward + 0.846 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 711/1000 --- L(Train): 0.0091350 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.959 reward + 0.846 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 712/1000 --- L(Train): 0.0091357 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.96 reward + 0.845 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 713/1000 --- L(Train): 0.0091338 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.96 reward + 0.845 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 714/1000 --- L(Train): 0.0091326 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.96 reward + 0.845 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 715/1000 --- L(Train): 0.0091304 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.96 reward + 0.845 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 716/1000 --- L(Train): 0.0091294 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.96 reward + 0.845 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 717/1000 --- L(Train): 0.0091282 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.96 reward + 0.845 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 718/1000 --- L(Train): 0.0091289 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.96 reward + 0.845 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 719/1000 --- L(Train): 0.0091272 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.961 reward + 0.845 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 720/1000 --- L(Train): 0.0091259 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.961 reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 721/1000 --- L(Train): 0.0091246 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.961 reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 722/1000 --- L(Train): 0.0091249 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.961 reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 723/1000 --- L(Train): 0.0091259 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.961 reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 724/1000 --- L(Train): 0.0091274 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.961 reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 725/1000 --- L(Train): 0.0091264 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.961 reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 726/1000 --- L(Train): 0.0091269 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.961 reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 727/1000 --- L(Train): 0.0091268 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.962 reward + 0.844 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 728/1000 --- L(Train): 0.0091263 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.962 reward + 0.843 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 729/1000 --- L(Train): 0.0091250 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.962 reward + 0.843 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\u001b[H\u001b[2J\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 730/1000 --- L(Train): 0.0091252 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.962 reward + 0.843 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 731/1000 --- L(Train): 0.0091249 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.962 reward + 0.843 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 732/1000 --- L(Train): 0.0091249 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.962 reward + 0.843 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 733/1000 --- L(Train): 0.0091239 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.962 reward + 0.843 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 734/1000 --- L(Train): 0.0091228 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.963 reward + 0.843 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 735/1000 --- L(Train): 0.0091223 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.963 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 736/1000 --- L(Train): 0.0091221 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.963 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 737/1000 --- L(Train): 0.0091218 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.963 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 738/1000 --- L(Train): 0.0091233 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.963 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 739/1000 --- L(Train): 0.0091211 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.963 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 740/1000 --- L(Train): 0.0091209 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.963 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 741/1000 --- L(Train): 0.0091197 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.964 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 742/1000 --- L(Train): 0.0091198 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.964 reward + 0.842 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 743/1000 --- L(Train): 0.0091191 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.964 reward + 0.841 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 744/1000 --- L(Train): 0.0091184 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.964 reward + 0.841 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 745/1000 --- L(Train): 0.0091182 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.964 reward + 0.841 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 746/1000 --- L(Train): 0.0091164 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.964 reward + 0.841 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 747/1000 --- L(Train): 0.0091137 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.964 reward + 0.841 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 748/1000 --- L(Train): 0.0091131 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.964 reward + 0.841 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 749/1000 --- L(Train): 0.0091129 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.965 reward + 0.841 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 750/1000 --- L(Train): 0.0091122 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.965 reward + 0.84 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 751/1000 --- L(Train): 0.0091097 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.965 reward + 0.84 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 752/1000 --- L(Train): 0.0091102 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.965 reward + 0.84 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 753/1000 --- L(Train): 0.0091097 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.965 reward + 0.84 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 754/1000 --- L(Train): 0.0091113 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.965 reward + 0.84 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 755/1000 --- L(Train): 0.0091103 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.965 reward + 0.84 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 756/1000 --- L(Train): 0.0091104 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.966 reward + 0.84 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 757/1000 --- L(Train): 0.0091101 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.966 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 758/1000 --- L(Train): 0.0091116 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.966 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 759/1000 --- L(Train): 0.0091083 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.966 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 760/1000 --- L(Train): 0.0091090 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.966 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 761/1000 --- L(Train): 0.0091118 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.966 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 762/1000 --- L(Train): 0.0091124 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.966 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 763/1000 --- L(Train): 0.0091110 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.967 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 764/1000 --- L(Train): 0.0091091 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.967 reward + 0.839 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 765/1000 --- L(Train): 0.0091084 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.967 reward + 0.838 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 766/1000 --- L(Train): 0.0091107 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.967 reward + 0.838 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 767/1000 --- L(Train): 0.0091102 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.967 reward + 0.838 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 768/1000 --- L(Train): 0.0091089 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.967 reward + 0.838 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 769/1000 --- L(Train): 0.0091055 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.967 reward + 0.838 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 770/1000 --- L(Train): 0.0091038 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.968 reward + 0.838 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 771/1000 --- L(Train): 0.0091027 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.968 reward + 0.838 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 772/1000 --- L(Train): 0.0091044 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.968 reward + 0.837 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 773/1000 --- L(Train): 0.0091037 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.968 reward + 0.837 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 774/1000 --- L(Train): 0.0091005 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.968 reward + 0.837 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 775/1000 --- L(Train): 0.0090982 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.968 reward + 0.837 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 776/1000 --- L(Train): 0.0090977 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.968 reward + 0.837 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 777/1000 --- L(Train): 0.0090971 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.969 reward + 0.837 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 778/1000 --- L(Train): 0.0090980 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.969 reward + 0.837 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 779/1000 --- L(Train): 0.0090989 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.969 reward + 0.836 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 780/1000 --- L(Train): 0.0090996 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.969 reward + 0.836 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 781/1000 --- L(Train): 0.0090975 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.969 reward + 0.836 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 782/1000 --- L(Train): 0.0090964 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.969 reward + 0.836 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 783/1000 --- L(Train): 0.0090963 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.969 reward + 0.836 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 784/1000 --- L(Train): 0.0090987 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.97 reward + 0.836 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 785/1000 --- L(Train): 0.0090984 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.97 reward + 0.836 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 786/1000 --- L(Train): 0.0090993 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.97 reward + 0.835 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 787/1000 --- L(Train): 0.0090973 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.97 reward + 0.835 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 788/1000 --- L(Train): 0.0090960 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.97 reward + 0.835 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 789/1000 --- L(Train): 0.0090954 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.97 reward + 0.835 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 790/1000 --- L(Train): 0.0090963 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.97 reward + 0.835 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 791/1000 --- L(Train): 0.0090952 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.971 reward + 0.835 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 792/1000 --- L(Train): 0.0090944 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.971 reward + 0.835 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 793/1000 --- L(Train): 0.0090929 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.971 reward + 0.834 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 794/1000 --- L(Train): 0.0090945 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.971 reward + 0.834 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 795/1000 --- L(Train): 0.0090937 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.971 reward + 0.834 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 796/1000 --- L(Train): 0.0090934 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.971 reward + 0.834 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 797/1000 --- L(Train): 0.0090927 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.971 reward + 0.834 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 798/1000 --- L(Train): 0.0090929 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.972 reward + 0.834 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 799/1000 --- L(Train): 0.0090905 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.972 reward + 0.834 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 800/1000 --- L(Train): 0.0090892 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.972 reward + 0.834 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 801/1000 --- L(Train): 0.0090889 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.972 reward + 0.833 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 802/1000 --- L(Train): 0.0090878 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.972 reward + 0.833 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 803/1000 --- L(Train): 0.0090865 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.972 reward + 0.833 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 804/1000 --- L(Train): 0.0090863 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.972 reward + 0.833 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 805/1000 --- L(Train): 0.0090857 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.973 reward + 0.833 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 806/1000 --- L(Train): 0.0090845 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.973 reward + 0.833 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 807/1000 --- L(Train): 0.0090836 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.973 reward + 0.833 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 808/1000 --- L(Train): 0.0090847 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.973 reward + 0.832 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 809/1000 --- L(Train): 0.0090845 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.973 reward + 0.832 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 810/1000 --- L(Train): 0.0090842 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.973 reward + 0.832 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 811/1000 --- L(Train): 0.0090809 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.973 reward + 0.832 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 812/1000 --- L(Train): 0.0090834 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.974 reward + 0.832 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 813/1000 --- L(Train): 0.0090838 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.974 reward + 0.832 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 814/1000 --- L(Train): 0.0090836 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.974 reward + 0.832 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 815/1000 --- L(Train): 0.0090814 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.004 value_reward_chosen[t] + 0.974 reward + 0.831 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 816/1000 --- L(Train): 0.0090815 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.974 reward + 0.831 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 817/1000 --- L(Train): 0.0090837 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.974 reward + 0.831 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 818/1000 --- L(Train): 0.0090838 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.974 reward + 0.831 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 819/1000 --- L(Train): 0.0090804 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.975 reward + 0.831 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 820/1000 --- L(Train): 0.0090785 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.975 reward + 0.831 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 821/1000 --- L(Train): 0.0090800 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.975 reward + 0.831 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 822/1000 --- L(Train): 0.0090812 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.975 reward + 0.83 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 823/1000 --- L(Train): 0.0090789 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.975 reward + 0.83 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 824/1000 --- L(Train): 0.0090774 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.975 reward + 0.83 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 825/1000 --- L(Train): 0.0090758 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.976 reward + 0.83 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 826/1000 --- L(Train): 0.0090773 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.976 reward + 0.83 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 827/1000 --- L(Train): 0.0090769 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.976 reward + 0.83 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 828/1000 --- L(Train): 0.0090763 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.976 reward + 0.829 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 829/1000 --- L(Train): 0.0090741 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.976 reward + 0.829 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 830/1000 --- L(Train): 0.0090720 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.976 reward + 0.829 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 831/1000 --- L(Train): 0.0090696 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.976 reward + 0.829 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 832/1000 --- L(Train): 0.0090709 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.977 reward + 0.829 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 833/1000 --- L(Train): 0.0090711 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.977 reward + 0.829 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 834/1000 --- L(Train): 0.0090698 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.977 reward + 0.829 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 835/1000 --- L(Train): 0.0090689 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.977 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 836/1000 --- L(Train): 0.0090684 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.977 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 837/1000 --- L(Train): 0.0090669 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.977 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 838/1000 --- L(Train): 0.0090691 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.977 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 839/1000 --- L(Train): 0.0090678 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.978 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 840/1000 --- L(Train): 0.0090665 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.978 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 841/1000 --- L(Train): 0.0090669 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.978 reward + 0.828 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 842/1000 --- L(Train): 0.0090676 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.978 reward + 0.827 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 843/1000 --- L(Train): 0.0090658 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.978 reward + 0.827 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 844/1000 --- L(Train): 0.0090677 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.978 reward + 0.827 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 845/1000 --- L(Train): 0.0090689 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.979 reward + 0.827 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 846/1000 --- L(Train): 0.0090690 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.979 reward + 0.827 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 847/1000 --- L(Train): 0.0090678 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.979 reward + 0.827 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 848/1000 --- L(Train): 0.0090663 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.979 reward + 0.827 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 849/1000 --- L(Train): 0.0090665 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.979 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 850/1000 --- L(Train): 0.0090682 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.979 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 851/1000 --- L(Train): 0.0090678 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.979 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 852/1000 --- L(Train): 0.0090678 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.98 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 853/1000 --- L(Train): 0.0090641 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.98 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 854/1000 --- L(Train): 0.0090617 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.98 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 855/1000 --- L(Train): 0.0090611 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.98 reward + 0.826 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 856/1000 --- L(Train): 0.0090617 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.98 reward + 0.825 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 857/1000 --- L(Train): 0.0090604 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.98 reward + 0.825 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 858/1000 --- L(Train): 0.0090591 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.98 reward + 0.825 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 859/1000 --- L(Train): 0.0090556 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.981 reward + 0.825 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\u001b[H\u001b[2J\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 860/1000 --- L(Train): 0.0090554 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.981 reward + 0.825 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 861/1000 --- L(Train): 0.0090545 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.981 reward + 0.825 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 862/1000 --- L(Train): 0.0090547 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.981 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 863/1000 --- L(Train): 0.0090550 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.981 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 864/1000 --- L(Train): 0.0090555 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.981 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 865/1000 --- L(Train): 0.0090534 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.982 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 866/1000 --- L(Train): 0.0090528 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.982 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 867/1000 --- L(Train): 0.0090530 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.982 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 868/1000 --- L(Train): 0.0090550 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.982 reward + 0.824 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 869/1000 --- L(Train): 0.0090553 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.982 reward + 0.823 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 870/1000 --- L(Train): 0.0090549 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.982 reward + 0.823 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 871/1000 --- L(Train): 0.0090516 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.982 reward + 0.823 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 872/1000 --- L(Train): 0.0090517 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.983 reward + 0.823 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 873/1000 --- L(Train): 0.0090534 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.983 reward + 0.823 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 874/1000 --- L(Train): 0.0090533 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.983 reward + 0.823 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 875/1000 --- L(Train): 0.0090525 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.983 reward + 0.823 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 876/1000 --- L(Train): 0.0090509 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.983 reward + 0.822 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 877/1000 --- L(Train): 0.0090506 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.983 reward + 0.822 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 878/1000 --- L(Train): 0.0090521 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.984 reward + 0.822 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 879/1000 --- L(Train): 0.0090498 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.984 reward + 0.822 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 880/1000 --- L(Train): 0.0090483 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.984 reward + 0.822 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 881/1000 --- L(Train): 0.0090481 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.984 reward + 0.822 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 882/1000 --- L(Train): 0.0090489 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.984 reward + 0.821 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 883/1000 --- L(Train): 0.0090464 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.984 reward + 0.821 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 884/1000 --- L(Train): 0.0090471 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.984 reward + 0.821 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 885/1000 --- L(Train): 0.0090467 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.985 reward + 0.821 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 886/1000 --- L(Train): 0.0090465 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.985 reward + 0.821 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 887/1000 --- L(Train): 0.0090443 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.985 reward + 0.821 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 888/1000 --- L(Train): 0.0090439 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.985 reward + 0.821 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 889/1000 --- L(Train): 0.0090438 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.985 reward + 0.82 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 890/1000 --- L(Train): 0.0090432 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.985 reward + 0.82 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 891/1000 --- L(Train): 0.0090400 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.986 reward + 0.82 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 892/1000 --- L(Train): 0.0090400 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.986 reward + 0.82 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 893/1000 --- L(Train): 0.0090398 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.986 reward + 0.82 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 894/1000 --- L(Train): 0.0090390 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.986 reward + 0.82 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 895/1000 --- L(Train): 0.0090370 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.986 reward + 0.819 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 896/1000 --- L(Train): 0.0090361 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.986 reward + 0.819 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 897/1000 --- L(Train): 0.0090359 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.986 reward + 0.819 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 898/1000 --- L(Train): 0.0090386 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.987 reward + 0.819 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 899/1000 --- L(Train): 0.0090377 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.987 reward + 0.819 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 900/1000 --- L(Train): 0.0090371 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.987 reward + 0.819 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 901/1000 --- L(Train): 0.0090374 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.987 reward + 0.819 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 902/1000 --- L(Train): 0.0090371 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.987 reward + 0.818 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 903/1000 --- L(Train): 0.0090367 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.987 reward + 0.818 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 904/1000 --- L(Train): 0.0090373 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.988 reward + 0.818 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 905/1000 --- L(Train): 0.0090368 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.988 reward + 0.818 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 906/1000 --- L(Train): 0.0090380 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.988 reward + 0.818 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 907/1000 --- L(Train): 0.0090373 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.988 reward + 0.818 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 908/1000 --- L(Train): 0.0090366 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.988 reward + 0.817 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 909/1000 --- L(Train): 0.0090344 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.988 reward + 0.817 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 910/1000 --- L(Train): 0.0090332 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.989 reward + 0.817 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 911/1000 --- L(Train): 0.0090319 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.989 reward + 0.817 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 912/1000 --- L(Train): 0.0090319 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.989 reward + 0.817 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 913/1000 --- L(Train): 0.0090304 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.989 reward + 0.817 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 914/1000 --- L(Train): 0.0090267 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.989 reward + 0.817 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 915/1000 --- L(Train): 0.0090255 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.989 reward + 0.816 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 916/1000 --- L(Train): 0.0090277 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.989 reward + 0.816 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 917/1000 --- L(Train): 0.0090277 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.99 reward + 0.816 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 918/1000 --- L(Train): 0.0090275 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.99 reward + 0.816 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 919/1000 --- L(Train): 0.0090247 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.99 reward + 0.816 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 920/1000 --- L(Train): 0.0090244 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.99 reward + 0.816 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 921/1000 --- L(Train): 0.0090245 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.99 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 922/1000 --- L(Train): 0.0090264 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.99 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 923/1000 --- L(Train): 0.0090259 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.991 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 924/1000 --- L(Train): 0.0090267 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.991 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 925/1000 --- L(Train): 0.0090254 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.991 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 926/1000 --- L(Train): 0.0090243 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.991 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 927/1000 --- L(Train): 0.0090222 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.991 reward + 0.815 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 928/1000 --- L(Train): 0.0090221 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.991 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 929/1000 --- L(Train): 0.0090229 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.992 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 930/1000 --- L(Train): 0.0090228 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.992 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 931/1000 --- L(Train): 0.0090240 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.992 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 932/1000 --- L(Train): 0.0090216 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.992 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 933/1000 --- L(Train): 0.0090205 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.992 reward + 0.814 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 934/1000 --- L(Train): 0.0090208 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.992 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 935/1000 --- L(Train): 0.0090191 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.993 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 936/1000 --- L(Train): 0.0090195 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.993 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 937/1000 --- L(Train): 0.0090189 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.993 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 938/1000 --- L(Train): 0.0090187 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.993 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 939/1000 --- L(Train): 0.0090174 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.993 reward + 0.813 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 940/1000 --- L(Train): 0.0090164 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.993 reward + 0.812 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 941/1000 --- L(Train): 0.0090173 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.993 reward + 0.812 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 942/1000 --- L(Train): 0.0090162 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.994 reward + 0.812 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 943/1000 --- L(Train): 0.0090140 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.994 reward + 0.812 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 944/1000 --- L(Train): 0.0090134 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.994 reward + 0.812 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 945/1000 --- L(Train): 0.0090129 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.994 reward + 0.812 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 946/1000 --- L(Train): 0.0090141 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.994 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 947/1000 --- L(Train): 0.0090135 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.994 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 948/1000 --- L(Train): 0.0090128 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.995 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 949/1000 --- L(Train): 0.0090105 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.995 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 950/1000 --- L(Train): 0.0090097 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.995 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 951/1000 --- L(Train): 0.0090075 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.995 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 952/1000 --- L(Train): 0.0090094 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.995 reward + 0.811 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 953/1000 --- L(Train): 0.0090094 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.995 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 954/1000 --- L(Train): 0.0090085 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.996 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 955/1000 --- L(Train): 0.0090064 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.996 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 956/1000 --- L(Train): 0.0090063 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.996 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 957/1000 --- L(Train): 0.0090069 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.996 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 958/1000 --- L(Train): 0.0090089 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.996 reward + 0.81 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 959/1000 --- L(Train): 0.0090067 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.996 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 960/1000 --- L(Train): 0.0090045 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.997 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 961/1000 --- L(Train): 0.0090054 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.997 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 962/1000 --- L(Train): 0.0090077 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.997 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 963/1000 --- L(Train): 0.0090058 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.997 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 964/1000 --- L(Train): 0.0090060 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.997 reward + 0.809 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 965/1000 --- L(Train): 0.0090046 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.997 reward + 0.808 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 966/1000 --- L(Train): 0.0090043 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.998 reward + 0.808 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 967/1000 --- L(Train): 0.0090022 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.998 reward + 0.808 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 968/1000 --- L(Train): 0.0090006 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.998 reward + 0.808 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 969/1000 --- L(Train): 0.0089992 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.998 reward + 0.808 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 970/1000 --- L(Train): 0.0089988 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.998 reward + 0.808 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 971/1000 --- L(Train): 0.0089981 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.998 reward + 0.807 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 972/1000 --- L(Train): 0.0089974 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.999 reward + 0.807 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 973/1000 --- L(Train): 0.0089947 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.999 reward + 0.807 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 974/1000 --- L(Train): 0.0089937 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.999 reward + 0.807 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 975/1000 --- L(Train): 0.0089945 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.999 reward + 0.807 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 976/1000 --- L(Train): 0.0089960 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.999 reward + 0.807 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 977/1000 --- L(Train): 0.0089962 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 0.999 reward + 0.806 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 978/1000 --- L(Train): 0.0089963 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.0 reward + 0.806 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 979/1000 --- L(Train): 0.0089939 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.0 reward + 0.806 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 980/1000 --- L(Train): 0.0089940 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.0 reward + 0.806 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 981/1000 --- L(Train): 0.0089939 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.0 reward + 0.806 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 982/1000 --- L(Train): 0.0089936 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.0 reward + 0.806 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 983/1000 --- L(Train): 0.0089933 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.0 reward + 0.805 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 984/1000 --- L(Train): 0.0089929 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.001 reward + 0.805 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 985/1000 --- L(Train): 0.0089929 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.001 reward + 0.805 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 986/1000 --- L(Train): 0.0089918 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.001 reward + 0.805 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 987/1000 --- L(Train): 0.0089904 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.001 reward + 0.805 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 988/1000 --- L(Train): 0.0089913 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.001 reward + 0.805 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 989/1000 --- L(Train): 0.0089911 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.001 reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 990/1000 --- L(Train): 0.0089912 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.002 reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 991/1000 --- L(Train): 0.0089885 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.002 reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 992/1000 --- L(Train): 0.0089895 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.002 reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 993/1000 --- L(Train): 0.0089879 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.002 reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 994/1000 --- L(Train): 0.0089871 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.002 reward + 0.804 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 995/1000 --- L(Train): 0.0089866 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.002 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 996/1000 --- L(Train): 0.0089874 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.003 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 997/1000 --- L(Train): 0.0089866 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.003 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 998/1000 --- L(Train): 0.0089838 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.003 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 999/1000 --- L(Train): 0.0089798 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.003 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 1000/1000 --- L(Train): 0.0089802 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.003 reward + 0.803 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 1001/1000 --- L(Train): 0.0089811 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.003 reward + 0.802 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: -, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\n",
            "Training result:\n",
            "L(Train): 0.4178554 --- L(Val, RNN): 0.4137070 --- L(Val, SINDy): 0.4148086 --- LR: 1.0000000e-02\n",
            "\n",
            "RNN training finished.\n",
            "Training took 366.50 seconds.\n",
            "Saving SPICE model to ../params/ganesh2024a/spice_ganesh2024a.pkl...\n",
            "================================================================================\n",
            "\n",
            "Training complete!\n",
            "\n",
            "Example SPICE model (participant 0):\n",
            "--------------------------------------------------------------------------------\n",
            "value_reward_chosen[t+1] = -0.591 1 + 0.003 value_reward_chosen[t] + 1.003 reward + 0.802 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.468 1 + 0.746 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "estimator = SpiceEstimator(\n",
        "        # model paramaeters\n",
        "        rnn_class=SPICERNN,\n",
        "        spice_config=spice_config,\n",
        "        n_actions=2,\n",
        "        n_participants=n_participants,\n",
        "        \n",
        "        # rnn training parameters\n",
        "        epochs=1000,\n",
        "        warmup_steps=200,\n",
        "        learning_rate=0.01,\n",
        "        \n",
        "        # sindy fitting parameters\n",
        "        sindy_weight=0.1,\n",
        "        sindy_threshold=0.05,\n",
        "        sindy_threshold_frequency=1,\n",
        "        sindy_threshold_terms=1,\n",
        "        sindy_cutoff_patience=100,\n",
        "        sindy_epochs=1000,\n",
        "        sindy_alpha=0.0001,\n",
        "        sindy_library_polynomial_degree=2,\n",
        "        sindy_ensemble_size=1,\n",
        "        \n",
        "        # additional generalization parameters\n",
        "        batch_size=1024,\n",
        "        bagging=True,\n",
        "        scheduler=True,\n",
        "        \n",
        "        verbose=True,\n",
        "        save_path_spice='../params/ganesh2024a/spice_ganesh2024a.pkl',\n",
        "    )\n",
        "\n",
        "print(f\"\\nStarting training on {estimator.device}...\")\n",
        "print(\"=\" * 80)\n",
        "estimator.fit(dataset.xs, dataset.ys, dataset.xs, dataset.ys)\n",
        "# estimator.load_spice(args.model)\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Print example SPICE model for first participant\n",
        "print(\"\\nExample SPICE model (participant 0):\")\n",
        "print(\"-\" * 80)\n",
        "estimator.print_spice_model(participant_id=0)\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "value_reward_chosen[t+1] = -0.933 1 + 0.008 value_reward_chosen[t] + 0.07 contr_diff + 1.015 reward + 0.0 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.234 1 + 1.0 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 8 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plotting\n",
        "participant_id = 7\n",
        "\n",
        "estimator.print_spice_model(participant_id)\n",
        "\n",
        "agents = {\n",
        "    'rnn': estimator.rnn_agent,\n",
        "    'spice': estimator.spice_agent,\n",
        "}\n",
        "\n",
        "fig, axs = plot_session(agents, dataset.xs[participant_id])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GRU for benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRU(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, n_actions):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.gru_features = 32\n",
        "        self.n_actions = n_actions\n",
        "        \n",
        "        self.linear_in = torch.nn.Linear(in_features=input_size, out_features=self.gru_features)\n",
        "        self.gru = torch.nn.GRU(input_size=self.gru_features,hidden_size=n_actions, batch_first=True)\n",
        "        self.linear_out = torch.nn.Linear(in_features=n_actions, out_features=n_actions)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \n",
        "        y = self.linear_in(inputs.nan_to_num(0))\n",
        "        y, _ = self.gru(y)\n",
        "        y = self.linear_out(y)\n",
        "        \n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 1000\n",
        "\n",
        "model = GRU(dataset.xs.shape[-1], 2)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000: Loss: 0.6943965554237366\n",
            "Epoch 2/1000: Loss: 0.6915597319602966\n",
            "Epoch 3/1000: Loss: 0.6901808977127075\n",
            "Epoch 4/1000: Loss: 0.6892239451408386\n",
            "Epoch 5/1000: Loss: 0.6884060502052307\n",
            "Epoch 6/1000: Loss: 0.6876288056373596\n",
            "Epoch 7/1000: Loss: 0.6867664456367493\n",
            "Epoch 8/1000: Loss: 0.6854802966117859\n",
            "Epoch 9/1000: Loss: 0.6831544041633606\n",
            "Epoch 10/1000: Loss: 0.6782125234603882\n",
            "Epoch 11/1000: Loss: 0.6663599610328674\n",
            "Epoch 12/1000: Loss: 0.6178154945373535\n",
            "Epoch 13/1000: Loss: 0.6695680022239685\n",
            "Epoch 14/1000: Loss: 0.5682570934295654\n",
            "Epoch 15/1000: Loss: 0.6566042304039001\n",
            "Epoch 16/1000: Loss: 0.6174185872077942\n",
            "Epoch 17/1000: Loss: 0.6314780712127686\n",
            "Epoch 18/1000: Loss: 0.6473832726478577\n",
            "Epoch 19/1000: Loss: 0.5603772401809692\n",
            "Epoch 20/1000: Loss: 0.5538023710250854\n",
            "Epoch 21/1000: Loss: 0.6224590539932251\n",
            "Epoch 22/1000: Loss: 0.5600453019142151\n",
            "Epoch 23/1000: Loss: 0.5497366189956665\n",
            "Epoch 24/1000: Loss: 0.6237532496452332\n",
            "Epoch 25/1000: Loss: 0.6231620907783508\n",
            "Epoch 26/1000: Loss: 0.5445185899734497\n",
            "Epoch 27/1000: Loss: 0.5436940789222717\n",
            "Epoch 28/1000: Loss: 0.545502781867981\n",
            "Epoch 29/1000: Loss: 0.5659019947052002\n",
            "Epoch 30/1000: Loss: 0.5371814966201782\n",
            "Epoch 31/1000: Loss: 0.5389012098312378\n",
            "Epoch 32/1000: Loss: 0.5340660810470581\n",
            "Epoch 33/1000: Loss: 0.5563908219337463\n",
            "Epoch 34/1000: Loss: 0.5312574505805969\n",
            "Epoch 35/1000: Loss: 0.5335786938667297\n",
            "Epoch 36/1000: Loss: 0.5336658358573914\n",
            "Epoch 37/1000: Loss: 0.5323601365089417\n",
            "Epoch 38/1000: Loss: 0.5290703177452087\n",
            "Epoch 39/1000: Loss: 0.5242569446563721\n",
            "Epoch 40/1000: Loss: 0.5309960246086121\n",
            "Epoch 41/1000: Loss: 0.5247856974601746\n",
            "Epoch 42/1000: Loss: 0.5233752131462097\n",
            "Epoch 43/1000: Loss: 0.5258795022964478\n",
            "Epoch 44/1000: Loss: 0.5255785584449768\n",
            "Epoch 45/1000: Loss: 0.52255779504776\n",
            "Epoch 46/1000: Loss: 0.5182379484176636\n",
            "Epoch 47/1000: Loss: 0.5222891569137573\n",
            "Epoch 48/1000: Loss: 0.5174703598022461\n",
            "Epoch 49/1000: Loss: 0.518810510635376\n",
            "Epoch 50/1000: Loss: 0.5193415880203247\n",
            "Epoch 51/1000: Loss: 0.5149747729301453\n",
            "Epoch 52/1000: Loss: 0.5179322361946106\n",
            "Epoch 53/1000: Loss: 0.5150200724601746\n",
            "Epoch 54/1000: Loss: 0.5153560042381287\n",
            "Epoch 55/1000: Loss: 0.5153216123580933\n",
            "Epoch 56/1000: Loss: 0.5126420855522156\n",
            "Epoch 57/1000: Loss: 0.5142713785171509\n",
            "Epoch 58/1000: Loss: 0.5113022923469543\n",
            "Epoch 59/1000: Loss: 0.5127520561218262\n",
            "Epoch 60/1000: Loss: 0.5102439522743225\n",
            "Epoch 61/1000: Loss: 0.5113956928253174\n",
            "Epoch 62/1000: Loss: 0.5090238451957703\n",
            "Epoch 63/1000: Loss: 0.5097647309303284\n",
            "Epoch 64/1000: Loss: 0.507798433303833\n",
            "Epoch 65/1000: Loss: 0.5083645582199097\n",
            "Epoch 66/1000: Loss: 0.5061607360839844\n",
            "Epoch 67/1000: Loss: 0.5065287947654724\n",
            "Epoch 68/1000: Loss: 0.5046411156654358\n",
            "Epoch 69/1000: Loss: 0.5050550699234009\n",
            "Epoch 70/1000: Loss: 0.5031148195266724\n",
            "Epoch 71/1000: Loss: 0.5030564665794373\n",
            "Epoch 72/1000: Loss: 0.5016434788703918\n",
            "Epoch 73/1000: Loss: 0.5015899538993835\n",
            "Epoch 74/1000: Loss: 0.5006421208381653\n",
            "Epoch 75/1000: Loss: 0.5003263354301453\n",
            "Epoch 76/1000: Loss: 0.4996854364871979\n",
            "Epoch 77/1000: Loss: 0.49945399165153503\n",
            "Epoch 78/1000: Loss: 0.4991215169429779\n",
            "Epoch 79/1000: Loss: 0.49894845485687256\n",
            "Epoch 80/1000: Loss: 0.4986639618873596\n",
            "Epoch 81/1000: Loss: 0.49861153960227966\n",
            "Epoch 82/1000: Loss: 0.4983713626861572\n",
            "Epoch 83/1000: Loss: 0.49834391474723816\n",
            "Epoch 84/1000: Loss: 0.49803397059440613\n",
            "Epoch 85/1000: Loss: 0.49802064895629883\n",
            "Epoch 86/1000: Loss: 0.4976765513420105\n",
            "Epoch 87/1000: Loss: 0.49757516384124756\n",
            "Epoch 88/1000: Loss: 0.4971773326396942\n",
            "Epoch 89/1000: Loss: 0.4970582127571106\n",
            "Epoch 90/1000: Loss: 0.49668264389038086\n",
            "Epoch 91/1000: Loss: 0.4964712858200073\n",
            "Epoch 92/1000: Loss: 0.49614524841308594\n",
            "Epoch 93/1000: Loss: 0.49582624435424805\n",
            "Epoch 94/1000: Loss: 0.4956737756729126\n",
            "Epoch 95/1000: Loss: 0.4953015148639679\n",
            "Epoch 96/1000: Loss: 0.4950486421585083\n",
            "Epoch 97/1000: Loss: 0.49489471316337585\n",
            "Epoch 98/1000: Loss: 0.49456295371055603\n",
            "Epoch 99/1000: Loss: 0.4942626953125\n",
            "Epoch 100/1000: Loss: 0.49391135573387146\n",
            "Epoch 101/1000: Loss: 0.4939754009246826\n",
            "Epoch 102/1000: Loss: 0.494453102350235\n",
            "Epoch 103/1000: Loss: 0.5005077719688416\n",
            "Epoch 104/1000: Loss: 0.4963057041168213\n",
            "Epoch 105/1000: Loss: 0.5227444171905518\n",
            "Epoch 106/1000: Loss: 0.539058268070221\n",
            "Epoch 107/1000: Loss: 0.71959388256073\n",
            "Epoch 108/1000: Loss: 0.6542525887489319\n",
            "Epoch 109/1000: Loss: 0.5226811170578003\n",
            "Epoch 110/1000: Loss: 0.6931084394454956\n",
            "Epoch 111/1000: Loss: 0.6330759525299072\n",
            "Epoch 112/1000: Loss: 0.5250989198684692\n",
            "Epoch 113/1000: Loss: 0.7038261890411377\n",
            "Epoch 114/1000: Loss: 0.7714199423789978\n",
            "Epoch 115/1000: Loss: 0.7784643173217773\n",
            "Epoch 116/1000: Loss: 0.7642384171485901\n",
            "Epoch 117/1000: Loss: 0.7345131039619446\n",
            "Epoch 118/1000: Loss: 0.6827904582023621\n",
            "Epoch 119/1000: Loss: 0.5981582999229431\n",
            "Epoch 120/1000: Loss: 0.5196682214736938\n",
            "Epoch 121/1000: Loss: 0.5213416218757629\n",
            "Epoch 122/1000: Loss: 0.5455493927001953\n",
            "Epoch 123/1000: Loss: 0.60531085729599\n",
            "Epoch 124/1000: Loss: 0.5211247205734253\n",
            "Epoch 125/1000: Loss: 0.5307952761650085\n",
            "Epoch 126/1000: Loss: 0.5259872078895569\n",
            "Epoch 127/1000: Loss: 0.5313771367073059\n",
            "Epoch 128/1000: Loss: 0.5618363618850708\n",
            "Epoch 129/1000: Loss: 0.5713813304901123\n",
            "Epoch 130/1000: Loss: 0.5607720613479614\n",
            "Epoch 131/1000: Loss: 0.534275233745575\n",
            "Epoch 132/1000: Loss: 0.5293499231338501\n",
            "Epoch 133/1000: Loss: 0.5334492921829224\n",
            "Epoch 134/1000: Loss: 0.5362657308578491\n",
            "Epoch 135/1000: Loss: 0.536071240901947\n",
            "Epoch 136/1000: Loss: 0.5352007746696472\n",
            "Epoch 137/1000: Loss: 0.5334317088127136\n",
            "Epoch 138/1000: Loss: 0.5291377902030945\n",
            "Epoch 139/1000: Loss: 0.524346649646759\n",
            "Epoch 140/1000: Loss: 0.5315818190574646\n",
            "Epoch 141/1000: Loss: 0.5275055766105652\n",
            "Epoch 142/1000: Loss: 0.5196105241775513\n",
            "Epoch 143/1000: Loss: 0.5222594738006592\n",
            "Epoch 144/1000: Loss: 0.5232014060020447\n",
            "Epoch 145/1000: Loss: 0.5212652087211609\n",
            "Epoch 146/1000: Loss: 0.5157903432846069\n",
            "Epoch 147/1000: Loss: 0.5127224922180176\n",
            "Epoch 148/1000: Loss: 0.5169495940208435\n",
            "Epoch 149/1000: Loss: 0.5114048719406128\n",
            "Epoch 150/1000: Loss: 0.5098723769187927\n",
            "Epoch 151/1000: Loss: 0.5123609304428101\n",
            "Epoch 152/1000: Loss: 0.5101207494735718\n",
            "Epoch 153/1000: Loss: 0.5062143206596375\n",
            "Epoch 154/1000: Loss: 0.509027361869812\n",
            "Epoch 155/1000: Loss: 0.5068022012710571\n",
            "Epoch 156/1000: Loss: 0.5046992301940918\n",
            "Epoch 157/1000: Loss: 0.5066208243370056\n",
            "Epoch 158/1000: Loss: 0.5051166415214539\n",
            "Epoch 159/1000: Loss: 0.5024988651275635\n",
            "Epoch 160/1000: Loss: 0.5047187209129333\n",
            "Epoch 161/1000: Loss: 0.5024471879005432\n",
            "Epoch 162/1000: Loss: 0.5017614960670471\n",
            "Epoch 163/1000: Loss: 0.5028811097145081\n",
            "Epoch 164/1000: Loss: 0.5012691617012024\n",
            "Epoch 165/1000: Loss: 0.49977585673332214\n",
            "Epoch 166/1000: Loss: 0.5011464953422546\n",
            "Epoch 167/1000: Loss: 0.4989749491214752\n",
            "Epoch 168/1000: Loss: 0.4990760087966919\n",
            "Epoch 169/1000: Loss: 0.49909508228302\n",
            "Epoch 170/1000: Loss: 0.4975530505180359\n",
            "Epoch 171/1000: Loss: 0.49606063961982727\n",
            "Epoch 172/1000: Loss: 0.491281121969223\n",
            "Epoch 173/1000: Loss: 0.512587308883667\n",
            "Epoch 174/1000: Loss: 0.5031880736351013\n",
            "Epoch 175/1000: Loss: 0.5014092922210693\n",
            "Epoch 176/1000: Loss: 0.4935559332370758\n",
            "Epoch 177/1000: Loss: 0.49461737275123596\n",
            "Epoch 178/1000: Loss: 0.49960535764694214\n",
            "Epoch 179/1000: Loss: 0.4941372573375702\n",
            "Epoch 180/1000: Loss: 0.4818689525127411\n",
            "Epoch 181/1000: Loss: 0.4908137321472168\n",
            "Epoch 182/1000: Loss: 0.500697672367096\n",
            "Epoch 183/1000: Loss: 0.4933325946331024\n",
            "Epoch 184/1000: Loss: 0.4797850251197815\n",
            "Epoch 185/1000: Loss: 0.4888691008090973\n",
            "Epoch 186/1000: Loss: 0.49264588952064514\n",
            "Epoch 187/1000: Loss: 0.49159863591194153\n",
            "Epoch 188/1000: Loss: 0.48753252625465393\n",
            "Epoch 189/1000: Loss: 0.47714152932167053\n",
            "Epoch 190/1000: Loss: 0.48763135075569153\n",
            "Epoch 191/1000: Loss: 0.49101701378822327\n",
            "Epoch 192/1000: Loss: 0.48376601934432983\n",
            "Epoch 193/1000: Loss: 0.47710248827934265\n",
            "Epoch 194/1000: Loss: 0.48384740948677063\n",
            "Epoch 195/1000: Loss: 0.4822087287902832\n",
            "Epoch 196/1000: Loss: 0.4755670130252838\n",
            "Epoch 197/1000: Loss: 0.4801870286464691\n",
            "Epoch 198/1000: Loss: 0.4790588319301605\n",
            "Epoch 199/1000: Loss: 0.4769168198108673\n",
            "Epoch 200/1000: Loss: 0.4748862385749817\n",
            "Epoch 201/1000: Loss: 0.4756571650505066\n",
            "Epoch 202/1000: Loss: 0.4760650098323822\n",
            "Epoch 203/1000: Loss: 0.47220274806022644\n",
            "Epoch 204/1000: Loss: 0.4720298647880554\n",
            "Epoch 205/1000: Loss: 0.4717268943786621\n",
            "Epoch 206/1000: Loss: 0.47079405188560486\n",
            "Epoch 207/1000: Loss: 0.46981319785118103\n",
            "Epoch 208/1000: Loss: 0.4699469804763794\n",
            "Epoch 209/1000: Loss: 0.4686299264431\n",
            "Epoch 210/1000: Loss: 0.46778762340545654\n",
            "Epoch 211/1000: Loss: 0.467683345079422\n",
            "Epoch 212/1000: Loss: 0.46665292978286743\n",
            "Epoch 213/1000: Loss: 0.4669923186302185\n",
            "Epoch 214/1000: Loss: 0.46547335386276245\n",
            "Epoch 215/1000: Loss: 0.46566295623779297\n",
            "Epoch 216/1000: Loss: 0.4649029076099396\n",
            "Epoch 217/1000: Loss: 0.46401259303092957\n",
            "Epoch 218/1000: Loss: 0.46372178196907043\n",
            "Epoch 219/1000: Loss: 0.4633641242980957\n",
            "Epoch 220/1000: Loss: 0.4624159038066864\n",
            "Epoch 221/1000: Loss: 0.4622858762741089\n",
            "Epoch 222/1000: Loss: 0.4618770182132721\n",
            "Epoch 223/1000: Loss: 0.46092432737350464\n",
            "Epoch 224/1000: Loss: 0.460904061794281\n",
            "Epoch 225/1000: Loss: 0.4598940908908844\n",
            "Epoch 226/1000: Loss: 0.4599277079105377\n",
            "Epoch 227/1000: Loss: 0.45914316177368164\n",
            "Epoch 228/1000: Loss: 0.45851996541023254\n",
            "Epoch 229/1000: Loss: 0.45816120505332947\n",
            "Epoch 230/1000: Loss: 0.45730894804000854\n",
            "Epoch 231/1000: Loss: 0.4568491578102112\n",
            "Epoch 232/1000: Loss: 0.45586809515953064\n",
            "Epoch 233/1000: Loss: 0.45495328307151794\n",
            "Epoch 234/1000: Loss: 0.45355525612831116\n",
            "Epoch 235/1000: Loss: 0.4511430859565735\n",
            "Epoch 236/1000: Loss: 0.4463634192943573\n",
            "Epoch 237/1000: Loss: 0.4536612331867218\n",
            "Epoch 238/1000: Loss: 0.4891408383846283\n",
            "Epoch 239/1000: Loss: 0.45034363865852356\n",
            "Epoch 240/1000: Loss: 0.46325764060020447\n",
            "Epoch 241/1000: Loss: 0.4480879008769989\n",
            "Epoch 242/1000: Loss: 0.4488241374492645\n",
            "Epoch 243/1000: Loss: 0.4525870382785797\n",
            "Epoch 244/1000: Loss: 0.45015251636505127\n",
            "Epoch 245/1000: Loss: 0.44640812277793884\n",
            "Epoch 246/1000: Loss: 0.44619616866111755\n",
            "Epoch 247/1000: Loss: 0.44629284739494324\n",
            "Epoch 248/1000: Loss: 0.4416259229183197\n",
            "Epoch 249/1000: Loss: 0.4450324773788452\n",
            "Epoch 250/1000: Loss: 0.44091323018074036\n",
            "Epoch 251/1000: Loss: 0.43884530663490295\n",
            "Epoch 252/1000: Loss: 0.44148263335227966\n",
            "Epoch 253/1000: Loss: 0.43839454650878906\n",
            "Epoch 254/1000: Loss: 0.43396589159965515\n",
            "Epoch 255/1000: Loss: 0.42959094047546387\n",
            "Epoch 256/1000: Loss: 0.42961210012435913\n",
            "Epoch 257/1000: Loss: 0.43017640709877014\n",
            "Epoch 258/1000: Loss: 0.42750483751296997\n",
            "Epoch 259/1000: Loss: 0.42679494619369507\n",
            "Epoch 260/1000: Loss: 0.4273010790348053\n",
            "Epoch 261/1000: Loss: 0.4262146055698395\n",
            "Epoch 262/1000: Loss: 0.4264313876628876\n",
            "Epoch 263/1000: Loss: 0.4258998930454254\n",
            "Epoch 264/1000: Loss: 0.42502984404563904\n",
            "Epoch 265/1000: Loss: 0.4263767898082733\n",
            "Epoch 266/1000: Loss: 0.42620575428009033\n",
            "Epoch 267/1000: Loss: 0.4240805208683014\n",
            "Epoch 268/1000: Loss: 0.4263927638530731\n",
            "Epoch 269/1000: Loss: 0.4248541295528412\n",
            "Epoch 270/1000: Loss: 0.42472147941589355\n",
            "Epoch 271/1000: Loss: 0.4242364466190338\n",
            "Epoch 272/1000: Loss: 0.4227699339389801\n",
            "Epoch 273/1000: Loss: 0.4237935543060303\n",
            "Epoch 274/1000: Loss: 0.4224582612514496\n",
            "Epoch 275/1000: Loss: 0.42320650815963745\n",
            "Epoch 276/1000: Loss: 0.4227299392223358\n",
            "Epoch 277/1000: Loss: 0.42264142632484436\n",
            "Epoch 278/1000: Loss: 0.42247313261032104\n",
            "Epoch 279/1000: Loss: 0.4217771887779236\n",
            "Epoch 280/1000: Loss: 0.4222182035446167\n",
            "Epoch 281/1000: Loss: 0.42149317264556885\n",
            "Epoch 282/1000: Loss: 0.4219297766685486\n",
            "Epoch 283/1000: Loss: 0.42147108912467957\n",
            "Epoch 284/1000: Loss: 0.4212968051433563\n",
            "Epoch 285/1000: Loss: 0.42145025730133057\n",
            "Epoch 286/1000: Loss: 0.42081141471862793\n",
            "Epoch 287/1000: Loss: 0.4210161864757538\n",
            "Epoch 288/1000: Loss: 0.42068421840667725\n",
            "Epoch 289/1000: Loss: 0.42064207792282104\n",
            "Epoch 290/1000: Loss: 0.4207056760787964\n",
            "Epoch 291/1000: Loss: 0.42031222581863403\n",
            "Epoch 292/1000: Loss: 0.42047250270843506\n",
            "Epoch 293/1000: Loss: 0.4203372597694397\n",
            "Epoch 294/1000: Loss: 0.42011427879333496\n",
            "Epoch 295/1000: Loss: 0.4202496111392975\n",
            "Epoch 296/1000: Loss: 0.4199583828449249\n",
            "Epoch 297/1000: Loss: 0.4198800027370453\n",
            "Epoch 298/1000: Loss: 0.4199749231338501\n",
            "Epoch 299/1000: Loss: 0.41972237825393677\n",
            "Epoch 300/1000: Loss: 0.4196476340293884\n",
            "Epoch 301/1000: Loss: 0.4196903705596924\n",
            "Epoch 302/1000: Loss: 0.41949719190597534\n",
            "Epoch 303/1000: Loss: 0.4194457530975342\n",
            "Epoch 304/1000: Loss: 0.41943496465682983\n",
            "Epoch 305/1000: Loss: 0.41926464438438416\n",
            "Epoch 306/1000: Loss: 0.41920530796051025\n",
            "Epoch 307/1000: Loss: 0.4191967844963074\n",
            "Epoch 308/1000: Loss: 0.4190683662891388\n",
            "Epoch 309/1000: Loss: 0.4189726412296295\n",
            "Epoch 310/1000: Loss: 0.418965220451355\n",
            "Epoch 311/1000: Loss: 0.4188826382160187\n",
            "Epoch 312/1000: Loss: 0.41877007484436035\n",
            "Epoch 313/1000: Loss: 0.41875314712524414\n",
            "Epoch 314/1000: Loss: 0.4187045395374298\n",
            "Epoch 315/1000: Loss: 0.4185827672481537\n",
            "Epoch 316/1000: Loss: 0.41853758692741394\n",
            "Epoch 317/1000: Loss: 0.41852009296417236\n",
            "Epoch 318/1000: Loss: 0.4184249937534332\n",
            "Epoch 319/1000: Loss: 0.41833922266960144\n",
            "Epoch 320/1000: Loss: 0.41830581426620483\n",
            "Epoch 321/1000: Loss: 0.4182605743408203\n",
            "Epoch 322/1000: Loss: 0.4181828796863556\n",
            "Epoch 323/1000: Loss: 0.418110728263855\n",
            "Epoch 324/1000: Loss: 0.41806933283805847\n",
            "Epoch 325/1000: Loss: 0.4180275797843933\n",
            "Epoch 326/1000: Loss: 0.4179580807685852\n",
            "Epoch 327/1000: Loss: 0.41788747906684875\n",
            "Epoch 328/1000: Loss: 0.4178342819213867\n",
            "Epoch 329/1000: Loss: 0.4177924692630768\n",
            "Epoch 330/1000: Loss: 0.41774633526802063\n",
            "Epoch 331/1000: Loss: 0.41768598556518555\n",
            "Epoch 332/1000: Loss: 0.4176240563392639\n",
            "Epoch 333/1000: Loss: 0.41756927967071533\n",
            "Epoch 334/1000: Loss: 0.41752365231513977\n",
            "Epoch 335/1000: Loss: 0.41748201847076416\n",
            "Epoch 336/1000: Loss: 0.4174371361732483\n",
            "Epoch 337/1000: Loss: 0.4173893332481384\n",
            "Epoch 338/1000: Loss: 0.4173368811607361\n",
            "Epoch 339/1000: Loss: 0.41728511452674866\n",
            "Epoch 340/1000: Loss: 0.41723424196243286\n",
            "Epoch 341/1000: Loss: 0.4171857535839081\n",
            "Epoch 342/1000: Loss: 0.4171393811702728\n",
            "Epoch 343/1000: Loss: 0.4170951843261719\n",
            "Epoch 344/1000: Loss: 0.41705283522605896\n",
            "Epoch 345/1000: Loss: 0.4170122444629669\n",
            "Epoch 346/1000: Loss: 0.41697436571121216\n",
            "Epoch 347/1000: Loss: 0.4169401526451111\n",
            "Epoch 348/1000: Loss: 0.4169160723686218\n",
            "Epoch 349/1000: Loss: 0.4169037938117981\n",
            "Epoch 350/1000: Loss: 0.4169384241104126\n",
            "Epoch 351/1000: Loss: 0.41701534390449524\n",
            "Epoch 352/1000: Loss: 0.4173588156700134\n",
            "Epoch 353/1000: Loss: 0.41775205731391907\n",
            "Epoch 354/1000: Loss: 0.4196271598339081\n",
            "Epoch 355/1000: Loss: 0.4196213483810425\n",
            "Epoch 356/1000: Loss: 0.4231160879135132\n",
            "Epoch 357/1000: Loss: 0.4183691442012787\n",
            "Epoch 358/1000: Loss: 0.41677916049957275\n",
            "Epoch 359/1000: Loss: 0.4170244038105011\n",
            "Epoch 360/1000: Loss: 0.41826990246772766\n",
            "Epoch 361/1000: Loss: 0.42183852195739746\n",
            "Epoch 362/1000: Loss: 0.41913825273513794\n",
            "Epoch 363/1000: Loss: 0.4187711477279663\n",
            "Epoch 364/1000: Loss: 0.41660359501838684\n",
            "Epoch 365/1000: Loss: 0.41708868741989136\n",
            "Epoch 366/1000: Loss: 0.4192546010017395\n",
            "Epoch 367/1000: Loss: 0.4174427092075348\n",
            "Epoch 368/1000: Loss: 0.416456401348114\n",
            "Epoch 369/1000: Loss: 0.4162644147872925\n",
            "Epoch 370/1000: Loss: 0.4169122576713562\n",
            "Epoch 371/1000: Loss: 0.41820573806762695\n",
            "Epoch 372/1000: Loss: 0.41729170083999634\n",
            "Epoch 373/1000: Loss: 0.4167434573173523\n",
            "Epoch 374/1000: Loss: 0.41615015268325806\n",
            "Epoch 375/1000: Loss: 0.41649237275123596\n",
            "Epoch 376/1000: Loss: 0.41739150881767273\n",
            "Epoch 377/1000: Loss: 0.4169142544269562\n",
            "Epoch 378/1000: Loss: 0.4166358709335327\n",
            "Epoch 379/1000: Loss: 0.4160539507865906\n",
            "Epoch 380/1000: Loss: 0.41593095660209656\n",
            "Epoch 381/1000: Loss: 0.41617316007614136\n",
            "Epoch 382/1000: Loss: 0.41631370782852173\n",
            "Epoch 383/1000: Loss: 0.4163914918899536\n",
            "Epoch 384/1000: Loss: 0.41603580117225647\n",
            "Epoch 385/1000: Loss: 0.41582155227661133\n",
            "Epoch 386/1000: Loss: 0.41576826572418213\n",
            "Epoch 387/1000: Loss: 0.4158525764942169\n",
            "Epoch 388/1000: Loss: 0.41601884365081787\n",
            "Epoch 389/1000: Loss: 0.4160357415676117\n",
            "Epoch 390/1000: Loss: 0.41607466340065\n",
            "Epoch 391/1000: Loss: 0.41585975885391235\n",
            "Epoch 392/1000: Loss: 0.41570329666137695\n",
            "Epoch 393/1000: Loss: 0.4155879318714142\n",
            "Epoch 394/1000: Loss: 0.41556116938591003\n",
            "Epoch 395/1000: Loss: 0.415597140789032\n",
            "Epoch 396/1000: Loss: 0.415657103061676\n",
            "Epoch 397/1000: Loss: 0.41578009724617004\n",
            "Epoch 398/1000: Loss: 0.41581645607948303\n",
            "Epoch 399/1000: Loss: 0.4159596562385559\n",
            "Epoch 400/1000: Loss: 0.4158610999584198\n",
            "Epoch 401/1000: Loss: 0.4159252345561981\n",
            "Epoch 402/1000: Loss: 0.4157882630825043\n",
            "Epoch 403/1000: Loss: 0.4158507287502289\n",
            "Epoch 404/1000: Loss: 0.41576051712036133\n",
            "Epoch 405/1000: Loss: 0.41589391231536865\n",
            "Epoch 406/1000: Loss: 0.4158279597759247\n",
            "Epoch 407/1000: Loss: 0.4160483777523041\n",
            "Epoch 408/1000: Loss: 0.41593798995018005\n",
            "Epoch 409/1000: Loss: 0.4162314832210541\n",
            "Epoch 410/1000: Loss: 0.41607093811035156\n",
            "Epoch 411/1000: Loss: 0.41648972034454346\n",
            "Epoch 412/1000: Loss: 0.41624510288238525\n",
            "Epoch 413/1000: Loss: 0.4167420566082001\n",
            "Epoch 414/1000: Loss: 0.4162557125091553\n",
            "Epoch 415/1000: Loss: 0.41647809743881226\n",
            "Epoch 416/1000: Loss: 0.4158724248409271\n",
            "Epoch 417/1000: Loss: 0.41575098037719727\n",
            "Epoch 418/1000: Loss: 0.4153915047645569\n",
            "Epoch 419/1000: Loss: 0.4152613580226898\n",
            "Epoch 420/1000: Loss: 0.4151165187358856\n",
            "Epoch 421/1000: Loss: 0.4150484502315521\n",
            "Epoch 422/1000: Loss: 0.4149876832962036\n",
            "Epoch 423/1000: Loss: 0.4149520993232727\n",
            "Epoch 424/1000: Loss: 0.4149186611175537\n",
            "Epoch 425/1000: Loss: 0.41489726305007935\n",
            "Epoch 426/1000: Loss: 0.4148807227611542\n",
            "Epoch 427/1000: Loss: 0.4148801863193512\n",
            "Epoch 428/1000: Loss: 0.41488978266716003\n",
            "Epoch 429/1000: Loss: 0.41495177149772644\n",
            "Epoch 430/1000: Loss: 0.4150422215461731\n",
            "Epoch 431/1000: Loss: 0.4153459668159485\n",
            "Epoch 432/1000: Loss: 0.4156504273414612\n",
            "Epoch 433/1000: Loss: 0.41690072417259216\n",
            "Epoch 434/1000: Loss: 0.4173039495944977\n",
            "Epoch 435/1000: Loss: 0.4207124710083008\n",
            "Epoch 436/1000: Loss: 0.4180811047554016\n",
            "Epoch 437/1000: Loss: 0.41835981607437134\n",
            "Epoch 438/1000: Loss: 0.41535213589668274\n",
            "Epoch 439/1000: Loss: 0.4146308898925781\n",
            "Epoch 440/1000: Loss: 0.41574421525001526\n",
            "Epoch 441/1000: Loss: 0.4173000156879425\n",
            "Epoch 442/1000: Loss: 0.4215949475765228\n",
            "Epoch 443/1000: Loss: 0.41775429248809814\n",
            "Epoch 444/1000: Loss: 0.41644760966300964\n",
            "Epoch 445/1000: Loss: 0.4147799611091614\n",
            "Epoch 446/1000: Loss: 0.4162847399711609\n",
            "Epoch 447/1000: Loss: 0.42124271392822266\n",
            "Epoch 448/1000: Loss: 0.41800200939178467\n",
            "Epoch 449/1000: Loss: 0.41681140661239624\n",
            "Epoch 450/1000: Loss: 0.41460201144218445\n",
            "Epoch 451/1000: Loss: 0.4156513810157776\n",
            "Epoch 452/1000: Loss: 0.41859450936317444\n",
            "Epoch 453/1000: Loss: 0.4158281683921814\n",
            "Epoch 454/1000: Loss: 0.4144796133041382\n",
            "Epoch 455/1000: Loss: 0.4147055447101593\n",
            "Epoch 456/1000: Loss: 0.4156569838523865\n",
            "Epoch 457/1000: Loss: 0.4167512357234955\n",
            "Epoch 458/1000: Loss: 0.41497719287872314\n",
            "Epoch 459/1000: Loss: 0.41435685753822327\n",
            "Epoch 460/1000: Loss: 0.4149963855743408\n",
            "Epoch 461/1000: Loss: 0.4154042601585388\n",
            "Epoch 462/1000: Loss: 0.4156087338924408\n",
            "Epoch 463/1000: Loss: 0.41459226608276367\n",
            "Epoch 464/1000: Loss: 0.4142635762691498\n",
            "Epoch 465/1000: Loss: 0.41461628675460815\n",
            "Epoch 466/1000: Loss: 0.4148353338241577\n",
            "Epoch 467/1000: Loss: 0.4148803651332855\n",
            "Epoch 468/1000: Loss: 0.4143928289413452\n",
            "Epoch 469/1000: Loss: 0.41417115926742554\n",
            "Epoch 470/1000: Loss: 0.41430163383483887\n",
            "Epoch 471/1000: Loss: 0.41449037194252014\n",
            "Epoch 472/1000: Loss: 0.41457414627075195\n",
            "Epoch 473/1000: Loss: 0.4143027365207672\n",
            "Epoch 474/1000: Loss: 0.41412121057510376\n",
            "Epoch 475/1000: Loss: 0.4141119122505188\n",
            "Epoch 476/1000: Loss: 0.4142209589481354\n",
            "Epoch 477/1000: Loss: 0.41433990001678467\n",
            "Epoch 478/1000: Loss: 0.4142637848854065\n",
            "Epoch 479/1000: Loss: 0.41414695978164673\n",
            "Epoch 480/1000: Loss: 0.41403260827064514\n",
            "Epoch 481/1000: Loss: 0.41401681303977966\n",
            "Epoch 482/1000: Loss: 0.4140719473361969\n",
            "Epoch 483/1000: Loss: 0.4141198694705963\n",
            "Epoch 484/1000: Loss: 0.4141456186771393\n",
            "Epoch 485/1000: Loss: 0.41407695412635803\n",
            "Epoch 486/1000: Loss: 0.4140031039714813\n",
            "Epoch 487/1000: Loss: 0.4139426648616791\n",
            "Epoch 488/1000: Loss: 0.41392338275909424\n",
            "Epoch 489/1000: Loss: 0.4139358401298523\n",
            "Epoch 490/1000: Loss: 0.4139598309993744\n",
            "Epoch 491/1000: Loss: 0.41398268938064575\n",
            "Epoch 492/1000: Loss: 0.41396990418434143\n",
            "Epoch 493/1000: Loss: 0.4139506220817566\n",
            "Epoch 494/1000: Loss: 0.4139110743999481\n",
            "Epoch 495/1000: Loss: 0.413876473903656\n",
            "Epoch 496/1000: Loss: 0.41384580731391907\n",
            "Epoch 497/1000: Loss: 0.41382622718811035\n",
            "Epoch 498/1000: Loss: 0.41381528973579407\n",
            "Epoch 499/1000: Loss: 0.41381126642227173\n",
            "Epoch 500/1000: Loss: 0.4138126075267792\n",
            "Epoch 501/1000: Loss: 0.4138147532939911\n",
            "Epoch 502/1000: Loss: 0.4138195812702179\n",
            "Epoch 503/1000: Loss: 0.4138212203979492\n",
            "Epoch 504/1000: Loss: 0.4138280153274536\n",
            "Epoch 505/1000: Loss: 0.4138278663158417\n",
            "Epoch 506/1000: Loss: 0.41383999586105347\n",
            "Epoch 507/1000: Loss: 0.41384443640708923\n",
            "Epoch 508/1000: Loss: 0.41387295722961426\n",
            "Epoch 509/1000: Loss: 0.41389063000679016\n",
            "Epoch 510/1000: Loss: 0.4139576852321625\n",
            "Epoch 511/1000: Loss: 0.41400131583213806\n",
            "Epoch 512/1000: Loss: 0.4141589403152466\n",
            "Epoch 513/1000: Loss: 0.41425561904907227\n",
            "Epoch 514/1000: Loss: 0.41463562846183777\n",
            "Epoch 515/1000: Loss: 0.4147857129573822\n",
            "Epoch 516/1000: Loss: 0.4156187176704407\n",
            "Epoch 517/1000: Loss: 0.41558966040611267\n",
            "Epoch 518/1000: Loss: 0.41686612367630005\n",
            "Epoch 519/1000: Loss: 0.4160407781600952\n",
            "Epoch 520/1000: Loss: 0.41674476861953735\n",
            "Epoch 521/1000: Loss: 0.41528424620628357\n",
            "Epoch 522/1000: Loss: 0.4148375391960144\n",
            "Epoch 523/1000: Loss: 0.41403791308403015\n",
            "Epoch 524/1000: Loss: 0.41369086503982544\n",
            "Epoch 525/1000: Loss: 0.41361260414123535\n",
            "Epoch 526/1000: Loss: 0.4137572646141052\n",
            "Epoch 527/1000: Loss: 0.41408082842826843\n",
            "Epoch 528/1000: Loss: 0.4143093526363373\n",
            "Epoch 529/1000: Loss: 0.41478049755096436\n",
            "Epoch 530/1000: Loss: 0.4146635830402374\n",
            "Epoch 531/1000: Loss: 0.41492000222206116\n",
            "Epoch 532/1000: Loss: 0.4144769012928009\n",
            "Epoch 533/1000: Loss: 0.4143446683883667\n",
            "Epoch 534/1000: Loss: 0.4139392077922821\n",
            "Epoch 535/1000: Loss: 0.41371843218803406\n",
            "Epoch 536/1000: Loss: 0.41357508301734924\n",
            "Epoch 537/1000: Loss: 0.41354408860206604\n",
            "Epoch 538/1000: Loss: 0.4135986864566803\n",
            "Epoch 539/1000: Loss: 0.4136997163295746\n",
            "Epoch 540/1000: Loss: 0.4138582944869995\n",
            "Epoch 541/1000: Loss: 0.41394495964050293\n",
            "Epoch 542/1000: Loss: 0.4141431450843811\n",
            "Epoch 543/1000: Loss: 0.41413813829421997\n",
            "Epoch 544/1000: Loss: 0.4143209755420685\n",
            "Epoch 545/1000: Loss: 0.41420120000839233\n",
            "Epoch 546/1000: Loss: 0.41429537534713745\n",
            "Epoch 547/1000: Loss: 0.4140985608100891\n",
            "Epoch 548/1000: Loss: 0.4140866994857788\n",
            "Epoch 549/1000: Loss: 0.4139043092727661\n",
            "Epoch 550/1000: Loss: 0.41384992003440857\n",
            "Epoch 551/1000: Loss: 0.41372618079185486\n",
            "Epoch 552/1000: Loss: 0.4136771559715271\n",
            "Epoch 553/1000: Loss: 0.4136073887348175\n",
            "Epoch 554/1000: Loss: 0.41357675194740295\n",
            "Epoch 555/1000: Loss: 0.41354161500930786\n",
            "Epoch 556/1000: Loss: 0.41352736949920654\n",
            "Epoch 557/1000: Loss: 0.4135116934776306\n",
            "Epoch 558/1000: Loss: 0.413510262966156\n",
            "Epoch 559/1000: Loss: 0.41350746154785156\n",
            "Epoch 560/1000: Loss: 0.4135205149650574\n",
            "Epoch 561/1000: Loss: 0.4135333299636841\n",
            "Epoch 562/1000: Loss: 0.41357678174972534\n",
            "Epoch 563/1000: Loss: 0.4136221706867218\n",
            "Epoch 564/1000: Loss: 0.41374799609184265\n",
            "Epoch 565/1000: Loss: 0.4138684868812561\n",
            "Epoch 566/1000: Loss: 0.41423219442367554\n",
            "Epoch 567/1000: Loss: 0.41449853777885437\n",
            "Epoch 568/1000: Loss: 0.41552022099494934\n",
            "Epoch 569/1000: Loss: 0.41580238938331604\n",
            "Epoch 570/1000: Loss: 0.41802334785461426\n",
            "Epoch 571/1000: Loss: 0.41702786087989807\n",
            "Epoch 572/1000: Loss: 0.41878020763397217\n",
            "Epoch 573/1000: Loss: 0.4159047305583954\n",
            "Epoch 574/1000: Loss: 0.41494908928871155\n",
            "Epoch 575/1000: Loss: 0.4137132167816162\n",
            "Epoch 576/1000: Loss: 0.41338425874710083\n",
            "Epoch 577/1000: Loss: 0.41368934512138367\n",
            "Epoch 578/1000: Loss: 0.41429293155670166\n",
            "Epoch 579/1000: Loss: 0.4153921604156494\n",
            "Epoch 580/1000: Loss: 0.41511857509613037\n",
            "Epoch 581/1000: Loss: 0.4154615104198456\n",
            "Epoch 582/1000: Loss: 0.4144158959388733\n",
            "Epoch 583/1000: Loss: 0.4138888120651245\n",
            "Epoch 584/1000: Loss: 0.41343972086906433\n",
            "Epoch 585/1000: Loss: 0.41336777806282043\n",
            "Epoch 586/1000: Loss: 0.413582980632782\n",
            "Epoch 587/1000: Loss: 0.4138510227203369\n",
            "Epoch 588/1000: Loss: 0.41421258449554443\n",
            "Epoch 589/1000: Loss: 0.4141196608543396\n",
            "Epoch 590/1000: Loss: 0.4141375720500946\n",
            "Epoch 591/1000: Loss: 0.41378411650657654\n",
            "Epoch 592/1000: Loss: 0.4135671854019165\n",
            "Epoch 593/1000: Loss: 0.4133773446083069\n",
            "Epoch 594/1000: Loss: 0.41331854462623596\n",
            "Epoch 595/1000: Loss: 0.41336527466773987\n",
            "Epoch 596/1000: Loss: 0.4134658873081207\n",
            "Epoch 597/1000: Loss: 0.4136027991771698\n",
            "Epoch 598/1000: Loss: 0.4136468172073364\n",
            "Epoch 599/1000: Loss: 0.4137149751186371\n",
            "Epoch 600/1000: Loss: 0.41363540291786194\n",
            "Epoch 601/1000: Loss: 0.4135989546775818\n",
            "Epoch 602/1000: Loss: 0.4134872555732727\n",
            "Epoch 603/1000: Loss: 0.41341695189476013\n",
            "Epoch 604/1000: Loss: 0.413345068693161\n",
            "Epoch 605/1000: Loss: 0.41330212354660034\n",
            "Epoch 606/1000: Loss: 0.4132801294326782\n",
            "Epoch 607/1000: Loss: 0.4132768213748932\n",
            "Epoch 608/1000: Loss: 0.413286954164505\n",
            "Epoch 609/1000: Loss: 0.41330578923225403\n",
            "Epoch 610/1000: Loss: 0.4133334755897522\n",
            "Epoch 611/1000: Loss: 0.4133586287498474\n",
            "Epoch 612/1000: Loss: 0.41339969635009766\n",
            "Epoch 613/1000: Loss: 0.4134278893470764\n",
            "Epoch 614/1000: Loss: 0.41349154710769653\n",
            "Epoch 615/1000: Loss: 0.4135269224643707\n",
            "Epoch 616/1000: Loss: 0.4136364758014679\n",
            "Epoch 617/1000: Loss: 0.4136859178543091\n",
            "Epoch 618/1000: Loss: 0.41388434171676636\n",
            "Epoch 619/1000: Loss: 0.41395291686058044\n",
            "Epoch 620/1000: Loss: 0.4143160879611969\n",
            "Epoch 621/1000: Loss: 0.41436874866485596\n",
            "Epoch 622/1000: Loss: 0.41497117280960083\n",
            "Epoch 623/1000: Loss: 0.4148651361465454\n",
            "Epoch 624/1000: Loss: 0.4156193435192108\n",
            "Epoch 625/1000: Loss: 0.4151388108730316\n",
            "Epoch 626/1000: Loss: 0.41564497351646423\n",
            "Epoch 627/1000: Loss: 0.4148139953613281\n",
            "Epoch 628/1000: Loss: 0.41473501920700073\n",
            "Epoch 629/1000: Loss: 0.4140187203884125\n",
            "Epoch 630/1000: Loss: 0.41368958353996277\n",
            "Epoch 631/1000: Loss: 0.4133691191673279\n",
            "Epoch 632/1000: Loss: 0.4132307767868042\n",
            "Epoch 633/1000: Loss: 0.4132210612297058\n",
            "Epoch 634/1000: Loss: 0.4133070111274719\n",
            "Epoch 635/1000: Loss: 0.41346731781959534\n",
            "Epoch 636/1000: Loss: 0.4136020243167877\n",
            "Epoch 637/1000: Loss: 0.4138307571411133\n",
            "Epoch 638/1000: Loss: 0.4138551950454712\n",
            "Epoch 639/1000: Loss: 0.41403692960739136\n",
            "Epoch 640/1000: Loss: 0.4138965606689453\n",
            "Epoch 641/1000: Loss: 0.41393017768859863\n",
            "Epoch 642/1000: Loss: 0.4137110412120819\n",
            "Epoch 643/1000: Loss: 0.4136255979537964\n",
            "Epoch 644/1000: Loss: 0.413448303937912\n",
            "Epoch 645/1000: Loss: 0.4133509695529938\n",
            "Epoch 646/1000: Loss: 0.41325974464416504\n",
            "Epoch 647/1000: Loss: 0.4132092297077179\n",
            "Epoch 648/1000: Loss: 0.41318270564079285\n",
            "Epoch 649/1000: Loss: 0.4131768047809601\n",
            "Epoch 650/1000: Loss: 0.41318607330322266\n",
            "Epoch 651/1000: Loss: 0.41320592164993286\n",
            "Epoch 652/1000: Loss: 0.4132370948791504\n",
            "Epoch 653/1000: Loss: 0.4132690727710724\n",
            "Epoch 654/1000: Loss: 0.41332101821899414\n",
            "Epoch 655/1000: Loss: 0.41336068511009216\n",
            "Epoch 656/1000: Loss: 0.41344699263572693\n",
            "Epoch 657/1000: Loss: 0.4134993851184845\n",
            "Epoch 658/1000: Loss: 0.4136560261249542\n",
            "Epoch 659/1000: Loss: 0.4137304425239563\n",
            "Epoch 660/1000: Loss: 0.4140278995037079\n",
            "Epoch 661/1000: Loss: 0.41411641240119934\n",
            "Epoch 662/1000: Loss: 0.41465649008750916\n",
            "Epoch 663/1000: Loss: 0.4146636724472046\n",
            "Epoch 664/1000: Loss: 0.41546720266342163\n",
            "Epoch 665/1000: Loss: 0.4151400327682495\n",
            "Epoch 666/1000: Loss: 0.4158761203289032\n",
            "Epoch 667/1000: Loss: 0.4150596559047699\n",
            "Epoch 668/1000: Loss: 0.4151914715766907\n",
            "Epoch 669/1000: Loss: 0.4142981767654419\n",
            "Epoch 670/1000: Loss: 0.41395434737205505\n",
            "Epoch 671/1000: Loss: 0.4134622812271118\n",
            "Epoch 672/1000: Loss: 0.4132230281829834\n",
            "Epoch 673/1000: Loss: 0.4131316542625427\n",
            "Epoch 674/1000: Loss: 0.41317489743232727\n",
            "Epoch 675/1000: Loss: 0.41331639885902405\n",
            "Epoch 676/1000: Loss: 0.41347363591194153\n",
            "Epoch 677/1000: Loss: 0.4137214422225952\n",
            "Epoch 678/1000: Loss: 0.4137836694717407\n",
            "Epoch 679/1000: Loss: 0.4139997065067291\n",
            "Epoch 680/1000: Loss: 0.413868248462677\n",
            "Epoch 681/1000: Loss: 0.41391628980636597\n",
            "Epoch 682/1000: Loss: 0.4136740267276764\n",
            "Epoch 683/1000: Loss: 0.41357308626174927\n",
            "Epoch 684/1000: Loss: 0.41337382793426514\n",
            "Epoch 685/1000: Loss: 0.4132612645626068\n",
            "Epoch 686/1000: Loss: 0.41316691040992737\n",
            "Epoch 687/1000: Loss: 0.41311946511268616\n",
            "Epoch 688/1000: Loss: 0.41310420632362366\n",
            "Epoch 689/1000: Loss: 0.41311395168304443\n",
            "Epoch 690/1000: Loss: 0.41314244270324707\n",
            "Epoch 691/1000: Loss: 0.41318005323410034\n",
            "Epoch 692/1000: Loss: 0.4132353663444519\n",
            "Epoch 693/1000: Loss: 0.41327834129333496\n",
            "Epoch 694/1000: Loss: 0.41335541009902954\n",
            "Epoch 695/1000: Loss: 0.4133915901184082\n",
            "Epoch 696/1000: Loss: 0.41349661350250244\n",
            "Epoch 697/1000: Loss: 0.4135248363018036\n",
            "Epoch 698/1000: Loss: 0.4136788249015808\n",
            "Epoch 699/1000: Loss: 0.41370058059692383\n",
            "Epoch 700/1000: Loss: 0.4139348864555359\n",
            "Epoch 701/1000: Loss: 0.41393786668777466\n",
            "Epoch 702/1000: Loss: 0.41427791118621826\n",
            "Epoch 703/1000: Loss: 0.414213091135025\n",
            "Epoch 704/1000: Loss: 0.41462936997413635\n",
            "Epoch 705/1000: Loss: 0.41441482305526733\n",
            "Epoch 706/1000: Loss: 0.4147740602493286\n",
            "Epoch 707/1000: Loss: 0.414379745721817\n",
            "Epoch 708/1000: Loss: 0.41452035307884216\n",
            "Epoch 709/1000: Loss: 0.4140581488609314\n",
            "Epoch 710/1000: Loss: 0.4139713943004608\n",
            "Epoch 711/1000: Loss: 0.41361016035079956\n",
            "Epoch 712/1000: Loss: 0.41344913840293884\n",
            "Epoch 713/1000: Loss: 0.41325339674949646\n",
            "Epoch 714/1000: Loss: 0.41314783692359924\n",
            "Epoch 715/1000: Loss: 0.4130810797214508\n",
            "Epoch 716/1000: Loss: 0.4130575656890869\n",
            "Epoch 717/1000: Loss: 0.4130653738975525\n",
            "Epoch 718/1000: Loss: 0.4130952060222626\n",
            "Epoch 719/1000: Loss: 0.413144052028656\n",
            "Epoch 720/1000: Loss: 0.413193017244339\n",
            "Epoch 721/1000: Loss: 0.4132691025733948\n",
            "Epoch 722/1000: Loss: 0.4133150279521942\n",
            "Epoch 723/1000: Loss: 0.4134178161621094\n",
            "Epoch 724/1000: Loss: 0.41345059871673584\n",
            "Epoch 725/1000: Loss: 0.41358932852745056\n",
            "Epoch 726/1000: Loss: 0.41360411047935486\n",
            "Epoch 727/1000: Loss: 0.4137919843196869\n",
            "Epoch 728/1000: Loss: 0.4137786030769348\n",
            "Epoch 729/1000: Loss: 0.4140220582485199\n",
            "Epoch 730/1000: Loss: 0.4139571487903595\n",
            "Epoch 731/1000: Loss: 0.41423654556274414\n",
            "Epoch 732/1000: Loss: 0.41408684849739075\n",
            "Epoch 733/1000: Loss: 0.41434192657470703\n",
            "Epoch 734/1000: Loss: 0.4140944480895996\n",
            "Epoch 735/1000: Loss: 0.4142475128173828\n",
            "Epoch 736/1000: Loss: 0.41394278407096863\n",
            "Epoch 737/1000: Loss: 0.4139622151851654\n",
            "Epoch 738/1000: Loss: 0.41367819905281067\n",
            "Epoch 739/1000: Loss: 0.4136069118976593\n",
            "Epoch 740/1000: Loss: 0.4134012460708618\n",
            "Epoch 741/1000: Loss: 0.4133109152317047\n",
            "Epoch 742/1000: Loss: 0.41319143772125244\n",
            "Epoch 743/1000: Loss: 0.4131261706352234\n",
            "Epoch 744/1000: Loss: 0.41307011246681213\n",
            "Epoch 745/1000: Loss: 0.4130380153656006\n",
            "Epoch 746/1000: Loss: 0.4130188226699829\n",
            "Epoch 747/1000: Loss: 0.41301077604293823\n",
            "Epoch 748/1000: Loss: 0.4130106270313263\n",
            "Epoch 749/1000: Loss: 0.4130162298679352\n",
            "Epoch 750/1000: Loss: 0.4130268692970276\n",
            "Epoch 751/1000: Loss: 0.41304075717926025\n",
            "Epoch 752/1000: Loss: 0.4130622446537018\n",
            "Epoch 753/1000: Loss: 0.41308528184890747\n",
            "Epoch 754/1000: Loss: 0.41312676668167114\n",
            "Epoch 755/1000: Loss: 0.41316652297973633\n",
            "Epoch 756/1000: Loss: 0.4132525622844696\n",
            "Epoch 757/1000: Loss: 0.4133252799510956\n",
            "Epoch 758/1000: Loss: 0.41351383924484253\n",
            "Epoch 759/1000: Loss: 0.41364169120788574\n",
            "Epoch 760/1000: Loss: 0.4140576124191284\n",
            "Epoch 761/1000: Loss: 0.41423070430755615\n",
            "Epoch 762/1000: Loss: 0.4150749444961548\n",
            "Epoch 763/1000: Loss: 0.4150957763195038\n",
            "Epoch 764/1000: Loss: 0.4163966774940491\n",
            "Epoch 765/1000: Loss: 0.41571882367134094\n",
            "Epoch 766/1000: Loss: 0.41667863726615906\n",
            "Epoch 767/1000: Loss: 0.41522374749183655\n",
            "Epoch 768/1000: Loss: 0.41500088572502136\n",
            "Epoch 769/1000: Loss: 0.4138834774494171\n",
            "Epoch 770/1000: Loss: 0.4133544862270355\n",
            "Epoch 771/1000: Loss: 0.4130266010761261\n",
            "Epoch 772/1000: Loss: 0.41300269961357117\n",
            "Epoch 773/1000: Loss: 0.4132028818130493\n",
            "Epoch 774/1000: Loss: 0.4134773313999176\n",
            "Epoch 775/1000: Loss: 0.4139160215854645\n",
            "Epoch 776/1000: Loss: 0.4139741361141205\n",
            "Epoch 777/1000: Loss: 0.4142777919769287\n",
            "Epoch 778/1000: Loss: 0.4139302372932434\n",
            "Epoch 779/1000: Loss: 0.4138242304325104\n",
            "Epoch 780/1000: Loss: 0.4134116768836975\n",
            "Epoch 781/1000: Loss: 0.41317054629325867\n",
            "Epoch 782/1000: Loss: 0.4130042493343353\n",
            "Epoch 783/1000: Loss: 0.4129696786403656\n",
            "Epoch 784/1000: Loss: 0.41303783655166626\n",
            "Epoch 785/1000: Loss: 0.4131566286087036\n",
            "Epoch 786/1000: Loss: 0.41332757472991943\n",
            "Epoch 787/1000: Loss: 0.4134061932563782\n",
            "Epoch 788/1000: Loss: 0.41355791687965393\n",
            "Epoch 789/1000: Loss: 0.4135020077228546\n",
            "Epoch 790/1000: Loss: 0.41354385018348694\n",
            "Epoch 791/1000: Loss: 0.4133908450603485\n",
            "Epoch 792/1000: Loss: 0.41331982612609863\n",
            "Epoch 793/1000: Loss: 0.41317182779312134\n",
            "Epoch 794/1000: Loss: 0.4130812883377075\n",
            "Epoch 795/1000: Loss: 0.4130028188228607\n",
            "Epoch 796/1000: Loss: 0.41296157240867615\n",
            "Epoch 797/1000: Loss: 0.4129459857940674\n",
            "Epoch 798/1000: Loss: 0.41295018792152405\n",
            "Epoch 799/1000: Loss: 0.41296854615211487\n",
            "Epoch 800/1000: Loss: 0.41299542784690857\n",
            "Epoch 801/1000: Loss: 0.41303539276123047\n",
            "Epoch 802/1000: Loss: 0.41307133436203003\n",
            "Epoch 803/1000: Loss: 0.41313016414642334\n",
            "Epoch 804/1000: Loss: 0.4131650924682617\n",
            "Epoch 805/1000: Loss: 0.4132455885410309\n",
            "Epoch 806/1000: Loss: 0.41327765583992004\n",
            "Epoch 807/1000: Loss: 0.4133986830711365\n",
            "Epoch 808/1000: Loss: 0.4134344458580017\n",
            "Epoch 809/1000: Loss: 0.41363203525543213\n",
            "Epoch 810/1000: Loss: 0.4136704206466675\n",
            "Epoch 811/1000: Loss: 0.41399019956588745\n",
            "Epoch 812/1000: Loss: 0.4139944016933441\n",
            "Epoch 813/1000: Loss: 0.41445091366767883\n",
            "Epoch 814/1000: Loss: 0.41432127356529236\n",
            "Epoch 815/1000: Loss: 0.4148061275482178\n",
            "Epoch 816/1000: Loss: 0.4144398868083954\n",
            "Epoch 817/1000: Loss: 0.41472601890563965\n",
            "Epoch 818/1000: Loss: 0.41418567299842834\n",
            "Epoch 819/1000: Loss: 0.41415736079216003\n",
            "Epoch 820/1000: Loss: 0.4136751890182495\n",
            "Epoch 821/1000: Loss: 0.413485586643219\n",
            "Epoch 822/1000: Loss: 0.4132036566734314\n",
            "Epoch 823/1000: Loss: 0.4130553603172302\n",
            "Epoch 824/1000: Loss: 0.4129527509212494\n",
            "Epoch 825/1000: Loss: 0.41291290521621704\n",
            "Epoch 826/1000: Loss: 0.41291847825050354\n",
            "Epoch 827/1000: Loss: 0.4129560887813568\n",
            "Epoch 828/1000: Loss: 0.41301968693733215\n",
            "Epoch 829/1000: Loss: 0.4130813777446747\n",
            "Epoch 830/1000: Loss: 0.41317442059516907\n",
            "Epoch 831/1000: Loss: 0.41321858763694763\n",
            "Epoch 832/1000: Loss: 0.413322776556015\n",
            "Epoch 833/1000: Loss: 0.4133300483226776\n",
            "Epoch 834/1000: Loss: 0.41343453526496887\n",
            "Epoch 835/1000: Loss: 0.41340452432632446\n",
            "Epoch 836/1000: Loss: 0.41350454092025757\n",
            "Epoch 837/1000: Loss: 0.4134461581707001\n",
            "Epoch 838/1000: Loss: 0.41353875398635864\n",
            "Epoch 839/1000: Loss: 0.4134617745876312\n",
            "Epoch 840/1000: Loss: 0.4135449528694153\n",
            "Epoch 841/1000: Loss: 0.4134569466114044\n",
            "Epoch 842/1000: Loss: 0.41352975368499756\n",
            "Epoch 843/1000: Loss: 0.41343817114830017\n",
            "Epoch 844/1000: Loss: 0.41350215673446655\n",
            "Epoch 845/1000: Loss: 0.4134133756160736\n",
            "Epoch 846/1000: Loss: 0.4134732782840729\n",
            "Epoch 847/1000: Loss: 0.41339191794395447\n",
            "Epoch 848/1000: Loss: 0.4134528636932373\n",
            "Epoch 849/1000: Loss: 0.41338038444519043\n",
            "Epoch 850/1000: Loss: 0.413446843624115\n",
            "Epoch 851/1000: Loss: 0.4133816361427307\n",
            "Epoch 852/1000: Loss: 0.4134570062160492\n",
            "Epoch 853/1000: Loss: 0.41339635848999023\n",
            "Epoch 854/1000: Loss: 0.4134830832481384\n",
            "Epoch 855/1000: Loss: 0.4134237766265869\n",
            "Epoch 856/1000: Loss: 0.41352415084838867\n",
            "Epoch 857/1000: Loss: 0.41346243023872375\n",
            "Epoch 858/1000: Loss: 0.4135782718658447\n",
            "Epoch 859/1000: Loss: 0.41350945830345154\n",
            "Epoch 860/1000: Loss: 0.4136395752429962\n",
            "Epoch 861/1000: Loss: 0.41355764865875244\n",
            "Epoch 862/1000: Loss: 0.41369569301605225\n",
            "Epoch 863/1000: Loss: 0.41359400749206543\n",
            "Epoch 864/1000: Loss: 0.4137275218963623\n",
            "Epoch 865/1000: Loss: 0.4136029779911041\n",
            "Epoch 866/1000: Loss: 0.41371604800224304\n",
            "Epoch 867/1000: Loss: 0.4135729968547821\n",
            "Epoch 868/1000: Loss: 0.41365355253219604\n",
            "Epoch 869/1000: Loss: 0.4135042130947113\n",
            "Epoch 870/1000: Loss: 0.413549542427063\n",
            "Epoch 871/1000: Loss: 0.41340935230255127\n",
            "Epoch 872/1000: Loss: 0.41342657804489136\n",
            "Epoch 873/1000: Loss: 0.4133073687553406\n",
            "Epoch 874/1000: Loss: 0.41330838203430176\n",
            "Epoch 875/1000: Loss: 0.4132154583930969\n",
            "Epoch 876/1000: Loss: 0.4132115840911865\n",
            "Epoch 877/1000: Loss: 0.413144052028656\n",
            "Epoch 878/1000: Loss: 0.41314318776130676\n",
            "Epoch 879/1000: Loss: 0.4130972623825073\n",
            "Epoch 880/1000: Loss: 0.4131046533584595\n",
            "Epoch 881/1000: Loss: 0.41307640075683594\n",
            "Epoch 882/1000: Loss: 0.41309642791748047\n",
            "Epoch 883/1000: Loss: 0.4130832850933075\n",
            "Epoch 884/1000: Loss: 0.41312268376350403\n",
            "Epoch 885/1000: Loss: 0.41312453150749207\n",
            "Epoch 886/1000: Loss: 0.41319558024406433\n",
            "Epoch 887/1000: Loss: 0.41321462392807007\n",
            "Epoch 888/1000: Loss: 0.41334059834480286\n",
            "Epoch 889/1000: Loss: 0.41337835788726807\n",
            "Epoch 890/1000: Loss: 0.41359853744506836\n",
            "Epoch 891/1000: Loss: 0.4136447310447693\n",
            "Epoch 892/1000: Loss: 0.4140104651451111\n",
            "Epoch 893/1000: Loss: 0.4140152037143707\n",
            "Epoch 894/1000: Loss: 0.41454556584358215\n",
            "Epoch 895/1000: Loss: 0.4143887758255005\n",
            "Epoch 896/1000: Loss: 0.4149644672870636\n",
            "Epoch 897/1000: Loss: 0.41452258825302124\n",
            "Epoch 898/1000: Loss: 0.41486093401908875\n",
            "Epoch 899/1000: Loss: 0.4142208695411682\n",
            "Epoch 900/1000: Loss: 0.4141751229763031\n",
            "Epoch 901/1000: Loss: 0.4136233627796173\n",
            "Epoch 902/1000: Loss: 0.41339653730392456\n",
            "Epoch 903/1000: Loss: 0.4130944609642029\n",
            "Epoch 904/1000: Loss: 0.4129377007484436\n",
            "Epoch 905/1000: Loss: 0.4128457009792328\n",
            "Epoch 906/1000: Loss: 0.41282349824905396\n",
            "Epoch 907/1000: Loss: 0.4128519892692566\n",
            "Epoch 908/1000: Loss: 0.4129129648208618\n",
            "Epoch 909/1000: Loss: 0.41300690174102783\n",
            "Epoch 910/1000: Loss: 0.41308319568634033\n",
            "Epoch 911/1000: Loss: 0.41321104764938354\n",
            "Epoch 912/1000: Loss: 0.4132503569126129\n",
            "Epoch 913/1000: Loss: 0.41338273882865906\n",
            "Epoch 914/1000: Loss: 0.4133610427379608\n",
            "Epoch 915/1000: Loss: 0.4134747087955475\n",
            "Epoch 916/1000: Loss: 0.41340094804763794\n",
            "Epoch 917/1000: Loss: 0.41348710656166077\n",
            "Epoch 918/1000: Loss: 0.4133862257003784\n",
            "Epoch 919/1000: Loss: 0.41344720125198364\n",
            "Epoch 920/1000: Loss: 0.4133412539958954\n",
            "Epoch 921/1000: Loss: 0.41338351368904114\n",
            "Epoch 922/1000: Loss: 0.41328537464141846\n",
            "Epoch 923/1000: Loss: 0.41331571340560913\n",
            "Epoch 924/1000: Loss: 0.4132314622402191\n",
            "Epoch 925/1000: Loss: 0.41325756907463074\n",
            "Epoch 926/1000: Loss: 0.41318997740745544\n",
            "Epoch 927/1000: Loss: 0.41322049498558044\n",
            "Epoch 928/1000: Loss: 0.41317033767700195\n",
            "Epoch 929/1000: Loss: 0.4132140278816223\n",
            "Epoch 930/1000: Loss: 0.4131801128387451\n",
            "Epoch 931/1000: Loss: 0.41324582695961\n",
            "Epoch 932/1000: Loss: 0.4132254421710968\n",
            "Epoch 933/1000: Loss: 0.4133239984512329\n",
            "Epoch 934/1000: Loss: 0.41331231594085693\n",
            "Epoch 935/1000: Loss: 0.4134567081928253\n",
            "Epoch 936/1000: Loss: 0.4134441316127777\n",
            "Epoch 937/1000: Loss: 0.4136478900909424\n",
            "Epoch 938/1000: Loss: 0.41361570358276367\n",
            "Epoch 939/1000: Loss: 0.4138830006122589\n",
            "Epoch 940/1000: Loss: 0.41379809379577637\n",
            "Epoch 941/1000: Loss: 0.4141034781932831\n",
            "Epoch 942/1000: Loss: 0.4139261543750763\n",
            "Epoch 943/1000: Loss: 0.4141983389854431\n",
            "Epoch 944/1000: Loss: 0.41391634941101074\n",
            "Epoch 945/1000: Loss: 0.41406795382499695\n",
            "Epoch 946/1000: Loss: 0.4137323796749115\n",
            "Epoch 947/1000: Loss: 0.4137363135814667\n",
            "Epoch 948/1000: Loss: 0.4134361147880554\n",
            "Epoch 949/1000: Loss: 0.41335129737854004\n",
            "Epoch 950/1000: Loss: 0.41314393281936646\n",
            "Epoch 951/1000: Loss: 0.41304969787597656\n",
            "Epoch 952/1000: Loss: 0.4129350185394287\n",
            "Epoch 953/1000: Loss: 0.4128720760345459\n",
            "Epoch 954/1000: Loss: 0.4128207862377167\n",
            "Epoch 955/1000: Loss: 0.4127919375896454\n",
            "Epoch 956/1000: Loss: 0.41277578473091125\n",
            "Epoch 957/1000: Loss: 0.41277003288269043\n",
            "Epoch 958/1000: Loss: 0.4127718210220337\n",
            "Epoch 959/1000: Loss: 0.41277897357940674\n",
            "Epoch 960/1000: Loss: 0.4127911627292633\n",
            "Epoch 961/1000: Loss: 0.41280630230903625\n",
            "Epoch 962/1000: Loss: 0.41282954812049866\n",
            "Epoch 963/1000: Loss: 0.41285383701324463\n",
            "Epoch 964/1000: Loss: 0.4128977656364441\n",
            "Epoch 965/1000: Loss: 0.4129388630390167\n",
            "Epoch 966/1000: Loss: 0.4130282700061798\n",
            "Epoch 967/1000: Loss: 0.413101464509964\n",
            "Epoch 968/1000: Loss: 0.4132922291755676\n",
            "Epoch 969/1000: Loss: 0.4134155809879303\n",
            "Epoch 970/1000: Loss: 0.4138200581073761\n",
            "Epoch 971/1000: Loss: 0.4139738082885742\n",
            "Epoch 972/1000: Loss: 0.414751797914505\n",
            "Epoch 973/1000: Loss: 0.4147450923919678\n",
            "Epoch 974/1000: Loss: 0.41587314009666443\n",
            "Epoch 975/1000: Loss: 0.4152672290802002\n",
            "Epoch 976/1000: Loss: 0.41609227657318115\n",
            "Epoch 977/1000: Loss: 0.4148733615875244\n",
            "Epoch 978/1000: Loss: 0.4147759675979614\n",
            "Epoch 979/1000: Loss: 0.41377103328704834\n",
            "Epoch 980/1000: Loss: 0.41332411766052246\n",
            "Epoch 981/1000: Loss: 0.4129180908203125\n",
            "Epoch 982/1000: Loss: 0.4127611219882965\n",
            "Epoch 983/1000: Loss: 0.4127773940563202\n",
            "Epoch 984/1000: Loss: 0.41291549801826477\n",
            "Epoch 985/1000: Loss: 0.41315773129463196\n",
            "Epoch 986/1000: Loss: 0.41333064436912537\n",
            "Epoch 987/1000: Loss: 0.41364601254463196\n",
            "Epoch 988/1000: Loss: 0.4136096239089966\n",
            "Epoch 989/1000: Loss: 0.41377705335617065\n",
            "Epoch 990/1000: Loss: 0.41350990533828735\n",
            "Epoch 991/1000: Loss: 0.41342848539352417\n",
            "Epoch 992/1000: Loss: 0.4131429195404053\n",
            "Epoch 993/1000: Loss: 0.4129849374294281\n",
            "Epoch 994/1000: Loss: 0.412836492061615\n",
            "Epoch 995/1000: Loss: 0.4127620756626129\n",
            "Epoch 996/1000: Loss: 0.4127373695373535\n",
            "Epoch 997/1000: Loss: 0.41275227069854736\n",
            "Epoch 998/1000: Loss: 0.412796288728714\n",
            "Epoch 999/1000: Loss: 0.41285279393196106\n",
            "Epoch 1000/1000: Loss: 0.41293567419052124\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # random_indexes = torch.randint(dataset.xs.shape[0], (1, dataset.xs.shape[0]))[0]\n",
        "    \n",
        "    # Forward pass\n",
        "    logits = model(dataset.xs)\n",
        "    \n",
        "    # Reshape for loss computation\n",
        "    nan_mask = ~dataset.xs[..., 0].reshape(-1).isnan()\n",
        "    logits_flat = logits.reshape(-1, 2)\n",
        "    labels_flat = dataset.ys[..., 1].reshape(-1).nan_to_num(0).long()\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = criterion(logits_flat[nan_mask], labels_flat[nan_mask])\n",
        "    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "spice",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
