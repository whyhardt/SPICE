{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf7jlYw4NA0v",
        "outputId": "0969ca34-675d-422e-cbfb-7387d9bcd8ad"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/whyhardt/SPICE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oXIbg826NS5i",
        "outputId": "3825864a-cb2d-4ad5-f2e5-79a4e81dfc3e"
      },
      "outputs": [],
      "source": [
        "# !pip install -e SPICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f0uVlABYznR5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from spice.estimator import SpiceEstimator\n",
        "from spice.resources.spice_utils import SpiceConfig\n",
        "from spice.utils.convert_dataset import convert_dataset\n",
        "from spice.resources.rnn import BaseRNN\n",
        "from spice.utils.plotting import plot_session\n",
        "\n",
        "# For custom RNN\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load the data first with the `convert_dataset` method. This method returns a `SpiceDataset` object which we can use right away "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of dataset: torch.Size([1176, 24, 8])\n",
            "Number of participants: 98\n",
            "Number of actions in dataset: 2\n",
            "Number of additional inputs: 1\n"
          ]
        }
      ],
      "source": [
        "# Load your data\n",
        "dataset = convert_dataset(\n",
        "    file = '../data/ganesh2024a/ganesh2024a.csv',\n",
        "    df_participant_id='subjID',\n",
        "    df_choice='chose_high',\n",
        "    df_reward='reward',\n",
        "    df_block='blocks',\n",
        "    additional_inputs=['contrast_difference'],\n",
        "    timeshift_additional_inputs=True,\n",
        "    )\n",
        "\n",
        "# structure of dataset:\n",
        "# dataset has two main attributes: xs -> inputs; ys -> targets (next action)\n",
        "# shape: (n_participants*n_blocks*n_experiments, n_timesteps, features)\n",
        "# features are (n_actions * action, n_actions * reward, n_additional_inputs * additional_input, block_number, experiment_id, participant_id)\n",
        "\n",
        "# in order to set up the participant embedding we have to compute the number of unique participants in our data \n",
        "# to get the number of participants n_participants we do:\n",
        "n_participants = len(dataset.xs[..., -1].unique())\n",
        "\n",
        "print(f\"Shape of dataset: {dataset.xs.shape}\")\n",
        "print(f\"Number of participants: {n_participants}\")\n",
        "n_actions = dataset.ys.shape[-1]\n",
        "print(f\"Number of actions in dataset: {n_actions}\")\n",
        "print(f\"Number of additional inputs: {dataset.xs.shape[-1]-2*n_actions-3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPICE Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to define the configuration for SPICE with a `SpiceConfig` object.\n",
        "\n",
        "The `SpiceConfig` takes as arguments \n",
        "1. `library_setup (dict)`: Defining the variable names of each module.\n",
        "2. `memory_state (dict)`: Defining the memory state variables and their initial values.\n",
        "3. `states_in_logit (list)`: Defining which of the memory state variables are used later for the logit computation. This is necessary for some background processes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "spice_config = SpiceConfig(\n",
        "    library_setup={\n",
        "        'value_reward_chosen': ['contr_diff', 'reward'],\n",
        "        'value_reward_not_chosen': ['contr_diff'],\n",
        "        'value_choice': ['contr_diff', 'choice'],\n",
        "    },\n",
        "    \n",
        "    memory_state={\n",
        "            'value_reward': 0.,\n",
        "            'value_choice': 0.,\n",
        "        }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we are going to define the SPICE model which is a child of the `BaseRNN` and `torch.nn.Module` class and takes as required arguments:\n",
        "1. `spice_config (SpiceConfig)`: previously defined SpiceConfig object\n",
        "2. `n_actions (int)`: number of possible actions in your dataset (including non-displayed ones if applicable).\n",
        "3. `n_participants (int)`: number of participants in your dataset.\n",
        "\n",
        "As usual for a `torch.nn.Module` we have to define at least the `__init__` method and the `forward` method.\n",
        "The `forward` method gets called when computing a forward pass through the model and takes as inputs `(inputs (SpiceDataset.xs), prev_state (dict, default: None), batch_first (bool, default: False))` and returns `(logits (torch.Tensor, shape: (n_participants*n_blocks*n_experiments, timesteps, n_actions)), updated_state (dict))`. Two necessary method calls inside the forward pass are:\n",
        "1. `self.init_forward_pass(inputs, prev_state, batch_first) -> SpiceSignals`: returns a `SpiceSignals` object which carries all relevant information already processed.\n",
        "2. `self.post_forward_pass(SpiceSignals, batch_first) -> SpiceSignals`: does some re-arranging of the logits to adhere to `batch_first`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z0kOR2Qgz0FZ"
      },
      "outputs": [],
      "source": [
        "class SPICERNN(BaseRNN):\n",
        "    \n",
        "    def __init__(self, spice_config, **kwargs):\n",
        "        super().__init__(spice_config=spice_config, **kwargs)\n",
        "        \n",
        "        # participant embedding\n",
        "        self.participant_embedding = self.setup_embedding(num_embeddings=self.n_participants, embedding_size=self.embedding_size, dropout=0.)\n",
        "        \n",
        "        # set up the submodules\n",
        "        self.setup_module(key_module='value_reward_chosen', input_size=2+self.embedding_size)\n",
        "        self.setup_module(key_module='value_reward_not_chosen', input_size=1+self.embedding_size)\n",
        "        self.setup_module(key_module='value_choice', input_size=2+self.embedding_size)\n",
        "        \n",
        "    def forward(self, inputs, prev_state, batch_first=False):\n",
        "        \n",
        "        spice_signals = self.init_forward_pass(inputs, prev_state, batch_first)\n",
        "        \n",
        "        contr_diffs = spice_signals.additional_inputs.repeat(1, 1, self.n_actions)\n",
        "        rewards_chosen = (spice_signals.actions * spice_signals.rewards).sum(dim=-1, keepdim=True).repeat(1, 1, self.n_actions)\n",
        "        \n",
        "        # time-invariant participant features\n",
        "        participant_embeddings = self.participant_embedding(spice_signals.participant_ids)\n",
        "        \n",
        "        for timestep in spice_signals.timesteps:\n",
        "            \n",
        "            # update chosen value\n",
        "            self.call_module(\n",
        "                key_module='value_reward_chosen',\n",
        "                key_state='value_reward',\n",
        "                action_mask=spice_signals.actions[timestep],\n",
        "                inputs=(contr_diffs[timestep], rewards_chosen[timestep]),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "            )\n",
        "            \n",
        "            # update not chosen value\n",
        "            self.call_module(\n",
        "                key_module='value_reward_not_chosen',\n",
        "                key_state='value_reward',\n",
        "                action_mask=1-spice_signals.actions[timestep],\n",
        "                inputs=(contr_diffs[timestep]),  # add input rewards_chosen[timestep] for counterfactual updating (adjust in config as well)\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "            )\n",
        "            \n",
        "            # same for choice values\n",
        "            self.call_module(\n",
        "                key_module='value_choice',\n",
        "                key_state='value_choice',\n",
        "                action_mask=spice_signals.actions[timestep],\n",
        "                inputs=(contr_diffs[timestep], spice_signals.actions[timestep]),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "            )\n",
        "            \n",
        "            spice_signals.logits[timestep] = self.state['value_reward'] + self.state['value_choice']\n",
        "            \n",
        "        spice_signals = self.post_forward_pass(spice_signals, batch_first)\n",
        "        \n",
        "        return spice_signals.logits, self.get_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's setup now the `SpiceEstimator` object and fit it to the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_spice = '../params/ganesh2024a/spice_ganesh2024a.pkl'\n",
        "\n",
        "estimator = SpiceEstimator(\n",
        "        # model paramaeters\n",
        "        rnn_class=SPICERNN,\n",
        "        spice_config=spice_config,\n",
        "        n_actions=2,\n",
        "        n_participants=n_participants,\n",
        "        \n",
        "        # rnn training parameters\n",
        "        epochs=1000,\n",
        "        warmup_steps=200,\n",
        "        learning_rate=0.01,\n",
        "        \n",
        "        # sindy fitting parameters\n",
        "        sindy_weight=0.1,\n",
        "        sindy_threshold=0.05,\n",
        "        sindy_threshold_frequency=1,\n",
        "        sindy_threshold_terms=1,\n",
        "        sindy_cutoff_patience=100,\n",
        "        sindy_epochs=1000,\n",
        "        sindy_alpha=0.0001,\n",
        "        sindy_library_polynomial_degree=2,\n",
        "        sindy_ensemble_size=1,\n",
        "        \n",
        "        # additional generalization parameters\n",
        "        batch_size=1024,\n",
        "        bagging=True,\n",
        "        scheduler=True,\n",
        "        \n",
        "        verbose=True,\n",
        "        save_path_spice=path_spice,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "3EnmDiUMWq6e",
        "outputId": "e53b1bbd-4173-4d2c-bcdc-15832bc31bd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training on cpu...\n",
            "================================================================================\n",
            "\n",
            "Training the RNN...\n",
            "================================================================================\n",
            "Epoch 1/1000 --- L(Train): 0.6989506 --- L(Val, RNN): 0.6408207 --- L(Val, SINDy): 0.7097650 --- Time: 0.43s; --- Convergence: 6.80e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.006 1 + 0.999 value_reward_chosen[t] + -0.002 contr_diff + -0.003 reward + -0.0 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.001 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + -0.002 reward^2 \n",
            "value_reward_not_chosen[t+1] = 0.006 1 + 0.998 value_reward_not_chosen[t] + -0.007 contr_diff + -0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = -0.008 1 + 0.996 value_choice[t] + 0.006 contr_diff + -0.008 choice + 0.003 value_choice^2 + -0.002 value_choice*contr_diff + -0.003 value_choice*choice + -0.003 contr_diff^2 + 0.006 contr_diff*choice + -0.006 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 2/1000 --- L(Train): 0.6397673 --- L(Val, RNN): 0.5986323 --- L(Val, SINDy): 0.6759062 --- Time: 0.30s; --- Convergence: 3.61e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.011 1 + 0.996 value_reward_chosen[t] + -0.003 contr_diff + 0.0 reward + -0.0 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.002 value_reward_chosen*reward + -0.003 contr_diff^2 + -0.001 contr_diff*reward + 0.001 reward^2 \n",
            "value_reward_not_chosen[t+1] = 0.013 1 + 0.996 value_reward_not_chosen[t] + -0.014 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = -0.016 1 + 0.989 value_choice[t] + 0.009 contr_diff + -0.016 choice + 0.009 value_choice^2 + -0.001 value_choice*contr_diff + -0.01 value_choice*choice + -0.005 contr_diff^2 + 0.009 contr_diff*choice + -0.014 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 3/1000 --- L(Train): 0.6053283 --- L(Val, RNN): 0.5662715 --- L(Val, SINDy): 0.6216979 --- Time: 0.36s; --- Convergence: 1.97e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.01 1 + 0.993 value_reward_chosen[t] + -0.004 contr_diff + 0.006 reward + -0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.003 value_reward_chosen*reward + -0.005 contr_diff^2 + -0.001 contr_diff*reward + 0.007 reward^2 \n",
            "value_reward_not_chosen[t+1] = 0.018 1 + 0.994 value_reward_not_chosen[t] + -0.02 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.006 contr_diff^2 \n",
            "value_choice[t+1] = -0.011 1 + 0.981 value_choice[t] + 0.008 contr_diff + -0.011 choice + 0.014 value_choice^2 + 0.001 value_choice*contr_diff + -0.018 value_choice*choice + -0.004 contr_diff^2 + 0.007 contr_diff*choice + -0.01 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 4/1000 --- L(Train): 0.5640646 --- L(Val, RNN): 0.5404323 --- L(Val, SINDy): 0.5731428 --- Time: 0.40s; --- Convergence: 1.11e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.007 1 + 0.992 value_reward_chosen[t] + -0.004 contr_diff + 0.011 reward + -0.001 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.003 value_reward_chosen*reward + -0.005 contr_diff^2 + -0.001 contr_diff*reward + 0.012 reward^2 \n",
            "value_reward_not_chosen[t+1] = 0.015 1 + 0.991 value_reward_not_chosen[t] + -0.025 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.006 contr_diff^2 \n",
            "value_choice[t+1] = -0.004 1 + 0.977 value_choice[t] + 0.003 contr_diff + -0.004 choice + 0.019 value_choice^2 + 0.001 value_choice*contr_diff + -0.022 value_choice*choice + -0.0 contr_diff^2 + 0.002 contr_diff*choice + -0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 5/1000 --- L(Train): 0.5407761 --- L(Val, RNN): 0.5193925 --- L(Val, SINDy): 0.5379621 --- Time: 0.34s; --- Convergence: 6.61e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.004 1 + 0.989 value_reward_chosen[t] + -0.004 contr_diff + 0.009 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.005 value_reward_chosen*reward + -0.004 contr_diff^2 + -0.001 contr_diff*reward + 0.01 reward^2 \n",
            "value_reward_not_chosen[t+1] = 0.01 1 + 0.987 value_reward_not_chosen[t] + -0.028 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.003 1 + 0.979 value_choice[t] + -0.002 contr_diff + 0.003 choice + 0.026 value_choice^2 + -0.001 value_choice*contr_diff + -0.02 value_choice*choice + 0.004 contr_diff^2 + -0.003 contr_diff*choice + 0.005 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 6/1000 --- L(Train): 0.5092751 --- L(Val, RNN): 0.5019832 --- L(Val, SINDy): 0.5131063 --- Time: 0.27s; --- Convergence: 4.18e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.001 1 + 0.986 value_reward_chosen[t] + -0.004 contr_diff + 0.007 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.007 value_reward_chosen*reward + -0.003 contr_diff^2 + 0.0 contr_diff*reward + 0.008 reward^2 \n",
            "value_reward_not_chosen[t+1] = 0.003 1 + 0.984 value_reward_not_chosen[t] + -0.029 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.011 1 + 0.982 value_choice[t] + -0.009 contr_diff + 0.011 choice + 0.032 value_choice^2 + -0.004 value_choice*contr_diff + -0.017 value_choice*choice + 0.01 contr_diff^2 + -0.01 contr_diff*choice + 0.013 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 7/1000 --- L(Train): 0.5120275 --- L(Val, RNN): 0.4883202 --- L(Val, SINDy): 0.4967899 --- Time: 0.29s; --- Convergence: 2.77e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.006 1 + 0.982 value_reward_chosen[t] + -0.004 contr_diff + 0.007 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.01 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.008 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.005 1 + 0.98 value_reward_not_chosen[t] + -0.029 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.019 1 + 0.981 value_choice[t] + -0.015 contr_diff + 0.019 choice + 0.032 value_choice^2 + -0.008 value_choice*contr_diff + -0.018 value_choice*choice + 0.015 contr_diff^2 + -0.016 contr_diff*choice + 0.021 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 8/1000 --- L(Train): 0.4926227 --- L(Val, RNN): 0.4784069 --- L(Val, SINDy): 0.4886358 --- Time: 0.29s; --- Convergence: 1.88e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.012 1 + 0.977 value_reward_chosen[t] + -0.001 contr_diff + 0.012 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.014 value_reward_chosen*reward + 0.004 contr_diff^2 + 0.003 contr_diff*reward + 0.013 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.012 1 + 0.975 value_reward_not_chosen[t] + -0.026 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.007 contr_diff^2 \n",
            "value_choice[t+1] = 0.028 1 + 0.976 value_choice[t] + -0.022 contr_diff + 0.028 choice + 0.027 value_choice^2 + -0.011 value_choice*contr_diff + -0.022 value_choice*choice + 0.021 contr_diff^2 + -0.023 contr_diff*choice + 0.029 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 9/1000 --- L(Train): 0.4895141 --- L(Val, RNN): 0.4727741 --- L(Val, SINDy): 0.4882984 --- Time: 0.41s; --- Convergence: 1.22e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.019 1 + 0.971 value_reward_chosen[t] + 0.004 contr_diff + 0.017 reward + -0.002 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.02 value_reward_chosen*reward + 0.009 contr_diff^2 + 0.007 contr_diff*reward + 0.018 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.021 1 + 0.969 value_reward_not_chosen[t] + -0.023 contr_diff + -0.004 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.036 1 + 0.97 value_choice[t] + -0.029 contr_diff + 0.036 choice + 0.021 value_choice^2 + -0.011 value_choice*contr_diff + -0.029 value_choice*choice + 0.027 contr_diff^2 + -0.03 contr_diff*choice + 0.038 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 10/1000 --- L(Train): 0.4775447 --- L(Val, RNN): 0.4711669 --- L(Val, SINDy): 0.4946911 --- Time: 0.37s; --- Convergence: 6.92e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.026 1 + 0.964 value_reward_chosen[t] + 0.007 contr_diff + 0.02 reward + -0.004 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.026 value_reward_chosen*reward + 0.014 contr_diff^2 + 0.01 contr_diff*reward + 0.021 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.029 1 + 0.962 value_reward_not_chosen[t] + -0.02 contr_diff + -0.007 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.018 contr_diff^2 \n",
            "value_choice[t+1] = 0.045 1 + 0.963 value_choice[t] + -0.035 contr_diff + 0.045 choice + 0.014 value_choice^2 + -0.011 value_choice*contr_diff + -0.035 value_choice*choice + 0.032 contr_diff^2 + -0.036 contr_diff*choice + 0.047 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 11/1000 --- L(Train): 0.4690082 --- L(Val, RNN): 0.4714539 --- L(Val, SINDy): 0.5050050 --- Time: 0.41s; --- Convergence: 3.60e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.033 1 + 0.957 value_reward_chosen[t] + 0.012 contr_diff + 0.022 reward + -0.007 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.033 value_reward_chosen*reward + 0.02 contr_diff^2 + 0.015 contr_diff*reward + 0.023 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.038 1 + 0.955 value_reward_not_chosen[t] + -0.015 contr_diff + -0.011 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.024 contr_diff^2 \n",
            "value_choice[t+1] = 0.054 1 + 0.964 value_choice[t] + -0.038 contr_diff + 0.054 choice + 0.013 value_choice^2 + -0.006 value_choice*contr_diff + -0.035 value_choice*choice + 0.039 contr_diff^2 + -0.039 contr_diff*choice + 0.055 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 12/1000 --- L(Train): 0.4775236 --- L(Val, RNN): 0.4716891 --- L(Val, SINDy): 0.5145251 --- Time: 0.44s; --- Convergence: 1.92e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.042 1 + 0.949 value_reward_chosen[t] + 0.014 contr_diff + 0.025 reward + -0.012 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.04 value_reward_chosen*reward + 0.026 contr_diff^2 + 0.017 contr_diff*reward + 0.026 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.047 1 + 0.947 value_reward_not_chosen[t] + -0.01 contr_diff + -0.015 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.031 contr_diff^2 \n",
            "value_choice[t+1] = 0.063 1 + 0.967 value_choice[t] + -0.038 contr_diff + 0.063 choice + 0.017 value_choice^2 + -0.001 value_choice*contr_diff + -0.031 value_choice*choice + 0.045 contr_diff^2 + -0.039 contr_diff*choice + 0.065 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 13/1000 --- L(Train): 0.4651267 --- L(Val, RNN): 0.4704445 --- L(Val, SINDy): 0.5158859 --- Time: 0.38s; --- Convergence: 1.58e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.05 1 + 0.94 value_reward_chosen[t] + 0.013 contr_diff + 0.022 reward + -0.017 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.048 value_reward_chosen*reward + 0.033 contr_diff^2 + 0.016 contr_diff*reward + 0.023 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.057 1 + 0.939 value_reward_not_chosen[t] + -0.003 contr_diff + -0.02 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.038 contr_diff^2 \n",
            "value_choice[t+1] = 0.072 1 + 0.973 value_choice[t] + -0.036 contr_diff + 0.072 choice + 0.023 value_choice^2 + 0.006 value_choice*contr_diff + -0.026 value_choice*choice + 0.052 contr_diff^2 + -0.037 contr_diff*choice + 0.074 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 14/1000 --- L(Train): 0.4683891 --- L(Val, RNN): 0.4678108 --- L(Val, SINDy): 0.5102481 --- Time: 0.29s; --- Convergence: 2.11e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.058 1 + 0.932 value_reward_chosen[t] + 0.011 contr_diff + 0.019 reward + -0.024 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + -0.056 value_reward_chosen*reward + 0.041 contr_diff^2 + 0.014 contr_diff*reward + 0.02 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.067 1 + 0.93 value_reward_not_chosen[t] + 0.003 contr_diff + -0.027 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.046 contr_diff^2 \n",
            "value_choice[t+1] = 0.078 1 + 0.976 value_choice[t] + -0.032 contr_diff + 0.078 choice + 0.025 value_choice^2 + 0.014 value_choice*contr_diff + -0.023 value_choice*choice + 0.057 contr_diff^2 + -0.033 contr_diff*choice + 0.08 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 15/1000 --- L(Train): 0.4700564 --- L(Val, RNN): 0.4644897 --- L(Val, SINDy): 0.5009440 --- Time: 0.31s; --- Convergence: 2.71e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.068 1 + 0.923 value_reward_chosen[t] + 0.008 contr_diff + 0.017 reward + -0.031 value_reward_chosen^2 + -0.007 value_reward_chosen*contr_diff + -0.065 value_reward_chosen*reward + 0.049 contr_diff^2 + 0.011 contr_diff*reward + 0.018 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.077 1 + 0.921 value_reward_not_chosen[t] + 0.01 contr_diff + -0.033 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.054 contr_diff^2 \n",
            "value_choice[t+1] = 0.08 1 + 0.974 value_choice[t] + -0.027 contr_diff + 0.08 choice + 0.023 value_choice^2 + 0.021 value_choice*contr_diff + -0.025 value_choice*choice + 0.058 contr_diff^2 + -0.028 contr_diff*choice + 0.082 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 16/1000 --- L(Train): 0.4628086 --- L(Val, RNN): 0.4609782 --- L(Val, SINDy): 0.4913879 --- Time: 0.36s; --- Convergence: 3.11e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.076 1 + 0.913 value_reward_chosen[t] + 0.006 contr_diff + 0.013 reward + -0.038 value_reward_chosen^2 + -0.009 value_reward_chosen*contr_diff + -0.075 value_reward_chosen*reward + 0.056 contr_diff^2 + 0.009 contr_diff*reward + 0.014 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.085 1 + 0.912 value_reward_not_chosen[t] + 0.015 contr_diff + -0.038 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.061 contr_diff^2 \n",
            "value_choice[t+1] = 0.08 1 + 0.97 value_choice[t] + -0.022 contr_diff + 0.08 choice + 0.018 value_choice^2 + 0.028 value_choice*contr_diff + -0.028 value_choice*choice + 0.058 contr_diff^2 + -0.023 contr_diff*choice + 0.081 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 17/1000 --- L(Train): 0.4596286 --- L(Val, RNN): 0.4577130 --- L(Val, SINDy): 0.4823987 --- Time: 0.29s; --- Convergence: 3.19e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.085 1 + 0.903 value_reward_chosen[t] + 0.001 contr_diff + 0.01 reward + -0.046 value_reward_chosen^2 + -0.013 value_reward_chosen*contr_diff + -0.084 value_reward_chosen*reward + 0.065 contr_diff^2 + 0.004 contr_diff*reward + 0.011 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.094 1 + 0.902 value_reward_not_chosen[t] + 0.021 contr_diff + -0.043 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.069 contr_diff^2 \n",
            "value_choice[t+1] = 0.077 1 + 0.966 value_choice[t] + -0.017 contr_diff + 0.077 choice + 0.014 value_choice^2 + 0.035 value_choice*contr_diff + -0.032 value_choice*choice + 0.056 contr_diff^2 + -0.018 contr_diff*choice + 0.079 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 18/1000 --- L(Train): 0.4693031 --- L(Val, RNN): 0.4548655 --- L(Val, SINDy): 0.4743173 --- Time: 0.36s; --- Convergence: 3.02e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.095 1 + 0.893 value_reward_chosen[t] + -0.004 contr_diff + 0.008 reward + -0.054 value_reward_chosen^2 + -0.016 value_reward_chosen*contr_diff + -0.094 value_reward_chosen*reward + 0.074 contr_diff^2 + -0.001 contr_diff*reward + 0.009 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.101 1 + 0.892 value_reward_not_chosen[t] + 0.026 contr_diff + -0.047 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.077 contr_diff^2 \n",
            "value_choice[t+1] = 0.073 1 + 0.963 value_choice[t] + -0.012 contr_diff + 0.073 choice + 0.011 value_choice^2 + 0.04 value_choice*contr_diff + -0.036 value_choice*choice + 0.052 contr_diff^2 + -0.013 contr_diff*choice + 0.075 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 19/1000 --- L(Train): 0.4615328 --- L(Val, RNN): 0.4525275 --- L(Val, SINDy): 0.4675263 --- Time: 0.35s; --- Convergence: 2.68e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.104 1 + 0.882 value_reward_chosen[t] + -0.005 contr_diff + 0.008 reward + -0.063 value_reward_chosen^2 + -0.021 value_reward_chosen*contr_diff + -0.105 value_reward_chosen*reward + 0.083 contr_diff^2 + -0.003 contr_diff*reward + 0.008 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.106 1 + 0.882 value_reward_not_chosen[t] + 0.026 contr_diff + -0.048 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.085 contr_diff^2 \n",
            "value_choice[t+1] = 0.068 1 + 0.961 value_choice[t] + -0.007 contr_diff + 0.068 choice + 0.01 value_choice^2 + 0.043 value_choice*contr_diff + -0.038 value_choice*choice + 0.046 contr_diff^2 + -0.008 contr_diff*choice + 0.07 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 20/1000 --- L(Train): 0.4539353 --- L(Val, RNN): 0.4507542 --- L(Val, SINDy): 0.4624746 --- Time: 0.31s; --- Convergence: 2.23e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.115 1 + 0.872 value_reward_chosen[t] + -0.006 contr_diff + 0.01 reward + -0.072 value_reward_chosen^2 + -0.024 value_reward_chosen*contr_diff + -0.115 value_reward_chosen*reward + 0.092 contr_diff^2 + -0.004 contr_diff*reward + 0.01 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.11 1 + 0.871 value_reward_not_chosen[t] + 0.026 contr_diff + -0.048 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.093 contr_diff^2 \n",
            "value_choice[t+1] = 0.063 1 + 0.961 value_choice[t] + -0.002 contr_diff + 0.063 choice + 0.011 value_choice^2 + 0.045 value_choice*contr_diff + -0.038 value_choice*choice + 0.041 contr_diff^2 + -0.003 contr_diff*choice + 0.065 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 21/1000 --- L(Train): 0.4544056 --- L(Val, RNN): 0.4495764 --- L(Val, SINDy): 0.4589231 --- Time: 0.29s; --- Convergence: 1.70e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.125 1 + 0.861 value_reward_chosen[t] + -0.006 contr_diff + 0.013 reward + -0.081 value_reward_chosen^2 + -0.029 value_reward_chosen*contr_diff + -0.126 value_reward_chosen*reward + 0.101 contr_diff^2 + -0.006 contr_diff*reward + 0.014 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.114 1 + 0.86 value_reward_not_chosen[t] + 0.023 contr_diff + -0.049 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.1 contr_diff^2 \n",
            "value_choice[t+1] = 0.058 1 + 0.961 value_choice[t] + -0.0 contr_diff + 0.058 choice + 0.012 value_choice^2 + 0.044 value_choice*contr_diff + -0.038 value_choice*choice + 0.034 contr_diff^2 + -0.001 contr_diff*choice + 0.059 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 22/1000 --- L(Train): 0.4599746 --- L(Val, RNN): 0.4487923 --- L(Val, SINDy): 0.4554925 --- Time: 0.40s; --- Convergence: 1.24e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.136 1 + 0.849 value_reward_chosen[t] + -0.005 contr_diff + 0.015 reward + -0.091 value_reward_chosen^2 + -0.033 value_reward_chosen*contr_diff + -0.137 value_reward_chosen*reward + 0.111 contr_diff^2 + -0.007 contr_diff*reward + 0.016 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.117 1 + 0.849 value_reward_not_chosen[t] + 0.021 contr_diff + -0.048 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.107 contr_diff^2 \n",
            "value_choice[t+1] = 0.052 1 + 0.962 value_choice[t] + 0.001 contr_diff + 0.052 choice + 0.014 value_choice^2 + 0.042 value_choice*contr_diff + -0.037 value_choice*choice + 0.028 contr_diff^2 + 0.0 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 23/1000 --- L(Train): 0.4516614 --- L(Val, RNN): 0.4482671 --- L(Val, SINDy): 0.4521121 --- Time: 0.31s; --- Convergence: 8.84e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.147 1 + 0.838 value_reward_chosen[t] + -0.004 contr_diff + 0.016 reward + -0.101 value_reward_chosen^2 + -0.039 value_reward_chosen*contr_diff + -0.148 value_reward_chosen*reward + 0.12 contr_diff^2 + -0.009 contr_diff*reward + 0.017 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.118 1 + 0.838 value_reward_not_chosen[t] + 0.017 contr_diff + -0.046 value_reward_not_chosen^2 + 0.005 value_reward_not_chosen*contr_diff + -0.114 contr_diff^2 \n",
            "value_choice[t+1] = 0.047 1 + 0.963 value_choice[t] + -0.0 contr_diff + 0.047 choice + 0.016 value_choice^2 + 0.039 value_choice*contr_diff + -0.035 value_choice*choice + 0.021 contr_diff^2 + -0.001 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 24/1000 --- L(Train): 0.4468510 --- L(Val, RNN): 0.4477033 --- L(Val, SINDy): 0.4497763 --- Time: 0.34s; --- Convergence: 7.24e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.157 1 + 0.827 value_reward_chosen[t] + -0.003 contr_diff + 0.013 reward + -0.11 value_reward_chosen^2 + -0.045 value_reward_chosen*contr_diff + -0.16 value_reward_chosen*reward + 0.13 contr_diff^2 + -0.013 contr_diff*reward + 0.014 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.117 1 + 0.827 value_reward_not_chosen[t] + 0.014 contr_diff + -0.043 value_reward_not_chosen^2 + 0.007 value_reward_not_chosen*contr_diff + -0.12 contr_diff^2 \n",
            "value_choice[t+1] = 0.043 1 + 0.966 value_choice[t] + -0.003 contr_diff + 0.043 choice + 0.019 value_choice^2 + 0.035 value_choice*contr_diff + -0.032 value_choice*choice + 0.015 contr_diff^2 + -0.004 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 25/1000 --- L(Train): 0.4424058 --- L(Val, RNN): 0.4468991 --- L(Val, SINDy): 0.4483493 --- Time: 0.41s; --- Convergence: 7.64e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.167 1 + 0.815 value_reward_chosen[t] + -0.001 contr_diff + 0.011 reward + -0.12 value_reward_chosen^2 + -0.05 value_reward_chosen*contr_diff + -0.171 value_reward_chosen*reward + 0.139 contr_diff^2 + -0.014 contr_diff*reward + 0.012 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.116 1 + 0.815 value_reward_not_chosen[t] + 0.011 contr_diff + -0.039 value_reward_not_chosen^2 + 0.009 value_reward_not_chosen*contr_diff + -0.126 contr_diff^2 \n",
            "value_choice[t+1] = 0.039 1 + 0.969 value_choice[t] + -0.005 contr_diff + 0.039 choice + 0.022 value_choice^2 + 0.03 value_choice*contr_diff + -0.029 value_choice*choice + 0.009 contr_diff^2 + -0.006 contr_diff*choice + 0.04 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 26/1000 --- L(Train): 0.4301813 --- L(Val, RNN): 0.4459804 --- L(Val, SINDy): 0.4471935 --- Time: 0.31s; --- Convergence: 8.41e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.177 1 + 0.804 value_reward_chosen[t] + 0.002 contr_diff + 0.009 reward + -0.13 value_reward_chosen^2 + -0.053 value_reward_chosen*contr_diff + -0.182 value_reward_chosen*reward + 0.148 contr_diff^2 + -0.015 contr_diff*reward + 0.01 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.115 1 + 0.804 value_reward_not_chosen[t] + 0.008 contr_diff + -0.036 value_reward_not_chosen^2 + 0.009 value_reward_not_chosen*contr_diff + -0.132 contr_diff^2 \n",
            "value_choice[t+1] = 0.035 1 + 0.973 value_choice[t] + -0.008 contr_diff + 0.035 choice + 0.025 value_choice^2 + 0.026 value_choice*contr_diff + -0.026 value_choice*choice + 0.004 contr_diff^2 + -0.009 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 27/1000 --- L(Train): 0.4487169 --- L(Val, RNN): 0.4449351 --- L(Val, SINDy): 0.4466447 --- Time: 0.33s; --- Convergence: 9.43e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.187 1 + 0.792 value_reward_chosen[t] + 0.004 contr_diff + 0.007 reward + -0.14 value_reward_chosen^2 + -0.057 value_reward_chosen*contr_diff + -0.194 value_reward_chosen*reward + 0.156 contr_diff^2 + -0.017 contr_diff*reward + 0.008 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.114 1 + 0.793 value_reward_not_chosen[t] + 0.005 contr_diff + -0.033 value_reward_not_chosen^2 + 0.011 value_reward_not_chosen*contr_diff + -0.136 contr_diff^2 \n",
            "value_choice[t+1] = 0.033 1 + 0.976 value_choice[t] + -0.01 contr_diff + 0.033 choice + 0.028 value_choice^2 + 0.022 value_choice*contr_diff + -0.023 value_choice*choice + -0.0 contr_diff^2 + -0.011 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 28/1000 --- L(Train): 0.4463908 --- L(Val, RNN): 0.4438064 --- L(Val, SINDy): 0.4462127 --- Time: 0.29s; --- Convergence: 1.04e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.196 1 + 0.78 value_reward_chosen[t] + 0.007 contr_diff + 0.007 reward + -0.151 value_reward_chosen^2 + -0.059 value_reward_chosen*contr_diff + -0.205 value_reward_chosen*reward + 0.163 contr_diff^2 + -0.018 contr_diff*reward + 0.008 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.113 1 + 0.782 value_reward_not_chosen[t] + 0.002 contr_diff + -0.029 value_reward_not_chosen^2 + 0.012 value_reward_not_chosen*contr_diff + -0.141 contr_diff^2 \n",
            "value_choice[t+1] = 0.032 1 + 0.98 value_choice[t] + -0.012 contr_diff + 0.032 choice + 0.031 value_choice^2 + 0.018 value_choice*contr_diff + -0.019 value_choice*choice + -0.004 contr_diff^2 + -0.013 contr_diff*choice + 0.033 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 29/1000 --- L(Train): 0.4492468 --- L(Val, RNN): 0.4427207 --- L(Val, SINDy): 0.4455991 --- Time: 0.34s; --- Convergence: 1.06e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.204 1 + 0.768 value_reward_chosen[t] + 0.012 contr_diff + 0.009 reward + -0.162 value_reward_chosen^2 + -0.061 value_reward_chosen*contr_diff + -0.216 value_reward_chosen*reward + 0.17 contr_diff^2 + -0.017 contr_diff*reward + 0.01 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.112 1 + 0.771 value_reward_not_chosen[t] + -0.0 contr_diff + -0.026 value_reward_not_chosen^2 + 0.012 value_reward_not_chosen*contr_diff + -0.145 contr_diff^2 \n",
            "value_choice[t+1] = 0.031 1 + 0.983 value_choice[t] + -0.013 contr_diff + 0.031 choice + 0.033 value_choice^2 + 0.015 value_choice*contr_diff + -0.016 value_choice*choice + -0.006 contr_diff^2 + -0.014 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 30/1000 --- L(Train): 0.4538268 --- L(Val, RNN): 0.4417399 --- L(Val, SINDy): 0.4444873 --- Time: 0.37s; --- Convergence: 1.02e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.213 1 + 0.756 value_reward_chosen[t] + 0.015 contr_diff + 0.011 reward + -0.174 value_reward_chosen^2 + -0.063 value_reward_chosen*contr_diff + -0.228 value_reward_chosen*reward + 0.177 contr_diff^2 + -0.018 contr_diff*reward + 0.011 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.111 1 + 0.759 value_reward_not_chosen[t] + -0.001 contr_diff + -0.023 value_reward_not_chosen^2 + 0.013 value_reward_not_chosen*contr_diff + -0.149 contr_diff^2 \n",
            "value_choice[t+1] = 0.03 1 + 0.983 value_choice[t] + -0.014 contr_diff + 0.03 choice + 0.033 value_choice^2 + 0.012 value_choice*contr_diff + -0.016 value_choice*choice + -0.008 contr_diff^2 + -0.015 contr_diff*choice + 0.031 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 31/1000 --- L(Train): 0.4459712 --- L(Val, RNN): 0.4409661 --- L(Val, SINDy): 0.4441972 --- Time: 0.46s; --- Convergence: 8.97e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.223 1 + 0.744 value_reward_chosen[t] + 0.017 contr_diff + 0.015 reward + -0.185 value_reward_chosen^2 + -0.065 value_reward_chosen*contr_diff + -0.239 value_reward_chosen*reward + 0.183 contr_diff^2 + -0.019 contr_diff*reward + 0.015 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.111 1 + 0.747 value_reward_not_chosen[t] + -0.001 contr_diff + -0.023 value_reward_not_chosen^2 + 0.014 value_reward_not_chosen*contr_diff + -0.153 contr_diff^2 \n",
            "value_choice[t+1] = 0.029 1 + 0.982 value_choice[t] + -0.014 contr_diff + 0.029 choice + 0.031 value_choice^2 + 0.01 value_choice*contr_diff + -0.017 value_choice*choice + -0.01 contr_diff^2 + -0.015 contr_diff*choice + 0.031 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 32/1000 --- L(Train): 0.4412989 --- L(Val, RNN): 0.4403380 --- L(Val, SINDy): 0.4444178 --- Time: 0.29s; --- Convergence: 7.63e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.233 1 + 0.731 value_reward_chosen[t] + 0.021 contr_diff + 0.02 reward + -0.197 value_reward_chosen^2 + -0.065 value_reward_chosen*contr_diff + -0.251 value_reward_chosen*reward + 0.188 contr_diff^2 + -0.017 contr_diff*reward + 0.02 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.112 1 + 0.735 value_reward_not_chosen[t] + 0.001 contr_diff + -0.025 value_reward_not_chosen^2 + 0.013 value_reward_not_chosen*contr_diff + -0.156 contr_diff^2 \n",
            "value_choice[t+1] = 0.031 1 + 0.981 value_choice[t] + -0.013 contr_diff + 0.031 choice + 0.029 value_choice^2 + 0.009 value_choice*contr_diff + -0.018 value_choice*choice + -0.009 contr_diff^2 + -0.013 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 33/1000 --- L(Train): 0.4399400 --- L(Val, RNN): 0.4397188 --- L(Val, SINDy): 0.4447024 --- Time: 0.39s; --- Convergence: 6.91e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.242 1 + 0.719 value_reward_chosen[t] + 0.023 contr_diff + 0.025 reward + -0.208 value_reward_chosen^2 + -0.066 value_reward_chosen*contr_diff + -0.263 value_reward_chosen*reward + 0.193 contr_diff^2 + -0.016 contr_diff*reward + 0.026 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.113 1 + 0.724 value_reward_not_chosen[t] + 0.005 contr_diff + -0.028 value_reward_not_chosen^2 + 0.013 value_reward_not_chosen*contr_diff + -0.159 contr_diff^2 \n",
            "value_choice[t+1] = 0.033 1 + 0.981 value_choice[t] + -0.01 contr_diff + 0.033 choice + 0.027 value_choice^2 + 0.009 value_choice*contr_diff + -0.018 value_choice*choice + -0.008 contr_diff^2 + -0.011 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 34/1000 --- L(Train): 0.4468850 --- L(Val, RNN): 0.4391591 --- L(Val, SINDy): 0.4450260 --- Time: 0.31s; --- Convergence: 6.25e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.249 1 + 0.706 value_reward_chosen[t] + 0.026 contr_diff + 0.029 reward + -0.22 value_reward_chosen^2 + -0.066 value_reward_chosen*contr_diff + -0.275 value_reward_chosen*reward + 0.196 contr_diff^2 + -0.016 contr_diff*reward + 0.029 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.111 1 + 0.712 value_reward_not_chosen[t] + 0.011 contr_diff + -0.032 value_reward_not_chosen^2 + 0.011 value_reward_not_chosen*contr_diff + -0.16 contr_diff^2 \n",
            "value_choice[t+1] = 0.037 1 + 0.98 value_choice[t] + -0.007 contr_diff + 0.037 choice + 0.025 value_choice^2 + 0.01 value_choice*contr_diff + -0.019 value_choice*choice + -0.004 contr_diff^2 + -0.008 contr_diff*choice + 0.039 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 35/1000 --- L(Train): 0.4392131 --- L(Val, RNN): 0.4386636 --- L(Val, SINDy): 0.4453295 --- Time: 0.36s; --- Convergence: 5.60e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.254 1 + 0.693 value_reward_chosen[t] + 0.026 contr_diff + 0.03 reward + -0.233 value_reward_chosen^2 + -0.066 value_reward_chosen*contr_diff + -0.287 value_reward_chosen*reward + 0.199 contr_diff^2 + -0.016 contr_diff*reward + 0.031 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.107 1 + 0.7 value_reward_not_chosen[t] + 0.018 contr_diff + -0.034 value_reward_not_chosen^2 + 0.009 value_reward_not_chosen*contr_diff + -0.16 contr_diff^2 \n",
            "value_choice[t+1] = 0.042 1 + 0.98 value_choice[t] + -0.004 contr_diff + 0.042 choice + 0.021 value_choice^2 + 0.011 value_choice*contr_diff + -0.019 value_choice*choice + -0.0 contr_diff^2 + -0.005 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 36/1000 --- L(Train): 0.4421874 --- L(Val, RNN): 0.4380553 --- L(Val, SINDy): 0.4482693 --- Time: 0.30s; --- Convergence: 5.84e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.257 1 + 0.68 value_reward_chosen[t] + 0.026 contr_diff + 0.032 reward + -0.245 value_reward_chosen^2 + -0.065 value_reward_chosen*contr_diff + -0.299 value_reward_chosen*reward + 0.2 contr_diff^2 + -0.016 contr_diff*reward + 0.032 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.1 1 + 0.688 value_reward_not_chosen[t] + 0.026 contr_diff + -0.038 value_reward_not_chosen^2 + 0.006 value_reward_not_chosen*contr_diff + -0.157 contr_diff^2 \n",
            "value_choice[t+1] = 0.048 1 + 0.98 value_choice[t] + -0.001 contr_diff + 0.048 choice + 0.018 value_choice^2 + 0.012 value_choice*contr_diff + -0.019 value_choice*choice + 0.004 contr_diff^2 + -0.002 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 37/1000 --- L(Train): 0.4519296 --- L(Val, RNN): 0.4373521 --- L(Val, SINDy): 0.4483671 --- Time: 0.31s; --- Convergence: 6.44e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.261 1 + 0.667 value_reward_chosen[t] + 0.023 contr_diff + 0.035 reward + -0.258 value_reward_chosen^2 + -0.067 value_reward_chosen*contr_diff + -0.311 value_reward_chosen*reward + 0.2 contr_diff^2 + -0.018 contr_diff*reward + 0.036 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.091 1 + 0.676 value_reward_not_chosen[t] + 0.036 contr_diff + -0.041 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.151 contr_diff^2 \n",
            "value_choice[t+1] = 0.052 1 + 0.979 value_choice[t] + 0.001 contr_diff + 0.052 choice + 0.014 value_choice^2 + 0.012 value_choice*contr_diff + -0.02 value_choice*choice + 0.008 contr_diff^2 + 0.0 contr_diff*choice + 0.053 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 38/1000 --- L(Train): 0.4350812 --- L(Val, RNN): 0.4366230 --- L(Val, SINDy): 0.4467309 --- Time: 0.28s; --- Convergence: 6.86e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.265 1 + 0.654 value_reward_chosen[t] + 0.019 contr_diff + 0.041 reward + -0.271 value_reward_chosen^2 + -0.068 value_reward_chosen*contr_diff + -0.322 value_reward_chosen*reward + 0.2 contr_diff^2 + -0.02 contr_diff*reward + 0.041 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.082 1 + 0.664 value_reward_not_chosen[t] + 0.046 contr_diff + -0.046 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.145 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.977 value_choice[t] + 0.003 contr_diff + 0.055 choice + 0.011 value_choice^2 + 0.011 value_choice*contr_diff + -0.021 value_choice*choice + 0.012 contr_diff^2 + 0.002 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 39/1000 --- L(Train): 0.4368492 --- L(Val, RNN): 0.4359302 --- L(Val, SINDy): 0.4453634 --- Time: 0.31s; --- Convergence: 6.90e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.27 1 + 0.642 value_reward_chosen[t] + 0.017 contr_diff + 0.048 reward + -0.284 value_reward_chosen^2 + -0.067 value_reward_chosen*contr_diff + -0.332 value_reward_chosen*reward + 0.198 contr_diff^2 + -0.02 contr_diff*reward + 0.049 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.072 1 + 0.653 value_reward_not_chosen[t] + 0.055 contr_diff + -0.051 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + -0.137 contr_diff^2 \n",
            "value_choice[t+1] = 0.057 1 + 0.977 value_choice[t] + 0.003 contr_diff + 0.057 choice + 0.008 value_choice^2 + 0.009 value_choice*contr_diff + -0.022 value_choice*choice + 0.014 contr_diff^2 + 0.003 contr_diff*choice + 0.059 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 40/1000 --- L(Train): 0.4458511 --- L(Val, RNN): 0.4353245 --- L(Val, SINDy): 0.4442788 --- Time: 0.45s; --- Convergence: 6.48e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.276 1 + 0.63 value_reward_chosen[t] + 0.016 contr_diff + 0.057 reward + -0.296 value_reward_chosen^2 + -0.064 value_reward_chosen*contr_diff + -0.34 value_reward_chosen*reward + 0.197 contr_diff^2 + -0.02 contr_diff*reward + 0.058 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.061 1 + 0.642 value_reward_not_chosen[t] + 0.064 contr_diff + -0.058 value_reward_not_chosen^2 + -0.011 value_reward_not_chosen*contr_diff + -0.13 contr_diff^2 \n",
            "value_choice[t+1] = 0.058 1 + 0.977 value_choice[t] + 0.004 contr_diff + 0.058 choice + 0.006 value_choice^2 + 0.007 value_choice*contr_diff + -0.021 value_choice*choice + 0.015 contr_diff^2 + 0.003 contr_diff*choice + 0.06 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 41/1000 --- L(Train): 0.4361637 --- L(Val, RNN): 0.4347251 --- L(Val, SINDy): 0.4431199 --- Time: 0.30s; --- Convergence: 6.24e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.283 1 + 0.619 value_reward_chosen[t] + 0.014 contr_diff + 0.066 reward + -0.308 value_reward_chosen^2 + -0.06 value_reward_chosen*contr_diff + -0.348 value_reward_chosen*reward + 0.194 contr_diff^2 + -0.019 contr_diff*reward + 0.066 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.051 1 + 0.631 value_reward_not_chosen[t] + 0.073 contr_diff + -0.067 value_reward_not_chosen^2 + -0.016 value_reward_not_chosen*contr_diff + -0.122 contr_diff^2 \n",
            "value_choice[t+1] = 0.058 1 + 0.978 value_choice[t] + 0.004 contr_diff + 0.058 choice + 0.006 value_choice^2 + 0.004 value_choice*contr_diff + -0.02 value_choice*choice + 0.016 contr_diff^2 + 0.003 contr_diff*choice + 0.06 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 42/1000 --- L(Train): 0.4328451 --- L(Val, RNN): 0.4341613 --- L(Val, SINDy): 0.4417430 --- Time: 0.29s; --- Convergence: 5.94e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.288 1 + 0.608 value_reward_chosen[t] + 0.014 contr_diff + 0.073 reward + -0.32 value_reward_chosen^2 + -0.055 value_reward_chosen*contr_diff + -0.357 value_reward_chosen*reward + 0.191 contr_diff^2 + -0.018 contr_diff*reward + 0.073 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.04 1 + 0.621 value_reward_not_chosen[t] + 0.081 contr_diff + -0.076 value_reward_not_chosen^2 + -0.021 value_reward_not_chosen*contr_diff + -0.114 contr_diff^2 \n",
            "value_choice[t+1] = 0.057 1 + 0.98 value_choice[t] + 0.004 contr_diff + 0.057 choice + 0.006 value_choice^2 + 0.002 value_choice*contr_diff + -0.019 value_choice*choice + 0.015 contr_diff^2 + 0.004 contr_diff*choice + 0.059 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 43/1000 --- L(Train): 0.4332119 --- L(Val, RNN): 0.4336311 --- L(Val, SINDy): 0.4397190 --- Time: 0.37s; --- Convergence: 5.62e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.289 1 + 0.596 value_reward_chosen[t] + 0.016 contr_diff + 0.078 reward + -0.333 value_reward_chosen^2 + -0.047 value_reward_chosen*contr_diff + -0.365 value_reward_chosen*reward + 0.186 contr_diff^2 + -0.016 contr_diff*reward + 0.078 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.03 1 + 0.61 value_reward_not_chosen[t] + 0.089 contr_diff + -0.086 value_reward_not_chosen^2 + -0.027 value_reward_not_chosen*contr_diff + -0.107 contr_diff^2 \n",
            "value_choice[t+1] = 0.054 1 + 0.98 value_choice[t] + 0.004 contr_diff + 0.054 choice + 0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.019 value_choice*choice + 0.014 contr_diff^2 + 0.004 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 44/1000 --- L(Train): 0.4333129 --- L(Val, RNN): 0.4330528 --- L(Val, SINDy): 0.4388298 --- Time: 0.33s; --- Convergence: 5.70e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.287 1 + 0.584 value_reward_chosen[t] + 0.019 contr_diff + 0.082 reward + -0.346 value_reward_chosen^2 + -0.039 value_reward_chosen*contr_diff + -0.373 value_reward_chosen*reward + 0.179 contr_diff^2 + -0.014 contr_diff*reward + 0.082 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.021 1 + 0.6 value_reward_not_chosen[t] + 0.095 contr_diff + -0.097 value_reward_not_chosen^2 + -0.034 value_reward_not_chosen*contr_diff + -0.1 contr_diff^2 \n",
            "value_choice[t+1] = 0.051 1 + 0.981 value_choice[t] + 0.004 contr_diff + 0.05 choice + 0.006 value_choice^2 + -0.004 value_choice*contr_diff + -0.018 value_choice*choice + 0.011 contr_diff^2 + 0.003 contr_diff*choice + 0.052 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 45/1000 --- L(Train): 0.4383850 --- L(Val, RNN): 0.4324060 --- L(Val, SINDy): 0.4348480 --- Time: 0.47s; --- Convergence: 6.08e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.281 1 + 0.572 value_reward_chosen[t] + 0.025 contr_diff + 0.085 reward + -0.359 value_reward_chosen^2 + -0.029 value_reward_chosen*contr_diff + -0.38 value_reward_chosen*reward + 0.17 contr_diff^2 + -0.009 contr_diff*reward + 0.086 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.016 1 + 0.59 value_reward_not_chosen[t] + 0.098 contr_diff + -0.109 value_reward_not_chosen^2 + -0.042 value_reward_not_chosen*contr_diff + -0.097 contr_diff^2 \n",
            "value_choice[t+1] = 0.047 1 + 0.983 value_choice[t] + 0.003 contr_diff + 0.047 choice + 0.008 value_choice^2 + -0.008 value_choice*contr_diff + -0.016 value_choice*choice + 0.009 contr_diff^2 + 0.002 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 46/1000 --- L(Train): 0.4327628 --- L(Val, RNN): 0.4317138 --- L(Val, SINDy): 0.4338791 --- Time: 0.32s; --- Convergence: 6.50e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.273 1 + 0.559 value_reward_chosen[t] + 0.029 contr_diff + 0.092 reward + -0.373 value_reward_chosen^2 + -0.021 value_reward_chosen*contr_diff + -0.385 value_reward_chosen*reward + 0.161 contr_diff^2 + -0.005 contr_diff*reward + 0.092 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.016 1 + 0.579 value_reward_not_chosen[t] + 0.1 contr_diff + -0.121 value_reward_not_chosen^2 + -0.051 value_reward_not_chosen*contr_diff + -0.095 contr_diff^2 \n",
            "value_choice[t+1] = 0.045 1 + 0.986 value_choice[t] + 0.002 contr_diff + 0.045 choice + 0.01 value_choice^2 + -0.011 value_choice*contr_diff + -0.013 value_choice*choice + 0.006 contr_diff^2 + 0.001 contr_diff*choice + 0.046 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 47/1000 --- L(Train): 0.4410988 --- L(Val, RNN): 0.4310609 --- L(Val, SINDy): 0.4328248 --- Time: 0.33s; --- Convergence: 6.52e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.264 1 + 0.546 value_reward_chosen[t] + 0.036 contr_diff + 0.101 reward + -0.387 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + -0.384 value_reward_chosen*reward + 0.15 contr_diff^2 + -0.003 contr_diff*reward + 0.101 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.019 1 + 0.569 value_reward_not_chosen[t] + 0.1 contr_diff + -0.134 value_reward_not_chosen^2 + -0.06 value_reward_not_chosen*contr_diff + -0.098 contr_diff^2 \n",
            "value_choice[t+1] = 0.042 1 + 0.987 value_choice[t] + 0.002 contr_diff + 0.042 choice + 0.01 value_choice^2 + -0.011 value_choice*contr_diff + -0.012 value_choice*choice + 0.005 contr_diff^2 + 0.002 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 48/1000 --- L(Train): 0.4363113 --- L(Val, RNN): 0.4304158 --- L(Val, SINDy): 0.4319159 --- Time: 0.32s; --- Convergence: 6.48e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.253 1 + 0.532 value_reward_chosen[t] + 0.035 contr_diff + 0.111 reward + -0.401 value_reward_chosen^2 + -0.008 value_reward_chosen*contr_diff + -0.382 value_reward_chosen*reward + 0.139 contr_diff^2 + -0.003 contr_diff*reward + 0.111 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.026 1 + 0.559 value_reward_not_chosen[t] + 0.099 contr_diff + -0.147 value_reward_not_chosen^2 + -0.068 value_reward_not_chosen*contr_diff + -0.102 contr_diff^2 \n",
            "value_choice[t+1] = 0.041 1 + 0.987 value_choice[t] + 0.004 contr_diff + 0.041 choice + 0.01 value_choice^2 + -0.01 value_choice*contr_diff + -0.011 value_choice*choice + 0.004 contr_diff^2 + 0.003 contr_diff*choice + 0.043 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 49/1000 --- L(Train): 0.4194239 --- L(Val, RNN): 0.4296897 --- L(Val, SINDy): 0.4313591 --- Time: 0.33s; --- Convergence: 6.87e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.241 1 + 0.517 value_reward_chosen[t] + 0.029 contr_diff + 0.122 reward + -0.416 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + -0.378 value_reward_chosen*reward + 0.126 contr_diff^2 + -0.007 contr_diff*reward + 0.123 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.034 1 + 0.551 value_reward_not_chosen[t] + 0.095 contr_diff + -0.16 value_reward_not_chosen^2 + -0.076 value_reward_not_chosen*contr_diff + -0.109 contr_diff^2 \n",
            "value_choice[t+1] = 0.042 1 + 0.988 value_choice[t] + 0.006 contr_diff + 0.042 choice + 0.009 value_choice^2 + -0.008 value_choice*contr_diff + -0.011 value_choice*choice + 0.006 contr_diff^2 + 0.006 contr_diff*choice + 0.043 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 50/1000 --- L(Train): 0.4265115 --- L(Val, RNN): 0.4289716 --- L(Val, SINDy): 0.4318672 --- Time: 0.45s; --- Convergence: 7.03e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.228 1 + 0.503 value_reward_chosen[t] + 0.024 contr_diff + 0.135 reward + -0.431 value_reward_chosen^2 + -0.011 value_reward_chosen*contr_diff + -0.372 value_reward_chosen*reward + 0.113 contr_diff^2 + -0.011 contr_diff*reward + 0.135 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.044 1 + 0.545 value_reward_not_chosen[t] + 0.09 contr_diff + -0.173 value_reward_not_chosen^2 + -0.083 value_reward_not_chosen*contr_diff + -0.118 contr_diff^2 \n",
            "value_choice[t+1] = 0.044 1 + 0.988 value_choice[t] + 0.007 contr_diff + 0.044 choice + 0.007 value_choice^2 + -0.005 value_choice*contr_diff + -0.011 value_choice*choice + 0.009 contr_diff^2 + 0.006 contr_diff*choice + 0.045 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 51/1000 --- L(Train): 0.4201156 --- L(Val, RNN): 0.4284204 --- L(Val, SINDy): 0.4314379 --- Time: 0.34s; --- Convergence: 6.27e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.214 1 + 0.488 value_reward_chosen[t] + 0.017 contr_diff + 0.148 reward + -0.446 value_reward_chosen^2 + -0.014 value_reward_chosen*contr_diff + -0.365 value_reward_chosen*reward + 0.1 contr_diff^2 + -0.016 contr_diff*reward + 0.148 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.056 1 + 0.542 value_reward_not_chosen[t] + 0.083 contr_diff + -0.187 value_reward_not_chosen^2 + -0.087 value_reward_not_chosen*contr_diff + -0.129 contr_diff^2 \n",
            "value_choice[t+1] = 0.046 1 + 0.987 value_choice[t] + 0.007 contr_diff + 0.045 choice + 0.004 value_choice^2 + -0.003 value_choice*contr_diff + -0.012 value_choice*choice + 0.013 contr_diff^2 + 0.006 contr_diff*choice + 0.047 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 52/1000 --- L(Train): 0.4209555 --- L(Val, RNN): 0.4278685 --- L(Val, SINDy): 0.4308125 --- Time: 0.37s; --- Convergence: 5.89e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.201 1 + 0.473 value_reward_chosen[t] + 0.01 contr_diff + 0.161 reward + -0.461 value_reward_chosen^2 + -0.018 value_reward_chosen*contr_diff + -0.356 value_reward_chosen*reward + 0.087 contr_diff^2 + -0.021 contr_diff*reward + 0.162 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.068 1 + 0.542 value_reward_not_chosen[t] + 0.075 contr_diff + -0.201 value_reward_not_chosen^2 + -0.09 value_reward_not_chosen*contr_diff + -0.139 contr_diff^2 \n",
            "value_choice[t+1] = 0.048 1 + 0.986 value_choice[t] + 0.006 contr_diff + 0.048 choice + 0.002 value_choice^2 + -0.001 value_choice*contr_diff + -0.013 value_choice*choice + 0.016 contr_diff^2 + 0.006 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 53/1000 --- L(Train): 0.4164794 --- L(Val, RNN): 0.4273245 --- L(Val, SINDy): 0.4301040 --- Time: 0.44s; --- Convergence: 5.67e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.187 1 + 0.458 value_reward_chosen[t] + 0.002 contr_diff + 0.176 reward + -0.476 value_reward_chosen^2 + -0.023 value_reward_chosen*contr_diff + -0.347 value_reward_chosen*reward + 0.073 contr_diff^2 + -0.025 contr_diff*reward + 0.176 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.081 1 + 0.544 value_reward_not_chosen[t] + 0.066 contr_diff + -0.216 value_reward_not_chosen^2 + -0.091 value_reward_not_chosen*contr_diff + -0.151 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.986 value_choice[t] + 0.004 contr_diff + 0.05 choice + -0.0 value_choice^2 + -0.001 value_choice*contr_diff + -0.013 value_choice*choice + 0.019 contr_diff^2 + 0.003 contr_diff*choice + 0.052 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 54/1000 --- L(Train): 0.4280657 --- L(Val, RNN): 0.4267840 --- L(Val, SINDy): 0.4286969 --- Time: 0.30s; --- Convergence: 5.54e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.174 1 + 0.443 value_reward_chosen[t] + -0.001 contr_diff + 0.19 reward + -0.491 value_reward_chosen^2 + -0.024 value_reward_chosen*contr_diff + -0.335 value_reward_chosen*reward + 0.059 contr_diff^2 + -0.025 contr_diff*reward + 0.191 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.095 1 + 0.549 value_reward_not_chosen[t] + 0.057 contr_diff + -0.231 value_reward_not_chosen^2 + -0.09 value_reward_not_chosen*contr_diff + -0.162 contr_diff^2 \n",
            "value_choice[t+1] = 0.053 1 + 0.987 value_choice[t] + 0.001 contr_diff + 0.053 choice + -0.001 value_choice^2 + -0.002 value_choice*contr_diff + -0.012 value_choice*choice + 0.021 contr_diff^2 + 0.0 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 55/1000 --- L(Train): 0.4321833 --- L(Val, RNN): 0.4262613 --- L(Val, SINDy): 0.4277136 --- Time: 0.40s; --- Convergence: 5.38e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.16 1 + 0.428 value_reward_chosen[t] + -0.006 contr_diff + 0.206 reward + -0.505 value_reward_chosen^2 + -0.028 value_reward_chosen*contr_diff + -0.323 value_reward_chosen*reward + 0.045 contr_diff^2 + -0.025 contr_diff*reward + 0.206 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.109 1 + 0.555 value_reward_not_chosen[t] + 0.046 contr_diff + -0.246 value_reward_not_chosen^2 + -0.085 value_reward_not_chosen*contr_diff + -0.174 contr_diff^2 \n",
            "value_choice[t+1] = 0.053 1 + 0.986 value_choice[t] + -0.002 contr_diff + 0.053 choice + -0.002 value_choice^2 + -0.002 value_choice*contr_diff + -0.012 value_choice*choice + 0.021 contr_diff^2 + -0.002 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 56/1000 --- L(Train): 0.4321109 --- L(Val, RNN): 0.4257664 --- L(Val, SINDy): 0.4277558 --- Time: 0.32s; --- Convergence: 5.17e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.146 1 + 0.413 value_reward_chosen[t] + -0.008 contr_diff + 0.221 reward + -0.521 value_reward_chosen^2 + -0.03 value_reward_chosen*contr_diff + -0.309 value_reward_chosen*reward + 0.031 contr_diff^2 + -0.024 contr_diff*reward + 0.222 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.123 1 + 0.564 value_reward_not_chosen[t] + 0.035 contr_diff + -0.261 value_reward_not_chosen^2 + -0.077 value_reward_not_chosen*contr_diff + -0.186 contr_diff^2 \n",
            "value_choice[t+1] = 0.051 1 + 0.986 value_choice[t] + -0.002 contr_diff + 0.051 choice + -0.003 value_choice^2 + -0.001 value_choice*contr_diff + -0.012 value_choice*choice + 0.018 contr_diff^2 + -0.003 contr_diff*choice + 0.053 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 57/1000 --- L(Train): 0.4229356 --- L(Val, RNN): 0.4253267 --- L(Val, SINDy): 0.4269052 --- Time: 0.31s; --- Convergence: 4.78e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.133 1 + 0.398 value_reward_chosen[t] + -0.011 contr_diff + 0.237 reward + -0.535 value_reward_chosen^2 + -0.032 value_reward_chosen*contr_diff + -0.295 value_reward_chosen*reward + 0.017 contr_diff^2 + -0.023 contr_diff*reward + 0.238 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.137 1 + 0.574 value_reward_not_chosen[t] + 0.025 contr_diff + -0.275 value_reward_not_chosen^2 + -0.069 value_reward_not_chosen*contr_diff + -0.197 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.988 value_choice[t] + -0.002 contr_diff + 0.05 choice + -0.002 value_choice^2 + -0.0 value_choice*contr_diff + -0.011 value_choice*choice + 0.016 contr_diff^2 + -0.003 contr_diff*choice + 0.052 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 58/1000 --- L(Train): 0.4122690 --- L(Val, RNN): 0.4248681 --- L(Val, SINDy): 0.4261307 --- Time: 0.33s; --- Convergence: 4.68e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.122 1 + 0.386 value_reward_chosen[t] + -0.012 contr_diff + 0.253 reward + -0.545 value_reward_chosen^2 + -0.034 value_reward_chosen*contr_diff + -0.28 value_reward_chosen*reward + 0.004 contr_diff^2 + -0.019 contr_diff*reward + 0.254 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.149 1 + 0.584 value_reward_not_chosen[t] + 0.016 contr_diff + -0.282 value_reward_not_chosen^2 + -0.059 value_reward_not_chosen*contr_diff + -0.204 contr_diff^2 \n",
            "value_choice[t+1] = 0.049 1 + 0.989 value_choice[t] + -0.001 contr_diff + 0.049 choice + -0.001 value_choice^2 + 0.001 value_choice*contr_diff + -0.009 value_choice*choice + 0.013 contr_diff^2 + -0.002 contr_diff*choice + 0.051 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 59/1000 --- L(Train): 0.4312513 --- L(Val, RNN): 0.4243651 --- L(Val, SINDy): 0.4256451 --- Time: 0.33s; --- Convergence: 4.86e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.115 1 + 0.378 value_reward_chosen[t] + -0.012 contr_diff + 0.27 reward + -0.55 value_reward_chosen^2 + -0.035 value_reward_chosen*contr_diff + -0.264 value_reward_chosen*reward + -0.008 contr_diff^2 + -0.012 contr_diff*reward + 0.271 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.158 1 + 0.594 value_reward_not_chosen[t] + 0.012 contr_diff + -0.281 value_reward_not_chosen^2 + -0.048 value_reward_not_chosen*contr_diff + -0.205 contr_diff^2 \n",
            "value_choice[t+1] = 0.048 1 + 0.99 value_choice[t] + -0.0 contr_diff + 0.048 choice + -0.0 value_choice^2 + 0.003 value_choice*contr_diff + -0.009 value_choice*choice + 0.01 contr_diff^2 + -0.001 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 60/1000 --- L(Train): 0.4289392 --- L(Val, RNN): 0.4237531 --- L(Val, SINDy): 0.4251977 --- Time: 0.32s; --- Convergence: 5.49e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.109 1 + 0.372 value_reward_chosen[t] + -0.014 contr_diff + 0.286 reward + -0.552 value_reward_chosen^2 + -0.037 value_reward_chosen*contr_diff + -0.248 value_reward_chosen*reward + -0.018 contr_diff^2 + -0.006 contr_diff*reward + 0.287 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.164 1 + 0.602 value_reward_not_chosen[t] + 0.01 contr_diff + -0.276 value_reward_not_chosen^2 + -0.036 value_reward_not_chosen*contr_diff + -0.203 contr_diff^2 \n",
            "value_choice[t+1] = 0.046 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.046 choice + -0.001 value_choice^2 + 0.004 value_choice*contr_diff + -0.009 value_choice*choice + 0.007 contr_diff^2 + -0.0 contr_diff*choice + 0.048 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 61/1000 --- L(Train): 0.4252118 --- L(Val, RNN): 0.4231731 --- L(Val, SINDy): 0.4248483 --- Time: 0.31s; --- Convergence: 5.64e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.105 1 + 0.367 value_reward_chosen[t] + -0.016 contr_diff + 0.302 reward + -0.553 value_reward_chosen^2 + -0.039 value_reward_chosen*contr_diff + -0.232 value_reward_chosen*reward + -0.028 contr_diff^2 + 0.0 contr_diff*reward + 0.303 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.167 1 + 0.609 value_reward_not_chosen[t] + 0.01 contr_diff + -0.269 value_reward_not_chosen^2 + -0.025 value_reward_not_chosen*contr_diff + -0.2 contr_diff^2 \n",
            "value_choice[t+1] = 0.046 1 + 0.99 value_choice[t] + 0.0 contr_diff + 0.046 choice + -0.001 value_choice^2 + 0.005 value_choice*contr_diff + -0.009 value_choice*choice + 0.006 contr_diff^2 + -0.0 contr_diff*choice + 0.048 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 62/1000 --- L(Train): 0.4205473 --- L(Val, RNN): 0.4228171 --- L(Val, SINDy): 0.4242209 --- Time: 0.35s; --- Convergence: 4.60e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.099 1 + 0.365 value_reward_chosen[t] + -0.018 contr_diff + 0.317 reward + -0.552 value_reward_chosen^2 + -0.041 value_reward_chosen*contr_diff + -0.217 value_reward_chosen*reward + -0.038 contr_diff^2 + 0.004 contr_diff*reward + 0.318 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.167 1 + 0.613 value_reward_not_chosen[t] + 0.01 contr_diff + -0.259 value_reward_not_chosen^2 + -0.014 value_reward_not_chosen*contr_diff + -0.194 contr_diff^2 \n",
            "value_choice[t+1] = 0.048 1 + 0.991 value_choice[t] + -0.0 contr_diff + 0.048 choice + -0.001 value_choice^2 + 0.005 value_choice*contr_diff + -0.008 value_choice*choice + 0.005 contr_diff^2 + -0.001 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\n",
            "Training interrupted. Continuing with further operations...\n",
            "\n",
            "================================================================================\n",
            "Starting second stage SINDy fitting (threshold=0.05, single model)\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 1/1000 --- L(Train): 0.1353876 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.011 1 + 0.99 value_reward_chosen[t] + 0.011 contr_diff + 0.01 reward + -0.012 value_reward_chosen^2 + 0.009 value_reward_chosen*contr_diff + -0.01 value_reward_chosen*reward + -0.008 contr_diff^2 + -0.01 contr_diff*reward + 0.011 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.009 1 + 0.99 value_reward_not_chosen[t] + 0.009 contr_diff + -0.009 value_reward_not_chosen^2 + -0.009 value_reward_not_chosen*contr_diff + 0.009 contr_diff^2 \n",
            "value_choice[t+1] = 0.009 1 + 1.012 value_choice[t] + 0.01 contr_diff + 0.01 choice + 0.011 value_choice^2 + -0.009 value_choice*contr_diff + 0.01 value_choice*choice + 0.012 contr_diff^2 + 0.009 contr_diff*choice + 0.01 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 2/1000 --- L(Train): 0.1249563 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.021 1 + 0.98 value_reward_chosen[t] + 0.021 contr_diff + 0.02 reward + -0.022 value_reward_chosen^2 + 0.013 value_reward_chosen*contr_diff + -0.02 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.006 contr_diff*reward + 0.021 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.019 1 + 0.982 value_reward_not_chosen[t] + 0.011 contr_diff + -0.018 value_reward_not_chosen^2 + -0.012 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.018 1 + 1.019 value_choice[t] + 0.005 contr_diff + 0.019 choice + 0.017 value_choice^2 + -0.009 value_choice*contr_diff + 0.018 value_choice*choice + 0.018 contr_diff^2 + 0.006 contr_diff*choice + 0.019 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 3/1000 --- L(Train): 0.1189471 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.03 1 + 0.97 value_reward_chosen[t] + 0.031 contr_diff + 0.03 reward + -0.032 value_reward_chosen^2 + 0.014 value_reward_chosen*contr_diff + -0.03 value_reward_chosen*reward + 0.005 contr_diff^2 + -0.0 contr_diff*reward + 0.031 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.029 1 + 0.973 value_reward_not_chosen[t] + 0.009 contr_diff + -0.025 value_reward_not_chosen^2 + -0.012 value_reward_not_chosen*contr_diff + -0.007 contr_diff^2 \n",
            "value_choice[t+1] = 0.025 1 + 1.022 value_choice[t] + -0.002 contr_diff + 0.026 choice + 0.017 value_choice^2 + -0.005 value_choice*contr_diff + 0.02 value_choice*choice + 0.016 contr_diff^2 + 0.0 contr_diff*choice + 0.025 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 4/1000 --- L(Train): 0.1132147 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.039 1 + 0.96 value_reward_chosen[t] + 0.04 contr_diff + 0.04 reward + -0.042 value_reward_chosen^2 + 0.012 value_reward_chosen*contr_diff + -0.04 value_reward_chosen*reward + 0.008 contr_diff^2 + 0.007 contr_diff*reward + 0.041 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.038 1 + 0.964 value_reward_not_chosen[t] + 0.006 contr_diff + -0.032 value_reward_not_chosen^2 + -0.01 value_reward_not_chosen*contr_diff + -0.013 contr_diff^2 \n",
            "value_choice[t+1] = 0.029 1 + 1.02 value_choice[t] + -0.004 contr_diff + 0.031 choice + 0.012 value_choice^2 + 0.002 value_choice*contr_diff + 0.018 value_choice*choice + 0.011 contr_diff^2 + -0.006 contr_diff*choice + 0.029 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 5/1000 --- L(Train): 0.1070987 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.045 1 + 0.95 value_reward_chosen[t] + 0.049 contr_diff + 0.05 reward + -0.052 value_reward_chosen^2 + 0.009 value_reward_chosen*contr_diff + -0.05 value_reward_chosen*reward + 0.008 contr_diff^2 + 0.01 contr_diff*reward + 0.051 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.048 1 + 0.954 value_reward_not_chosen[t] + 0.001 contr_diff + -0.038 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + -0.017 contr_diff^2 \n",
            "value_choice[t+1] = 0.032 1 + 1.016 value_choice[t] + -0.003 contr_diff + 0.033 choice + 0.007 value_choice^2 + 0.005 value_choice*contr_diff + 0.014 value_choice*choice + 0.004 contr_diff^2 + -0.009 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 6/1000 --- L(Train): 0.1012283 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.049 1 + 0.94 value_reward_chosen[t] + 0.057 contr_diff + 0.059 reward + -0.062 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + -0.061 value_reward_chosen*reward + 0.006 contr_diff^2 + 0.011 contr_diff*reward + 0.061 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.056 1 + 0.945 value_reward_not_chosen[t] + -0.004 contr_diff + -0.042 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.019 contr_diff^2 \n",
            "value_choice[t+1] = 0.033 1 + 1.011 value_choice[t] + 0.0 contr_diff + 0.034 choice + 0.0 value_choice^2 + 0.007 value_choice*contr_diff + 0.009 value_choice*choice + -0.003 contr_diff^2 + -0.008 contr_diff*choice + 0.033 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 7/1000 --- L(Train): 0.0961295 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.051 1 + 0.93 value_reward_chosen[t] + 0.065 contr_diff + 0.069 reward + -0.072 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.009 contr_diff*reward + 0.07 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.065 1 + 0.935 value_reward_not_chosen[t] + -0.006 contr_diff + -0.045 value_reward_not_chosen^2 + 0.004 value_reward_not_chosen*contr_diff + -0.02 contr_diff^2 \n",
            "value_choice[t+1] = 0.035 1 + 1.006 value_choice[t] + 0.001 contr_diff + 0.036 choice + -0.006 value_choice^2 + 0.006 value_choice*contr_diff + 0.003 value_choice*choice + -0.006 contr_diff^2 + -0.006 contr_diff*choice + 0.034 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 8/1000 --- L(Train): 0.0918714 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.05 1 + 0.92 value_reward_chosen[t] + 0.071 contr_diff + 0.078 reward + -0.082 value_reward_chosen^2 + -0.005 value_reward_chosen*contr_diff + -0.081 value_reward_chosen*reward + -0.004 contr_diff^2 + 0.006 contr_diff*reward + 0.08 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.072 1 + 0.925 value_reward_not_chosen[t] + -0.004 contr_diff + -0.047 value_reward_not_chosen^2 + 0.006 value_reward_not_chosen*contr_diff + -0.019 contr_diff^2 \n",
            "value_choice[t+1] = 0.037 1 + 1.002 value_choice[t] + 0.0 contr_diff + 0.037 choice + -0.012 value_choice^2 + 0.004 value_choice*contr_diff + -0.001 value_choice*choice + -0.007 contr_diff^2 + -0.003 contr_diff*choice + 0.035 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 9/1000 --- L(Train): 0.0881347 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.048 1 + 0.91 value_reward_chosen[t] + 0.077 contr_diff + 0.088 reward + -0.093 value_reward_chosen^2 + -0.006 value_reward_chosen*contr_diff + -0.091 value_reward_chosen*reward + -0.008 contr_diff^2 + 0.002 contr_diff*reward + 0.089 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.079 1 + 0.915 value_reward_not_chosen[t] + -0.001 contr_diff + -0.047 value_reward_not_chosen^2 + 0.005 value_reward_not_chosen*contr_diff + -0.017 contr_diff^2 \n",
            "value_choice[t+1] = 0.039 1 + 1.0 value_choice[t] + -0.003 contr_diff + 0.04 choice + -0.016 value_choice^2 + 0.0 value_choice*contr_diff + -0.004 value_choice*choice + -0.004 contr_diff^2 + 0.001 contr_diff*choice + 0.037 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 10/1000 --- L(Train): 0.0845599 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.044 1 + 0.9 value_reward_chosen[t] + 0.081 contr_diff + 0.097 reward + -0.103 value_reward_chosen^2 + -0.005 value_reward_chosen*contr_diff + -0.101 value_reward_chosen*reward + -0.011 contr_diff^2 + -0.003 contr_diff*reward + 0.098 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.085 1 + 0.905 value_reward_not_chosen[t] + 0.004 contr_diff + -0.046 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.014 contr_diff^2 \n",
            "value_choice[t+1] = 0.043 1 + 0.998 value_choice[t] + -0.004 contr_diff + 0.043 choice + -0.018 value_choice^2 + -0.004 value_choice*contr_diff + -0.005 value_choice*choice + -0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.04 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 11/1000 --- L(Train): 0.0810317 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.039 1 + 0.89 value_reward_chosen[t] + 0.083 contr_diff + 0.106 reward + -0.113 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.111 value_reward_chosen*reward + -0.013 contr_diff^2 + -0.007 contr_diff*reward + 0.107 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.09 1 + 0.895 value_reward_not_chosen[t] + 0.007 contr_diff + -0.043 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.01 contr_diff^2 \n",
            "value_choice[t+1] = 0.047 1 + 0.999 value_choice[t] + -0.003 contr_diff + 0.047 choice + -0.019 value_choice^2 + -0.007 value_choice*contr_diff + -0.005 value_choice*choice + 0.004 contr_diff^2 + 0.004 contr_diff*choice + 0.044 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 12/1000 --- L(Train): 0.0776312 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.034 1 + 0.88 value_reward_chosen[t] + 0.085 contr_diff + 0.115 reward + -0.123 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.121 value_reward_chosen*reward + -0.015 contr_diff^2 + -0.009 contr_diff*reward + 0.116 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.095 1 + 0.885 value_reward_not_chosen[t] + 0.009 contr_diff + -0.039 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + -0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.052 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.052 choice + -0.019 value_choice^2 + -0.007 value_choice*contr_diff + -0.004 value_choice*choice + 0.008 contr_diff^2 + 0.002 contr_diff*choice + 0.048 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 13/1000 --- L(Train): 0.0744846 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.027 1 + 0.87 value_reward_chosen[t] + 0.085 contr_diff + 0.124 reward + -0.133 value_reward_chosen^2 + 0.006 value_reward_chosen*contr_diff + -0.131 value_reward_chosen*reward + -0.015 contr_diff^2 + -0.009 contr_diff*reward + 0.125 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.098 1 + 0.874 value_reward_not_chosen[t] + 0.01 contr_diff + -0.035 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.056 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.056 choice + -0.018 value_choice^2 + -0.007 value_choice*contr_diff + -0.002 value_choice*choice + 0.009 contr_diff^2 + 0.0 contr_diff*choice + 0.051 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 14/1000 --- L(Train): 0.0716189 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.02 1 + 0.86 value_reward_chosen[t] + 0.084 contr_diff + 0.133 reward + -0.143 value_reward_chosen^2 + 0.008 value_reward_chosen*contr_diff + -0.141 value_reward_chosen*reward + -0.015 contr_diff^2 + -0.009 contr_diff*reward + 0.134 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.101 1 + 0.864 value_reward_not_chosen[t] + 0.01 contr_diff + -0.03 value_reward_not_chosen^2 + -0.006 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.059 1 + 1.003 value_choice[t] + 0.002 contr_diff + 0.059 choice + -0.018 value_choice^2 + -0.005 value_choice*contr_diff + -0.001 value_choice*choice + 0.01 contr_diff^2 + -0.003 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 15/1000 --- L(Train): 0.0689389 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.013 1 + 0.85 value_reward_chosen[t] + 0.082 contr_diff + 0.142 reward + -0.153 value_reward_chosen^2 + 0.009 value_reward_chosen*contr_diff + -0.151 value_reward_chosen*reward + -0.014 contr_diff^2 + -0.008 contr_diff*reward + 0.143 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.104 1 + 0.853 value_reward_not_chosen[t] + 0.009 contr_diff + -0.024 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + 0.006 contr_diff^2 \n",
            "value_choice[t+1] = 0.062 1 + 1.003 value_choice[t] + 0.002 contr_diff + 0.061 choice + -0.017 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.008 contr_diff^2 + -0.005 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 16/1000 --- L(Train): 0.0663288 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = 0.006 1 + 0.841 value_reward_chosen[t] + 0.08 contr_diff + 0.15 reward + -0.163 value_reward_chosen^2 + 0.008 value_reward_chosen*contr_diff + -0.161 value_reward_chosen*reward + -0.012 contr_diff^2 + -0.006 contr_diff*reward + 0.151 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.106 1 + 0.843 value_reward_not_chosen[t] + 0.007 contr_diff + -0.017 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.006 contr_diff^2 \n",
            "value_choice[t+1] = 0.063 1 + 1.003 value_choice[t] + 0.001 contr_diff + 0.062 choice + -0.017 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.005 contr_diff^2 + -0.005 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 17/1000 --- L(Train): 0.0637422 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.001 1 + 0.831 value_reward_chosen[t] + 0.076 contr_diff + 0.159 reward + -0.173 value_reward_chosen^2 + 0.007 value_reward_chosen*contr_diff + -0.17 value_reward_chosen*reward + -0.01 contr_diff^2 + -0.003 contr_diff*reward + 0.16 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.108 1 + 0.833 value_reward_not_chosen[t] + 0.005 contr_diff + -0.011 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.005 contr_diff^2 \n",
            "value_choice[t+1] = 0.063 1 + 1.002 value_choice[t] + -0.001 contr_diff + 0.062 choice + -0.018 value_choice^2 + 0.004 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.056 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 18/1000 --- L(Train): 0.0612113 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.008 1 + 0.822 value_reward_chosen[t] + 0.071 contr_diff + 0.168 reward + -0.182 value_reward_chosen^2 + 0.005 value_reward_chosen*contr_diff + -0.179 value_reward_chosen*reward + -0.007 contr_diff^2 + 0.0 contr_diff*reward + 0.169 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.11 1 + 0.823 value_reward_not_chosen[t] + 0.002 contr_diff + -0.004 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.062 1 + 1.001 value_choice[t] + -0.001 contr_diff + 0.061 choice + -0.018 value_choice^2 + 0.005 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + -0.001 contr_diff*choice + 0.054 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 19/1000 --- L(Train): 0.0587956 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.015 1 + 0.813 value_reward_chosen[t] + 0.066 contr_diff + 0.176 reward + -0.192 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.188 value_reward_chosen*reward + -0.003 contr_diff^2 + 0.002 contr_diff*reward + 0.177 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.112 1 + 0.813 value_reward_not_chosen[t] + -0.001 contr_diff + 0.003 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.061 1 + 0.999 value_choice[t] + -0.0 contr_diff + 0.059 choice + -0.019 value_choice^2 + 0.005 value_choice*contr_diff + -0.003 value_choice*choice + -0.007 contr_diff^2 + 0.002 contr_diff*choice + 0.052 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 20/1000 --- L(Train): 0.0565193 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.022 1 + 0.803 value_reward_chosen[t] + 0.061 contr_diff + 0.185 reward + -0.201 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.197 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.186 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.113 1 + 0.804 value_reward_not_chosen[t] + -0.002 contr_diff + 0.008 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.059 1 + 0.997 value_choice[t] + 0.001 contr_diff + 0.057 choice + -0.02 value_choice^2 + 0.003 value_choice*contr_diff + -0.004 value_choice*choice + -0.01 contr_diff^2 + 0.004 contr_diff*choice + 0.049 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 21/1000 --- L(Train): 0.0543565 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.028 1 + 0.794 value_reward_chosen[t] + 0.055 contr_diff + 0.193 reward + -0.21 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.206 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.0 contr_diff*reward + 0.194 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.116 1 + 0.795 value_reward_not_chosen[t] + -0.001 contr_diff + 0.011 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.057 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.055 choice + -0.02 value_choice^2 + 0.0 value_choice*contr_diff + -0.005 value_choice*choice + -0.011 contr_diff^2 + 0.004 contr_diff*choice + 0.047 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 22/1000 --- L(Train): 0.0522700 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.034 1 + 0.785 value_reward_chosen[t] + 0.049 contr_diff + 0.202 reward + -0.219 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.214 value_reward_chosen*reward + 0.003 contr_diff^2 + -0.002 contr_diff*reward + 0.203 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.118 1 + 0.787 value_reward_not_chosen[t] + 0.002 contr_diff + 0.013 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.056 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.054 choice + -0.019 value_choice^2 + -0.003 value_choice*contr_diff + -0.005 value_choice*choice + -0.011 contr_diff^2 + 0.003 contr_diff*choice + 0.045 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 23/1000 --- L(Train): 0.0502443 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.04 1 + 0.776 value_reward_chosen[t] + 0.044 contr_diff + 0.21 reward + -0.228 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.222 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.003 contr_diff*reward + 0.211 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.12 1 + 0.779 value_reward_not_chosen[t] + 0.003 contr_diff + 0.013 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.052 choice + -0.018 value_choice^2 + -0.005 value_choice*contr_diff + -0.004 value_choice*choice + -0.01 contr_diff^2 + 0.001 contr_diff*choice + 0.043 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 24/1000 --- L(Train): 0.0482880 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.044 1 + 0.768 value_reward_chosen[t] + 0.038 contr_diff + 0.218 reward + -0.236 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.23 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.219 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.123 1 + 0.772 value_reward_not_chosen[t] + 0.005 contr_diff + 0.012 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.997 value_choice[t] + 0.0 contr_diff + 0.052 choice + -0.016 value_choice^2 + -0.006 value_choice*contr_diff + -0.003 value_choice*choice + -0.009 contr_diff^2 + -0.002 contr_diff*choice + 0.042 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 25/1000 --- L(Train): 0.0464187 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.049 1 + 0.759 value_reward_chosen[t] + 0.032 contr_diff + 0.227 reward + -0.245 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.237 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.003 contr_diff*reward + 0.228 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.125 1 + 0.765 value_reward_not_chosen[t] + 0.005 contr_diff + 0.01 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.998 value_choice[t] + 0.0 contr_diff + 0.051 choice + -0.014 value_choice^2 + -0.006 value_choice*contr_diff + -0.001 value_choice*choice + -0.006 contr_diff^2 + -0.004 contr_diff*choice + 0.041 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 26/1000 --- L(Train): 0.0446533 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.053 1 + 0.751 value_reward_chosen[t] + 0.027 contr_diff + 0.235 reward + -0.253 value_reward_chosen^2 + 0.004 value_reward_chosen*contr_diff + -0.245 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.236 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.128 1 + 0.759 value_reward_not_chosen[t] + 0.006 contr_diff + 0.008 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.999 value_choice[t] + -0.001 contr_diff + 0.051 choice + -0.012 value_choice^2 + -0.005 value_choice*contr_diff + 0.001 value_choice*choice + -0.003 contr_diff^2 + -0.004 contr_diff*choice + 0.04 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 27/1000 --- L(Train): 0.0429834 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.057 1 + 0.742 value_reward_chosen[t] + 0.022 contr_diff + 0.243 reward + -0.261 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.251 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.244 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.131 1 + 0.753 value_reward_not_chosen[t] + 0.005 contr_diff + 0.004 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 1.0 value_choice[t] + -0.0 contr_diff + 0.051 choice + -0.01 value_choice^2 + -0.002 value_choice*contr_diff + 0.002 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.039 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 28/1000 --- L(Train): 0.0413956 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.06 1 + 0.734 value_reward_chosen[t] + 0.018 contr_diff + 0.251 reward + -0.269 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.258 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.003 contr_diff*reward + 0.252 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.134 1 + 0.748 value_reward_not_chosen[t] + 0.005 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.054 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.05 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + 0.003 value_choice*choice + 0.004 contr_diff^2 + -0.001 contr_diff*choice + 0.038 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 29/1000 --- L(Train): 0.0398865 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.063 1 + 0.726 value_reward_chosen[t] + 0.014 contr_diff + 0.259 reward + -0.276 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.264 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.003 contr_diff*reward + 0.26 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.137 1 + 0.743 value_reward_not_chosen[t] + 0.004 contr_diff + -0.003 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.053 1 + 1.001 value_choice[t] + 0.001 contr_diff + 0.049 choice + -0.007 value_choice^2 + 0.003 value_choice*contr_diff + 0.003 value_choice*choice + 0.005 contr_diff^2 + 0.002 contr_diff*choice + 0.036 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 30/1000 --- L(Train): 0.0384513 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.066 1 + 0.718 value_reward_chosen[t] + 0.011 contr_diff + 0.267 reward + -0.283 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.27 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.002 contr_diff*reward + 0.268 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.139 1 + 0.738 value_reward_not_chosen[t] + 0.002 contr_diff + -0.005 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.052 1 + 1.001 value_choice[t] + 0.0 contr_diff + 0.047 choice + -0.006 value_choice^2 + 0.003 value_choice*contr_diff + 0.003 value_choice*choice + 0.005 contr_diff^2 + 0.003 contr_diff*choice + 0.034 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 31/1000 --- L(Train): 0.0370968 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.069 1 + 0.71 value_reward_chosen[t] + 0.008 contr_diff + 0.275 reward + -0.291 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.276 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.275 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.141 1 + 0.734 value_reward_not_chosen[t] + 0.001 contr_diff + -0.006 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.051 1 + 1.0 value_choice[t] + -0.002 contr_diff + 0.046 choice + -0.006 value_choice^2 + 0.003 value_choice*contr_diff + 0.002 value_choice*choice + 0.004 contr_diff^2 + 0.003 contr_diff*choice + 0.032 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 32/1000 --- L(Train): 0.0358177 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.072 1 + 0.703 value_reward_chosen[t] + 0.006 contr_diff + 0.282 reward + -0.298 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.282 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.283 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.73 value_reward_not_chosen[t] + -0.001 contr_diff + -0.005 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.999 value_choice[t] + -0.003 contr_diff + 0.045 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.003 contr_diff^2 + 0.002 contr_diff*choice + 0.03 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 33/1000 --- L(Train): 0.0346115 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.075 1 + 0.695 value_reward_chosen[t] + 0.005 contr_diff + 0.289 reward + -0.305 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.287 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.29 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.145 1 + 0.726 value_reward_not_chosen[t] + -0.001 contr_diff + -0.004 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.998 value_choice[t] + -0.003 contr_diff + 0.044 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.029 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 34/1000 --- L(Train): 0.0334692 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.078 1 + 0.688 value_reward_chosen[t] + 0.004 contr_diff + 0.297 reward + -0.311 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.292 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.297 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.146 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.998 value_choice[t] + -0.002 contr_diff + 0.043 choice + -0.006 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + -0.002 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 35/1000 --- L(Train): 0.0323954 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.081 1 + 0.681 value_reward_chosen[t] + 0.003 contr_diff + 0.304 reward + -0.318 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.297 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.304 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.147 1 + 0.718 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.05 1 + 0.998 value_choice[t] + -0.0 contr_diff + 0.043 choice + -0.005 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.004 contr_diff^2 + -0.004 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 36/1000 --- L(Train): 0.0313797 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.085 1 + 0.673 value_reward_chosen[t] + 0.003 contr_diff + 0.31 reward + -0.324 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.302 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.311 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.148 1 + 0.715 value_reward_not_chosen[t] + 0.004 contr_diff + 0.003 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.051 1 + 0.998 value_choice[t] + 0.003 contr_diff + 0.044 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.004 contr_diff^2 + -0.004 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 37/1000 --- L(Train): 0.0304146 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.089 1 + 0.666 value_reward_chosen[t] + 0.004 contr_diff + 0.317 reward + -0.331 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.306 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.317 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.148 1 + 0.712 value_reward_not_chosen[t] + 0.005 contr_diff + 0.003 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.052 1 + 0.999 value_choice[t] + 0.004 contr_diff + 0.045 choice + -0.004 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + -0.003 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 38/1000 --- L(Train): 0.0295013 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.092 1 + 0.659 value_reward_chosen[t] + 0.004 contr_diff + 0.323 reward + -0.337 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.311 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.323 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.149 1 + 0.709 value_reward_not_chosen[t] + 0.005 contr_diff + 0.003 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.054 1 + 0.999 value_choice[t] + 0.004 contr_diff + 0.046 choice + -0.004 value_choice^2 + 0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 39/1000 --- L(Train): 0.0286399 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.096 1 + 0.653 value_reward_chosen[t] + 0.005 contr_diff + 0.329 reward + -0.343 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.315 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.329 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.149 1 + 0.707 value_reward_not_chosen[t] + 0.005 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.055 1 + 0.999 value_choice[t] + 0.003 contr_diff + 0.047 choice + -0.003 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.002 contr_diff^2 + 0.0 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 40/1000 --- L(Train): 0.0278237 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.1 1 + 0.646 value_reward_chosen[t] + 0.005 contr_diff + 0.335 reward + -0.349 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.318 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.335 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.705 value_reward_not_chosen[t] + 0.004 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.056 1 + 1.0 value_choice[t] + 0.001 contr_diff + 0.048 choice + -0.003 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.003 contr_diff^2 + 0.001 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 41/1000 --- L(Train): 0.0270533 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.104 1 + 0.64 value_reward_chosen[t] + 0.006 contr_diff + 0.341 reward + -0.354 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.322 value_reward_chosen*reward + -0.004 contr_diff^2 + 0.002 contr_diff*reward + 0.341 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.703 value_reward_not_chosen[t] + 0.004 contr_diff + -0.002 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.057 1 + 0.999 value_choice[t] + -0.002 contr_diff + 0.048 choice + -0.004 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.004 contr_diff^2 + 0.001 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 42/1000 --- L(Train): 0.0263197 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.109 1 + 0.633 value_reward_chosen[t] + 0.007 contr_diff + 0.347 reward + -0.36 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.325 value_reward_chosen*reward + -0.005 contr_diff^2 + 0.001 contr_diff*reward + 0.346 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.702 value_reward_not_chosen[t] + 0.003 contr_diff + -0.003 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.058 1 + 0.999 value_choice[t] + -0.003 contr_diff + 0.049 choice + -0.004 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.004 contr_diff^2 + 0.0 contr_diff*choice + 0.028 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 43/1000 --- L(Train): 0.0256242 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.113 1 + 0.627 value_reward_chosen[t] + 0.007 contr_diff + 0.352 reward + -0.365 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.328 value_reward_chosen*reward + -0.004 contr_diff^2 + -0.001 contr_diff*reward + 0.352 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.7 value_reward_not_chosen[t] + 0.001 contr_diff + -0.003 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.059 1 + 0.998 value_choice[t] + -0.004 contr_diff + 0.049 choice + -0.005 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.027 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 44/1000 --- L(Train): 0.0249687 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.116 1 + 0.621 value_reward_chosen[t] + 0.007 contr_diff + 0.357 reward + -0.37 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.331 value_reward_chosen*reward + -0.003 contr_diff^2 + -0.001 contr_diff*reward + 0.357 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.7 value_reward_not_chosen[t] + -0.0 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.059 1 + 0.997 value_choice[t] + -0.003 contr_diff + 0.049 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + -0.003 value_choice*choice + 0.0 contr_diff^2 + -0.003 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 45/1000 --- L(Train): 0.0243485 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.12 1 + 0.615 value_reward_chosen[t] + 0.007 contr_diff + 0.362 reward + -0.375 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.334 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.362 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.699 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.06 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.05 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.003 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 46/1000 --- L(Train): 0.0237631 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.124 1 + 0.609 value_reward_chosen[t] + 0.006 contr_diff + 0.367 reward + -0.38 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.336 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.366 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.699 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.061 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.05 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.003 value_choice*choice + -0.004 contr_diff^2 + -0.001 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 47/1000 --- L(Train): 0.0232105 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.127 1 + 0.603 value_reward_chosen[t] + 0.005 contr_diff + 0.372 reward + -0.384 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.338 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.371 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.699 value_reward_not_chosen[t] + 0.001 contr_diff + 0.003 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.062 1 + 0.996 value_choice[t] + 0.002 contr_diff + 0.051 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + -0.003 value_choice*choice + -0.004 contr_diff^2 + 0.001 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 48/1000 --- L(Train): 0.0226844 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.131 1 + 0.598 value_reward_chosen[t] + 0.004 contr_diff + 0.377 reward + -0.388 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.34 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.375 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.699 value_reward_not_chosen[t] + 0.001 contr_diff + 0.003 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.063 1 + 0.996 value_choice[t] + 0.002 contr_diff + 0.052 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.003 value_choice*choice + -0.003 contr_diff^2 + 0.002 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 49/1000 --- L(Train): 0.0221864 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.133 1 + 0.592 value_reward_chosen[t] + 0.003 contr_diff + 0.381 reward + -0.392 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.341 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.38 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.7 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.064 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.052 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.026 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 50/1000 --- L(Train): 0.0217170 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.136 1 + 0.587 value_reward_chosen[t] + 0.001 contr_diff + 0.385 reward + -0.396 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.342 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.384 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.701 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.066 1 + 0.997 value_choice[t] + -0.001 contr_diff + 0.053 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.025 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 51/1000 --- L(Train): 0.0212722 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.139 1 + 0.582 value_reward_chosen[t] + -0.001 contr_diff + 0.389 reward + -0.4 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.343 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.388 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.702 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.066 1 + 0.997 value_choice[t] + -0.002 contr_diff + 0.054 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.025 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 52/1000 --- L(Train): 0.0208500 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.141 1 + 0.577 value_reward_chosen[t] + 0.001 contr_diff + 0.393 reward + -0.403 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.344 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.391 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.703 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.067 1 + 0.997 value_choice[t] + -0.002 contr_diff + 0.054 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.003 contr_diff^2 + -0.003 contr_diff*choice + 0.024 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 53/1000 --- L(Train): 0.0204512 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.143 1 + 0.572 value_reward_chosen[t] + 0.001 contr_diff + 0.397 reward + -0.407 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.345 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.395 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.704 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.067 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.054 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.024 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 54/1000 --- L(Train): 0.0200735 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.145 1 + 0.567 value_reward_chosen[t] + 0.002 contr_diff + 0.4 reward + -0.41 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.345 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.398 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.151 1 + 0.705 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.067 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.053 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.022 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 55/1000 --- L(Train): 0.0197156 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.147 1 + 0.563 value_reward_chosen[t] + 0.002 contr_diff + 0.404 reward + -0.413 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.345 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.401 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.706 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.068 1 + 0.996 value_choice[t] + 0.002 contr_diff + 0.053 choice + -0.009 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.021 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 56/1000 --- L(Train): 0.0193737 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.148 1 + 0.558 value_reward_chosen[t] + 0.001 contr_diff + 0.407 reward + -0.415 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.345 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.404 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.15 1 + 0.707 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.068 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.053 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.003 contr_diff^2 + 0.002 contr_diff*choice + 0.02 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 57/1000 --- L(Train): 0.0190477 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.15 1 + 0.554 value_reward_chosen[t] + 0.0 contr_diff + 0.41 reward + -0.418 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.344 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.407 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.149 1 + 0.708 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.068 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.053 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.004 contr_diff^2 + 0.001 contr_diff*choice + 0.019 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 58/1000 --- L(Train): 0.0187381 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.152 1 + 0.55 value_reward_chosen[t] + -0.001 contr_diff + 0.413 reward + -0.42 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.344 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.41 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.149 1 + 0.709 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.068 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.052 choice + -0.009 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.003 contr_diff^2 + -0.0 contr_diff*choice + 0.018 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 59/1000 --- L(Train): 0.0184415 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.153 1 + 0.545 value_reward_chosen[t] + 0.0 contr_diff + 0.416 reward + -0.422 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.343 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.412 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.148 1 + 0.71 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.069 1 + 0.996 value_choice[t] + -0.003 contr_diff + 0.052 choice + -0.009 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.017 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 60/1000 --- L(Train): 0.0181580 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.541 value_reward_chosen[t] + 0.001 contr_diff + 0.418 reward + -0.424 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.343 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.415 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.148 1 + 0.711 value_reward_not_chosen[t] + -0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.069 1 + 0.996 value_choice[t] + -0.003 contr_diff + 0.053 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + 0.002 contr_diff^2 + 0.0 contr_diff*choice + 0.016 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 61/1000 --- L(Train): 0.0178854 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.537 value_reward_chosen[t] + 0.001 contr_diff + 0.42 reward + -0.426 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.342 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.417 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.147 1 + 0.712 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.07 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.053 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.003 contr_diff^2 + 0.0 contr_diff*choice + 0.016 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 62/1000 --- L(Train): 0.0176252 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.533 value_reward_chosen[t] + 0.001 contr_diff + 0.423 reward + -0.428 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.341 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.419 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.146 1 + 0.713 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.07 1 + 0.997 value_choice[t] + -0.0 contr_diff + 0.053 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + 0.001 value_choice*choice + 0.003 contr_diff^2 + -0.001 contr_diff*choice + 0.015 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 63/1000 --- L(Train): 0.0173770 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.53 value_reward_chosen[t] + -0.0 contr_diff + 0.425 reward + -0.43 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.339 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.421 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.146 1 + 0.714 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.071 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.053 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.003 contr_diff^2 + -0.001 contr_diff*choice + 0.014 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 64/1000 --- L(Train): 0.0171362 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.526 value_reward_chosen[t] + 0.001 contr_diff + 0.427 reward + -0.431 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.338 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.422 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.146 1 + 0.715 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.071 1 + 0.997 value_choice[t] + 0.004 contr_diff + 0.053 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.013 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 65/1000 --- L(Train): 0.0169067 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.522 value_reward_chosen[t] + 0.002 contr_diff + 0.428 reward + -0.433 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.337 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.424 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.145 1 + 0.716 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.072 1 + 0.997 value_choice[t] + 0.004 contr_diff + 0.053 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.012 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 66/1000 --- L(Train): 0.0166839 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.519 value_reward_chosen[t] + 0.002 contr_diff + 0.43 reward + -0.434 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.335 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.425 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.145 1 + 0.717 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.072 1 + 0.997 value_choice[t] + 0.003 contr_diff + 0.053 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.002 contr_diff^2 + 0.002 contr_diff*choice + 0.011 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 67/1000 --- L(Train): 0.0164695 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.515 value_reward_chosen[t] + 0.001 contr_diff + 0.431 reward + -0.435 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.334 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.426 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.718 value_reward_not_chosen[t] + -0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.073 1 + 0.997 value_choice[t] + 0.001 contr_diff + 0.053 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.011 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 68/1000 --- L(Train): 0.0162634 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.512 value_reward_chosen[t] + -0.0 contr_diff + 0.433 reward + -0.436 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.332 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.427 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.719 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.074 1 + 0.998 value_choice[t] + -0.002 contr_diff + 0.053 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.01 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 69/1000 --- L(Train): 0.0160637 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.166 1 + 0.509 value_reward_chosen[t] + 0.001 contr_diff + 0.434 reward + -0.437 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.33 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.428 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.72 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.074 1 + 0.998 value_choice[t] + -0.004 contr_diff + 0.054 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.009 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 70/1000 --- L(Train): 0.0158700 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.505 value_reward_chosen[t] + 0.001 contr_diff + 0.435 reward + -0.438 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.328 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.429 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.075 1 + 0.998 value_choice[t] + -0.004 contr_diff + 0.054 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.003 contr_diff^2 + -0.0 contr_diff*choice + 0.008 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 71/1000 --- L(Train): 0.0156852 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.502 value_reward_chosen[t] + 0.001 contr_diff + 0.436 reward + -0.439 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.326 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.43 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.076 1 + 0.998 value_choice[t] + -0.004 contr_diff + 0.054 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.007 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 72/1000 --- L(Train): 0.0155035 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.499 value_reward_chosen[t] + -0.001 contr_diff + 0.437 reward + -0.439 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.324 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.431 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.076 1 + 0.998 value_choice[t] + -0.002 contr_diff + 0.054 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.007 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 73/1000 --- L(Train): 0.0153252 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.496 value_reward_chosen[t] + 0.0 contr_diff + 0.438 reward + -0.44 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.322 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.431 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.724 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.077 1 + 0.998 value_choice[t] + 0.0 contr_diff + 0.054 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.006 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 74/1000 --- L(Train): 0.0151530 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.493 value_reward_chosen[t] + 0.001 contr_diff + 0.438 reward + -0.44 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.32 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.724 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.078 1 + 0.998 value_choice[t] + 0.001 contr_diff + 0.054 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.005 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 75/1000 --- L(Train): 0.0149865 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.49 value_reward_chosen[t] + 0.0 contr_diff + 0.439 reward + -0.44 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.317 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + -0.0 contr_diff + -0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.078 1 + 0.998 value_choice[t] + 0.001 contr_diff + 0.054 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.004 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 76/1000 --- L(Train): 0.0148249 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.487 value_reward_chosen[t] + -0.001 contr_diff + 0.44 reward + -0.44 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.315 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.079 1 + 0.998 value_choice[t] + -0.0 contr_diff + 0.055 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.003 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 77/1000 --- L(Train): 0.0146669 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.485 value_reward_chosen[t] + 0.0 contr_diff + 0.44 reward + -0.44 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.312 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.08 1 + 0.998 value_choice[t] + -0.0 contr_diff + 0.055 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 78/1000 --- L(Train): 0.0145113 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.482 value_reward_chosen[t] + 0.001 contr_diff + 0.44 reward + -0.44 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.31 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.433 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.726 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.08 1 + 0.998 value_choice[t] + 0.001 contr_diff + 0.055 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.002 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 79/1000 --- L(Train): 0.0143580 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.479 value_reward_chosen[t] + 0.0 contr_diff + 0.441 reward + -0.44 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.307 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.433 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.726 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.081 1 + 0.998 value_choice[t] + 0.0 contr_diff + 0.055 choice + -0.008 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 80/1000 --- L(Train): 0.0142110 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.477 value_reward_chosen[t] + -0.001 contr_diff + 0.441 reward + -0.44 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.305 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.082 1 + 0.998 value_choice[t] + -0.001 contr_diff + 0.055 choice + -0.008 value_choice^2 + -0.003 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 81/1000 --- L(Train): 0.0140704 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.474 value_reward_chosen[t] + -0.0 contr_diff + 0.441 reward + -0.44 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.302 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.083 1 + 0.998 value_choice[t] + -0.001 contr_diff + 0.056 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 82/1000 --- L(Train): 0.0139286 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.471 value_reward_chosen[t] + 0.003 contr_diff + 0.441 reward + -0.439 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.3 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.083 1 + 0.998 value_choice[t] + -0.0 contr_diff + 0.056 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 83/1000 --- L(Train): 0.0137879 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.469 value_reward_chosen[t] + 0.005 contr_diff + 0.441 reward + -0.439 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.297 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.084 1 + 0.998 value_choice[t] + 0.002 contr_diff + 0.056 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 84/1000 --- L(Train): 0.0136518 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.466 value_reward_chosen[t] + 0.005 contr_diff + 0.441 reward + -0.439 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.294 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.432 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.084 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.056 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 85/1000 --- L(Train): 0.0135203 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.464 value_reward_chosen[t] + 0.005 contr_diff + 0.441 reward + -0.438 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.292 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.431 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.085 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.056 choice + -0.009 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 86/1000 --- L(Train): 0.0133903 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.461 value_reward_chosen[t] + 0.004 contr_diff + 0.441 reward + -0.438 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.289 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.431 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.726 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.085 1 + 0.997 value_choice[t] + 0.0 contr_diff + 0.056 choice + -0.009 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 87/1000 --- L(Train): 0.0132622 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.459 value_reward_chosen[t] + 0.002 contr_diff + 0.441 reward + -0.437 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.286 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.43 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.142 1 + 0.725 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.086 1 + 0.997 value_choice[t] + -0.002 contr_diff + 0.056 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 88/1000 --- L(Train): 0.0131357 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.457 value_reward_chosen[t] + -0.001 contr_diff + 0.441 reward + -0.436 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.284 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.43 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.086 1 + 0.997 value_choice[t] + -0.003 contr_diff + 0.055 choice + -0.009 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.003 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 89/1000 --- L(Train): 0.0130134 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.454 value_reward_chosen[t] + -0.001 contr_diff + 0.441 reward + -0.436 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.281 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.429 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.086 1 + 0.997 value_choice[t] + -0.004 contr_diff + 0.055 choice + -0.009 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 90/1000 --- L(Train): 0.0128941 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.452 value_reward_chosen[t] + 0.0 contr_diff + 0.44 reward + -0.435 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.278 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.429 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.997 value_choice[t] + -0.003 contr_diff + 0.055 choice + -0.009 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 91/1000 --- L(Train): 0.0127742 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.45 value_reward_chosen[t] + 0.001 contr_diff + 0.44 reward + -0.434 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.275 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.428 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.997 value_choice[t] + -0.001 contr_diff + 0.055 choice + -0.009 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 92/1000 --- L(Train): 0.0126582 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.166 1 + 0.447 value_reward_chosen[t] + 0.001 contr_diff + 0.44 reward + -0.433 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.273 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.427 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.725 value_reward_not_chosen[t] + 0.003 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.054 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 93/1000 --- L(Train): 0.0125449 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.166 1 + 0.445 value_reward_chosen[t] + -0.0 contr_diff + 0.439 reward + -0.433 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.27 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.427 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.724 value_reward_not_chosen[t] + 0.006 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.997 value_choice[t] + 0.003 contr_diff + 0.054 choice + -0.009 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 94/1000 --- L(Train): 0.0124330 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.443 value_reward_chosen[t] + 0.001 contr_diff + 0.439 reward + -0.432 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.267 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.426 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.724 value_reward_not_chosen[t] + 0.007 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.997 value_choice[t] + 0.003 contr_diff + 0.053 choice + -0.009 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 95/1000 --- L(Train): 0.0123226 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.44 value_reward_chosen[t] + 0.001 contr_diff + 0.439 reward + -0.431 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.265 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.425 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.724 value_reward_not_chosen[t] + 0.008 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.087 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.053 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 96/1000 --- L(Train): 0.0122144 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.438 value_reward_chosen[t] + 0.0 contr_diff + 0.439 reward + -0.43 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.262 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.425 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.724 value_reward_not_chosen[t] + 0.009 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + 0.0 contr_diff + 0.052 choice + -0.009 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 97/1000 --- L(Train): 0.0121085 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.436 value_reward_chosen[t] + -0.001 contr_diff + 0.438 reward + -0.429 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.259 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.424 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.723 value_reward_not_chosen[t] + 0.008 contr_diff + 0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + -0.003 contr_diff + 0.052 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 98/1000 --- L(Train): 0.0120062 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.434 value_reward_chosen[t] + -0.0 contr_diff + 0.438 reward + -0.428 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.256 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.423 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.723 value_reward_not_chosen[t] + 0.008 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + -0.004 contr_diff + 0.051 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 99/1000 --- L(Train): 0.0119041 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.432 value_reward_chosen[t] + 0.002 contr_diff + 0.438 reward + -0.427 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.254 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.003 contr_diff*reward + 0.423 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.723 value_reward_not_chosen[t] + 0.006 contr_diff + -0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + -0.005 contr_diff + 0.051 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 100/1000 --- L(Train): 0.0118013 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.43 value_reward_chosen[t] + 0.004 contr_diff + 0.437 reward + -0.426 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.251 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.002 contr_diff*reward + 0.422 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.723 value_reward_not_chosen[t] + 0.005 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + -0.004 contr_diff + 0.05 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 101/1000 --- L(Train): 0.0117029 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.427 value_reward_chosen[t] + 0.004 contr_diff + 0.437 reward + -0.425 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.248 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.001 contr_diff*reward + 0.421 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + -0.003 contr_diff + 0.05 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 102/1000 --- L(Train): 0.0116076 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.425 value_reward_chosen[t] + 0.003 contr_diff + 0.436 reward + -0.424 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.246 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.421 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.088 1 + 0.997 value_choice[t] + -0.0 contr_diff + 0.05 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 103/1000 --- L(Train): 0.0115134 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.423 value_reward_chosen[t] + 0.002 contr_diff + 0.436 reward + -0.423 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.243 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.42 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.089 1 + 0.997 value_choice[t] + 0.003 contr_diff + 0.049 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 104/1000 --- L(Train): 0.0114195 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.421 value_reward_chosen[t] + -0.0 contr_diff + 0.436 reward + -0.422 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.24 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.419 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.089 1 + 0.997 value_choice[t] + 0.005 contr_diff + 0.049 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 105/1000 --- L(Train): 0.0113265 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.419 value_reward_chosen[t] + -0.0 contr_diff + 0.435 reward + -0.42 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.238 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.418 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.089 1 + 0.997 value_choice[t] + 0.005 contr_diff + 0.049 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 106/1000 --- L(Train): 0.0112344 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.417 value_reward_chosen[t] + 0.001 contr_diff + 0.435 reward + -0.419 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.235 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.418 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.09 1 + 0.997 value_choice[t] + 0.004 contr_diff + 0.049 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 107/1000 --- L(Train): 0.0111453 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.415 value_reward_chosen[t] + 0.002 contr_diff + 0.435 reward + -0.418 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.233 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.417 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.004 contr_diff + 0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.09 1 + 0.997 value_choice[t] + 0.002 contr_diff + 0.048 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 108/1000 --- L(Train): 0.0110557 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.412 value_reward_chosen[t] + 0.002 contr_diff + 0.434 reward + -0.417 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.23 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.416 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.005 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.09 1 + 0.997 value_choice[t] + -0.0 contr_diff + 0.048 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 109/1000 --- L(Train): 0.0109689 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.41 value_reward_chosen[t] + 0.001 contr_diff + 0.434 reward + -0.416 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.228 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.416 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.005 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.09 1 + 0.997 value_choice[t] + -0.002 contr_diff + 0.047 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 110/1000 --- L(Train): 0.0108829 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.408 value_reward_chosen[t] + -0.001 contr_diff + 0.434 reward + -0.415 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.225 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.415 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.005 contr_diff + -0.002 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.091 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.047 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 111/1000 --- L(Train): 0.0107985 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.406 value_reward_chosen[t] + -0.0 contr_diff + 0.434 reward + -0.414 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.223 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.414 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.004 contr_diff + -0.002 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.091 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.047 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 112/1000 --- L(Train): 0.0107144 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.404 value_reward_chosen[t] + 0.002 contr_diff + 0.433 reward + -0.412 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.22 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.414 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.003 contr_diff + -0.002 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.092 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.047 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 113/1000 --- L(Train): 0.0106319 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.402 value_reward_chosen[t] + 0.003 contr_diff + 0.433 reward + -0.411 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.218 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.413 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.092 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.047 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 114/1000 --- L(Train): 0.0105499 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.4 value_reward_chosen[t] + 0.003 contr_diff + 0.433 reward + -0.41 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.215 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.412 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.092 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.046 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 115/1000 --- L(Train): 0.0104706 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.398 value_reward_chosen[t] + 0.002 contr_diff + 0.432 reward + -0.409 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.213 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.412 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + -0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.093 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.046 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 116/1000 --- L(Train): 0.0103934 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.396 value_reward_chosen[t] + 0.001 contr_diff + 0.432 reward + -0.408 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.211 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.411 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.093 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.046 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 117/1000 --- L(Train): 0.0103156 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.394 value_reward_chosen[t] + -0.002 contr_diff + 0.432 reward + -0.406 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.208 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.411 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.094 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.046 choice + -0.008 value_choice^2 + 0.003 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 118/1000 --- L(Train): 0.0102380 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.392 value_reward_chosen[t] + -0.002 contr_diff + 0.432 reward + -0.405 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.206 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.41 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.004 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.094 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.046 choice + -0.008 value_choice^2 + 0.003 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 119/1000 --- L(Train): 0.0101628 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.39 value_reward_chosen[t] + -0.0 contr_diff + 0.432 reward + -0.404 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.204 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.409 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.005 contr_diff + -0.002 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.095 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.045 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 120/1000 --- L(Train): 0.0100908 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.388 value_reward_chosen[t] + 0.003 contr_diff + 0.432 reward + -0.403 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.201 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.409 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.006 contr_diff + -0.002 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.095 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.045 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 121/1000 --- L(Train): 0.0100191 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.386 value_reward_chosen[t] + 0.005 contr_diff + 0.431 reward + -0.401 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.199 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.408 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.721 value_reward_not_chosen[t] + 0.007 contr_diff + -0.001 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.095 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.045 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 122/1000 --- L(Train): 0.0099447 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.384 value_reward_chosen[t] + 0.006 contr_diff + 0.431 reward + -0.4 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.197 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.408 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.006 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.096 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.044 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 123/1000 --- L(Train): 0.0098723 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.382 value_reward_chosen[t] + 0.006 contr_diff + 0.431 reward + -0.399 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.195 value_reward_chosen*reward + -0.003 contr_diff^2 + 0.001 contr_diff*reward + 0.407 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.006 contr_diff + 0.002 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.096 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.044 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 124/1000 --- L(Train): 0.0098037 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.38 value_reward_chosen[t] + 0.005 contr_diff + 0.431 reward + -0.397 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.193 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.407 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.005 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.096 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.044 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 125/1000 --- L(Train): 0.0097360 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.378 value_reward_chosen[t] + 0.003 contr_diff + 0.431 reward + -0.396 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.191 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.406 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.096 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.043 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 126/1000 --- L(Train): 0.0096687 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.376 value_reward_chosen[t] + 0.0 contr_diff + 0.431 reward + -0.395 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.188 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.406 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.097 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.043 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 127/1000 --- L(Train): 0.0096019 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.374 value_reward_chosen[t] + -0.003 contr_diff + 0.431 reward + -0.394 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.186 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.405 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + -0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.097 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.042 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.003 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 128/1000 --- L(Train): 0.0095362 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.372 value_reward_chosen[t] + -0.004 contr_diff + 0.431 reward + -0.392 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.184 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.405 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.097 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.042 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 129/1000 --- L(Train): 0.0094719 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.37 value_reward_chosen[t] + -0.003 contr_diff + 0.431 reward + -0.391 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.182 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.404 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.097 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.041 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 130/1000 --- L(Train): 0.0094099 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.368 value_reward_chosen[t] + -0.0 contr_diff + 0.431 reward + -0.39 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.18 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.404 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.002 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.098 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.041 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 131/1000 --- L(Train): 0.0093470 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.366 value_reward_chosen[t] + 0.004 contr_diff + 0.43 reward + -0.388 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.179 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.003 contr_diff*reward + 0.404 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + 0.003 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.098 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.041 choice + -0.008 value_choice^2 + 0.003 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 132/1000 --- L(Train): 0.0092867 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.364 value_reward_chosen[t] + 0.006 contr_diff + 0.43 reward + -0.387 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.177 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.403 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.003 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.099 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.041 choice + -0.007 value_choice^2 + 0.003 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 133/1000 --- L(Train): 0.0092260 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.362 value_reward_chosen[t] + 0.007 contr_diff + 0.43 reward + -0.385 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.175 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.403 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.099 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.04 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 134/1000 --- L(Train): 0.0091651 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.36 value_reward_chosen[t] + 0.008 contr_diff + 0.43 reward + -0.384 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.173 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.402 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.004 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.099 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.04 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 135/1000 --- L(Train): 0.0091064 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.358 value_reward_chosen[t] + 0.007 contr_diff + 0.43 reward + -0.383 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.171 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.004 contr_diff*reward + 0.402 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.004 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.099 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.04 choice + -0.008 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 136/1000 --- L(Train): 0.0090491 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.356 value_reward_chosen[t] + 0.005 contr_diff + 0.43 reward + -0.381 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.169 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.005 contr_diff*reward + 0.401 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.004 contr_diff + -0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.1 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.039 choice + -0.008 value_choice^2 + -0.004 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 137/1000 --- L(Train): 0.0089939 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.354 value_reward_chosen[t] + 0.003 contr_diff + 0.43 reward + -0.38 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.167 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.004 contr_diff*reward + 0.401 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.1 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.039 choice + -0.008 value_choice^2 + -0.004 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 138/1000 --- L(Train): 0.0089413 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.352 value_reward_chosen[t] + -0.0 contr_diff + 0.43 reward + -0.379 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.166 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.401 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.101 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.039 choice + -0.008 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + 0.003 contr_diff^2 + 0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 139/1000 --- L(Train): 0.0088828 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.35 value_reward_chosen[t] + -0.001 contr_diff + 0.43 reward + -0.377 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.164 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.4 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.101 1 + 0.996 value_choice[t] + 0.002 contr_diff + 0.038 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.003 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 140/1000 --- L(Train): 0.0088254 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.348 value_reward_chosen[t] + -0.0 contr_diff + 0.43 reward + -0.376 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.162 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.4 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.101 1 + 0.996 value_choice[t] + 0.002 contr_diff + 0.038 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 141/1000 --- L(Train): 0.0087736 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.346 value_reward_chosen[t] + 0.002 contr_diff + 0.431 reward + -0.374 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.16 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.004 contr_diff*reward + 0.4 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.102 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.038 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 142/1000 --- L(Train): 0.0087227 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.344 value_reward_chosen[t] + 0.003 contr_diff + 0.431 reward + -0.373 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.159 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.005 contr_diff*reward + 0.399 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.102 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.037 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 143/1000 --- L(Train): 0.0086709 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.342 value_reward_chosen[t] + 0.003 contr_diff + 0.431 reward + -0.371 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.157 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.004 contr_diff*reward + 0.399 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.103 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.037 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 144/1000 --- L(Train): 0.0086198 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.341 value_reward_chosen[t] + 0.002 contr_diff + 0.431 reward + -0.37 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.156 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.003 contr_diff*reward + 0.398 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.005 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.103 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.037 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 145/1000 --- L(Train): 0.0085685 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.339 value_reward_chosen[t] + 0.001 contr_diff + 0.431 reward + -0.368 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.154 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.398 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.104 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.037 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 146/1000 --- L(Train): 0.0085187 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.337 value_reward_chosen[t] + -0.002 contr_diff + 0.431 reward + -0.367 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.152 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.398 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.104 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.036 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 147/1000 --- L(Train): 0.0084748 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.335 value_reward_chosen[t] + -0.002 contr_diff + 0.431 reward + -0.365 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.151 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.005 contr_diff*reward + 0.397 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.104 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.036 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 148/1000 --- L(Train): 0.0084290 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.333 value_reward_chosen[t] + -0.001 contr_diff + 0.431 reward + -0.364 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.149 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.006 contr_diff*reward + 0.397 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.105 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.035 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 149/1000 --- L(Train): 0.0083797 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.331 value_reward_chosen[t] + 0.002 contr_diff + 0.431 reward + -0.362 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.148 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.005 contr_diff*reward + 0.397 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.105 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.035 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 150/1000 --- L(Train): 0.0083330 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.154 1 + 0.33 value_reward_chosen[t] + 0.004 contr_diff + 0.431 reward + -0.361 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.146 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.004 contr_diff*reward + 0.396 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.105 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.035 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 151/1000 --- L(Train): 0.0082878 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.328 value_reward_chosen[t] + 0.004 contr_diff + 0.431 reward + -0.359 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.145 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.396 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.106 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.034 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 152/1000 --- L(Train): 0.0082445 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.326 value_reward_chosen[t] + 0.004 contr_diff + 0.431 reward + -0.358 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.143 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.396 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.106 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.034 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 153/1000 --- L(Train): 0.0082022 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.324 value_reward_chosen[t] + 0.003 contr_diff + 0.431 reward + -0.356 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.142 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.395 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.106 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.033 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 154/1000 --- L(Train): 0.0081587 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.322 value_reward_chosen[t] + 0.001 contr_diff + 0.431 reward + -0.354 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.141 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.004 contr_diff*reward + 0.395 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.106 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.033 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 155/1000 --- L(Train): 0.0081151 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.321 value_reward_chosen[t] + -0.002 contr_diff + 0.432 reward + -0.353 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.139 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.395 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.107 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.033 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 156/1000 --- L(Train): 0.0080747 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.319 value_reward_chosen[t] + -0.003 contr_diff + 0.432 reward + -0.351 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.138 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.394 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.107 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.032 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 157/1000 --- L(Train): 0.0080344 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.317 value_reward_chosen[t] + -0.002 contr_diff + 0.432 reward + -0.35 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.137 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.394 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.108 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.032 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 158/1000 --- L(Train): 0.0079959 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.315 value_reward_chosen[t] + 0.001 contr_diff + 0.432 reward + -0.348 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.135 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.394 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.108 1 + 0.995 value_choice[t] + -0.0 contr_diff + 0.032 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 159/1000 --- L(Train): 0.0079556 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.314 value_reward_chosen[t] + 0.002 contr_diff + 0.432 reward + -0.346 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.134 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.393 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.108 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.031 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 160/1000 --- L(Train): 0.0079151 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.312 value_reward_chosen[t] + 0.002 contr_diff + 0.432 reward + -0.345 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.133 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.393 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.109 1 + 0.995 value_choice[t] + 0.003 contr_diff + 0.031 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 161/1000 --- L(Train): 0.0078756 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.31 value_reward_chosen[t] + 0.002 contr_diff + 0.432 reward + -0.343 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.131 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.393 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + -0.001 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.109 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.031 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 162/1000 --- L(Train): 0.0078413 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.155 1 + 0.308 value_reward_chosen[t] + 0.0 contr_diff + 0.432 reward + -0.341 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.13 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.392 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.11 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.03 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 163/1000 --- L(Train): 0.0078034 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.307 value_reward_chosen[t] + -0.002 contr_diff + 0.432 reward + -0.34 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.129 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.392 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.11 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.03 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 164/1000 --- L(Train): 0.0077655 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.305 value_reward_chosen[t] + -0.002 contr_diff + 0.433 reward + -0.338 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.128 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.392 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.11 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.029 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 165/1000 --- L(Train): 0.0077305 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.303 value_reward_chosen[t] + -0.001 contr_diff + 0.433 reward + -0.336 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.126 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.391 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.111 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.029 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 166/1000 --- L(Train): 0.0076967 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.302 value_reward_chosen[t] + 0.002 contr_diff + 0.433 reward + -0.334 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.125 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.391 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.111 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.028 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 167/1000 --- L(Train): 0.0076625 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.3 value_reward_chosen[t] + 0.003 contr_diff + 0.433 reward + -0.333 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.124 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.391 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.111 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.028 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 168/1000 --- L(Train): 0.0076266 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.299 value_reward_chosen[t] + 0.004 contr_diff + 0.433 reward + -0.331 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.123 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.39 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + -0.003 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.112 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.028 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 169/1000 --- L(Train): 0.0075934 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.156 1 + 0.297 value_reward_chosen[t] + 0.003 contr_diff + 0.433 reward + -0.329 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.122 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.39 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.003 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.112 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.027 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 170/1000 --- L(Train): 0.0075616 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.295 value_reward_chosen[t] + 0.002 contr_diff + 0.433 reward + -0.327 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.121 value_reward_chosen*reward + 0.003 contr_diff^2 + -0.001 contr_diff*reward + 0.39 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.113 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.027 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 171/1000 --- L(Train): 0.0075302 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.294 value_reward_chosen[t] + -0.0 contr_diff + 0.433 reward + -0.326 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.12 value_reward_chosen*reward + 0.003 contr_diff^2 + -0.001 contr_diff*reward + 0.39 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.113 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.027 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 172/1000 --- L(Train): 0.0074971 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.292 value_reward_chosen[t] + -0.001 contr_diff + 0.434 reward + -0.324 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.118 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.389 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.003 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.113 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.026 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 173/1000 --- L(Train): 0.0074659 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.29 value_reward_chosen[t] + 0.0 contr_diff + 0.434 reward + -0.322 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.117 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.389 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.004 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.114 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.026 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 174/1000 --- L(Train): 0.0074361 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.289 value_reward_chosen[t] + 0.001 contr_diff + 0.434 reward + -0.32 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.116 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.389 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.004 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.114 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.026 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 175/1000 --- L(Train): 0.0074060 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.157 1 + 0.287 value_reward_chosen[t] + -0.0 contr_diff + 0.434 reward + -0.318 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.115 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.388 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.004 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.115 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.026 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 176/1000 --- L(Train): 0.0073764 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.286 value_reward_chosen[t] + 0.001 contr_diff + 0.434 reward + -0.317 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.114 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.388 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.116 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.025 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 177/1000 --- L(Train): 0.0073462 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.284 value_reward_chosen[t] + 0.0 contr_diff + 0.434 reward + -0.315 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.113 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.388 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.116 1 + 0.996 value_choice[t] + -0.002 contr_diff + 0.025 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.003 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 178/1000 --- L(Train): 0.0073173 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.283 value_reward_chosen[t] + -0.0 contr_diff + 0.434 reward + -0.313 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.112 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.387 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.116 1 + 0.996 value_choice[t] + -0.001 contr_diff + 0.024 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.003 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 179/1000 --- L(Train): 0.0072886 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.281 value_reward_chosen[t] + 0.0 contr_diff + 0.435 reward + -0.311 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.111 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.387 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.116 1 + 0.995 value_choice[t] + -0.0 contr_diff + 0.024 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.003 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 180/1000 --- L(Train): 0.0072605 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.28 value_reward_chosen[t] + -0.0 contr_diff + 0.435 reward + -0.309 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.11 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.387 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.116 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.023 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 181/1000 --- L(Train): 0.0072334 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.158 1 + 0.278 value_reward_chosen[t] + 0.001 contr_diff + 0.435 reward + -0.307 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.109 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.386 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.117 1 + 0.995 value_choice[t] + 0.003 contr_diff + 0.022 choice + -0.008 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 182/1000 --- L(Train): 0.0072071 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.277 value_reward_chosen[t] + 0.001 contr_diff + 0.435 reward + -0.305 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.108 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.386 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.002 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.117 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.022 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.003 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 183/1000 --- L(Train): 0.0071808 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.275 value_reward_chosen[t] + 0.0 contr_diff + 0.435 reward + -0.304 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.107 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.386 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.117 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.022 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.001 value_choice*choice + -0.003 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 184/1000 --- L(Train): 0.0071551 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.274 value_reward_chosen[t] + -0.001 contr_diff + 0.435 reward + -0.302 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.106 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.385 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.118 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.021 choice + -0.007 value_choice^2 + -0.003 value_choice*contr_diff + 0.001 value_choice*choice + -0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 185/1000 --- L(Train): 0.0071307 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.272 value_reward_chosen[t] + -0.001 contr_diff + 0.436 reward + -0.3 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.106 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.385 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.119 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.021 choice + -0.007 value_choice^2 + -0.003 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 186/1000 --- L(Train): 0.0071074 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.159 1 + 0.271 value_reward_chosen[t] + 0.0 contr_diff + 0.436 reward + -0.298 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.105 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.385 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.119 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.021 choice + -0.007 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 187/1000 --- L(Train): 0.0070821 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.269 value_reward_chosen[t] + 0.001 contr_diff + 0.436 reward + -0.296 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.104 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.385 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.12 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.02 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.004 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 188/1000 --- L(Train): 0.0070570 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.268 value_reward_chosen[t] + 0.001 contr_diff + 0.436 reward + -0.294 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.103 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.384 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.12 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.02 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.004 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 189/1000 --- L(Train): 0.0070344 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.266 value_reward_chosen[t] + -0.001 contr_diff + 0.436 reward + -0.292 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.102 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.384 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.12 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.02 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.003 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 190/1000 --- L(Train): 0.0070120 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.16 1 + 0.265 value_reward_chosen[t] + -0.0 contr_diff + 0.436 reward + -0.29 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.101 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.384 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.121 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.019 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + -0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 191/1000 --- L(Train): 0.0069894 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.264 value_reward_chosen[t] + 0.001 contr_diff + 0.437 reward + -0.288 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.1 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.383 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.121 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.019 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 192/1000 --- L(Train): 0.0069678 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.262 value_reward_chosen[t] + 0.002 contr_diff + 0.437 reward + -0.286 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.1 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.383 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.122 1 + 0.996 value_choice[t] + -0.0 contr_diff + 0.019 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.002 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 193/1000 --- L(Train): 0.0069466 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.261 value_reward_chosen[t] + 0.002 contr_diff + 0.437 reward + -0.284 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.099 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.383 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.122 1 + 0.996 value_choice[t] + 0.001 contr_diff + 0.018 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 194/1000 --- L(Train): 0.0069243 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.259 value_reward_chosen[t] + 0.001 contr_diff + 0.437 reward + -0.282 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.098 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.382 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.123 1 + 0.996 value_choice[t] + 0.0 contr_diff + 0.018 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 195/1000 --- L(Train): 0.0069027 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.161 1 + 0.258 value_reward_chosen[t] + -0.001 contr_diff + 0.437 reward + -0.28 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.097 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.382 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.123 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.017 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 196/1000 --- L(Train): 0.0068831 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.257 value_reward_chosen[t] + -0.002 contr_diff + 0.438 reward + -0.278 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.097 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.382 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.123 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.016 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 197/1000 --- L(Train): 0.0068637 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.255 value_reward_chosen[t] + -0.001 contr_diff + 0.438 reward + -0.276 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.096 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.382 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.123 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.016 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 198/1000 --- L(Train): 0.0068424 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.254 value_reward_chosen[t] + 0.002 contr_diff + 0.438 reward + -0.274 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.095 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.381 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + 0.003 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.123 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.015 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 199/1000 --- L(Train): 0.0068214 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.162 1 + 0.253 value_reward_chosen[t] + 0.003 contr_diff + 0.438 reward + -0.272 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.094 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.381 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.003 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.124 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.015 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 200/1000 --- L(Train): 0.0068021 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.251 value_reward_chosen[t] + 0.003 contr_diff + 0.438 reward + -0.27 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.094 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.381 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.125 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.015 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.002 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 201/1000 --- L(Train): 0.0067833 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.25 value_reward_chosen[t] + 0.003 contr_diff + 0.439 reward + -0.268 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.093 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.38 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.125 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.015 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 202/1000 --- L(Train): 0.0067662 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.249 value_reward_chosen[t] + 0.001 contr_diff + 0.439 reward + -0.266 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.092 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.38 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.126 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.014 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 203/1000 --- L(Train): 0.0067477 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.163 1 + 0.247 value_reward_chosen[t] + -0.001 contr_diff + 0.439 reward + -0.264 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.092 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.38 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.127 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.014 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 204/1000 --- L(Train): 0.0067289 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.246 value_reward_chosen[t] + -0.002 contr_diff + 0.439 reward + -0.262 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.091 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.379 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.127 1 + 0.995 value_choice[t] + -0.0 contr_diff + 0.014 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.003 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 205/1000 --- L(Train): 0.0067132 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.245 value_reward_chosen[t] + -0.001 contr_diff + 0.439 reward + -0.26 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.091 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.379 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.128 1 + 0.996 value_choice[t] + 0.002 contr_diff + 0.013 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.003 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 206/1000 --- L(Train): 0.0066963 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.243 value_reward_chosen[t] + 0.001 contr_diff + 0.44 reward + -0.258 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.09 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.379 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.128 1 + 0.996 value_choice[t] + 0.003 contr_diff + 0.013 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 207/1000 --- L(Train): 0.0066804 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.164 1 + 0.242 value_reward_chosen[t] + 0.002 contr_diff + 0.44 reward + -0.256 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.089 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.379 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.128 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.012 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + -0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 208/1000 --- L(Train): 0.0066638 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.241 value_reward_chosen[t] + 0.002 contr_diff + 0.44 reward + -0.254 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.089 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.378 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.129 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.012 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 209/1000 --- L(Train): 0.0066474 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.239 value_reward_chosen[t] + 0.001 contr_diff + 0.44 reward + -0.252 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.088 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.378 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.129 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.011 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 210/1000 --- L(Train): 0.0066324 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.238 value_reward_chosen[t] + -0.001 contr_diff + 0.44 reward + -0.25 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.088 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.378 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.129 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.011 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 211/1000 --- L(Train): 0.0066165 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.165 1 + 0.237 value_reward_chosen[t] + -0.001 contr_diff + 0.441 reward + -0.248 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.087 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.377 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.13 1 + 0.995 value_choice[t] + -0.002 contr_diff + 0.011 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 212/1000 --- L(Train): 0.0065990 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.166 1 + 0.236 value_reward_chosen[t] + 0.0 contr_diff + 0.441 reward + -0.246 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.086 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.377 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.13 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.01 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 213/1000 --- L(Train): 0.0065849 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.166 1 + 0.234 value_reward_chosen[t] + 0.0 contr_diff + 0.441 reward + -0.244 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.086 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.377 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.13 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.009 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 214/1000 --- L(Train): 0.0065709 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.166 1 + 0.233 value_reward_chosen[t] + -0.0 contr_diff + 0.441 reward + -0.242 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.085 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.377 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.131 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.009 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 215/1000 --- L(Train): 0.0065559 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.232 value_reward_chosen[t] + 0.0 contr_diff + 0.442 reward + -0.239 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.085 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.376 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.131 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.008 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 216/1000 --- L(Train): 0.0065399 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.231 value_reward_chosen[t] + 0.0 contr_diff + 0.442 reward + -0.237 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.084 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.376 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.131 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.007 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 217/1000 --- L(Train): 0.0065263 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.229 value_reward_chosen[t] + -0.001 contr_diff + 0.442 reward + -0.235 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.084 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.376 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.132 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.007 choice + -0.008 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 218/1000 --- L(Train): 0.0065135 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.167 1 + 0.228 value_reward_chosen[t] + -0.001 contr_diff + 0.442 reward + -0.233 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.083 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.375 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.133 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.007 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 219/1000 --- L(Train): 0.0065011 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.227 value_reward_chosen[t] + 0.001 contr_diff + 0.443 reward + -0.231 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.083 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.375 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.133 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.007 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 220/1000 --- L(Train): 0.0064874 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.226 value_reward_chosen[t] + 0.002 contr_diff + 0.443 reward + -0.229 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.082 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.375 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.134 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.007 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 221/1000 --- L(Train): 0.0064752 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.168 1 + 0.225 value_reward_chosen[t] + 0.001 contr_diff + 0.443 reward + -0.227 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.082 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.375 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.134 1 + 0.995 value_choice[t] + -0.0 contr_diff + 0.006 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 222/1000 --- L(Train): 0.0064619 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.223 value_reward_chosen[t] + -0.0 contr_diff + 0.443 reward + -0.225 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.082 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.374 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.135 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.006 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 223/1000 --- L(Train): 0.0064485 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.222 value_reward_chosen[t] + 0.0 contr_diff + 0.443 reward + -0.223 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.081 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.374 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.135 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.005 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 224/1000 --- L(Train): 0.0064349 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.169 1 + 0.221 value_reward_chosen[t] + -0.001 contr_diff + 0.444 reward + -0.221 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.081 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.374 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.135 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.004 choice + -0.008 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 225/1000 --- L(Train): 0.0064223 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.17 1 + 0.22 value_reward_chosen[t] + -0.0 contr_diff + 0.444 reward + -0.218 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.08 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.374 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.136 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.004 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 226/1000 --- L(Train): 0.0064113 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.17 1 + 0.219 value_reward_chosen[t] + 0.002 contr_diff + 0.444 reward + -0.216 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.08 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.373 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.136 1 + 0.995 value_choice[t] + 0.0 contr_diff + 0.004 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 227/1000 --- L(Train): 0.0063975 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.17 1 + 0.218 value_reward_chosen[t] + 0.003 contr_diff + 0.444 reward + -0.214 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.079 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.373 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.137 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.003 choice + -0.008 value_choice^2 + -0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 228/1000 --- L(Train): 0.0063860 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.17 1 + 0.216 value_reward_chosen[t] + 0.002 contr_diff + 0.445 reward + -0.212 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.079 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.373 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.0 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.137 1 + 0.995 value_choice[t] + -0.001 contr_diff + 0.003 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 229/1000 --- L(Train): 0.0063769 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.171 1 + 0.215 value_reward_chosen[t] + 0.001 contr_diff + 0.445 reward + -0.21 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.079 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.372 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.138 1 + 0.995 value_choice[t] + -0.0 contr_diff + 0.003 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 230/1000 --- L(Train): 0.0063661 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.171 1 + 0.214 value_reward_chosen[t] + -0.001 contr_diff + 0.445 reward + -0.208 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.078 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.0 contr_diff*reward + 0.372 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.139 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.002 choice + -0.007 value_choice^2 + 0.003 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 231/1000 --- L(Train): 0.0063530 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.171 1 + 0.213 value_reward_chosen[t] + -0.001 contr_diff + 0.446 reward + -0.206 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.078 value_reward_chosen*reward + 0.003 contr_diff^2 + -0.0 contr_diff*reward + 0.372 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.139 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.002 choice + -0.007 value_choice^2 + 0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 232/1000 --- L(Train): 0.0063430 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.172 1 + 0.212 value_reward_chosen[t] + -0.0 contr_diff + 0.446 reward + -0.204 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.078 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.372 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.139 1 + 0.995 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 233/1000 --- L(Train): 0.0063344 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.172 1 + 0.211 value_reward_chosen[t] + 0.002 contr_diff + 0.446 reward + -0.202 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.077 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.371 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.14 1 + 0.995 value_choice[t] + -0.0 contr_diff + 0.0 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 234/1000 --- L(Train): 0.0063241 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.172 1 + 0.21 value_reward_chosen[t] + 0.003 contr_diff + 0.446 reward + -0.199 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.077 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.371 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.14 1 + 0.995 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 235/1000 --- L(Train): 0.0063150 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.173 1 + 0.209 value_reward_chosen[t] + 0.003 contr_diff + 0.447 reward + -0.197 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.077 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.371 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.14 1 + 0.995 value_choice[t] + -0.0 contr_diff + -0.0 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 236/1000 --- L(Train): 0.0063048 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.173 1 + 0.207 value_reward_chosen[t] + 0.002 contr_diff + 0.447 reward + -0.195 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.076 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.37 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 237/1000 --- L(Train): 0.0062955 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.173 1 + 0.206 value_reward_chosen[t] + -0.0 contr_diff + 0.447 reward + -0.193 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.076 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.37 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.995 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 238/1000 --- L(Train): 0.0062846 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.174 1 + 0.205 value_reward_chosen[t] + -0.001 contr_diff + 0.447 reward + -0.191 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.076 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.37 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.995 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 239/1000 --- L(Train): 0.0062750 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.174 1 + 0.204 value_reward_chosen[t] + 0.0 contr_diff + 0.448 reward + -0.189 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.075 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.37 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.003 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.008 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 240/1000 --- L(Train): 0.0062666 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.174 1 + 0.203 value_reward_chosen[t] + 0.0 contr_diff + 0.448 reward + -0.187 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.075 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.369 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.003 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\u001b[H\u001b[2J\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 241/1000 --- L(Train): 0.0062578 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.175 1 + 0.202 value_reward_chosen[t] + -0.001 contr_diff + 0.448 reward + -0.184 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.075 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.369 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.008 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 242/1000 --- L(Train): 0.0062479 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.175 1 + 0.201 value_reward_chosen[t] + -0.0 contr_diff + 0.448 reward + -0.182 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.074 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.369 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + -0.0 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 243/1000 --- L(Train): 0.0062392 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.175 1 + 0.2 value_reward_chosen[t] + 0.001 contr_diff + 0.449 reward + -0.18 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.074 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.369 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + 0.002 contr_diff + -0.0 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 244/1000 --- L(Train): 0.0062304 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.176 1 + 0.199 value_reward_chosen[t] + 0.002 contr_diff + 0.449 reward + -0.178 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.074 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.368 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.007 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 245/1000 --- L(Train): 0.0062215 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.176 1 + 0.198 value_reward_chosen[t] + 0.002 contr_diff + 0.449 reward + -0.176 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.074 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.368 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + 0.002 contr_diff + -0.0 choice + -0.007 value_choice^2 + -0.002 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 246/1000 --- L(Train): 0.0062140 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.176 1 + 0.197 value_reward_chosen[t] + 0.0 contr_diff + 0.45 reward + -0.174 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.073 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.368 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 247/1000 --- L(Train): 0.0062062 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.177 1 + 0.196 value_reward_chosen[t] + -0.002 contr_diff + 0.45 reward + -0.172 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.073 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.368 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.003 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 248/1000 --- L(Train): 0.0061980 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.177 1 + 0.195 value_reward_chosen[t] + -0.002 contr_diff + 0.45 reward + -0.169 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.073 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.367 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.993 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.003 value_choice*contr_diff + -0.001 value_choice*choice + 0.003 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 249/1000 --- L(Train): 0.0061883 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.177 1 + 0.194 value_reward_chosen[t] + -0.002 contr_diff + 0.45 reward + -0.167 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.073 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.367 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.993 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.003 value_choice*contr_diff + -0.001 value_choice*choice + 0.002 contr_diff^2 + -0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 250/1000 --- L(Train): 0.0061800 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.178 1 + 0.193 value_reward_chosen[t] + 0.0 contr_diff + 0.451 reward + -0.165 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.072 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.367 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.993 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "value_reward_not_chosen: 0, 0, 0, 0, 0, 0\n",
            "value_choice: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 251/1000 --- L(Train): 0.0061733 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.178 1 + 0.191 value_reward_chosen[t] + 0.001 contr_diff + 0.451 reward + -0.163 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.072 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.366 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 1, 0, 0, 1, 0, 1, 1, 0\n",
            "value_reward_not_chosen: 0, 0, 1, 1, 1, 1\n",
            "value_choice: 0, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 252/1000 --- L(Train): 0.0061664 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.178 1 + 0.19 value_reward_chosen[t] + 0.001 contr_diff + 0.451 reward + -0.161 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.072 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.366 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.994 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.007 value_choice^2 + -0.003 value_choice*contr_diff + -0.0 value_choice*choice + -0.003 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 2, 0, 0, 2, 0, 2, 2, 0\n",
            "value_reward_not_chosen: 0, 0, 2, 2, 2, 2\n",
            "value_choice: 0, 2, 2, 2, 2, 2, 2, 2, 2, 2\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 253/1000 --- L(Train): 0.0061577 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.179 1 + 0.189 value_reward_chosen[t] + 0.0 contr_diff + 0.452 reward + -0.159 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.072 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.366 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.993 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.007 value_choice^2 + -0.004 value_choice*contr_diff + 0.0 value_choice*choice + -0.004 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 3, 0, 0, 3, 0, 3, 3, 0\n",
            "value_reward_not_chosen: 0, 0, 3, 3, 3, 3\n",
            "value_choice: 0, 3, 3, 3, 3, 3, 3, 3, 3, 3\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 254/1000 --- L(Train): 0.0061494 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.179 1 + 0.188 value_reward_chosen[t] + -0.002 contr_diff + 0.452 reward + -0.157 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.072 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.366 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.993 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.007 value_choice^2 + -0.004 value_choice*contr_diff + -0.0 value_choice*choice + -0.003 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 4, 0, 0, 4, 0, 4, 4, 0\n",
            "value_reward_not_chosen: 0, 0, 4, 4, 4, 4\n",
            "value_choice: 0, 4, 4, 4, 4, 4, 4, 4, 4, 4\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 255/1000 --- L(Train): 0.0061432 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.179 1 + 0.187 value_reward_chosen[t] + -0.002 contr_diff + 0.452 reward + -0.154 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.365 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.993 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.007 value_choice^2 + -0.004 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 5, 0, 0, 5, 0, 5, 5, 0\n",
            "value_reward_not_chosen: 0, 0, 5, 5, 5, 5\n",
            "value_choice: 0, 5, 5, 5, 5, 5, 5, 5, 5, 5\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 256/1000 --- L(Train): 0.0061381 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.18 1 + 0.186 value_reward_chosen[t] + -0.001 contr_diff + 0.452 reward + -0.152 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.365 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.993 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.007 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 6, 0, 0, 6, 0, 6, 6, 0\n",
            "value_reward_not_chosen: 0, 0, 6, 6, 6, 6\n",
            "value_choice: 0, 6, 6, 6, 6, 6, 6, 6, 6, 6\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 257/1000 --- L(Train): 0.0061310 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.18 1 + 0.185 value_reward_chosen[t] + 0.001 contr_diff + 0.453 reward + -0.15 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.365 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.993 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.003 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 7, 0, 0, 7, 0, 7, 7, 0\n",
            "value_reward_not_chosen: 0, 0, 7, 7, 7, 7\n",
            "value_choice: 0, 7, 7, 7, 7, 7, 7, 7, 7, 7\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 258/1000 --- L(Train): 0.0061223 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.18 1 + 0.184 value_reward_chosen[t] + 0.002 contr_diff + 0.453 reward + -0.148 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.003 contr_diff*reward + 0.365 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.003 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 8, 0, 0, 8, 0, 8, 8, 0\n",
            "value_reward_not_chosen: 0, 0, 8, 8, 8, 8\n",
            "value_choice: 0, 8, 8, 8, 8, 8, 8, 8, 8, 8\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 259/1000 --- L(Train): 0.0061156 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.181 1 + 0.183 value_reward_chosen[t] + 0.002 contr_diff + 0.453 reward + -0.146 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.003 contr_diff*reward + 0.364 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.007 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 9, 0, 0, 9, 0, 9, 9, 0\n",
            "value_reward_not_chosen: 0, 0, 9, 9, 9, 9\n",
            "value_choice: 0, 9, 9, 9, 9, 9, 9, 9, 9, 9\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 260/1000 --- L(Train): 0.0061092 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.181 1 + 0.182 value_reward_chosen[t] + 0.001 contr_diff + 0.454 reward + -0.144 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.071 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.364 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.007 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 10, 0, 0, 10, 0, 10, 10, 0\n",
            "value_reward_not_chosen: 0, 0, 10, 10, 10, 10\n",
            "value_choice: 0, 10, 10, 10, 10, 10, 10, 10, 10, 10\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 261/1000 --- L(Train): 0.0061034 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.181 1 + 0.181 value_reward_chosen[t] + -0.001 contr_diff + 0.454 reward + -0.142 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.07 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.364 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 11, 0, 0, 11, 0, 11, 11, 0\n",
            "value_reward_not_chosen: 0, 0, 11, 11, 11, 11\n",
            "value_choice: 0, 11, 11, 11, 11, 11, 11, 11, 11, 11\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 262/1000 --- L(Train): 0.0060976 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.182 1 + 0.18 value_reward_chosen[t] + -0.001 contr_diff + 0.454 reward + -0.139 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.07 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.364 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.007 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 12, 0, 0, 12, 0, 12, 12, 0\n",
            "value_reward_not_chosen: 0, 0, 12, 12, 12, 12\n",
            "value_choice: 0, 12, 12, 12, 12, 12, 12, 12, 12, 12\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 263/1000 --- L(Train): 0.0060923 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.182 1 + 0.179 value_reward_chosen[t] + -0.0 contr_diff + 0.455 reward + -0.137 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.07 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.363 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + -0.0 contr_diff + 0.001 choice + -0.007 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 13, 0, 0, 13, 0, 13, 13, 0\n",
            "value_reward_not_chosen: 0, 0, 13, 13, 13, 13\n",
            "value_choice: 0, 13, 13, 13, 13, 13, 13, 13, 13, 13\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 264/1000 --- L(Train): 0.0060844 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.182 1 + 0.179 value_reward_chosen[t] + 0.002 contr_diff + 0.455 reward + -0.135 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.07 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.363 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.002 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 14, 0, 0, 14, 0, 14, 14, 0\n",
            "value_reward_not_chosen: 0, 0, 14, 14, 14, 14\n",
            "value_choice: 0, 14, 14, 14, 14, 14, 14, 14, 14, 14\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 265/1000 --- L(Train): 0.0060802 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.183 1 + 0.178 value_reward_chosen[t] + 0.003 contr_diff + 0.455 reward + -0.133 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.07 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.363 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + 0.003 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.0 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 15, 0, 0, 15, 0, 15, 15, 0\n",
            "value_reward_not_chosen: 0, 0, 15, 15, 15, 15\n",
            "value_choice: 0, 15, 15, 15, 15, 15, 15, 15, 15, 15\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 266/1000 --- L(Train): 0.0060730 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.183 1 + 0.177 value_reward_chosen[t] + 0.003 contr_diff + 0.455 reward + -0.131 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.07 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.362 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 16, 0, 0, 16, 0, 16, 16, 0\n",
            "value_reward_not_chosen: 0, 0, 16, 16, 16, 16\n",
            "value_choice: 0, 16, 16, 16, 16, 16, 16, 16, 16, 16\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 267/1000 --- L(Train): 0.0060672 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.183 1 + 0.176 value_reward_chosen[t] + 0.001 contr_diff + 0.456 reward + -0.129 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.362 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 17, 0, 0, 17, 0, 17, 17, 0\n",
            "value_reward_not_chosen: 0, 0, 17, 17, 17, 17\n",
            "value_choice: 0, 17, 17, 17, 17, 17, 17, 17, 17, 17\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 268/1000 --- L(Train): 0.0060611 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.184 1 + 0.175 value_reward_chosen[t] + -0.001 contr_diff + 0.456 reward + -0.127 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.362 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.141 1 + 0.992 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 18, 0, 0, 18, 0, 18, 18, 0\n",
            "value_reward_not_chosen: 0, 0, 18, 18, 18, 18\n",
            "value_choice: 0, 18, 18, 18, 18, 18, 18, 18, 18, 18\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 269/1000 --- L(Train): 0.0060553 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.184 1 + 0.174 value_reward_chosen[t] + -0.001 contr_diff + 0.456 reward + -0.125 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.362 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.992 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 19, 0, 0, 19, 0, 19, 19, 0\n",
            "value_reward_not_chosen: 0, 0, 19, 19, 19, 19\n",
            "value_choice: 0, 19, 19, 19, 19, 19, 19, 19, 19, 19\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 270/1000 --- L(Train): 0.0060498 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.184 1 + 0.173 value_reward_chosen[t] + -0.0 contr_diff + 0.457 reward + -0.122 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.361 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.992 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 20, 0, 0, 20, 0, 20, 20, 0\n",
            "value_reward_not_chosen: 0, 0, 20, 20, 20, 20\n",
            "value_choice: 0, 20, 20, 20, 20, 20, 20, 20, 20, 20\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 271/1000 --- L(Train): 0.0060449 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.185 1 + 0.172 value_reward_chosen[t] + 0.002 contr_diff + 0.457 reward + -0.12 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.361 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.992 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 21, 0, 0, 21, 0, 21, 21, 0\n",
            "value_reward_not_chosen: 0, 0, 21, 21, 21, 21\n",
            "value_choice: 0, 21, 21, 21, 21, 21, 21, 21, 21, 21\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 272/1000 --- L(Train): 0.0060398 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.185 1 + 0.171 value_reward_chosen[t] + 0.002 contr_diff + 0.457 reward + -0.118 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.361 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.992 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 22, 0, 0, 22, 0, 22, 22, 0\n",
            "value_reward_not_chosen: 0, 0, 22, 22, 22, 22\n",
            "value_choice: 0, 22, 22, 22, 22, 22, 22, 22, 22, 22\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 273/1000 --- L(Train): 0.0060347 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.186 1 + 0.17 value_reward_chosen[t] + 0.002 contr_diff + 0.458 reward + -0.116 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.361 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 23, 0, 0, 23, 0, 23, 23, 0\n",
            "value_reward_not_chosen: 0, 0, 23, 23, 23, 23\n",
            "value_choice: 0, 23, 23, 23, 23, 23, 23, 23, 23, 23\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 274/1000 --- L(Train): 0.0060302 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.186 1 + 0.169 value_reward_chosen[t] + 0.001 contr_diff + 0.458 reward + -0.114 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.069 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.36 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 24, 0, 0, 24, 0, 24, 24, 0\n",
            "value_reward_not_chosen: 0, 0, 24, 24, 24, 24\n",
            "value_choice: 0, 24, 24, 24, 24, 24, 24, 24, 24, 24\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 275/1000 --- L(Train): 0.0060264 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.186 1 + 0.168 value_reward_chosen[t] + -0.001 contr_diff + 0.458 reward + -0.112 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.36 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 25, 0, 0, 25, 0, 25, 25, 0\n",
            "value_reward_not_chosen: 0, 0, 25, 25, 25, 25\n",
            "value_choice: 0, 25, 25, 25, 25, 25, 25, 25, 25, 25\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 276/1000 --- L(Train): 0.0060198 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.187 1 + 0.167 value_reward_chosen[t] + -0.002 contr_diff + 0.459 reward + -0.11 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.36 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.992 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 26, 0, 0, 26, 0, 26, 26, 0\n",
            "value_reward_not_chosen: 0, 0, 26, 26, 26, 26\n",
            "value_choice: 0, 26, 26, 26, 26, 26, 26, 26, 26, 26\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 277/1000 --- L(Train): 0.0060139 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.187 1 + 0.166 value_reward_chosen[t] + -0.001 contr_diff + 0.459 reward + -0.108 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.359 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.003 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.992 value_choice[t] + -0.0 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 27, 0, 0, 27, 0, 27, 27, 0\n",
            "value_reward_not_chosen: 0, 0, 27, 27, 27, 27\n",
            "value_choice: 0, 27, 27, 27, 27, 27, 27, 27, 27, 27\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 278/1000 --- L(Train): 0.0060105 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.187 1 + 0.165 value_reward_chosen[t] + 0.001 contr_diff + 0.459 reward + -0.105 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.359 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.003 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 28, 0, 0, 28, 0, 28, 28, 0\n",
            "value_reward_not_chosen: 0, 0, 28, 28, 28, 28\n",
            "value_choice: 0, 28, 28, 28, 28, 28, 28, 28, 28, 28\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 279/1000 --- L(Train): 0.0060064 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.188 1 + 0.164 value_reward_chosen[t] + 0.001 contr_diff + 0.46 reward + -0.103 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.359 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 29, 0, 0, 29, 0, 29, 29, 0\n",
            "value_reward_not_chosen: 0, 0, 29, 29, 29, 29\n",
            "value_choice: 0, 29, 29, 29, 29, 29, 29, 29, 29, 29\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 280/1000 --- L(Train): 0.0060017 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.188 1 + 0.164 value_reward_chosen[t] + 0.001 contr_diff + 0.46 reward + -0.101 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.359 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 30, 0, 0, 30, 0, 30, 30, 0\n",
            "value_reward_not_chosen: 0, 0, 30, 30, 30, 30\n",
            "value_choice: 0, 30, 30, 30, 30, 30, 30, 30, 30, 30\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 281/1000 --- L(Train): 0.0059965 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.188 1 + 0.163 value_reward_chosen[t] + -0.0 contr_diff + 0.46 reward + -0.099 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.358 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 31, 0, 0, 31, 0, 31, 31, 0\n",
            "value_reward_not_chosen: 0, 0, 31, 31, 31, 31\n",
            "value_choice: 0, 31, 31, 31, 31, 31, 31, 31, 31, 31\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 282/1000 --- L(Train): 0.0059921 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.189 1 + 0.162 value_reward_chosen[t] + 0.0 contr_diff + 0.461 reward + -0.097 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.358 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.004 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.004 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 32, 0, 0, 32, 0, 32, 32, 0\n",
            "value_reward_not_chosen: 0, 0, 32, 32, 32, 32\n",
            "value_choice: 0, 32, 32, 32, 32, 32, 32, 32, 32, 32\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 283/1000 --- L(Train): 0.0059882 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.189 1 + 0.161 value_reward_chosen[t] + -0.001 contr_diff + 0.461 reward + -0.095 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.358 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.004 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.004 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 33, 0, 0, 33, 0, 33, 33, 0\n",
            "value_reward_not_chosen: 0, 0, 33, 33, 33, 33\n",
            "value_choice: 0, 33, 33, 33, 33, 33, 33, 33, 33, 33\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 284/1000 --- L(Train): 0.0059840 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.189 1 + 0.16 value_reward_chosen[t] + -0.0 contr_diff + 0.461 reward + -0.093 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.358 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.003 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 34, 0, 0, 34, 0, 34, 34, 0\n",
            "value_reward_not_chosen: 0, 0, 34, 34, 34, 34\n",
            "value_choice: 0, 34, 34, 34, 34, 34, 34, 34, 34, 34\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 285/1000 --- L(Train): 0.0059808 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.19 1 + 0.159 value_reward_chosen[t] + 0.001 contr_diff + 0.462 reward + -0.091 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.357 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 35, 0, 0, 35, 0, 35, 35, 0\n",
            "value_reward_not_chosen: 0, 0, 35, 35, 35, 35\n",
            "value_choice: 0, 35, 35, 35, 35, 35, 35, 35, 35, 35\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 286/1000 --- L(Train): 0.0059768 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.19 1 + 0.158 value_reward_chosen[t] + 0.002 contr_diff + 0.462 reward + -0.089 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.357 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.721 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 36, 0, 0, 36, 0, 36, 36, 0\n",
            "value_reward_not_chosen: 0, 0, 36, 36, 36, 36\n",
            "value_choice: 0, 36, 36, 36, 36, 36, 36, 36, 36, 36\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 287/1000 --- L(Train): 0.0059749 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.191 1 + 0.157 value_reward_chosen[t] + 0.001 contr_diff + 0.462 reward + -0.087 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.068 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.357 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.721 value_reward_not_chosen[t] + 0.0 contr_diff + 0.003 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 37, 0, 0, 37, 0, 37, 37, 0\n",
            "value_reward_not_chosen: 0, 0, 37, 37, 37, 37\n",
            "value_choice: 0, 37, 37, 37, 37, 37, 37, 37, 37, 37\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 288/1000 --- L(Train): 0.0059703 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.191 1 + 0.156 value_reward_chosen[t] + -0.0 contr_diff + 0.462 reward + -0.084 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.357 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.003 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 38, 0, 0, 38, 0, 38, 38, 0\n",
            "value_reward_not_chosen: 0, 0, 38, 38, 38, 38\n",
            "value_choice: 0, 38, 38, 38, 38, 38, 38, 38, 38, 38\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 289/1000 --- L(Train): 0.0059649 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.191 1 + 0.156 value_reward_chosen[t] + -0.0 contr_diff + 0.463 reward + -0.082 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.356 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 39, 0, 0, 39, 0, 39, 39, 0\n",
            "value_reward_not_chosen: 0, 0, 39, 39, 39, 39\n",
            "value_choice: 0, 39, 39, 39, 39, 39, 39, 39, 39, 39\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 290/1000 --- L(Train): 0.0059602 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.191 1 + 0.155 value_reward_chosen[t] + 0.001 contr_diff + 0.463 reward + -0.08 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.356 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 40, 0, 0, 40, 0, 40, 40, 0\n",
            "value_reward_not_chosen: 0, 0, 40, 40, 40, 40\n",
            "value_choice: 0, 40, 40, 40, 40, 40, 40, 40, 40, 40\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 291/1000 --- L(Train): 0.0059591 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.192 1 + 0.154 value_reward_chosen[t] + 0.001 contr_diff + 0.463 reward + -0.078 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.356 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 41, 0, 0, 41, 0, 41, 41, 0\n",
            "value_reward_not_chosen: 0, 0, 41, 41, 41, 41\n",
            "value_choice: 0, 41, 41, 41, 41, 41, 41, 41, 41, 41\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 292/1000 --- L(Train): 0.0059553 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.192 1 + 0.153 value_reward_chosen[t] + -0.0 contr_diff + 0.464 reward + -0.076 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.355 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.003 contr_diff + -0.003 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.0 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.002 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 42, 0, 0, 42, 0, 42, 42, 0\n",
            "value_reward_not_chosen: 0, 0, 42, 42, 42, 42\n",
            "value_choice: 0, 42, 42, 42, 42, 42, 42, 42, 42, 42\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 293/1000 --- L(Train): 0.0059504 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.193 1 + 0.152 value_reward_chosen[t] + 0.0 contr_diff + 0.464 reward + -0.074 value_reward_chosen^2 + -0.004 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.355 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.002 contr_diff + -0.003 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.003 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 43, 0, 0, 43, 0, 43, 43, 0\n",
            "value_reward_not_chosen: 0, 0, 43, 43, 43, 43\n",
            "value_choice: 0, 43, 43, 43, 43, 43, 43, 43, 43, 43\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 294/1000 --- L(Train): 0.0059477 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.193 1 + 0.151 value_reward_chosen[t] + -0.0 contr_diff + 0.464 reward + -0.072 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.355 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 44, 0, 0, 44, 0, 44, 44, 0\n",
            "value_reward_not_chosen: 0, 0, 44, 44, 44, 44\n",
            "value_choice: 0, 44, 44, 44, 44, 44, 44, 44, 44, 44\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 295/1000 --- L(Train): 0.0059445 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.193 1 + 0.15 value_reward_chosen[t] + 0.0 contr_diff + 0.465 reward + -0.07 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.355 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 45, 0, 0, 45, 0, 45, 45, 0\n",
            "value_reward_not_chosen: 0, 0, 45, 45, 45, 45\n",
            "value_choice: 0, 45, 45, 45, 45, 45, 45, 45, 45, 45\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 296/1000 --- L(Train): 0.0059395 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.194 1 + 0.149 value_reward_chosen[t] + -0.0 contr_diff + 0.465 reward + -0.068 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.354 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 46, 0, 0, 46, 0, 46, 46, 0\n",
            "value_reward_not_chosen: 0, 0, 46, 46, 46, 46\n",
            "value_choice: 0, 46, 46, 46, 46, 46, 46, 46, 46, 46\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 297/1000 --- L(Train): 0.0059363 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.194 1 + 0.149 value_reward_chosen[t] + 0.001 contr_diff + 0.465 reward + -0.066 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.354 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.004 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 47, 0, 0, 47, 0, 47, 47, 0\n",
            "value_reward_not_chosen: 0, 0, 47, 47, 47, 47\n",
            "value_choice: 0, 47, 47, 47, 47, 47, 47, 47, 47, 47\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 298/1000 --- L(Train): 0.0059353 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.194 1 + 0.148 value_reward_chosen[t] + 0.001 contr_diff + 0.466 reward + -0.064 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.354 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.004 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.002 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 48, 0, 0, 48, 0, 48, 48, 0\n",
            "value_reward_not_chosen: 0, 0, 48, 48, 48, 48\n",
            "value_choice: 0, 48, 48, 48, 48, 48, 48, 48, 48, 48\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 299/1000 --- L(Train): 0.0059346 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.195 1 + 0.147 value_reward_chosen[t] + -0.001 contr_diff + 0.466 reward + -0.062 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.003 contr_diff^2 + -0.001 contr_diff*reward + 0.353 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.003 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + 0.0 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 49, 0, 0, 49, 0, 49, 49, 0\n",
            "value_reward_not_chosen: 0, 0, 49, 49, 49, 49\n",
            "value_choice: 0, 49, 49, 49, 49, 49, 49, 49, 49, 49\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 300/1000 --- L(Train): 0.0059301 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.195 1 + 0.146 value_reward_chosen[t] + -0.0 contr_diff + 0.466 reward + -0.06 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.353 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 50, 0, 0, 50, 0, 50, 50, 0\n",
            "value_reward_not_chosen: 0, 0, 50, 50, 50, 50\n",
            "value_choice: 0, 50, 50, 50, 50, 50, 50, 50, 50, 50\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 301/1000 --- L(Train): 0.0059247 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.195 1 + 0.145 value_reward_chosen[t] + 0.001 contr_diff + 0.467 reward + -0.058 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.353 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.991 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 51, 0, 0, 51, 0, 51, 51, 0\n",
            "value_reward_not_chosen: 0, 0, 51, 51, 51, 51\n",
            "value_choice: 0, 51, 51, 51, 51, 51, 51, 51, 51, 51\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 302/1000 --- L(Train): 0.0059218 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.196 1 + 0.144 value_reward_chosen[t] + 0.001 contr_diff + 0.467 reward + -0.056 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.353 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + 0.0 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 52, 0, 0, 52, 0, 52, 52, 0\n",
            "value_reward_not_chosen: 0, 0, 52, 52, 52, 52\n",
            "value_choice: 0, 52, 52, 52, 52, 52, 52, 52, 52, 52\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 303/1000 --- L(Train): 0.0059213 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.196 1 + 0.144 value_reward_chosen[t] + 0.001 contr_diff + 0.468 reward + -0.054 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.352 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.723 value_reward_not_chosen[t] + -0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.0 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 53, 0, 0, 53, 0, 53, 53, 0\n",
            "value_reward_not_chosen: 0, 0, 53, 53, 53, 53\n",
            "value_choice: 0, 53, 53, 53, 53, 53, 53, 53, 53, 53\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 304/1000 --- L(Train): 0.0059184 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.197 1 + 0.143 value_reward_chosen[t] + -0.001 contr_diff + 0.468 reward + -0.051 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.352 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 54, 0, 0, 54, 0, 54, 54, 0\n",
            "value_reward_not_chosen: 0, 0, 54, 54, 54, 54\n",
            "value_choice: 0, 54, 54, 54, 54, 54, 54, 54, 54, 54\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 305/1000 --- L(Train): 0.0059136 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.197 1 + 0.142 value_reward_chosen[t] + -0.001 contr_diff + 0.468 reward + -0.049 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.352 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.003 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 55, 0, 1, 55, 0, 55, 55, 0\n",
            "value_reward_not_chosen: 0, 0, 55, 55, 55, 55\n",
            "value_choice: 0, 55, 55, 55, 55, 55, 55, 55, 55, 55\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 306/1000 --- L(Train): 0.0059123 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.197 1 + 0.141 value_reward_chosen[t] + -0.0 contr_diff + 0.469 reward + -0.047 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.352 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.003 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 56, 0, 2, 56, 0, 56, 56, 0\n",
            "value_reward_not_chosen: 0, 0, 56, 56, 56, 56\n",
            "value_choice: 0, 56, 56, 56, 56, 56, 56, 56, 56, 56\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 307/1000 --- L(Train): 0.0059093 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.198 1 + 0.14 value_reward_chosen[t] + 0.002 contr_diff + 0.469 reward + -0.045 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.0 contr_diff*reward + 0.351 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + 0.0 value_choice*choice + 0.002 contr_diff^2 + 0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 57, 0, 3, 57, 0, 57, 57, 0\n",
            "value_reward_not_chosen: 0, 0, 57, 57, 57, 57\n",
            "value_choice: 0, 57, 57, 57, 57, 57, 57, 57, 57, 57\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 308/1000 --- L(Train): 0.0059048 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.198 1 + 0.14 value_reward_chosen[t] + 0.003 contr_diff + 0.469 reward + -0.043 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.351 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 58, 0, 4, 58, 0, 58, 58, 0\n",
            "value_reward_not_chosen: 0, 0, 58, 58, 58, 58\n",
            "value_choice: 0, 58, 58, 58, 58, 58, 58, 58, 58, 58\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 309/1000 --- L(Train): 0.0059028 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.198 1 + 0.139 value_reward_chosen[t] + 0.002 contr_diff + 0.47 reward + -0.041 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.351 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 59, 0, 5, 59, 0, 59, 59, 0\n",
            "value_reward_not_chosen: 0, 0, 59, 59, 59, 59\n",
            "value_choice: 0, 59, 59, 59, 59, 59, 59, 59, 59, 59\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 310/1000 --- L(Train): 0.0059003 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.199 1 + 0.138 value_reward_chosen[t] + 0.001 contr_diff + 0.47 reward + -0.039 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.0 contr_diff*reward + 0.35 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 60, 0, 6, 60, 0, 60, 60, 0\n",
            "value_reward_not_chosen: 0, 0, 60, 60, 60, 60\n",
            "value_choice: 0, 60, 60, 60, 60, 60, 60, 60, 60, 60\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 311/1000 --- L(Train): 0.0058988 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.199 1 + 0.137 value_reward_chosen[t] + -0.001 contr_diff + 0.47 reward + -0.037 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.35 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 61, 0, 7, 61, 0, 61, 61, 0\n",
            "value_reward_not_chosen: 0, 0, 61, 61, 61, 61\n",
            "value_choice: 0, 61, 61, 61, 61, 61, 61, 61, 61, 61\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 312/1000 --- L(Train): 0.0058953 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.199 1 + 0.136 value_reward_chosen[t] + -0.002 contr_diff + 0.471 reward + -0.035 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.35 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.003 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 62, 0, 8, 62, 0, 62, 62, 0\n",
            "value_reward_not_chosen: 0, 0, 62, 62, 62, 62\n",
            "value_choice: 0, 62, 62, 62, 62, 62, 62, 62, 62, 62\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 313/1000 --- L(Train): 0.0058934 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.2 1 + 0.135 value_reward_chosen[t] + -0.001 contr_diff + 0.471 reward + -0.033 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.35 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.002 contr_diff^2 + 0.002 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 63, 0, 9, 63, 0, 63, 63, 0\n",
            "value_reward_not_chosen: 0, 0, 63, 63, 63, 63\n",
            "value_choice: 0, 63, 63, 63, 63, 63, 63, 63, 63, 63\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 314/1000 --- L(Train): 0.0058905 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.2 1 + 0.135 value_reward_chosen[t] + 0.001 contr_diff + 0.471 reward + -0.031 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.349 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.991 value_choice[t] + 0.0 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + 0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 64, 0, 10, 64, 0, 64, 64, 0\n",
            "value_reward_not_chosen: 0, 0, 64, 64, 64, 64\n",
            "value_choice: 0, 64, 64, 64, 64, 64, 64, 64, 64, 64\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 315/1000 --- L(Train): 0.0058888 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.2 1 + 0.134 value_reward_chosen[t] + 0.001 contr_diff + 0.472 reward + -0.029 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.349 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 65, 0, 11, 65, 0, 65, 65, 0\n",
            "value_reward_not_chosen: 0, 0, 65, 65, 65, 65\n",
            "value_choice: 0, 65, 65, 65, 65, 65, 65, 65, 65, 65\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 316/1000 --- L(Train): 0.0058867 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.201 1 + 0.133 value_reward_chosen[t] + 0.001 contr_diff + 0.472 reward + -0.027 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.349 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.003 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + -0.001 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 66, 0, 12, 66, 0, 66, 66, 0\n",
            "value_reward_not_chosen: 0, 0, 66, 66, 66, 66\n",
            "value_choice: 0, 66, 66, 66, 66, 66, 66, 66, 66, 66\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 317/1000 --- L(Train): 0.0058826 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.201 1 + 0.132 value_reward_chosen[t] + -0.001 contr_diff + 0.472 reward + -0.025 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.348 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.002 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + -0.0 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 67, 0, 13, 67, 0, 67, 67, 0\n",
            "value_reward_not_chosen: 0, 0, 67, 67, 67, 67\n",
            "value_choice: 0, 67, 67, 67, 67, 67, 67, 67, 67, 67\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 318/1000 --- L(Train): 0.0058793 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.201 1 + 0.131 value_reward_chosen[t] + -0.001 contr_diff + 0.473 reward + -0.023 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.348 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + -0.002 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 68, 0, 14, 68, 0, 68, 68, 0\n",
            "value_reward_not_chosen: 0, 0, 68, 68, 68, 68\n",
            "value_choice: 0, 68, 68, 68, 68, 68, 68, 68, 68, 68\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 319/1000 --- L(Train): 0.0058784 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.131 value_reward_chosen[t] + 0.001 contr_diff + 0.473 reward + -0.021 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.348 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 69, 0, 15, 69, 0, 69, 69, 0\n",
            "value_reward_not_chosen: 0, 0, 69, 69, 69, 69\n",
            "value_choice: 0, 69, 69, 69, 69, 69, 69, 69, 69, 69\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 320/1000 --- L(Train): 0.0058761 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.13 value_reward_chosen[t] + 0.001 contr_diff + 0.473 reward + -0.019 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.348 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 70, 0, 16, 70, 0, 70, 70, 0\n",
            "value_reward_not_chosen: 0, 0, 70, 70, 70, 70\n",
            "value_choice: 0, 70, 70, 70, 70, 70, 70, 70, 70, 70\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 321/1000 --- L(Train): 0.0058739 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.129 value_reward_chosen[t] + -0.0 contr_diff + 0.474 reward + -0.017 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.347 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.004 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 71, 0, 17, 71, 0, 71, 71, 0\n",
            "value_reward_not_chosen: 0, 0, 71, 71, 71, 71\n",
            "value_choice: 0, 71, 71, 71, 71, 71, 71, 71, 71, 71\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 322/1000 --- L(Train): 0.0058728 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.128 value_reward_chosen[t] + -0.0 contr_diff + 0.474 reward + -0.015 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.347 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.002 choice + -0.005 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.004 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 72, 0, 18, 72, 0, 72, 72, 0\n",
            "value_reward_not_chosen: 0, 0, 72, 72, 72, 72\n",
            "value_choice: 0, 72, 72, 72, 72, 72, 72, 72, 72, 72\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 323/1000 --- L(Train): 0.0058712 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.128 value_reward_chosen[t] + 0.001 contr_diff + 0.474 reward + -0.013 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.0 contr_diff*reward + 0.347 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.002 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.003 contr_diff*choice + -0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 73, 0, 19, 73, 0, 73, 73, 0\n",
            "value_reward_not_chosen: 0, 0, 73, 73, 73, 73\n",
            "value_choice: 0, 73, 73, 73, 73, 73, 73, 73, 73, 73\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 324/1000 --- L(Train): 0.0058686 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.127 value_reward_chosen[t] + 0.002 contr_diff + 0.475 reward + -0.011 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.0 contr_diff*reward + 0.346 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.0 contr_diff + 0.002 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 74, 0, 20, 74, 0, 74, 74, 0\n",
            "value_reward_not_chosen: 0, 0, 74, 74, 74, 74\n",
            "value_choice: 0, 74, 74, 74, 74, 74, 74, 74, 74, 74\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 325/1000 --- L(Train): 0.0058667 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.126 value_reward_chosen[t] + 0.001 contr_diff + 0.475 reward + -0.009 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.346 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.143 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.002 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 75, 0, 21, 75, 0, 75, 75, 0\n",
            "value_reward_not_chosen: 0, 0, 75, 75, 75, 75\n",
            "value_choice: 0, 75, 75, 75, 75, 75, 75, 75, 75, 75\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 326/1000 --- L(Train): 0.0058626 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.125 value_reward_chosen[t] + -0.001 contr_diff + 0.476 reward + -0.008 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.346 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 76, 0, 22, 76, 0, 76, 76, 0\n",
            "value_reward_not_chosen: 0, 0, 76, 76, 76, 76\n",
            "value_choice: 0, 76, 76, 76, 76, 76, 76, 76, 76, 76\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 327/1000 --- L(Train): 0.0058605 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.125 value_reward_chosen[t] + -0.001 contr_diff + 0.476 reward + -0.006 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.346 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.003 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 77, 0, 23, 77, 0, 77, 77, 0\n",
            "value_reward_not_chosen: 0, 0, 77, 77, 77, 77\n",
            "value_choice: 0, 77, 77, 77, 77, 77, 77, 77, 77, 77\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 328/1000 --- L(Train): 0.0058594 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.124 value_reward_chosen[t] + -0.0 contr_diff + 0.476 reward + -0.004 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.345 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.002 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.003 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 78, 0, 24, 78, 0, 78, 78, 0\n",
            "value_reward_not_chosen: 0, 0, 78, 78, 78, 78\n",
            "value_choice: 0, 78, 78, 78, 78, 78, 78, 78, 78, 78\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 329/1000 --- L(Train): 0.0058581 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.123 value_reward_chosen[t] + 0.002 contr_diff + 0.477 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.345 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + -0.001 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 79, 0, 25, 79, 0, 79, 79, 0\n",
            "value_reward_not_chosen: 0, 0, 79, 79, 79, 79\n",
            "value_choice: 0, 79, 79, 79, 79, 79, 79, 79, 79, 79\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 330/1000 --- L(Train): 0.0058547 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.122 value_reward_chosen[t] + 0.002 contr_diff + 0.477 reward + 0.0 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.002 contr_diff*reward + 0.345 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 80, 0, 26, 80, 0, 80, 80, 0\n",
            "value_reward_not_chosen: 0, 0, 80, 80, 80, 80\n",
            "value_choice: 0, 80, 80, 80, 80, 80, 80, 80, 80, 80\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 331/1000 --- L(Train): 0.0058521 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.121 value_reward_chosen[t] + 0.002 contr_diff + 0.477 reward + 0.001 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.0 contr_diff^2 + 0.001 contr_diff*reward + 0.344 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 81, 0, 27, 81, 0, 81, 81, 0\n",
            "value_reward_not_chosen: 0, 0, 81, 81, 81, 81\n",
            "value_choice: 0, 81, 81, 81, 81, 81, 81, 81, 81, 81\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 332/1000 --- L(Train): 0.0058498 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.121 value_reward_chosen[t] + 0.001 contr_diff + 0.478 reward + 0.002 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.001 contr_diff*reward + 0.344 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + 0.003 value_reward_not_chosen^2 + -0.004 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.0 contr_diff^2 + 0.003 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 82, 0, 28, 82, 0, 82, 82, 0\n",
            "value_reward_not_chosen: 0, 0, 82, 82, 82, 82\n",
            "value_choice: 0, 82, 82, 82, 82, 82, 82, 82, 82, 82\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 333/1000 --- L(Train): 0.0058491 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.12 value_reward_chosen[t] + -0.002 contr_diff + 0.478 reward + 0.002 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.344 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.003 value_reward_not_chosen^2 + -0.003 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.0 contr_diff + 0.002 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + 0.002 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 83, 0, 29, 83, 0, 83, 83, 0\n",
            "value_reward_not_chosen: 0, 0, 83, 83, 83, 83\n",
            "value_choice: 0, 83, 83, 83, 83, 83, 83, 83, 83, 83\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 334/1000 --- L(Train): 0.0058496 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.119 value_reward_chosen[t] + -0.002 contr_diff + 0.478 reward + 0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.002 contr_diff*reward + 0.344 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.002 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.002 choice + -0.005 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.0 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 84, 0, 30, 84, 0, 84, 84, 0\n",
            "value_reward_not_chosen: 0, 0, 84, 84, 84, 84\n",
            "value_choice: 0, 84, 84, 84, 84, 84, 84, 84, 84, 84\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 335/1000 --- L(Train): 0.0058472 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.119 value_reward_chosen[t] + -0.002 contr_diff + 0.479 reward + 0.0 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.343 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.0 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.0 contr_diff + 0.002 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 85, 0, 31, 85, 0, 85, 85, 0\n",
            "value_reward_not_chosen: 0, 0, 85, 85, 85, 85\n",
            "value_choice: 0, 85, 85, 85, 85, 85, 85, 85, 85, 85\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 336/1000 --- L(Train): 0.0058437 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.118 value_reward_chosen[t] + -0.0 contr_diff + 0.479 reward + -0.001 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.067 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.343 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.002 value_choice*choice + 0.001 contr_diff^2 + -0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 86, 0, 32, 86, 0, 86, 86, 0\n",
            "value_reward_not_chosen: 0, 0, 86, 86, 86, 86\n",
            "value_choice: 0, 86, 86, 86, 86, 86, 86, 86, 86, 86\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 337/1000 --- L(Train): 0.0058425 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.117 value_reward_chosen[t] + 0.002 contr_diff + 0.48 reward + -0.002 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.066 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.343 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.001 value_reward_not_chosen^2 + 0.0 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.002 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 87, 0, 33, 87, 0, 87, 87, 0\n",
            "value_reward_not_chosen: 0, 0, 87, 87, 87, 87\n",
            "value_choice: 0, 87, 87, 87, 87, 87, 87, 87, 87, 87\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 338/1000 --- L(Train): 0.0058408 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.117 value_reward_chosen[t] + 0.004 contr_diff + 0.48 reward + -0.003 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.066 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.343 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 88, 0, 34, 88, 0, 88, 88, 0\n",
            "value_reward_not_chosen: 0, 0, 88, 88, 88, 88\n",
            "value_choice: 0, 88, 88, 88, 88, 88, 88, 88, 88, 88\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 339/1000 --- L(Train): 0.0058405 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.116 value_reward_chosen[t] + 0.004 contr_diff + 0.481 reward + -0.003 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.066 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.001 contr_diff*reward + 0.343 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.0 contr_diff + 0.0 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 89, 0, 35, 89, 0, 89, 89, 0\n",
            "value_reward_not_chosen: 0, 0, 89, 89, 89, 89\n",
            "value_choice: 0, 89, 89, 89, 89, 89, 89, 89, 89, 89\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 340/1000 --- L(Train): 0.0058372 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.116 value_reward_chosen[t] + 0.003 contr_diff + 0.481 reward + -0.003 value_reward_chosen^2 + -0.0 value_reward_chosen*contr_diff + -0.065 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.342 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.0 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.001 contr_diff + -0.0 choice + -0.005 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 90, 0, 36, 90, 0, 90, 90, 0\n",
            "value_reward_not_chosen: 0, 0, 90, 90, 90, 90\n",
            "value_choice: 0, 90, 90, 90, 90, 90, 90, 90, 90, 90\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 341/1000 --- L(Train): 0.0058357 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.115 value_reward_chosen[t] + 0.001 contr_diff + 0.482 reward + -0.003 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.065 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.342 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.0 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.0 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 91, 0, 37, 91, 0, 91, 91, 0\n",
            "value_reward_not_chosen: 0, 0, 91, 91, 91, 91\n",
            "value_choice: 0, 91, 91, 91, 91, 91, 91, 91, 91, 91\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 342/1000 --- L(Train): 0.0058356 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.115 value_reward_chosen[t] + -0.002 contr_diff + 0.482 reward + -0.002 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.064 value_reward_chosen*reward + -0.001 contr_diff^2 + -0.002 contr_diff*reward + 0.342 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 92, 0, 38, 92, 0, 92, 92, 0\n",
            "value_reward_not_chosen: 0, 0, 92, 92, 92, 92\n",
            "value_choice: 0, 92, 92, 92, 92, 92, 92, 92, 92, 92\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 343/1000 --- L(Train): 0.0058331 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.114 value_reward_chosen[t] + -0.003 contr_diff + 0.482 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.064 value_reward_chosen*reward + 0.001 contr_diff^2 + -0.0 contr_diff*reward + 0.342 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 93, 0, 39, 93, 0, 93, 93, 0\n",
            "value_reward_not_chosen: 0, 0, 93, 93, 93, 93\n",
            "value_choice: 0, 93, 93, 93, 93, 93, 93, 93, 93, 93\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 344/1000 --- L(Train): 0.0058308 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.113 value_reward_chosen[t] + -0.003 contr_diff + 0.483 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.064 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.002 contr_diff*reward + 0.341 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.002 contr_diff + 0.001 choice + -0.005 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff^2 + -0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 94, 0, 40, 94, 0, 94, 94, 0\n",
            "value_reward_not_chosen: 0, 0, 94, 94, 94, 94\n",
            "value_choice: 0, 94, 94, 94, 94, 94, 94, 94, 94, 94\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 345/1000 --- L(Train): 0.0058296 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.113 value_reward_chosen[t] + -0.002 contr_diff + 0.483 reward + -0.0 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.063 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.003 contr_diff*reward + 0.341 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + -0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + -0.0 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + 0.0 value_choice*choice + -0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 95, 0, 41, 95, 0, 95, 95, 0\n",
            "value_reward_not_chosen: 0, 0, 95, 95, 95, 95\n",
            "value_choice: 0, 95, 95, 95, 95, 95, 95, 95, 95, 95\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 346/1000 --- L(Train): 0.0058296 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.112 value_reward_chosen[t] + 0.0 contr_diff + 0.483 reward + 0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.063 value_reward_chosen*reward + 0.001 contr_diff^2 + 0.003 contr_diff*reward + 0.341 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen^2 + -0.002 value_reward_not_chosen*contr_diff + -0.0 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff + -0.0 value_choice*choice + -0.0 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 96, 0, 42, 96, 0, 96, 96, 0\n",
            "value_reward_not_chosen: 0, 0, 96, 96, 96, 96\n",
            "value_choice: 0, 96, 96, 96, 96, 96, 96, 96, 96, 96\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 347/1000 --- L(Train): 0.0058271 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.112 value_reward_chosen[t] + 0.001 contr_diff + 0.484 reward + 0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.063 value_reward_chosen*reward + -0.001 contr_diff^2 + 0.002 contr_diff*reward + 0.34 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.0 contr_diff + -0.001 value_reward_not_chosen^2 + -0.001 value_reward_not_chosen*contr_diff + 0.002 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.003 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.001 value_choice*choice + 0.001 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 97, 0, 43, 97, 0, 97, 97, 0\n",
            "value_reward_not_chosen: 0, 0, 97, 97, 97, 97\n",
            "value_choice: 0, 97, 97, 97, 97, 97, 97, 97, 97, 97\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 348/1000 --- L(Train): 0.0058253 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.112 value_reward_chosen[t] + 0.001 contr_diff + 0.484 reward + 0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.062 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.0 contr_diff*reward + 0.34 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + -0.002 value_reward_not_chosen^2 + -0.0 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.002 contr_diff^2 + -0.0 contr_diff*choice + -0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 98, 0, 44, 98, 0, 98, 98, 0\n",
            "value_reward_not_chosen: 0, 0, 98, 98, 98, 98\n",
            "value_choice: 0, 98, 98, 98, 98, 98, 98, 98, 98, 98\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 349/1000 --- L(Train): 0.0058233 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 26):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.111 value_reward_chosen[t] + 0.0 contr_diff + 0.484 reward + 0.0 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.062 value_reward_chosen*reward + -0.002 contr_diff^2 + -0.001 contr_diff*reward + 0.339 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.004 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.0 choice + -0.005 value_choice^2 + -0.001 value_choice*contr_diff + 0.0 value_choice*choice + 0.001 contr_diff^2 + 0.001 contr_diff*choice + 0.0 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 99, 0, 45, 99, 0, 99, 99, 0\n",
            "value_reward_not_chosen: 0, 0, 99, 99, 99, 99\n",
            "value_choice: 0, 99, 99, 99, 99, 99, 99, 99, 99, 99\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 350/1000 --- L(Train): 0.0058197 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 25):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.111 value_reward_chosen[t] + -0.002 contr_diff + 0.484 reward + -0.001 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.061 value_reward_chosen*reward + -0.0 contr_diff^2 + -0.001 contr_diff*reward + 0.339 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.0 value_reward_not_chosen^2 + 0.002 value_reward_not_chosen*contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.001 contr_diff + -0.0 choice + -0.005 value_choice^2 + 0.001 value_choice*contr_diff + -0.0 value_choice*choice + 0.0 contr_diff*choice + 0.001 choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 100, 0, 46, 100, 0, 100, 100, 0\n",
            "value_reward_not_chosen: 0, 0, 100, 100, 100, 100\n",
            "value_choice: 0, 100, 100, 100, 100, 100, 100, -, 100, 100\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 351/1000 --- L(Train): 0.0058194 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 24):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.11 value_reward_chosen[t] + -0.003 contr_diff + 0.485 reward + -0.002 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.061 value_reward_chosen*reward + 0.002 contr_diff^2 + 0.001 contr_diff*reward + 0.339 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + 0.001 value_reward_not_chosen^2 + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.002 contr_diff + -0.0 choice + -0.005 value_choice^2 + 0.002 value_choice*contr_diff + -0.0 value_choice*choice + -0.001 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 101, 0, 47, 101, 0, 101, 101, 0\n",
            "value_reward_not_chosen: 0, 0, 101, 101, 101, 101\n",
            "value_choice: 0, 101, 101, 101, 101, 101, 101, -, 101, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 352/1000 --- L(Train): 0.0058183 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 23):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.11 value_reward_chosen[t] + -0.002 contr_diff + 0.485 reward + -0.002 value_reward_chosen^2 + -0.003 value_reward_chosen*contr_diff + -0.06 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.001 contr_diff*reward + 0.338 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.003 contr_diff + -0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.002 contr_diff + -0.0 choice + -0.006 value_choice^2 + 0.001 value_choice*contr_diff + -0.001 value_choice*choice + -0.001 contr_diff*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 102, 0, 48, 102, 0, 102, 102, 0\n",
            "value_reward_not_chosen: 0, 0, 102, -, 102, 102\n",
            "value_choice: 0, 102, 102, 102, 102, 102, 102, -, 102, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 353/1000 --- L(Train): 0.0058162 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 22):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.11 value_reward_chosen[t] + -0.0 contr_diff + 0.486 reward + -0.002 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.059 value_reward_chosen*reward + 0.003 contr_diff^2 + 0.0 contr_diff*reward + 0.338 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.0 choice + -0.006 value_choice^2 + -0.0 value_choice*contr_diff + -0.0 value_choice*choice \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 103, 0, 49, 103, 0, 103, 103, 0\n",
            "value_reward_not_chosen: 0, 0, 103, -, 103, 103\n",
            "value_choice: 0, 103, 103, 103, 103, 103, 103, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 354/1000 --- L(Train): 0.0058141 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 21):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.109 value_reward_chosen[t] + 0.002 contr_diff + 0.486 reward + -0.002 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.059 value_reward_chosen*reward + 0.002 contr_diff^2 + -0.002 contr_diff*reward + 0.338 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + -0.002 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 + -0.001 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 104, 0, 50, 104, 0, 104, 104, 0\n",
            "value_reward_not_chosen: 0, 0, 104, -, 104, 104\n",
            "value_choice: 0, 104, 104, 104, 104, 104, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 355/1000 --- L(Train): 0.0058116 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 20):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.109 value_reward_chosen[t] + 0.004 contr_diff + 0.486 reward + -0.001 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.058 value_reward_chosen*reward + -0.002 contr_diff*reward + 0.338 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + -0.001 value_reward_not_chosen*contr_diff + -0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.002 contr_diff + 0.001 choice + -0.006 value_choice^2 + 0.0 value_choice*contr_diff \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 105, 0, 51, 105, 0, -, 105, 0\n",
            "value_reward_not_chosen: 0, 0, 105, -, 105, 105\n",
            "value_choice: 0, 105, 105, 105, 105, 105, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 356/1000 --- L(Train): 0.0058098 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 19):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.109 value_reward_chosen[t] + 0.004 contr_diff + 0.487 reward + -0.001 value_reward_chosen^2 + 0.001 value_reward_chosen*contr_diff + -0.058 value_reward_chosen*reward + -0.001 contr_diff*reward + 0.337 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + -0.001 contr_diff + 0.001 value_reward_not_chosen*contr_diff + -0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 106, 0, 52, 106, 0, -, 106, 0\n",
            "value_reward_not_chosen: 0, 0, 106, -, 106, 106\n",
            "value_choice: 0, 106, 106, 106, 106, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 357/1000 --- L(Train): 0.0058074 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 18):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.108 value_reward_chosen[t] + 0.003 contr_diff + 0.487 reward + 0.0 value_reward_chosen^2 + 0.0 value_reward_chosen*contr_diff + -0.057 value_reward_chosen*reward + 0.337 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.001 value_reward_not_chosen*contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.0 contr_diff + 0.001 choice + -0.006 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 107, 0, 53, 107, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, 107, -, 107, 107\n",
            "value_choice: 0, 107, 107, 107, 107, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 358/1000 --- L(Train): 0.0058048 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 17):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.108 value_reward_chosen[t] + 0.001 contr_diff + 0.487 reward + 0.0 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.057 value_reward_chosen*reward + 0.336 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.001 contr_diff + 0.001 choice + -0.006 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 108, 0, 54, 108, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, 108, -, -, 108\n",
            "value_choice: 0, 108, 108, 108, 108, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 359/1000 --- L(Train): 0.0058032 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 16):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.108 value_reward_chosen[t] + -0.002 contr_diff + 0.487 reward + 0.0 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.056 value_reward_chosen*reward + 0.336 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.003 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + 0.0 choice + -0.006 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 109, 0, 55, 109, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, 109, -, -, 109\n",
            "value_choice: 0, 109, -, 109, 109, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 360/1000 --- L(Train): 0.0058015 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 15):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.107 value_reward_chosen[t] + -0.003 contr_diff + 0.488 reward + -0.001 value_reward_chosen^2 + -0.002 value_reward_chosen*contr_diff + -0.056 value_reward_chosen*reward + 0.335 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.002 contr_diff + 0.001 contr_diff^2 \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.006 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 110, 0, 56, 110, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, 110, -, -, 110\n",
            "value_choice: 0, 110, -, -, 110, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 361/1000 --- L(Train): 0.0058010 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 14):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.107 value_reward_chosen[t] + -0.003 contr_diff + 0.488 reward + -0.001 value_reward_chosen^2 + -0.001 value_reward_chosen*contr_diff + -0.056 value_reward_chosen*reward + 0.335 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] + 0.001 contr_diff \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.005 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 111, 0, 57, 111, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, 111, -, -, -\n",
            "value_choice: 0, 111, -, -, 111, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 362/1000 --- L(Train): 0.0057985 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 13):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.107 value_reward_chosen[t] + -0.002 contr_diff + 0.488 reward + -0.002 value_reward_chosen^2 + 0.002 value_reward_chosen*contr_diff + -0.055 value_reward_chosen*reward + 0.335 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.005 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, 112, 0, 58, 112, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, 112, -, -, 112, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 363/1000 --- L(Train): 0.0057960 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 12):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.107 value_reward_chosen[t] + 0.488 reward + -0.001 value_reward_chosen^2 + 0.003 value_reward_chosen*contr_diff + -0.055 value_reward_chosen*reward + 0.334 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.005 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 59, 113, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, 113, -, -, 113, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 364/1000 --- L(Train): 0.0057939 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 11):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.106 value_reward_chosen[t] + 0.489 reward + -0.001 value_reward_chosen^2 + -0.054 value_reward_chosen*reward + 0.334 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.143 1 + 0.99 value_choice[t] + -0.006 value_choice^2 \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 60, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, 114, -, -, 114, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 365/1000 --- L(Train): 0.0057973 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 10):\n",
            "value_reward_chosen[t+1] = -0.205 1 + 0.106 value_reward_chosen[t] + 0.489 reward + -0.0 value_reward_chosen^2 + -0.054 value_reward_chosen*reward + 0.333 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.142 1 + 0.99 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 61, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, 115, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 366/1000 --- L(Train): 0.0058204 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 9):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.106 value_reward_chosen[t] + 0.489 reward + 0.0 value_reward_chosen^2 + -0.053 value_reward_chosen*reward + 0.333 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.141 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 62, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 367/1000 --- L(Train): 0.0059190 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.106 value_reward_chosen[t] + 0.49 reward + 0.001 value_reward_chosen^2 + -0.053 value_reward_chosen*reward + 0.333 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.138 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 63, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 368/1000 --- L(Train): 0.0060194 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.105 value_reward_chosen[t] + 0.49 reward + 0.0 value_reward_chosen^2 + -0.052 value_reward_chosen*reward + 0.332 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.133 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 64, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 369/1000 --- L(Train): 0.0060125 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.105 value_reward_chosen[t] + 0.49 reward + -0.001 value_reward_chosen^2 + -0.052 value_reward_chosen*reward + 0.332 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.128 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 65, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 370/1000 --- L(Train): 0.0060429 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.105 value_reward_chosen[t] + 0.49 reward + -0.001 value_reward_chosen^2 + -0.052 value_reward_chosen*reward + 0.331 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 66, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 371/1000 --- L(Train): 0.0060457 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.105 value_reward_chosen[t] + 0.491 reward + -0.001 value_reward_chosen^2 + -0.052 value_reward_chosen*reward + 0.331 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.117 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 67, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 372/1000 --- L(Train): 0.0060073 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.104 value_reward_chosen[t] + 0.491 reward + -0.001 value_reward_chosen^2 + -0.051 value_reward_chosen*reward + 0.331 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.114 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 68, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 373/1000 --- L(Train): 0.0059874 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.104 value_reward_chosen[t] + 0.491 reward + -0.001 value_reward_chosen^2 + -0.051 value_reward_chosen*reward + 0.33 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.111 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 69, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 374/1000 --- L(Train): 0.0059848 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.104 value_reward_chosen[t] + 0.492 reward + -0.0 value_reward_chosen^2 + -0.051 value_reward_chosen*reward + 0.33 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.111 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 70, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 375/1000 --- L(Train): 0.0059928 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.104 value_reward_chosen[t] + 0.492 reward + 0.0 value_reward_chosen^2 + -0.05 value_reward_chosen*reward + 0.329 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.111 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 71, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 376/1000 --- L(Train): 0.0060048 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.103 value_reward_chosen[t] + 0.492 reward + 0.001 value_reward_chosen^2 + -0.05 value_reward_chosen*reward + 0.329 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.113 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 72, -, 0, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 377/1000 --- L(Train): 0.0060078 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.103 value_reward_chosen[t] + 0.492 reward + 0.0 value_reward_chosen^2 + -0.05 value_reward_chosen*reward + 0.328 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.116 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 73, -, 1, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 378/1000 --- L(Train): 0.0059990 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.103 value_reward_chosen[t] + 0.493 reward + -0.001 value_reward_chosen^2 + -0.05 value_reward_chosen*reward + 0.328 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.12 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 74, -, 2, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 379/1000 --- L(Train): 0.0059832 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.103 value_reward_chosen[t] + 0.493 reward + -0.001 value_reward_chosen^2 + -0.049 value_reward_chosen*reward + 0.328 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 75, -, 3, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 380/1000 --- L(Train): 0.0059668 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.102 value_reward_chosen[t] + 0.493 reward + -0.001 value_reward_chosen^2 + -0.049 value_reward_chosen*reward + 0.327 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.126 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 76, -, 4, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 381/1000 --- L(Train): 0.0059538 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.102 value_reward_chosen[t] + 0.494 reward + -0.001 value_reward_chosen^2 + -0.049 value_reward_chosen*reward + 0.327 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.129 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 77, -, 5, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 382/1000 --- L(Train): 0.0059438 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.102 value_reward_chosen[t] + 0.494 reward + -0.001 value_reward_chosen^2 + -0.049 value_reward_chosen*reward + 0.327 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.13 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 78, -, 6, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 383/1000 --- L(Train): 0.0059356 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.102 value_reward_chosen[t] + 0.494 reward + -0.0 value_reward_chosen^2 + -0.048 value_reward_chosen*reward + 0.326 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.13 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 79, -, 7, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 384/1000 --- L(Train): 0.0059348 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.102 value_reward_chosen[t] + 0.495 reward + 0.001 value_reward_chosen^2 + -0.048 value_reward_chosen*reward + 0.326 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.13 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 80, -, 8, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 385/1000 --- L(Train): 0.0059363 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.102 value_reward_chosen[t] + 0.495 reward + 0.001 value_reward_chosen^2 + -0.048 value_reward_chosen*reward + 0.325 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.129 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 81, -, 9, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 386/1000 --- L(Train): 0.0059377 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.101 value_reward_chosen[t] + 0.495 reward + 0.0 value_reward_chosen^2 + -0.048 value_reward_chosen*reward + 0.325 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.127 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 82, -, 10, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 387/1000 --- L(Train): 0.0059358 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.101 value_reward_chosen[t] + 0.496 reward + -0.001 value_reward_chosen^2 + -0.048 value_reward_chosen*reward + 0.325 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.125 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 83, -, 11, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 388/1000 --- L(Train): 0.0059313 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.101 value_reward_chosen[t] + 0.496 reward + -0.001 value_reward_chosen^2 + -0.047 value_reward_chosen*reward + 0.324 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 84, -, 12, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 389/1000 --- L(Train): 0.0059242 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.101 value_reward_chosen[t] + 0.496 reward + -0.001 value_reward_chosen^2 + -0.047 value_reward_chosen*reward + 0.324 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.12 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 85, -, 13, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 390/1000 --- L(Train): 0.0059182 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.101 value_reward_chosen[t] + 0.497 reward + -0.001 value_reward_chosen^2 + -0.047 value_reward_chosen*reward + 0.323 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.119 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 86, -, 14, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 391/1000 --- L(Train): 0.0059140 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.497 reward + -0.001 value_reward_chosen^2 + -0.047 value_reward_chosen*reward + 0.323 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.118 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 87, -, 15, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 392/1000 --- L(Train): 0.0059098 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.497 reward + -0.0 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.323 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.118 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 88, -, 16, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 393/1000 --- L(Train): 0.0059194 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.498 reward + 0.001 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.322 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.118 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 89, -, 17, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 394/1000 --- L(Train): 0.0059177 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.498 reward + 0.001 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.322 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.119 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 90, -, 18, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 395/1000 --- L(Train): 0.0059168 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.498 reward + 0.001 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.322 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.121 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 91, -, 19, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 396/1000 --- L(Train): 0.0059146 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.499 reward + -0.0 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.321 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 92, -, 20, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 397/1000 --- L(Train): 0.0059114 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.1 value_reward_chosen[t] + 0.499 reward + -0.001 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.321 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 93, -, 21, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 398/1000 --- L(Train): 0.0059080 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.499 reward + -0.001 value_reward_chosen^2 + -0.046 value_reward_chosen*reward + 0.32 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 94, -, 22, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 399/1000 --- L(Train): 0.0059050 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.5 reward + -0.001 value_reward_chosen^2 + -0.045 value_reward_chosen*reward + 0.32 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.125 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 95, -, 23, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 400/1000 --- L(Train): 0.0059043 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.5 reward + -0.0 value_reward_chosen^2 + -0.045 value_reward_chosen*reward + 0.32 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.126 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 96, -, 24, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 401/1000 --- L(Train): 0.0059030 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.5 reward + 0.0 value_reward_chosen^2 + -0.045 value_reward_chosen*reward + 0.319 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.126 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 97, -, 25, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 402/1000 --- L(Train): 0.0059039 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.501 reward + 0.0 value_reward_chosen^2 + -0.045 value_reward_chosen*reward + 0.319 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.126 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 98, -, 26, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 403/1000 --- L(Train): 0.0059045 --- L(Val, SINDy): 0.0000000 --- Time: 0.01s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.501 reward + -0.0 value_reward_chosen^2 + -0.045 value_reward_chosen*reward + 0.319 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.125 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, 99, -, 27, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 404/1000 --- L(Train): 0.0059033 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.502 reward + -0.045 value_reward_chosen*reward + 0.318 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 28, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 405/1000 --- L(Train): 0.0059007 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.099 value_reward_chosen[t] + 0.502 reward + -0.044 value_reward_chosen*reward + 0.318 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 29, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 406/1000 --- L(Train): 0.0059011 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.502 reward + -0.044 value_reward_chosen*reward + 0.318 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 30, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 407/1000 --- L(Train): 0.0058986 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.503 reward + -0.044 value_reward_chosen*reward + 0.317 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 31, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 408/1000 --- L(Train): 0.0058962 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.503 reward + -0.044 value_reward_chosen*reward + 0.317 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.121 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 32, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 409/1000 --- L(Train): 0.0058947 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.503 reward + -0.044 value_reward_chosen*reward + 0.316 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.121 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 33, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 410/1000 --- L(Train): 0.0058937 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.504 reward + -0.044 value_reward_chosen*reward + 0.316 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.121 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 34, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 411/1000 --- L(Train): 0.0058904 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.504 reward + -0.044 value_reward_chosen*reward + 0.316 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.121 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 35, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 412/1000 --- L(Train): 0.0058890 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.504 reward + -0.044 value_reward_chosen*reward + 0.315 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 36, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 413/1000 --- L(Train): 0.0058863 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.505 reward + -0.044 value_reward_chosen*reward + 0.315 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 37, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 414/1000 --- L(Train): 0.0058862 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 8):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.098 value_reward_chosen[t] + 0.505 reward + -0.044 value_reward_chosen*reward + 0.315 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 38, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 415/1000 --- L(Train): 0.0058852 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.505 reward + -0.043 value_reward_chosen*reward + 0.314 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 39, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 416/1000 --- L(Train): 0.0058832 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.506 reward + -0.043 value_reward_chosen*reward + 0.314 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 40, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 417/1000 --- L(Train): 0.0058824 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.506 reward + -0.043 value_reward_chosen*reward + 0.313 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 41, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\u001b[H\u001b[2J\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 418/1000 --- L(Train): 0.0058830 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.507 reward + -0.043 value_reward_chosen*reward + 0.313 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 42, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 419/1000 --- L(Train): 0.0058803 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.507 reward + -0.043 value_reward_chosen*reward + 0.313 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 43, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 420/1000 --- L(Train): 0.0058790 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.507 reward + -0.043 value_reward_chosen*reward + 0.312 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.124 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 44, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 421/1000 --- L(Train): 0.0058801 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.508 reward + -0.043 value_reward_chosen*reward + 0.312 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 45, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 422/1000 --- L(Train): 0.0058795 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.508 reward + -0.043 value_reward_chosen*reward + 0.311 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 46, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 423/1000 --- L(Train): 0.0058778 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.508 reward + -0.043 value_reward_chosen*reward + 0.311 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 47, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 424/1000 --- L(Train): 0.0058780 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.509 reward + -0.043 value_reward_chosen*reward + 0.311 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 48, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 425/1000 --- L(Train): 0.0058778 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.509 reward + -0.043 value_reward_chosen*reward + 0.31 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 49, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 426/1000 --- L(Train): 0.0058767 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.203 1 + 0.097 value_reward_chosen[t] + 0.509 reward + -0.042 value_reward_chosen*reward + 0.31 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 50, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 427/1000 --- L(Train): 0.0058755 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.097 value_reward_chosen[t] + 0.51 reward + -0.042 value_reward_chosen*reward + 0.31 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 51, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 428/1000 --- L(Train): 0.0058749 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.51 reward + -0.042 value_reward_chosen*reward + 0.309 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 52, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 429/1000 --- L(Train): 0.0058746 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.511 reward + -0.042 value_reward_chosen*reward + 0.309 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 53, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 430/1000 --- L(Train): 0.0058747 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.511 reward + -0.042 value_reward_chosen*reward + 0.308 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 54, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 431/1000 --- L(Train): 0.0058743 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.511 reward + -0.042 value_reward_chosen*reward + 0.308 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 55, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 432/1000 --- L(Train): 0.0058737 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.512 reward + -0.042 value_reward_chosen*reward + 0.308 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 56, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 433/1000 --- L(Train): 0.0058723 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.512 reward + -0.042 value_reward_chosen*reward + 0.307 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 57, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "================================================================================\n",
            "Epoch 434/1000 --- L(Train): 0.0058712 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.512 reward + -0.042 value_reward_chosen*reward + 0.307 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 58, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
            "Epoch 435/1000 --- L(Train): 0.0058693 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.513 reward + -0.042 value_reward_chosen*reward + 0.306 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 59, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 436/1000 --- L(Train): 0.0058695 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.513 reward + -0.042 value_reward_chosen*reward + 0.306 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 60, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 437/1000 --- L(Train): 0.0058685 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.514 reward + -0.042 value_reward_chosen*reward + 0.306 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 61, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 438/1000 --- L(Train): 0.0058673 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.514 reward + -0.042 value_reward_chosen*reward + 0.305 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 62, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 439/1000 --- L(Train): 0.0058661 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.514 reward + -0.042 value_reward_chosen*reward + 0.305 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 63, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 440/1000 --- L(Train): 0.0058652 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.515 reward + -0.041 value_reward_chosen*reward + 0.305 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 64, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 441/1000 --- L(Train): 0.0058635 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.515 reward + -0.041 value_reward_chosen*reward + 0.304 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 65, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 442/1000 --- L(Train): 0.0058629 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.515 reward + -0.041 value_reward_chosen*reward + 0.304 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 66, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 443/1000 --- L(Train): 0.0058600 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.516 reward + -0.041 value_reward_chosen*reward + 0.303 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 67, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 444/1000 --- L(Train): 0.0058591 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.516 reward + -0.041 value_reward_chosen*reward + 0.303 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.122 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 68, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 445/1000 --- L(Train): 0.0058597 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.517 reward + -0.041 value_reward_chosen*reward + 0.303 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 69, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 446/1000 --- L(Train): 0.0058596 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.517 reward + -0.041 value_reward_chosen*reward + 0.302 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 70, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 447/1000 --- L(Train): 0.0058580 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.517 reward + -0.041 value_reward_chosen*reward + 0.302 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 71, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 448/1000 --- L(Train): 0.0058579 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.096 value_reward_chosen[t] + 0.518 reward + -0.041 value_reward_chosen*reward + 0.301 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 72, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 449/1000 --- L(Train): 0.0058563 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.518 reward + -0.041 value_reward_chosen*reward + 0.301 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 73, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 450/1000 --- L(Train): 0.0058558 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.519 reward + -0.041 value_reward_chosen*reward + 0.301 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 74, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 451/1000 --- L(Train): 0.0058563 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.519 reward + -0.041 value_reward_chosen*reward + 0.3 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 75, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 452/1000 --- L(Train): 0.0058552 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.519 reward + -0.041 value_reward_chosen*reward + 0.3 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 76, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 453/1000 --- L(Train): 0.0058547 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.52 reward + -0.041 value_reward_chosen*reward + 0.299 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 77, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 454/1000 --- L(Train): 0.0058561 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.52 reward + -0.041 value_reward_chosen*reward + 0.299 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 78, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 455/1000 --- L(Train): 0.0058533 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.52 reward + -0.041 value_reward_chosen*reward + 0.299 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 79, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 456/1000 --- L(Train): 0.0058529 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.521 reward + -0.041 value_reward_chosen*reward + 0.298 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 80, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 457/1000 --- L(Train): 0.0058542 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.521 reward + -0.041 value_reward_chosen*reward + 0.298 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 81, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 458/1000 --- L(Train): 0.0058538 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.522 reward + -0.041 value_reward_chosen*reward + 0.297 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 82, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 459/1000 --- L(Train): 0.0058519 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.522 reward + -0.041 value_reward_chosen*reward + 0.297 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 83, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 460/1000 --- L(Train): 0.0058523 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.522 reward + -0.041 value_reward_chosen*reward + 0.297 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 84, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 461/1000 --- L(Train): 0.0058511 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.523 reward + -0.041 value_reward_chosen*reward + 0.296 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 85, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 462/1000 --- L(Train): 0.0058505 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.523 reward + -0.041 value_reward_chosen*reward + 0.296 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 86, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 463/1000 --- L(Train): 0.0058505 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.524 reward + -0.041 value_reward_chosen*reward + 0.295 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 87, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 464/1000 --- L(Train): 0.0058498 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.524 reward + -0.04 value_reward_chosen*reward + 0.295 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 88, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 465/1000 --- L(Train): 0.0058486 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.524 reward + -0.04 value_reward_chosen*reward + 0.295 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 89, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 466/1000 --- L(Train): 0.0058484 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.525 reward + -0.04 value_reward_chosen*reward + 0.294 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 90, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 467/1000 --- L(Train): 0.0058459 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.525 reward + -0.04 value_reward_chosen*reward + 0.294 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 91, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 468/1000 --- L(Train): 0.0058447 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.526 reward + -0.04 value_reward_chosen*reward + 0.293 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 92, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 469/1000 --- L(Train): 0.0058438 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.526 reward + -0.04 value_reward_chosen*reward + 0.293 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 93, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 470/1000 --- L(Train): 0.0058441 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.526 reward + -0.04 value_reward_chosen*reward + 0.293 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 94, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 471/1000 --- L(Train): 0.0058430 --- L(Val, SINDy): 0.0000000 --- Time: 0.04s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.527 reward + -0.04 value_reward_chosen*reward + 0.292 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 95, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 472/1000 --- L(Train): 0.0058431 --- L(Val, SINDy): 0.0000000 --- Time: 0.03s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.527 reward + -0.04 value_reward_chosen*reward + 0.292 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 96, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 473/1000 --- L(Train): 0.0058426 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.528 reward + -0.04 value_reward_chosen*reward + 0.291 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 97, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 474/1000 --- L(Train): 0.0058418 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.528 reward + -0.04 value_reward_chosen*reward + 0.291 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 98, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 475/1000 --- L(Train): 0.0058412 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.528 reward + -0.04 value_reward_chosen*reward + 0.291 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, 99, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 476/1000 --- L(Train): 0.0058408 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.202 1 + 0.095 value_reward_chosen[t] + 0.529 reward + 0.29 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 477/1000 --- L(Train): 0.0058414 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.204 1 + 0.094 value_reward_chosen[t] + 0.529 reward + 0.29 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 478/1000 --- L(Train): 0.0058411 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.206 1 + 0.094 value_reward_chosen[t] + 0.529 reward + 0.288 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 479/1000 --- L(Train): 0.0058394 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.209 1 + 0.093 value_reward_chosen[t] + 0.528 reward + 0.287 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n",
            "\u001b[H\u001b[2J================================================================================\n",
            "Epoch 480/1000 --- L(Train): 0.0058387 --- L(Val, SINDy): 0.0000000 --- Time: 0.02s;\n",
            "--------------------------------------------------------------------------------\n",
            "SPICE Model (Coefficients: 7):\n",
            "value_reward_chosen[t+1] = -0.212 1 + 0.093 value_reward_chosen[t] + 0.528 reward + 0.286 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.144 1 + 0.722 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 0.123 1 + 1.0 value_choice[t] \n",
            "--------------------------------------------------------------------------------\n",
            "Cutoff patience:\n",
            "value_reward_chosen: 0, 0, -, 0, -, -, -, -, -, 0\n",
            "value_reward_not_chosen: 0, 0, -, -, -, -\n",
            "value_choice: 0, -, -, -, -, -, -, -, -, -\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nStarting training on {estimator.device}...\")\n",
        "print(\"=\" * 80)\n",
        "estimator.fit(dataset.xs, dataset.ys, dataset.xs, dataset.ys)\n",
        "# estimator.load_spice(args.model)\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Print example SPICE model for first participant\n",
        "print(\"\\nExample SPICE model (participant 0):\")\n",
        "print(\"-\" * 80)\n",
        "estimator.print_spice_model(participant_id=0)\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "estimator.load_spice(path_spice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRU for benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('../..')\n",
        "from weinhardt2025.benchmarking.benchmarking_gru import GRU, training, setup_agent_gru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000: L(Train): 0.6775030493736267; L(Test): 0.5822796821594238\n",
            "Epoch 2/1000: L(Train): 0.5848801732063293; L(Test): 0.5116956830024719\n",
            "Epoch 3/1000: L(Train): 0.5086604356765747; L(Test): 0.46075400710105896\n",
            "Epoch 4/1000: L(Train): 0.46378234028816223; L(Test): 0.4428197145462036\n",
            "Epoch 5/1000: L(Train): 0.4437780976295471; L(Test): 0.4543968141078949\n",
            "Epoch 6/1000: L(Train): 0.4604732096195221; L(Test): 0.4685104191303253\n",
            "Epoch 7/1000: L(Train): 0.4695923328399658; L(Test): 0.4697604775428772\n",
            "Epoch 8/1000: L(Train): 0.47187739610671997; L(Test): 0.45993420481681824\n",
            "Epoch 9/1000: L(Train): 0.45509061217308044; L(Test): 0.4462375044822693\n",
            "Epoch 10/1000: L(Train): 0.45772090554237366; L(Test): 0.43464934825897217\n",
            "Epoch 11/1000: L(Train): 0.4478502571582794; L(Test): 0.42908936738967896\n",
            "Epoch 12/1000: L(Train): 0.42654502391815186; L(Test): 0.4298192262649536\n",
            "Epoch 13/1000: L(Train): 0.4358992874622345; L(Test): 0.4333026111125946\n",
            "Epoch 14/1000: L(Train): 0.4346815049648285; L(Test): 0.434948593378067\n",
            "Epoch 15/1000: L(Train): 0.4480709433555603; L(Test): 0.43316909670829773\n",
            "Epoch 16/1000: L(Train): 0.4371027946472168; L(Test): 0.42868679761886597\n",
            "Epoch 17/1000: L(Train): 0.44104406237602234; L(Test): 0.4237605631351471\n",
            "Epoch 18/1000: L(Train): 0.4289761185646057; L(Test): 0.42000898718833923\n",
            "Epoch 19/1000: L(Train): 0.42560648918151855; L(Test): 0.41827303171157837\n",
            "Epoch 20/1000: L(Train): 0.42354023456573486; L(Test): 0.4184154272079468\n",
            "Epoch 21/1000: L(Train): 0.42701807618141174; L(Test): 0.4194008708000183\n",
            "Epoch 22/1000: L(Train): 0.41926309466362; L(Test): 0.4202488958835602\n",
            "Epoch 23/1000: L(Train): 0.42934128642082214; L(Test): 0.420134037733078\n",
            "Epoch 24/1000: L(Train): 0.41832661628723145; L(Test): 0.41905319690704346\n",
            "Epoch 25/1000: L(Train): 0.42227062582969666; L(Test): 0.4173011779785156\n",
            "Epoch 26/1000: L(Train): 0.40834367275238037; L(Test): 0.41554105281829834\n",
            "Epoch 27/1000: L(Train): 0.4065655767917633; L(Test): 0.4142434895038605\n",
            "Epoch 28/1000: L(Train): 0.41234272718429565; L(Test): 0.4136400520801544\n",
            "Epoch 29/1000: L(Train): 0.4227108657360077; L(Test): 0.41367238759994507\n",
            "Epoch 30/1000: L(Train): 0.4073338210582733; L(Test): 0.4139382839202881\n",
            "Epoch 31/1000: L(Train): 0.41121339797973633; L(Test): 0.4139077365398407\n",
            "Epoch 32/1000: L(Train): 0.40936070680618286; L(Test): 0.4136117696762085\n",
            "Epoch 33/1000: L(Train): 0.41171973943710327; L(Test): 0.41315603256225586\n",
            "Epoch 34/1000: L(Train): 0.41790056228637695; L(Test): 0.41270318627357483\n",
            "Epoch 35/1000: L(Train): 0.4111061990261078; L(Test): 0.41221874952316284\n",
            "Epoch 36/1000: L(Train): 0.42139577865600586; L(Test): 0.41202661395072937\n",
            "Epoch 37/1000: L(Train): 0.4218769669532776; L(Test): 0.4120561480522156\n",
            "Epoch 38/1000: L(Train): 0.4195542335510254; L(Test): 0.41210684180259705\n",
            "Epoch 39/1000: L(Train): 0.42310765385627747; L(Test): 0.4119018316268921\n",
            "Epoch 40/1000: L(Train): 0.4186907112598419; L(Test): 0.4116019010543823\n",
            "Epoch 41/1000: L(Train): 0.41957566142082214; L(Test): 0.41136598587036133\n",
            "Epoch 42/1000: L(Train): 0.4020448327064514; L(Test): 0.4111669361591339\n",
            "Epoch 43/1000: L(Train): 0.41643092036247253; L(Test): 0.4109114706516266\n",
            "Epoch 44/1000: L(Train): 0.4182683527469635; L(Test): 0.41060179471969604\n",
            "Epoch 45/1000: L(Train): 0.4125562310218811; L(Test): 0.41058751940727234\n",
            "Epoch 46/1000: L(Train): 0.42045822739601135; L(Test): 0.4105865955352783\n",
            "Epoch 47/1000: L(Train): 0.4176332354545593; L(Test): 0.4103698432445526\n",
            "Epoch 48/1000: L(Train): 0.41615915298461914; L(Test): 0.41018110513687134\n",
            "Epoch 49/1000: L(Train): 0.40932977199554443; L(Test): 0.41015246510505676\n",
            "Epoch 50/1000: L(Train): 0.41379258036613464; L(Test): 0.40993666648864746\n",
            "Epoch 51/1000: L(Train): 0.4072631597518921; L(Test): 0.40961387753486633\n",
            "Epoch 52/1000: L(Train): 0.40392062067985535; L(Test): 0.4093307852745056\n",
            "Epoch 53/1000: L(Train): 0.41831305623054504; L(Test): 0.40930038690567017\n",
            "Epoch 54/1000: L(Train): 0.4121413230895996; L(Test): 0.40923306345939636\n",
            "Epoch 55/1000: L(Train): 0.40858587622642517; L(Test): 0.4090598523616791\n",
            "Epoch 56/1000: L(Train): 0.3994540572166443; L(Test): 0.40903112292289734\n",
            "Epoch 57/1000: L(Train): 0.41929492354393005; L(Test): 0.40900859236717224\n",
            "Epoch 58/1000: L(Train): 0.40054836869239807; L(Test): 0.4086509346961975\n",
            "Epoch 59/1000: L(Train): 0.4148124158382416; L(Test): 0.40827757120132446\n",
            "Epoch 60/1000: L(Train): 0.4131239652633667; L(Test): 0.4081760048866272\n",
            "Epoch 61/1000: L(Train): 0.4023897647857666; L(Test): 0.4080423414707184\n",
            "Epoch 62/1000: L(Train): 0.41339483857154846; L(Test): 0.4078676700592041\n",
            "Epoch 63/1000: L(Train): 0.3981713652610779; L(Test): 0.40771353244781494\n",
            "Epoch 64/1000: L(Train): 0.4015633761882782; L(Test): 0.40752214193344116\n",
            "Epoch 65/1000: L(Train): 0.41974571347236633; L(Test): 0.4073527753353119\n",
            "Epoch 66/1000: L(Train): 0.40783238410949707; L(Test): 0.40723079442977905\n",
            "Epoch 67/1000: L(Train): 0.4042106866836548; L(Test): 0.40710774064064026\n",
            "Epoch 68/1000: L(Train): 0.4203040301799774; L(Test): 0.4069744348526001\n",
            "Epoch 69/1000: L(Train): 0.4092864990234375; L(Test): 0.4067211151123047\n",
            "Epoch 70/1000: L(Train): 0.40534916520118713; L(Test): 0.40650853514671326\n",
            "Epoch 71/1000: L(Train): 0.41017910838127136; L(Test): 0.40633711218833923\n",
            "Epoch 72/1000: L(Train): 0.4106224477291107; L(Test): 0.4061552584171295\n",
            "Epoch 73/1000: L(Train): 0.40960824489593506; L(Test): 0.4060097932815552\n",
            "Epoch 74/1000: L(Train): 0.4070279002189636; L(Test): 0.40580087900161743\n",
            "Epoch 75/1000: L(Train): 0.40541326999664307; L(Test): 0.40563175082206726\n",
            "Epoch 76/1000: L(Train): 0.4057105481624603; L(Test): 0.4054684340953827\n",
            "Epoch 77/1000: L(Train): 0.41227149963378906; L(Test): 0.40534353256225586\n",
            "Epoch 78/1000: L(Train): 0.40781158208847046; L(Test): 0.405206561088562\n",
            "Epoch 79/1000: L(Train): 0.40388020873069763; L(Test): 0.4049983024597168\n",
            "Epoch 80/1000: L(Train): 0.4169110953807831; L(Test): 0.40475985407829285\n",
            "Epoch 81/1000: L(Train): 0.4146789610385895; L(Test): 0.4045946002006531\n",
            "Epoch 82/1000: L(Train): 0.4006470739841461; L(Test): 0.4044358432292938\n",
            "Epoch 83/1000: L(Train): 0.4140966832637787; L(Test): 0.40418606996536255\n",
            "Epoch 84/1000: L(Train): 0.40420809388160706; L(Test): 0.40401792526245117\n",
            "Epoch 85/1000: L(Train): 0.4145548939704895; L(Test): 0.40391385555267334\n",
            "Epoch 86/1000: L(Train): 0.3995579183101654; L(Test): 0.40362444519996643\n",
            "Epoch 87/1000: L(Train): 0.41195961833000183; L(Test): 0.4034135043621063\n",
            "Epoch 88/1000: L(Train): 0.41006147861480713; L(Test): 0.40324127674102783\n",
            "Epoch 89/1000: L(Train): 0.4018332064151764; L(Test): 0.4030284881591797\n",
            "Epoch 90/1000: L(Train): 0.4141203463077545; L(Test): 0.40283873677253723\n",
            "Epoch 91/1000: L(Train): 0.3939473628997803; L(Test): 0.40275654196739197\n",
            "Epoch 92/1000: L(Train): 0.4041439890861511; L(Test): 0.4025256335735321\n",
            "Epoch 93/1000: L(Train): 0.4162517488002777; L(Test): 0.4022969603538513\n",
            "Epoch 94/1000: L(Train): 0.40581977367401123; L(Test): 0.40207234025001526\n",
            "Epoch 95/1000: L(Train): 0.4118462800979614; L(Test): 0.4018283486366272\n",
            "Epoch 96/1000: L(Train): 0.40853309631347656; L(Test): 0.4016362726688385\n",
            "Epoch 97/1000: L(Train): 0.40758994221687317; L(Test): 0.4014853835105896\n",
            "Epoch 98/1000: L(Train): 0.4050458073616028; L(Test): 0.4013316035270691\n",
            "Epoch 99/1000: L(Train): 0.4085710942745209; L(Test): 0.4010806679725647\n",
            "Epoch 100/1000: L(Train): 0.3952159881591797; L(Test): 0.4008486866950989\n",
            "Epoch 101/1000: L(Train): 0.39557528495788574; L(Test): 0.4006865322589874\n",
            "Epoch 102/1000: L(Train): 0.4006715714931488; L(Test): 0.40075019001960754\n",
            "Epoch 103/1000: L(Train): 0.40587499737739563; L(Test): 0.400655597448349\n",
            "Epoch 104/1000: L(Train): 0.4073139727115631; L(Test): 0.4002443552017212\n",
            "Epoch 105/1000: L(Train): 0.394977331161499; L(Test): 0.39973777532577515\n",
            "Epoch 106/1000: L(Train): 0.4010106921195984; L(Test): 0.39952144026756287\n",
            "Epoch 107/1000: L(Train): 0.4037368595600128; L(Test): 0.39976203441619873\n",
            "Epoch 108/1000: L(Train): 0.39165765047073364; L(Test): 0.39939042925834656\n",
            "Epoch 109/1000: L(Train): 0.40621858835220337; L(Test): 0.39891961216926575\n",
            "Epoch 110/1000: L(Train): 0.40619781613349915; L(Test): 0.3996352255344391\n",
            "Epoch 111/1000: L(Train): 0.39449357986450195; L(Test): 0.3994731307029724\n",
            "Epoch 112/1000: L(Train): 0.4005439877510071; L(Test): 0.39866042137145996\n",
            "Epoch 113/1000: L(Train): 0.40308088064193726; L(Test): 0.3989187180995941\n",
            "Epoch 114/1000: L(Train): 0.40158602595329285; L(Test): 0.3989814221858978\n",
            "Epoch 115/1000: L(Train): 0.4073323607444763; L(Test): 0.39788517355918884\n",
            "Epoch 116/1000: L(Train): 0.39908501505851746; L(Test): 0.398014634847641\n",
            "Epoch 117/1000: L(Train): 0.40251603722572327; L(Test): 0.39801424741744995\n",
            "Epoch 118/1000: L(Train): 0.40814706683158875; L(Test): 0.39742910861968994\n",
            "Epoch 119/1000: L(Train): 0.3958868980407715; L(Test): 0.39733630418777466\n",
            "Epoch 120/1000: L(Train): 0.39729753136634827; L(Test): 0.39751601219177246\n",
            "Epoch 121/1000: L(Train): 0.40977537631988525; L(Test): 0.3971770405769348\n",
            "Epoch 122/1000: L(Train): 0.4050152897834778; L(Test): 0.39721187949180603\n",
            "Epoch 123/1000: L(Train): 0.40412646532058716; L(Test): 0.3969956338405609\n",
            "Epoch 124/1000: L(Train): 0.40656596422195435; L(Test): 0.3964991271495819\n",
            "Epoch 125/1000: L(Train): 0.4013272821903229; L(Test): 0.3966890573501587\n",
            "Epoch 126/1000: L(Train): 0.3987025022506714; L(Test): 0.3967083990573883\n",
            "Epoch 127/1000: L(Train): 0.40271100401878357; L(Test): 0.39627891778945923\n",
            "Epoch 128/1000: L(Train): 0.3881005048751831; L(Test): 0.39641904830932617\n",
            "Epoch 129/1000: L(Train): 0.4047245383262634; L(Test): 0.39613157510757446\n",
            "Epoch 130/1000: L(Train): 0.3876614272594452; L(Test): 0.396251380443573\n",
            "Epoch 131/1000: L(Train): 0.40744537115097046; L(Test): 0.3960098624229431\n",
            "Epoch 132/1000: L(Train): 0.39565175771713257; L(Test): 0.3957219123840332\n",
            "Epoch 133/1000: L(Train): 0.4048318564891815; L(Test): 0.39586004614830017\n",
            "Epoch 134/1000: L(Train): 0.39657506346702576; L(Test): 0.3955589234828949\n",
            "Epoch 135/1000: L(Train): 0.39942678809165955; L(Test): 0.3954681158065796\n",
            "Epoch 136/1000: L(Train): 0.4104648232460022; L(Test): 0.3955700695514679\n",
            "Epoch 137/1000: L(Train): 0.3986908793449402; L(Test): 0.39536768198013306\n",
            "Epoch 138/1000: L(Train): 0.3978949189186096; L(Test): 0.3952672779560089\n",
            "Epoch 139/1000: L(Train): 0.3965984284877777; L(Test): 0.3954218924045563\n",
            "Epoch 140/1000: L(Train): 0.40413033962249756; L(Test): 0.3949722349643707\n",
            "Epoch 141/1000: L(Train): 0.3873257040977478; L(Test): 0.3950655460357666\n",
            "Epoch 142/1000: L(Train): 0.40262824296951294; L(Test): 0.3949292302131653\n",
            "Epoch 143/1000: L(Train): 0.4054909646511078; L(Test): 0.39468392729759216\n",
            "Epoch 144/1000: L(Train): 0.3938847482204437; L(Test): 0.3947242796421051\n",
            "Epoch 145/1000: L(Train): 0.40417107939720154; L(Test): 0.39448612928390503\n",
            "Epoch 146/1000: L(Train): 0.4062364101409912; L(Test): 0.39455437660217285\n",
            "Epoch 147/1000: L(Train): 0.4028485417366028; L(Test): 0.394347220659256\n",
            "Epoch 148/1000: L(Train): 0.39073747396469116; L(Test): 0.39424973726272583\n",
            "Epoch 149/1000: L(Train): 0.40094250440597534; L(Test): 0.39420562982559204\n",
            "Epoch 150/1000: L(Train): 0.3930656909942627; L(Test): 0.39402514696121216\n",
            "Epoch 151/1000: L(Train): 0.4046580195426941; L(Test): 0.3939816951751709\n",
            "Epoch 152/1000: L(Train): 0.40829217433929443; L(Test): 0.39393407106399536\n",
            "Epoch 153/1000: L(Train): 0.39658573269844055; L(Test): 0.3936842083930969\n",
            "Epoch 154/1000: L(Train): 0.39217618107795715; L(Test): 0.39374494552612305\n",
            "Epoch 155/1000: L(Train): 0.3988645672798157; L(Test): 0.39389997720718384\n",
            "Epoch 156/1000: L(Train): 0.4025803506374359; L(Test): 0.3937276303768158\n",
            "Epoch 157/1000: L(Train): 0.4062480628490448; L(Test): 0.3934429883956909\n",
            "Epoch 158/1000: L(Train): 0.40148451924324036; L(Test): 0.39327019453048706\n",
            "Epoch 159/1000: L(Train): 0.39495572447776794; L(Test): 0.3931426405906677\n",
            "Epoch 160/1000: L(Train): 0.39405909180641174; L(Test): 0.3930896520614624\n",
            "Epoch 161/1000: L(Train): 0.3940631151199341; L(Test): 0.3932000696659088\n",
            "Epoch 162/1000: L(Train): 0.39258354902267456; L(Test): 0.39334630966186523\n",
            "Epoch 163/1000: L(Train): 0.3971058428287506; L(Test): 0.3931276202201843\n",
            "Epoch 164/1000: L(Train): 0.39481866359710693; L(Test): 0.39266303181648254\n",
            "Epoch 165/1000: L(Train): 0.40388721227645874; L(Test): 0.3926232159137726\n",
            "Epoch 166/1000: L(Train): 0.39701181650161743; L(Test): 0.3928183615207672\n",
            "Epoch 167/1000: L(Train): 0.39177805185317993; L(Test): 0.3926326632499695\n",
            "Epoch 168/1000: L(Train): 0.39522585272789; L(Test): 0.39234495162963867\n",
            "Epoch 169/1000: L(Train): 0.40035560727119446; L(Test): 0.3923340439796448\n",
            "Epoch 170/1000: L(Train): 0.4015325903892517; L(Test): 0.3923872411251068\n",
            "Epoch 171/1000: L(Train): 0.4006845951080322; L(Test): 0.39221295714378357\n",
            "Epoch 172/1000: L(Train): 0.4069453775882721; L(Test): 0.39205530285835266\n",
            "Epoch 173/1000: L(Train): 0.39383459091186523; L(Test): 0.3921656310558319\n",
            "Epoch 174/1000: L(Train): 0.4005928337574005; L(Test): 0.3919316232204437\n",
            "Epoch 175/1000: L(Train): 0.40076929330825806; L(Test): 0.39181533455848694\n",
            "Epoch 176/1000: L(Train): 0.4013441503047943; L(Test): 0.39177456498146057\n",
            "Epoch 177/1000: L(Train): 0.3910083472728729; L(Test): 0.3916490077972412\n",
            "Epoch 178/1000: L(Train): 0.397370845079422; L(Test): 0.39188528060913086\n",
            "Epoch 179/1000: L(Train): 0.39007559418678284; L(Test): 0.39132192730903625\n",
            "Epoch 180/1000: L(Train): 0.39328476786613464; L(Test): 0.39120247960090637\n",
            "Epoch 181/1000: L(Train): 0.39681756496429443; L(Test): 0.3911495804786682\n",
            "Epoch 182/1000: L(Train): 0.39130502939224243; L(Test): 0.39086613059043884\n",
            "Epoch 183/1000: L(Train): 0.38793691992759705; L(Test): 0.3912249505519867\n",
            "Epoch 184/1000: L(Train): 0.3873286843299866; L(Test): 0.39122408628463745\n",
            "Epoch 185/1000: L(Train): 0.4146783649921417; L(Test): 0.39061814546585083\n",
            "Epoch 186/1000: L(Train): 0.3918525278568268; L(Test): 0.3914528489112854\n",
            "Epoch 187/1000: L(Train): 0.38101863861083984; L(Test): 0.3915584087371826\n",
            "Epoch 188/1000: L(Train): 0.38581040501594543; L(Test): 0.3901470899581909\n",
            "Epoch 189/1000: L(Train): 0.4079987406730652; L(Test): 0.39173173904418945\n",
            "Epoch 190/1000: L(Train): 0.39038553833961487; L(Test): 0.3905951976776123\n",
            "Epoch 191/1000: L(Train): 0.39610743522644043; L(Test): 0.3901360034942627\n",
            "Epoch 192/1000: L(Train): 0.3957677185535431; L(Test): 0.39122799038887024\n",
            "Epoch 193/1000: L(Train): 0.39243635535240173; L(Test): 0.38994693756103516\n",
            "Epoch 194/1000: L(Train): 0.4007355570793152; L(Test): 0.39014488458633423\n",
            "Epoch 195/1000: L(Train): 0.3854837119579315; L(Test): 0.3910066783428192\n",
            "Epoch 196/1000: L(Train): 0.3983597755432129; L(Test): 0.3896214962005615\n",
            "Epoch 197/1000: L(Train): 0.3841816782951355; L(Test): 0.38929370045661926\n",
            "Epoch 198/1000: L(Train): 0.38648107647895813; L(Test): 0.38974544405937195\n",
            "Epoch 199/1000: L(Train): 0.3969575762748718; L(Test): 0.38917988538742065\n",
            "Epoch 200/1000: L(Train): 0.4020318388938904; L(Test): 0.3891764283180237\n",
            "Epoch 201/1000: L(Train): 0.39231956005096436; L(Test): 0.3894882798194885\n",
            "Epoch 202/1000: L(Train): 0.4021630585193634; L(Test): 0.3890978693962097\n",
            "Epoch 203/1000: L(Train): 0.39169779419898987; L(Test): 0.38886547088623047\n",
            "Epoch 204/1000: L(Train): 0.39588040113449097; L(Test): 0.3887212872505188\n",
            "Epoch 205/1000: L(Train): 0.3903101682662964; L(Test): 0.38863280415534973\n",
            "Epoch 206/1000: L(Train): 0.3942551016807556; L(Test): 0.38851431012153625\n",
            "Epoch 207/1000: L(Train): 0.38886985182762146; L(Test): 0.38832417130470276\n",
            "Epoch 208/1000: L(Train): 0.38264843821525574; L(Test): 0.3880493640899658\n",
            "Epoch 209/1000: L(Train): 0.39592260122299194; L(Test): 0.387809693813324\n",
            "Epoch 210/1000: L(Train): 0.39461836218833923; L(Test): 0.3877179026603699\n",
            "Epoch 211/1000: L(Train): 0.39162591099739075; L(Test): 0.38757696747779846\n",
            "Epoch 212/1000: L(Train): 0.3867045044898987; L(Test): 0.38781294226646423\n",
            "Epoch 213/1000: L(Train): 0.3949167728424072; L(Test): 0.3875162601470947\n",
            "Epoch 214/1000: L(Train): 0.3813190162181854; L(Test): 0.38716280460357666\n",
            "Epoch 215/1000: L(Train): 0.38456636667251587; L(Test): 0.38708388805389404\n",
            "Epoch 216/1000: L(Train): 0.39115071296691895; L(Test): 0.3869456350803375\n",
            "Epoch 217/1000: L(Train): 0.3842627704143524; L(Test): 0.3867802619934082\n",
            "Epoch 218/1000: L(Train): 0.38574692606925964; L(Test): 0.38672590255737305\n",
            "Epoch 219/1000: L(Train): 0.39928439259529114; L(Test): 0.3865722417831421\n",
            "Epoch 220/1000: L(Train): 0.38767868280410767; L(Test): 0.3863994777202606\n",
            "Epoch 221/1000: L(Train): 0.39037203788757324; L(Test): 0.3862112760543823\n",
            "Epoch 222/1000: L(Train): 0.38504135608673096; L(Test): 0.3860824406147003\n",
            "Epoch 223/1000: L(Train): 0.3759777247905731; L(Test): 0.3861309587955475\n",
            "Epoch 224/1000: L(Train): 0.39412182569503784; L(Test): 0.38614535331726074\n",
            "Epoch 225/1000: L(Train): 0.38196638226509094; L(Test): 0.3859955966472626\n",
            "Epoch 226/1000: L(Train): 0.3988748788833618; L(Test): 0.3857481777667999\n",
            "Epoch 227/1000: L(Train): 0.3919414281845093; L(Test): 0.38570258021354675\n",
            "Epoch 228/1000: L(Train): 0.3875442445278168; L(Test): 0.3855452537536621\n",
            "Epoch 229/1000: L(Train): 0.39587539434432983; L(Test): 0.38530638813972473\n",
            "Epoch 230/1000: L(Train): 0.39252331852912903; L(Test): 0.38550642132759094\n",
            "Epoch 231/1000: L(Train): 0.3849691152572632; L(Test): 0.3852079212665558\n",
            "Epoch 232/1000: L(Train): 0.379450261592865; L(Test): 0.38497036695480347\n",
            "Epoch 233/1000: L(Train): 0.39506274461746216; L(Test): 0.3850495517253876\n",
            "Epoch 234/1000: L(Train): 0.3844500780105591; L(Test): 0.3846643269062042\n",
            "Epoch 235/1000: L(Train): 0.3961111307144165; L(Test): 0.3845650255680084\n",
            "Epoch 236/1000: L(Train): 0.3910316526889801; L(Test): 0.38461604714393616\n",
            "Epoch 237/1000: L(Train): 0.3830512762069702; L(Test): 0.3842697739601135\n",
            "Epoch 238/1000: L(Train): 0.39424729347229004; L(Test): 0.3839965760707855\n",
            "Epoch 239/1000: L(Train): 0.38288307189941406; L(Test): 0.3837294578552246\n",
            "Epoch 240/1000: L(Train): 0.3978593051433563; L(Test): 0.38388144969940186\n",
            "Epoch 241/1000: L(Train): 0.38465169072151184; L(Test): 0.38369640707969666\n",
            "Epoch 242/1000: L(Train): 0.3815571069717407; L(Test): 0.3833397328853607\n",
            "Epoch 243/1000: L(Train): 0.38463711738586426; L(Test): 0.38360923528671265\n",
            "Epoch 244/1000: L(Train): 0.3922501504421234; L(Test): 0.3836213946342468\n",
            "Epoch 245/1000: L(Train): 0.3852193057537079; L(Test): 0.3831750154495239\n",
            "Epoch 246/1000: L(Train): 0.38450565934181213; L(Test): 0.3830430507659912\n",
            "Epoch 247/1000: L(Train): 0.38219407200813293; L(Test): 0.38282155990600586\n",
            "Epoch 248/1000: L(Train): 0.3914318382740021; L(Test): 0.3825247883796692\n",
            "Epoch 249/1000: L(Train): 0.39085665345191956; L(Test): 0.38230764865875244\n",
            "Epoch 250/1000: L(Train): 0.38004833459854126; L(Test): 0.38220709562301636\n",
            "Epoch 251/1000: L(Train): 0.384394109249115; L(Test): 0.3819037675857544\n",
            "Epoch 252/1000: L(Train): 0.3894943594932556; L(Test): 0.38166168332099915\n",
            "Epoch 253/1000: L(Train): 0.380254864692688; L(Test): 0.38153085112571716\n",
            "Epoch 254/1000: L(Train): 0.3917715847492218; L(Test): 0.3814013600349426\n",
            "Epoch 255/1000: L(Train): 0.3822014331817627; L(Test): 0.38138219714164734\n",
            "Epoch 256/1000: L(Train): 0.37026306986808777; L(Test): 0.3816523849964142\n",
            "Epoch 257/1000: L(Train): 0.3947160243988037; L(Test): 0.3813512921333313\n",
            "Epoch 258/1000: L(Train): 0.3892723321914673; L(Test): 0.38119012117385864\n",
            "Epoch 259/1000: L(Train): 0.3819199204444885; L(Test): 0.38076579570770264\n",
            "Epoch 260/1000: L(Train): 0.3886692523956299; L(Test): 0.3807946741580963\n",
            "Epoch 261/1000: L(Train): 0.38174912333488464; L(Test): 0.3807544708251953\n",
            "Epoch 262/1000: L(Train): 0.393301784992218; L(Test): 0.3806506097316742\n",
            "Epoch 263/1000: L(Train): 0.39083072543144226; L(Test): 0.3805360794067383\n",
            "Epoch 264/1000: L(Train): 0.3763427436351776; L(Test): 0.3801456391811371\n",
            "Epoch 265/1000: L(Train): 0.38933128118515015; L(Test): 0.3800632059574127\n",
            "Epoch 266/1000: L(Train): 0.37601956725120544; L(Test): 0.3799152076244354\n",
            "Epoch 267/1000: L(Train): 0.39029747247695923; L(Test): 0.37972745299339294\n",
            "Epoch 268/1000: L(Train): 0.39333006739616394; L(Test): 0.3792002499103546\n",
            "Epoch 269/1000: L(Train): 0.3771425485610962; L(Test): 0.3791905641555786\n",
            "Epoch 270/1000: L(Train): 0.3929210901260376; L(Test): 0.3792773485183716\n",
            "Epoch 271/1000: L(Train): 0.3887118995189667; L(Test): 0.3788762390613556\n",
            "Epoch 272/1000: L(Train): 0.3920416235923767; L(Test): 0.3786850869655609\n",
            "Epoch 273/1000: L(Train): 0.3816060423851013; L(Test): 0.3785876929759979\n",
            "Epoch 274/1000: L(Train): 0.3731382489204407; L(Test): 0.3784269094467163\n",
            "Epoch 275/1000: L(Train): 0.3766193687915802; L(Test): 0.3784237205982208\n",
            "Epoch 276/1000: L(Train): 0.36850428581237793; L(Test): 0.3781757056713104\n",
            "Epoch 277/1000: L(Train): 0.39168620109558105; L(Test): 0.3782746195793152\n",
            "Epoch 278/1000: L(Train): 0.38609811663627625; L(Test): 0.3777022361755371\n",
            "Epoch 279/1000: L(Train): 0.3860715925693512; L(Test): 0.3781149685382843\n",
            "Epoch 280/1000: L(Train): 0.3937332034111023; L(Test): 0.37744957208633423\n",
            "Epoch 281/1000: L(Train): 0.39327484369277954; L(Test): 0.37737324833869934\n",
            "Epoch 282/1000: L(Train): 0.37816333770751953; L(Test): 0.37741196155548096\n",
            "Epoch 283/1000: L(Train): 0.390316367149353; L(Test): 0.37662142515182495\n",
            "Epoch 284/1000: L(Train): 0.37964698672294617; L(Test): 0.37679776549339294\n",
            "Epoch 285/1000: L(Train): 0.38635551929473877; L(Test): 0.37617236375808716\n",
            "Epoch 286/1000: L(Train): 0.3839186429977417; L(Test): 0.37660789489746094\n",
            "Epoch 287/1000: L(Train): 0.37999534606933594; L(Test): 0.37617263197898865\n",
            "Epoch 288/1000: L(Train): 0.37315070629119873; L(Test): 0.3760108947753906\n",
            "Epoch 289/1000: L(Train): 0.3805663585662842; L(Test): 0.3765662610530853\n",
            "Epoch 290/1000: L(Train): 0.3969949781894684; L(Test): 0.3755965232849121\n",
            "Epoch 291/1000: L(Train): 0.37193170189857483; L(Test): 0.3757152557373047\n",
            "Epoch 292/1000: L(Train): 0.37244346737861633; L(Test): 0.37554097175598145\n",
            "Epoch 293/1000: L(Train): 0.3794393241405487; L(Test): 0.3753642439842224\n",
            "Epoch 294/1000: L(Train): 0.3770923614501953; L(Test): 0.3751489818096161\n",
            "Epoch 295/1000: L(Train): 0.3725311756134033; L(Test): 0.3746917247772217\n",
            "Epoch 296/1000: L(Train): 0.383008748292923; L(Test): 0.3745662271976471\n",
            "Epoch 297/1000: L(Train): 0.37620165944099426; L(Test): 0.37461233139038086\n",
            "Epoch 298/1000: L(Train): 0.38235974311828613; L(Test): 0.3745931088924408\n",
            "Epoch 299/1000: L(Train): 0.3861347734928131; L(Test): 0.37442147731781006\n",
            "Epoch 300/1000: L(Train): 0.3784308433532715; L(Test): 0.3744334578514099\n",
            "Epoch 301/1000: L(Train): 0.3776423931121826; L(Test): 0.37381812930107117\n",
            "Epoch 302/1000: L(Train): 0.37949109077453613; L(Test): 0.37367480993270874\n",
            "Epoch 303/1000: L(Train): 0.38278695940971375; L(Test): 0.3736133575439453\n",
            "Epoch 304/1000: L(Train): 0.3867563009262085; L(Test): 0.3738813102245331\n",
            "Epoch 305/1000: L(Train): 0.37730321288108826; L(Test): 0.3732168972492218\n",
            "Epoch 306/1000: L(Train): 0.37664496898651123; L(Test): 0.3729390501976013\n",
            "Epoch 307/1000: L(Train): 0.3735463321208954; L(Test): 0.37305426597595215\n",
            "Epoch 308/1000: L(Train): 0.3854554295539856; L(Test): 0.3724324405193329\n",
            "Epoch 309/1000: L(Train): 0.3843848407268524; L(Test): 0.3725947141647339\n",
            "Epoch 310/1000: L(Train): 0.3732938766479492; L(Test): 0.3726130425930023\n",
            "Epoch 311/1000: L(Train): 0.37901291251182556; L(Test): 0.3723536431789398\n",
            "Epoch 312/1000: L(Train): 0.3856949210166931; L(Test): 0.3717363774776459\n",
            "Epoch 313/1000: L(Train): 0.37346479296684265; L(Test): 0.3713424801826477\n",
            "Epoch 314/1000: L(Train): 0.3743670582771301; L(Test): 0.3715154230594635\n",
            "Epoch 315/1000: L(Train): 0.3783227503299713; L(Test): 0.37129533290863037\n",
            "Epoch 316/1000: L(Train): 0.3688727617263794; L(Test): 0.3711830973625183\n",
            "Epoch 317/1000: L(Train): 0.36875373125076294; L(Test): 0.37127721309661865\n",
            "Epoch 318/1000: L(Train): 0.3819449245929718; L(Test): 0.3706931173801422\n",
            "Epoch 319/1000: L(Train): 0.3769873082637787; L(Test): 0.370613694190979\n",
            "Epoch 320/1000: L(Train): 0.37573325634002686; L(Test): 0.36983972787857056\n",
            "Epoch 321/1000: L(Train): 0.3758018910884857; L(Test): 0.3708426058292389\n",
            "Epoch 322/1000: L(Train): 0.3786359131336212; L(Test): 0.3699418306350708\n",
            "Epoch 323/1000: L(Train): 0.37914586067199707; L(Test): 0.36925503611564636\n",
            "Epoch 324/1000: L(Train): 0.3858092725276947; L(Test): 0.369392067193985\n",
            "Epoch 325/1000: L(Train): 0.3695739507675171; L(Test): 0.36989954113960266\n",
            "Epoch 326/1000: L(Train): 0.37677091360092163; L(Test): 0.3691105842590332\n",
            "Epoch 327/1000: L(Train): 0.3758392035961151; L(Test): 0.36929821968078613\n",
            "Epoch 328/1000: L(Train): 0.3700334131717682; L(Test): 0.36877548694610596\n",
            "Epoch 329/1000: L(Train): 0.3840082883834839; L(Test): 0.36843007802963257\n",
            "Epoch 330/1000: L(Train): 0.3774626851081848; L(Test): 0.36852678656578064\n",
            "Epoch 331/1000: L(Train): 0.37444543838500977; L(Test): 0.3677249550819397\n",
            "Epoch 332/1000: L(Train): 0.37236759066581726; L(Test): 0.36781662702560425\n",
            "Epoch 333/1000: L(Train): 0.3787791132926941; L(Test): 0.3680213391780853\n",
            "Epoch 334/1000: L(Train): 0.38072121143341064; L(Test): 0.3679214417934418\n",
            "Epoch 335/1000: L(Train): 0.38447391986846924; L(Test): 0.36694321036338806\n",
            "Epoch 336/1000: L(Train): 0.37565329670906067; L(Test): 0.3678778409957886\n",
            "Epoch 337/1000: L(Train): 0.3778233528137207; L(Test): 0.3670215308666229\n",
            "Epoch 338/1000: L(Train): 0.37394043803215027; L(Test): 0.3673246204853058\n",
            "Epoch 339/1000: L(Train): 0.3766842782497406; L(Test): 0.36703231930732727\n",
            "Epoch 340/1000: L(Train): 0.37967637181282043; L(Test): 0.3668517768383026\n",
            "Epoch 341/1000: L(Train): 0.36819419264793396; L(Test): 0.3665330708026886\n",
            "Epoch 342/1000: L(Train): 0.37230199575424194; L(Test): 0.36580991744995117\n",
            "Epoch 343/1000: L(Train): 0.37182313203811646; L(Test): 0.3668038547039032\n",
            "Epoch 344/1000: L(Train): 0.3647826910018921; L(Test): 0.3658909499645233\n",
            "Epoch 345/1000: L(Train): 0.37442195415496826; L(Test): 0.3652000427246094\n",
            "Epoch 346/1000: L(Train): 0.376102477312088; L(Test): 0.3655162751674652\n",
            "Epoch 347/1000: L(Train): 0.36031046509742737; L(Test): 0.36541494727134705\n",
            "Epoch 348/1000: L(Train): 0.36385825276374817; L(Test): 0.3656332194805145\n",
            "Epoch 349/1000: L(Train): 0.37207505106925964; L(Test): 0.3653336465358734\n",
            "Epoch 350/1000: L(Train): 0.37122777104377747; L(Test): 0.36498764157295227\n",
            "Epoch 351/1000: L(Train): 0.37092670798301697; L(Test): 0.3642641305923462\n",
            "Epoch 352/1000: L(Train): 0.3623042404651642; L(Test): 0.36441195011138916\n",
            "Epoch 353/1000: L(Train): 0.36574074625968933; L(Test): 0.3642953932285309\n",
            "Epoch 354/1000: L(Train): 0.3686508536338806; L(Test): 0.3641384541988373\n",
            "Epoch 355/1000: L(Train): 0.3798709809780121; L(Test): 0.3637251853942871\n",
            "Epoch 356/1000: L(Train): 0.37642034888267517; L(Test): 0.36315762996673584\n",
            "Epoch 357/1000: L(Train): 0.36275961995124817; L(Test): 0.36398324370384216\n",
            "Epoch 358/1000: L(Train): 0.3648349940776825; L(Test): 0.3636636435985565\n",
            "Epoch 359/1000: L(Train): 0.36537298560142517; L(Test): 0.36323100328445435\n",
            "Epoch 360/1000: L(Train): 0.3716930150985718; L(Test): 0.3630558252334595\n",
            "Epoch 361/1000: L(Train): 0.36996543407440186; L(Test): 0.36258357763290405\n",
            "Epoch 362/1000: L(Train): 0.36896491050720215; L(Test): 0.36241069436073303\n",
            "Epoch 363/1000: L(Train): 0.3752954304218292; L(Test): 0.36176809668540955\n",
            "Epoch 364/1000: L(Train): 0.37074658274650574; L(Test): 0.36186501383781433\n",
            "Epoch 365/1000: L(Train): 0.3737689256668091; L(Test): 0.36203068494796753\n",
            "Epoch 366/1000: L(Train): 0.364592045545578; L(Test): 0.3618353009223938\n",
            "Epoch 367/1000: L(Train): 0.36237430572509766; L(Test): 0.3616797626018524\n",
            "Epoch 368/1000: L(Train): 0.3834841847419739; L(Test): 0.3619532883167267\n",
            "Epoch 369/1000: L(Train): 0.35745304822921753; L(Test): 0.36136555671691895\n",
            "Epoch 370/1000: L(Train): 0.37100160121917725; L(Test): 0.3611944019794464\n",
            "Epoch 371/1000: L(Train): 0.3560691475868225; L(Test): 0.3605646789073944\n",
            "Epoch 372/1000: L(Train): 0.36349818110466003; L(Test): 0.3613024055957794\n",
            "Epoch 373/1000: L(Train): 0.36194470524787903; L(Test): 0.36043286323547363\n",
            "Epoch 374/1000: L(Train): 0.3727381229400635; L(Test): 0.3606642186641693\n",
            "Epoch 375/1000: L(Train): 0.3689091205596924; L(Test): 0.3608959913253784\n",
            "Epoch 376/1000: L(Train): 0.364871621131897; L(Test): 0.36089348793029785\n",
            "Epoch 377/1000: L(Train): 0.3554385304450989; L(Test): 0.3604945242404938\n",
            "Epoch 378/1000: L(Train): 0.3621910810470581; L(Test): 0.36010676622390747\n",
            "Epoch 379/1000: L(Train): 0.3734032213687897; L(Test): 0.3600817918777466\n",
            "Epoch 380/1000: L(Train): 0.36847296357154846; L(Test): 0.3599187135696411\n",
            "Epoch 381/1000: L(Train): 0.36523714661598206; L(Test): 0.3595465123653412\n",
            "Epoch 382/1000: L(Train): 0.3632785379886627; L(Test): 0.3599952161312103\n",
            "Epoch 383/1000: L(Train): 0.3693942725658417; L(Test): 0.3598420321941376\n",
            "Epoch 384/1000: L(Train): 0.36755505204200745; L(Test): 0.3596833348274231\n",
            "Epoch 385/1000: L(Train): 0.3546426594257355; L(Test): 0.35928258299827576\n",
            "Epoch 386/1000: L(Train): 0.3583431839942932; L(Test): 0.3598305284976959\n",
            "Epoch 387/1000: L(Train): 0.36335641145706177; L(Test): 0.3594318628311157\n",
            "Epoch 388/1000: L(Train): 0.3702467978000641; L(Test): 0.3591088056564331\n",
            "Epoch 389/1000: L(Train): 0.36895063519477844; L(Test): 0.35984089970588684\n",
            "Epoch 390/1000: L(Train): 0.3745841681957245; L(Test): 0.35842767357826233\n",
            "Epoch 391/1000: L(Train): 0.36178725957870483; L(Test): 0.3590618669986725\n",
            "Epoch 392/1000: L(Train): 0.3751237392425537; L(Test): 0.35884714126586914\n",
            "Epoch 393/1000: L(Train): 0.36166051030158997; L(Test): 0.3583643138408661\n",
            "Epoch 394/1000: L(Train): 0.36155936121940613; L(Test): 0.3579677939414978\n",
            "Epoch 395/1000: L(Train): 0.3557475805282593; L(Test): 0.3582365810871124\n",
            "Epoch 396/1000: L(Train): 0.3684283494949341; L(Test): 0.3577885627746582\n",
            "Epoch 397/1000: L(Train): 0.37494245171546936; L(Test): 0.3576084077358246\n",
            "Epoch 398/1000: L(Train): 0.3707207441329956; L(Test): 0.3572670519351959\n",
            "Epoch 399/1000: L(Train): 0.36578455567359924; L(Test): 0.35682412981987\n",
            "Epoch 400/1000: L(Train): 0.3645436465740204; L(Test): 0.3571271002292633\n",
            "Epoch 401/1000: L(Train): 0.3709784150123596; L(Test): 0.3570155203342438\n",
            "Epoch 402/1000: L(Train): 0.3638514578342438; L(Test): 0.35714924335479736\n",
            "Epoch 403/1000: L(Train): 0.36821866035461426; L(Test): 0.356245219707489\n",
            "Epoch 404/1000: L(Train): 0.3641473352909088; L(Test): 0.3563776910305023\n",
            "Epoch 405/1000: L(Train): 0.36925339698791504; L(Test): 0.35585877299308777\n",
            "Epoch 406/1000: L(Train): 0.3667632043361664; L(Test): 0.3555091917514801\n",
            "Epoch 407/1000: L(Train): 0.37131017446517944; L(Test): 0.35593339800834656\n",
            "Epoch 408/1000: L(Train): 0.3732837438583374; L(Test): 0.35528475046157837\n",
            "Epoch 409/1000: L(Train): 0.3614286184310913; L(Test): 0.35486361384391785\n",
            "Epoch 410/1000: L(Train): 0.3698531985282898; L(Test): 0.355888694524765\n",
            "Epoch 411/1000: L(Train): 0.3593684732913971; L(Test): 0.354930579662323\n",
            "Epoch 412/1000: L(Train): 0.3591935336589813; L(Test): 0.35580846667289734\n",
            "Epoch 413/1000: L(Train): 0.3534485101699829; L(Test): 0.35586273670196533\n",
            "Epoch 414/1000: L(Train): 0.3614274561405182; L(Test): 0.3544537127017975\n",
            "Epoch 415/1000: L(Train): 0.3563230037689209; L(Test): 0.3544357419013977\n",
            "Epoch 416/1000: L(Train): 0.36910638213157654; L(Test): 0.3536822497844696\n",
            "Epoch 417/1000: L(Train): 0.35817527770996094; L(Test): 0.3544919192790985\n",
            "Epoch 418/1000: L(Train): 0.36145326495170593; L(Test): 0.35403427481651306\n",
            "Epoch 419/1000: L(Train): 0.3678915798664093; L(Test): 0.35442814230918884\n",
            "Epoch 420/1000: L(Train): 0.36019861698150635; L(Test): 0.3541174530982971\n",
            "Epoch 421/1000: L(Train): 0.36151182651519775; L(Test): 0.3540467321872711\n",
            "Epoch 422/1000: L(Train): 0.36114412546157837; L(Test): 0.3539849817752838\n",
            "Epoch 423/1000: L(Train): 0.3593614995479584; L(Test): 0.35399916768074036\n",
            "Epoch 424/1000: L(Train): 0.36615076661109924; L(Test): 0.35364487767219543\n",
            "Epoch 425/1000: L(Train): 0.3637652099132538; L(Test): 0.35396111011505127\n",
            "Epoch 426/1000: L(Train): 0.36562487483024597; L(Test): 0.35291382670402527\n",
            "Epoch 427/1000: L(Train): 0.36384499073028564; L(Test): 0.3538166582584381\n",
            "Epoch 428/1000: L(Train): 0.36289894580841064; L(Test): 0.35303443670272827\n",
            "Epoch 429/1000: L(Train): 0.3543119430541992; L(Test): 0.3530882000923157\n",
            "Epoch 430/1000: L(Train): 0.3550903797149658; L(Test): 0.35275349020957947\n",
            "Epoch 431/1000: L(Train): 0.35504916310310364; L(Test): 0.3534013330936432\n",
            "Epoch 432/1000: L(Train): 0.3730110228061676; L(Test): 0.35320791602134705\n",
            "Epoch 433/1000: L(Train): 0.3638465404510498; L(Test): 0.3525055944919586\n",
            "Epoch 434/1000: L(Train): 0.3587416708469391; L(Test): 0.3523581922054291\n",
            "Epoch 435/1000: L(Train): 0.35919126868247986; L(Test): 0.3528115749359131\n",
            "Epoch 436/1000: L(Train): 0.35715949535369873; L(Test): 0.35275790095329285\n",
            "Epoch 437/1000: L(Train): 0.36227118968963623; L(Test): 0.35181179642677307\n",
            "Epoch 438/1000: L(Train): 0.3740837574005127; L(Test): 0.3524598181247711\n",
            "Epoch 439/1000: L(Train): 0.35516873002052307; L(Test): 0.35193169116973877\n",
            "Epoch 440/1000: L(Train): 0.3609733581542969; L(Test): 0.3522680699825287\n",
            "Epoch 441/1000: L(Train): 0.3547977805137634; L(Test): 0.3524768650531769\n",
            "Epoch 442/1000: L(Train): 0.36589428782463074; L(Test): 0.35207927227020264\n",
            "Epoch 443/1000: L(Train): 0.3599516451358795; L(Test): 0.3521385192871094\n",
            "Epoch 444/1000: L(Train): 0.36515992879867554; L(Test): 0.3517360985279083\n",
            "Epoch 445/1000: L(Train): 0.366668701171875; L(Test): 0.35254713892936707\n",
            "Epoch 446/1000: L(Train): 0.3640345633029938; L(Test): 0.35174641013145447\n",
            "Epoch 447/1000: L(Train): 0.3691938519477844; L(Test): 0.3514823615550995\n",
            "Epoch 448/1000: L(Train): 0.3630160093307495; L(Test): 0.3525379002094269\n",
            "Epoch 449/1000: L(Train): 0.35450658202171326; L(Test): 0.3513924181461334\n",
            "Epoch 450/1000: L(Train): 0.3504749834537506; L(Test): 0.35265836119651794\n",
            "Epoch 451/1000: L(Train): 0.36847758293151855; L(Test): 0.3515051305294037\n",
            "Epoch 452/1000: L(Train): 0.3729911744594574; L(Test): 0.35022643208503723\n",
            "Epoch 453/1000: L(Train): 0.3581751585006714; L(Test): 0.3513696491718292\n",
            "Epoch 454/1000: L(Train): 0.3616824448108673; L(Test): 0.35066840052604675\n",
            "Epoch 455/1000: L(Train): 0.35000133514404297; L(Test): 0.35202693939208984\n",
            "Epoch 456/1000: L(Train): 0.36932918429374695; L(Test): 0.3517744839191437\n",
            "Epoch 457/1000: L(Train): 0.35160061717033386; L(Test): 0.3521273136138916\n",
            "Epoch 458/1000: L(Train): 0.36189356446266174; L(Test): 0.3513372838497162\n",
            "Epoch 459/1000: L(Train): 0.3690348267555237; L(Test): 0.3503749668598175\n",
            "Epoch 460/1000: L(Train): 0.3701010048389435; L(Test): 0.35110288858413696\n",
            "Epoch 461/1000: L(Train): 0.3681189715862274; L(Test): 0.3501032292842865\n",
            "Epoch 462/1000: L(Train): 0.36749598383903503; L(Test): 0.35093510150909424\n",
            "Epoch 463/1000: L(Train): 0.3623802959918976; L(Test): 0.34901297092437744\n",
            "Epoch 464/1000: L(Train): 0.3539507985115051; L(Test): 0.3502569794654846\n",
            "Epoch 465/1000: L(Train): 0.36295846104621887; L(Test): 0.35039958357810974\n",
            "Epoch 466/1000: L(Train): 0.3616672456264496; L(Test): 0.3505599796772003\n",
            "Epoch 467/1000: L(Train): 0.36527106165885925; L(Test): 0.3497733771800995\n",
            "Epoch 468/1000: L(Train): 0.34905850887298584; L(Test): 0.3491513729095459\n",
            "Epoch 469/1000: L(Train): 0.35189929604530334; L(Test): 0.34968841075897217\n",
            "Epoch 470/1000: L(Train): 0.3627251386642456; L(Test): 0.3484990894794464\n",
            "Epoch 471/1000: L(Train): 0.3599492311477661; L(Test): 0.3490629196166992\n",
            "Epoch 472/1000: L(Train): 0.3668299615383148; L(Test): 0.3484173119068146\n",
            "Epoch 473/1000: L(Train): 0.35547417402267456; L(Test): 0.3493054509162903\n",
            "Epoch 474/1000: L(Train): 0.36542290449142456; L(Test): 0.34915927052497864\n",
            "Epoch 475/1000: L(Train): 0.3557303249835968; L(Test): 0.34796807169914246\n",
            "Epoch 476/1000: L(Train): 0.368511438369751; L(Test): 0.3489440083503723\n",
            "Epoch 477/1000: L(Train): 0.36253219842910767; L(Test): 0.34867236018180847\n",
            "Epoch 478/1000: L(Train): 0.35777512192726135; L(Test): 0.3487364649772644\n",
            "Epoch 479/1000: L(Train): 0.353326678276062; L(Test): 0.34747886657714844\n",
            "Epoch 480/1000: L(Train): 0.35724732279777527; L(Test): 0.3478313386440277\n",
            "Epoch 481/1000: L(Train): 0.3634280264377594; L(Test): 0.34787753224372864\n",
            "Epoch 482/1000: L(Train): 0.3517877459526062; L(Test): 0.347652405500412\n",
            "Epoch 483/1000: L(Train): 0.35617467761039734; L(Test): 0.34741654992103577\n",
            "Epoch 484/1000: L(Train): 0.3602063059806824; L(Test): 0.34690365195274353\n",
            "Epoch 485/1000: L(Train): 0.3614799976348877; L(Test): 0.34704500436782837\n",
            "Epoch 486/1000: L(Train): 0.36172565817832947; L(Test): 0.347110390663147\n",
            "Epoch 487/1000: L(Train): 0.35266315937042236; L(Test): 0.34724825620651245\n",
            "Epoch 488/1000: L(Train): 0.3688313364982605; L(Test): 0.34697115421295166\n",
            "Epoch 489/1000: L(Train): 0.357395201921463; L(Test): 0.34672117233276367\n",
            "Epoch 490/1000: L(Train): 0.35486435890197754; L(Test): 0.34726014733314514\n",
            "Epoch 491/1000: L(Train): 0.36396944522857666; L(Test): 0.34686753153800964\n",
            "Epoch 492/1000: L(Train): 0.35625892877578735; L(Test): 0.3465900719165802\n",
            "Epoch 493/1000: L(Train): 0.3570176959037781; L(Test): 0.34762945771217346\n",
            "Epoch 494/1000: L(Train): 0.36335259675979614; L(Test): 0.3472268581390381\n",
            "Epoch 495/1000: L(Train): 0.3506552278995514; L(Test): 0.3464824855327606\n",
            "Epoch 496/1000: L(Train): 0.364767849445343; L(Test): 0.34597480297088623\n",
            "Epoch 497/1000: L(Train): 0.3632197678089142; L(Test): 0.3460060954093933\n",
            "Epoch 498/1000: L(Train): 0.37047553062438965; L(Test): 0.3463854193687439\n",
            "Epoch 499/1000: L(Train): 0.35667893290519714; L(Test): 0.34611430764198303\n",
            "Epoch 500/1000: L(Train): 0.36672094464302063; L(Test): 0.34624049067497253\n",
            "Epoch 501/1000: L(Train): 0.3577333390712738; L(Test): 0.3455890119075775\n",
            "Epoch 502/1000: L(Train): 0.3562067151069641; L(Test): 0.3455938696861267\n",
            "Epoch 503/1000: L(Train): 0.3580434322357178; L(Test): 0.3453771770000458\n",
            "Epoch 504/1000: L(Train): 0.3551867604255676; L(Test): 0.34559905529022217\n",
            "Epoch 505/1000: L(Train): 0.346278578042984; L(Test): 0.34566351771354675\n",
            "Epoch 506/1000: L(Train): 0.3598743975162506; L(Test): 0.3451412618160248\n",
            "Epoch 507/1000: L(Train): 0.35452714562416077; L(Test): 0.3460051417350769\n",
            "Epoch 508/1000: L(Train): 0.3584476709365845; L(Test): 0.3452461361885071\n",
            "Epoch 509/1000: L(Train): 0.35860639810562134; L(Test): 0.34541013836860657\n",
            "Epoch 510/1000: L(Train): 0.3531433939933777; L(Test): 0.3461826741695404\n",
            "Epoch 511/1000: L(Train): 0.35756009817123413; L(Test): 0.3450579345226288\n",
            "Epoch 512/1000: L(Train): 0.3509972393512726; L(Test): 0.34484899044036865\n",
            "Epoch 513/1000: L(Train): 0.3565272390842438; L(Test): 0.3451871871948242\n",
            "Epoch 514/1000: L(Train): 0.3568410575389862; L(Test): 0.3453029692173004\n",
            "Epoch 515/1000: L(Train): 0.35638150572776794; L(Test): 0.3454306423664093\n",
            "Epoch 516/1000: L(Train): 0.36741989850997925; L(Test): 0.3448273837566376\n",
            "Epoch 517/1000: L(Train): 0.36700454354286194; L(Test): 0.3449217677116394\n",
            "Epoch 518/1000: L(Train): 0.35762494802474976; L(Test): 0.34469008445739746\n",
            "Epoch 519/1000: L(Train): 0.3579773008823395; L(Test): 0.34508568048477173\n",
            "Epoch 520/1000: L(Train): 0.35304364562034607; L(Test): 0.34541136026382446\n",
            "Epoch 521/1000: L(Train): 0.3626936972141266; L(Test): 0.3443000316619873\n",
            "Epoch 522/1000: L(Train): 0.35172948241233826; L(Test): 0.34520554542541504\n",
            "Epoch 523/1000: L(Train): 0.3581086993217468; L(Test): 0.3449808955192566\n",
            "Epoch 524/1000: L(Train): 0.35207781195640564; L(Test): 0.34452706575393677\n",
            "Epoch 525/1000: L(Train): 0.35369622707366943; L(Test): 0.34535595774650574\n",
            "Epoch 526/1000: L(Train): 0.358151912689209; L(Test): 0.34530919790267944\n",
            "Epoch 527/1000: L(Train): 0.35847195982933044; L(Test): 0.3445756137371063\n",
            "Epoch 528/1000: L(Train): 0.3554545044898987; L(Test): 0.34408125281333923\n",
            "Epoch 529/1000: L(Train): 0.3574230372905731; L(Test): 0.34487295150756836\n",
            "Epoch 530/1000: L(Train): 0.356563538312912; L(Test): 0.3442632853984833\n",
            "Epoch 531/1000: L(Train): 0.35642364621162415; L(Test): 0.34333521127700806\n",
            "Epoch 532/1000: L(Train): 0.36004573106765747; L(Test): 0.3448411524295807\n",
            "Epoch 533/1000: L(Train): 0.35030120611190796; L(Test): 0.34375524520874023\n",
            "Epoch 534/1000: L(Train): 0.36146223545074463; L(Test): 0.34343940019607544\n",
            "Epoch 535/1000: L(Train): 0.36144527792930603; L(Test): 0.34384024143218994\n",
            "Epoch 536/1000: L(Train): 0.35051774978637695; L(Test): 0.3444516360759735\n",
            "Epoch 537/1000: L(Train): 0.35712555050849915; L(Test): 0.3435470759868622\n",
            "Epoch 538/1000: L(Train): 0.3691040277481079; L(Test): 0.3431394100189209\n",
            "Epoch 539/1000: L(Train): 0.3617886006832123; L(Test): 0.3432195782661438\n",
            "Epoch 540/1000: L(Train): 0.34526684880256653; L(Test): 0.34334081411361694\n",
            "Epoch 541/1000: L(Train): 0.3479781746864319; L(Test): 0.3430313169956207\n",
            "Epoch 542/1000: L(Train): 0.36289578676223755; L(Test): 0.3427658677101135\n",
            "Epoch 543/1000: L(Train): 0.3473397493362427; L(Test): 0.343133807182312\n",
            "Epoch 544/1000: L(Train): 0.35145217180252075; L(Test): 0.3432829976081848\n",
            "Epoch 545/1000: L(Train): 0.36699220538139343; L(Test): 0.3429517447948456\n",
            "Epoch 546/1000: L(Train): 0.35501837730407715; L(Test): 0.34249147772789\n",
            "Epoch 547/1000: L(Train): 0.36072099208831787; L(Test): 0.3431898057460785\n",
            "Epoch 548/1000: L(Train): 0.3643508851528168; L(Test): 0.3437630534172058\n",
            "Epoch 549/1000: L(Train): 0.3603323996067047; L(Test): 0.34368544816970825\n",
            "Epoch 550/1000: L(Train): 0.3538719117641449; L(Test): 0.344043493270874\n",
            "Epoch 551/1000: L(Train): 0.35598456859588623; L(Test): 0.34376928210258484\n",
            "Epoch 552/1000: L(Train): 0.36044061183929443; L(Test): 0.3435837924480438\n",
            "Epoch 553/1000: L(Train): 0.3697597086429596; L(Test): 0.34399598836898804\n",
            "Epoch 554/1000: L(Train): 0.35362955927848816; L(Test): 0.3435985743999481\n",
            "Epoch 555/1000: L(Train): 0.3453586995601654; L(Test): 0.34346768260002136\n",
            "Epoch 556/1000: L(Train): 0.3450716435909271; L(Test): 0.34332406520843506\n",
            "Epoch 557/1000: L(Train): 0.3584998548030853; L(Test): 0.34343016147613525\n",
            "Epoch 558/1000: L(Train): 0.35826361179351807; L(Test): 0.3432784676551819\n",
            "Epoch 559/1000: L(Train): 0.35550954937934875; L(Test): 0.34333398938179016\n",
            "Epoch 560/1000: L(Train): 0.3463096022605896; L(Test): 0.34321892261505127\n",
            "Epoch 561/1000: L(Train): 0.3463221490383148; L(Test): 0.34245020151138306\n",
            "Epoch 562/1000: L(Train): 0.3452659249305725; L(Test): 0.3417363166809082\n",
            "Epoch 563/1000: L(Train): 0.349958598613739; L(Test): 0.3420068025588989\n",
            "Epoch 564/1000: L(Train): 0.3539127707481384; L(Test): 0.3418183922767639\n",
            "Epoch 565/1000: L(Train): 0.35710957646369934; L(Test): 0.34069690108299255\n",
            "Epoch 566/1000: L(Train): 0.3488321900367737; L(Test): 0.34138303995132446\n",
            "Epoch 567/1000: L(Train): 0.35191962122917175; L(Test): 0.3419892191886902\n",
            "Epoch 568/1000: L(Train): 0.35564953088760376; L(Test): 0.34074315428733826\n",
            "Epoch 569/1000: L(Train): 0.361686646938324; L(Test): 0.340593546628952\n",
            "Epoch 570/1000: L(Train): 0.35009729862213135; L(Test): 0.3414290249347687\n",
            "Epoch 571/1000: L(Train): 0.3552386164665222; L(Test): 0.3405553102493286\n",
            "Epoch 572/1000: L(Train): 0.34605836868286133; L(Test): 0.340587854385376\n",
            "Epoch 573/1000: L(Train): 0.3493139147758484; L(Test): 0.34047502279281616\n",
            "Epoch 574/1000: L(Train): 0.3492039144039154; L(Test): 0.33995339274406433\n",
            "Epoch 575/1000: L(Train): 0.3543490767478943; L(Test): 0.34011417627334595\n",
            "Epoch 576/1000: L(Train): 0.35327017307281494; L(Test): 0.3394201397895813\n",
            "Epoch 577/1000: L(Train): 0.3609246015548706; L(Test): 0.3390451669692993\n",
            "Epoch 578/1000: L(Train): 0.34399521350860596; L(Test): 0.33885738253593445\n",
            "Epoch 579/1000: L(Train): 0.3686268925666809; L(Test): 0.3392075300216675\n",
            "Epoch 580/1000: L(Train): 0.3545389473438263; L(Test): 0.33887597918510437\n",
            "Epoch 581/1000: L(Train): 0.3597782552242279; L(Test): 0.3394266963005066\n",
            "Epoch 582/1000: L(Train): 0.3506763279438019; L(Test): 0.33941560983657837\n",
            "Epoch 583/1000: L(Train): 0.35589471459388733; L(Test): 0.3387826681137085\n",
            "Epoch 584/1000: L(Train): 0.356731653213501; L(Test): 0.33972305059432983\n",
            "Epoch 585/1000: L(Train): 0.3545554280281067; L(Test): 0.33936217427253723\n",
            "Epoch 586/1000: L(Train): 0.3526929020881653; L(Test): 0.3398788571357727\n",
            "Epoch 587/1000: L(Train): 0.3540254831314087; L(Test): 0.33895066380500793\n",
            "Epoch 588/1000: L(Train): 0.3458627462387085; L(Test): 0.3386060297489166\n",
            "Epoch 589/1000: L(Train): 0.3517586290836334; L(Test): 0.3390825092792511\n",
            "Epoch 590/1000: L(Train): 0.34812405705451965; L(Test): 0.338661253452301\n",
            "Epoch 591/1000: L(Train): 0.35351458191871643; L(Test): 0.33898451924324036\n",
            "Epoch 592/1000: L(Train): 0.35208284854888916; L(Test): 0.33848491311073303\n",
            "Epoch 593/1000: L(Train): 0.34946492314338684; L(Test): 0.3382895588874817\n",
            "Epoch 594/1000: L(Train): 0.35871320962905884; L(Test): 0.33751919865608215\n",
            "Epoch 595/1000: L(Train): 0.3474341034889221; L(Test): 0.3377855122089386\n",
            "Epoch 596/1000: L(Train): 0.35313668847084045; L(Test): 0.33782222867012024\n",
            "Epoch 597/1000: L(Train): 0.36096620559692383; L(Test): 0.33820539712905884\n",
            "Epoch 598/1000: L(Train): 0.35033467411994934; L(Test): 0.33814576268196106\n",
            "Epoch 599/1000: L(Train): 0.35514959692955017; L(Test): 0.33717581629753113\n",
            "Epoch 600/1000: L(Train): 0.3555217683315277; L(Test): 0.33734041452407837\n",
            "Epoch 601/1000: L(Train): 0.3495502471923828; L(Test): 0.3379341959953308\n",
            "Epoch 602/1000: L(Train): 0.3509960174560547; L(Test): 0.3377399742603302\n",
            "Epoch 603/1000: L(Train): 0.35567569732666016; L(Test): 0.337663471698761\n",
            "Epoch 604/1000: L(Train): 0.3596802353858948; L(Test): 0.3373049795627594\n",
            "Epoch 605/1000: L(Train): 0.34796321392059326; L(Test): 0.33795222640037537\n",
            "Epoch 606/1000: L(Train): 0.36296793818473816; L(Test): 0.33830901980400085\n",
            "Epoch 607/1000: L(Train): 0.3458248972892761; L(Test): 0.3378460109233856\n",
            "Epoch 608/1000: L(Train): 0.349447637796402; L(Test): 0.33664020895957947\n",
            "Epoch 609/1000: L(Train): 0.350436806678772; L(Test): 0.3391891121864319\n",
            "Epoch 610/1000: L(Train): 0.36223074793815613; L(Test): 0.33815404772758484\n",
            "Epoch 611/1000: L(Train): 0.34633398056030273; L(Test): 0.3394981026649475\n",
            "Epoch 612/1000: L(Train): 0.3538810610771179; L(Test): 0.3387632369995117\n",
            "Epoch 613/1000: L(Train): 0.3566420376300812; L(Test): 0.33915984630584717\n",
            "Epoch 614/1000: L(Train): 0.3521731197834015; L(Test): 0.3394937217235565\n",
            "Epoch 615/1000: L(Train): 0.3596741259098053; L(Test): 0.33747807145118713\n",
            "Epoch 616/1000: L(Train): 0.3448501229286194; L(Test): 0.34020546078681946\n",
            "Epoch 617/1000: L(Train): 0.3598962128162384; L(Test): 0.34223586320877075\n",
            "Epoch 618/1000: L(Train): 0.3457035720348358; L(Test): 0.34031304717063904\n",
            "Epoch 619/1000: L(Train): 0.3467491865158081; L(Test): 0.3403168320655823\n",
            "Epoch 620/1000: L(Train): 0.35949745774269104; L(Test): 0.3374664783477783\n",
            "Epoch 621/1000: L(Train): 0.35013294219970703; L(Test): 0.3394871950149536\n",
            "Epoch 622/1000: L(Train): 0.34968388080596924; L(Test): 0.3418440818786621\n",
            "Epoch 623/1000: L(Train): 0.35899460315704346; L(Test): 0.3404606282711029\n",
            "Epoch 624/1000: L(Train): 0.3513506054878235; L(Test): 0.33971625566482544\n",
            "Epoch 625/1000: L(Train): 0.3514384925365448; L(Test): 0.3391869068145752\n",
            "Epoch 626/1000: L(Train): 0.34858033061027527; L(Test): 0.33857348561286926\n",
            "Epoch 627/1000: L(Train): 0.34957632422447205; L(Test): 0.3397408723831177\n",
            "Epoch 628/1000: L(Train): 0.3524163067340851; L(Test): 0.3391116261482239\n",
            "Epoch 629/1000: L(Train): 0.35293468832969666; L(Test): 0.3389267921447754\n",
            "Epoch 630/1000: L(Train): 0.36598849296569824; L(Test): 0.3391445577144623\n",
            "Epoch 631/1000: L(Train): 0.34854745864868164; L(Test): 0.3371342122554779\n",
            "Epoch 632/1000: L(Train): 0.35013940930366516; L(Test): 0.3376331627368927\n",
            "Epoch 633/1000: L(Train): 0.3482434153556824; L(Test): 0.3390500247478485\n",
            "Epoch 634/1000: L(Train): 0.3497154712677002; L(Test): 0.33719924092292786\n",
            "Epoch 635/1000: L(Train): 0.3542766273021698; L(Test): 0.3382580876350403\n",
            "Epoch 636/1000: L(Train): 0.35113468766212463; L(Test): 0.33862441778182983\n",
            "Epoch 637/1000: L(Train): 0.36471033096313477; L(Test): 0.33785563707351685\n",
            "Epoch 638/1000: L(Train): 0.3454011380672455; L(Test): 0.3390064239501953\n",
            "Epoch 639/1000: L(Train): 0.3548717200756073; L(Test): 0.3378690481185913\n",
            "Epoch 640/1000: L(Train): 0.35228776931762695; L(Test): 0.33682504296302795\n",
            "Epoch 641/1000: L(Train): 0.3461683988571167; L(Test): 0.3368382751941681\n",
            "Epoch 642/1000: L(Train): 0.33956751227378845; L(Test): 0.33667537569999695\n",
            "Epoch 643/1000: L(Train): 0.346257746219635; L(Test): 0.33634305000305176\n",
            "Epoch 644/1000: L(Train): 0.3545061945915222; L(Test): 0.3366079032421112\n",
            "Epoch 645/1000: L(Train): 0.3593509793281555; L(Test): 0.3359456956386566\n",
            "Epoch 646/1000: L(Train): 0.34960734844207764; L(Test): 0.33560243248939514\n",
            "Epoch 647/1000: L(Train): 0.3557755947113037; L(Test): 0.3353477418422699\n",
            "Epoch 648/1000: L(Train): 0.35372254252433777; L(Test): 0.3352740705013275\n",
            "Epoch 649/1000: L(Train): 0.34787651896476746; L(Test): 0.336845338344574\n",
            "Epoch 650/1000: L(Train): 0.3518455922603607; L(Test): 0.3370242714881897\n",
            "Epoch 651/1000: L(Train): 0.3501533269882202; L(Test): 0.3352636992931366\n",
            "Epoch 652/1000: L(Train): 0.3417731821537018; L(Test): 0.33568888902664185\n",
            "Epoch 653/1000: L(Train): 0.35781973600387573; L(Test): 0.3346151113510132\n",
            "Epoch 654/1000: L(Train): 0.3500387966632843; L(Test): 0.3341524600982666\n",
            "Epoch 655/1000: L(Train): 0.35130956768989563; L(Test): 0.3350234925746918\n",
            "Epoch 656/1000: L(Train): 0.3524307310581207; L(Test): 0.3348841369152069\n",
            "Epoch 657/1000: L(Train): 0.3522203266620636; L(Test): 0.3351324796676636\n",
            "Epoch 658/1000: L(Train): 0.3583020865917206; L(Test): 0.33537912368774414\n",
            "Epoch 659/1000: L(Train): 0.35207438468933105; L(Test): 0.3348691463470459\n",
            "Epoch 660/1000: L(Train): 0.35285839438438416; L(Test): 0.3352101445198059\n",
            "Epoch 661/1000: L(Train): 0.34765422344207764; L(Test): 0.33470419049263\n",
            "Epoch 662/1000: L(Train): 0.3496559262275696; L(Test): 0.33381423354148865\n",
            "Epoch 663/1000: L(Train): 0.35685545206069946; L(Test): 0.3337049186229706\n",
            "Epoch 664/1000: L(Train): 0.340566486120224; L(Test): 0.3333306312561035\n",
            "Epoch 665/1000: L(Train): 0.35667937994003296; L(Test): 0.3334234356880188\n",
            "Epoch 666/1000: L(Train): 0.35457468032836914; L(Test): 0.33388155698776245\n",
            "Epoch 667/1000: L(Train): 0.3486631512641907; L(Test): 0.334124892950058\n",
            "Epoch 668/1000: L(Train): 0.3467588424682617; L(Test): 0.33402019739151\n",
            "Epoch 669/1000: L(Train): 0.3391321301460266; L(Test): 0.33420565724372864\n",
            "Epoch 670/1000: L(Train): 0.3477634787559509; L(Test): 0.3338727653026581\n",
            "Epoch 671/1000: L(Train): 0.3523450195789337; L(Test): 0.33367615938186646\n",
            "Epoch 672/1000: L(Train): 0.3467947542667389; L(Test): 0.3336297571659088\n",
            "Epoch 673/1000: L(Train): 0.3485363721847534; L(Test): 0.3336455821990967\n",
            "Epoch 674/1000: L(Train): 0.34934282302856445; L(Test): 0.3341202139854431\n",
            "Epoch 675/1000: L(Train): 0.3363243341445923; L(Test): 0.3340311348438263\n",
            "Epoch 676/1000: L(Train): 0.3541165292263031; L(Test): 0.3331550061702728\n",
            "Epoch 677/1000: L(Train): 0.34718215465545654; L(Test): 0.3332822024822235\n",
            "Epoch 678/1000: L(Train): 0.3493994474411011; L(Test): 0.33443912863731384\n",
            "Epoch 679/1000: L(Train): 0.35705217719078064; L(Test): 0.33411499857902527\n",
            "Epoch 680/1000: L(Train): 0.3414052724838257; L(Test): 0.3334379196166992\n",
            "Epoch 681/1000: L(Train): 0.3512919545173645; L(Test): 0.33489111065864563\n",
            "Epoch 682/1000: L(Train): 0.33640405535697937; L(Test): 0.333412766456604\n",
            "Epoch 683/1000: L(Train): 0.34212538599967957; L(Test): 0.3346661627292633\n",
            "Epoch 684/1000: L(Train): 0.35447847843170166; L(Test): 0.3364886939525604\n",
            "Epoch 685/1000: L(Train): 0.34229281544685364; L(Test): 0.3361682891845703\n",
            "Epoch 686/1000: L(Train): 0.3534606397151947; L(Test): 0.3348880112171173\n",
            "Epoch 687/1000: L(Train): 0.3503868877887726; L(Test): 0.3345279395580292\n",
            "Epoch 688/1000: L(Train): 0.3472414016723633; L(Test): 0.3365233540534973\n",
            "Epoch 689/1000: L(Train): 0.33945906162261963; L(Test): 0.3342010974884033\n",
            "Epoch 690/1000: L(Train): 0.3511882722377777; L(Test): 0.3360975980758667\n",
            "Epoch 691/1000: L(Train): 0.35354548692703247; L(Test): 0.3358742594718933\n",
            "Epoch 692/1000: L(Train): 0.3589975833892822; L(Test): 0.33386948704719543\n",
            "Epoch 693/1000: L(Train): 0.3434655964374542; L(Test): 0.33569687604904175\n",
            "Epoch 694/1000: L(Train): 0.3357884883880615; L(Test): 0.3346005976200104\n",
            "Epoch 695/1000: L(Train): 0.34783461689949036; L(Test): 0.3345317542552948\n",
            "Epoch 696/1000: L(Train): 0.3395839333534241; L(Test): 0.33406010270118713\n",
            "Epoch 697/1000: L(Train): 0.3454589545726776; L(Test): 0.3330739438533783\n",
            "Epoch 698/1000: L(Train): 0.3497805893421173; L(Test): 0.3335515260696411\n",
            "Epoch 699/1000: L(Train): 0.35054126381874084; L(Test): 0.33378472924232483\n",
            "Epoch 700/1000: L(Train): 0.3431472182273865; L(Test): 0.3334943652153015\n",
            "Epoch 701/1000: L(Train): 0.35417231917381287; L(Test): 0.33460068702697754\n",
            "Epoch 702/1000: L(Train): 0.34730470180511475; L(Test): 0.334186315536499\n",
            "Epoch 703/1000: L(Train): 0.3494657278060913; L(Test): 0.33289670944213867\n",
            "Epoch 704/1000: L(Train): 0.3383655548095703; L(Test): 0.33282706141471863\n",
            "Epoch 705/1000: L(Train): 0.3481922745704651; L(Test): 0.3328486382961273\n",
            "Epoch 706/1000: L(Train): 0.34150636196136475; L(Test): 0.33226895332336426\n",
            "Epoch 707/1000: L(Train): 0.3529531955718994; L(Test): 0.33321860432624817\n",
            "Epoch 708/1000: L(Train): 0.3359070122241974; L(Test): 0.3329896926879883\n",
            "Epoch 709/1000: L(Train): 0.346005916595459; L(Test): 0.3320131301879883\n",
            "Epoch 710/1000: L(Train): 0.34707409143447876; L(Test): 0.3318828344345093\n",
            "Epoch 711/1000: L(Train): 0.3553105592727661; L(Test): 0.33136430382728577\n",
            "Epoch 712/1000: L(Train): 0.34141984581947327; L(Test): 0.3309977352619171\n",
            "Epoch 713/1000: L(Train): 0.3418499529361725; L(Test): 0.33193403482437134\n",
            "Epoch 714/1000: L(Train): 0.35533127188682556; L(Test): 0.3316296637058258\n",
            "Epoch 715/1000: L(Train): 0.3557915985584259; L(Test): 0.33145594596862793\n",
            "Epoch 716/1000: L(Train): 0.35677698254585266; L(Test): 0.3319302201271057\n",
            "Epoch 717/1000: L(Train): 0.3543332815170288; L(Test): 0.3310229480266571\n",
            "Epoch 718/1000: L(Train): 0.3444772958755493; L(Test): 0.33163967728614807\n",
            "Epoch 719/1000: L(Train): 0.33773502707481384; L(Test): 0.3324967920780182\n",
            "Epoch 720/1000: L(Train): 0.3515031337738037; L(Test): 0.33247023820877075\n",
            "Epoch 721/1000: L(Train): 0.35575759410858154; L(Test): 0.33188921213150024\n",
            "Epoch 722/1000: L(Train): 0.3539624810218811; L(Test): 0.3312012255191803\n",
            "Epoch 723/1000: L(Train): 0.3520260751247406; L(Test): 0.3319217264652252\n",
            "Epoch 724/1000: L(Train): 0.34521162509918213; L(Test): 0.3318359851837158\n",
            "Epoch 725/1000: L(Train): 0.3485899865627289; L(Test): 0.33125758171081543\n",
            "Epoch 726/1000: L(Train): 0.3348994255065918; L(Test): 0.3311881422996521\n",
            "Epoch 727/1000: L(Train): 0.3524834215641022; L(Test): 0.33171260356903076\n",
            "Epoch 728/1000: L(Train): 0.33769935369491577; L(Test): 0.330401748418808\n",
            "Epoch 729/1000: L(Train): 0.3480999767780304; L(Test): 0.33182254433631897\n",
            "Epoch 730/1000: L(Train): 0.34874439239501953; L(Test): 0.3310379087924957\n",
            "Epoch 731/1000: L(Train): 0.3406563103199005; L(Test): 0.3318268954753876\n",
            "Epoch 732/1000: L(Train): 0.34463098645210266; L(Test): 0.33184921741485596\n",
            "Epoch 733/1000: L(Train): 0.3418358266353607; L(Test): 0.3311789333820343\n",
            "Epoch 734/1000: L(Train): 0.354373037815094; L(Test): 0.33085745573043823\n",
            "Epoch 735/1000: L(Train): 0.3497413992881775; L(Test): 0.33143073320388794\n",
            "Epoch 736/1000: L(Train): 0.3461756408214569; L(Test): 0.3308049440383911\n",
            "Epoch 737/1000: L(Train): 0.3442855179309845; L(Test): 0.3304283618927002\n",
            "Epoch 738/1000: L(Train): 0.34091103076934814; L(Test): 0.3310468792915344\n",
            "Epoch 739/1000: L(Train): 0.34804078936576843; L(Test): 0.33061158657073975\n",
            "Epoch 740/1000: L(Train): 0.3490860164165497; L(Test): 0.3296050429344177\n",
            "Epoch 741/1000: L(Train): 0.3425828218460083; L(Test): 0.3303316831588745\n",
            "Epoch 742/1000: L(Train): 0.34626075625419617; L(Test): 0.3302512764930725\n",
            "Epoch 743/1000: L(Train): 0.3424745202064514; L(Test): 0.33022868633270264\n",
            "Epoch 744/1000: L(Train): 0.33699890971183777; L(Test): 0.33075273036956787\n",
            "Epoch 745/1000: L(Train): 0.3481135666370392; L(Test): 0.33079856634140015\n",
            "Epoch 746/1000: L(Train): 0.3562186360359192; L(Test): 0.3306475877761841\n",
            "Epoch 747/1000: L(Train): 0.34877607226371765; L(Test): 0.3302689492702484\n",
            "Epoch 748/1000: L(Train): 0.34477686882019043; L(Test): 0.3311292827129364\n",
            "Epoch 749/1000: L(Train): 0.338251531124115; L(Test): 0.33012086153030396\n",
            "Epoch 750/1000: L(Train): 0.3431224524974823; L(Test): 0.33058759570121765\n",
            "Epoch 751/1000: L(Train): 0.3397490382194519; L(Test): 0.33128634095191956\n",
            "Epoch 752/1000: L(Train): 0.3483112156391144; L(Test): 0.3314913809299469\n",
            "Epoch 753/1000: L(Train): 0.3351805508136749; L(Test): 0.3315390646457672\n",
            "Epoch 754/1000: L(Train): 0.3493596613407135; L(Test): 0.3314501941204071\n",
            "Epoch 755/1000: L(Train): 0.3426533341407776; L(Test): 0.3307763636112213\n",
            "Epoch 756/1000: L(Train): 0.34199780225753784; L(Test): 0.3314147889614105\n",
            "Epoch 757/1000: L(Train): 0.33883166313171387; L(Test): 0.3306465744972229\n",
            "Epoch 758/1000: L(Train): 0.3492960035800934; L(Test): 0.3306209146976471\n",
            "Epoch 759/1000: L(Train): 0.34582042694091797; L(Test): 0.3308831453323364\n",
            "Epoch 760/1000: L(Train): 0.33479371666908264; L(Test): 0.32985779643058777\n",
            "Epoch 761/1000: L(Train): 0.3402336835861206; L(Test): 0.33114150166511536\n",
            "Epoch 762/1000: L(Train): 0.34264418482780457; L(Test): 0.33101800084114075\n",
            "Epoch 763/1000: L(Train): 0.3454156219959259; L(Test): 0.3312031328678131\n",
            "Epoch 764/1000: L(Train): 0.35398218035697937; L(Test): 0.33052003383636475\n",
            "Epoch 765/1000: L(Train): 0.34284263849258423; L(Test): 0.32987478375434875\n",
            "Epoch 766/1000: L(Train): 0.35328027606010437; L(Test): 0.330746591091156\n",
            "Epoch 767/1000: L(Train): 0.3503655195236206; L(Test): 0.33050575852394104\n",
            "Epoch 768/1000: L(Train): 0.34220603108406067; L(Test): 0.330740749835968\n",
            "Epoch 769/1000: L(Train): 0.3460325598716736; L(Test): 0.3300461769104004\n",
            "Epoch 770/1000: L(Train): 0.35003218054771423; L(Test): 0.3296775817871094\n",
            "Epoch 771/1000: L(Train): 0.34941861033439636; L(Test): 0.3297417461872101\n",
            "Epoch 772/1000: L(Train): 0.34033286571502686; L(Test): 0.3297746479511261\n",
            "Epoch 773/1000: L(Train): 0.3368329107761383; L(Test): 0.32994017004966736\n",
            "Epoch 774/1000: L(Train): 0.3519967496395111; L(Test): 0.33006229996681213\n",
            "Epoch 775/1000: L(Train): 0.34258100390434265; L(Test): 0.3292865455150604\n",
            "Epoch 776/1000: L(Train): 0.33660152554512024; L(Test): 0.32921379804611206\n",
            "Epoch 777/1000: L(Train): 0.3458879590034485; L(Test): 0.3294510841369629\n",
            "Epoch 778/1000: L(Train): 0.3484254777431488; L(Test): 0.32978492975234985\n",
            "Epoch 779/1000: L(Train): 0.3403565585613251; L(Test): 0.32932794094085693\n",
            "Epoch 780/1000: L(Train): 0.34208914637565613; L(Test): 0.33048486709594727\n",
            "Epoch 781/1000: L(Train): 0.34831109642982483; L(Test): 0.331243634223938\n",
            "Epoch 782/1000: L(Train): 0.3417510688304901; L(Test): 0.33040347695350647\n",
            "Epoch 783/1000: L(Train): 0.3384273052215576; L(Test): 0.3297174572944641\n",
            "Epoch 784/1000: L(Train): 0.3435348570346832; L(Test): 0.3303327262401581\n",
            "Epoch 785/1000: L(Train): 0.35224929451942444; L(Test): 0.32985109090805054\n",
            "Epoch 786/1000: L(Train): 0.3491734564304352; L(Test): 0.329913467168808\n",
            "Epoch 787/1000: L(Train): 0.3490746021270752; L(Test): 0.33183348178863525\n",
            "Epoch 788/1000: L(Train): 0.34003809094429016; L(Test): 0.33215442299842834\n",
            "Epoch 789/1000: L(Train): 0.34283316135406494; L(Test): 0.33039069175720215\n",
            "Epoch 790/1000: L(Train): 0.344701886177063; L(Test): 0.32992997765541077\n",
            "Epoch 791/1000: L(Train): 0.335628479719162; L(Test): 0.3305562734603882\n",
            "Epoch 792/1000: L(Train): 0.357857346534729; L(Test): 0.3297104835510254\n",
            "Epoch 793/1000: L(Train): 0.3383324146270752; L(Test): 0.32961440086364746\n",
            "Epoch 794/1000: L(Train): 0.33665668964385986; L(Test): 0.3293696343898773\n",
            "Epoch 795/1000: L(Train): 0.3503727614879608; L(Test): 0.329471617937088\n",
            "Epoch 796/1000: L(Train): 0.3376924395561218; L(Test): 0.32952365279197693\n",
            "Epoch 797/1000: L(Train): 0.33929359912872314; L(Test): 0.32889288663864136\n",
            "Epoch 798/1000: L(Train): 0.33809414505958557; L(Test): 0.32880568504333496\n",
            "Epoch 799/1000: L(Train): 0.3451392352581024; L(Test): 0.32844579219818115\n",
            "Epoch 800/1000: L(Train): 0.3579063415527344; L(Test): 0.32854580879211426\n",
            "Epoch 801/1000: L(Train): 0.34720614552497864; L(Test): 0.3286111354827881\n",
            "Epoch 802/1000: L(Train): 0.34679505228996277; L(Test): 0.3280492126941681\n",
            "Epoch 803/1000: L(Train): 0.33475735783576965; L(Test): 0.32866179943084717\n",
            "Epoch 804/1000: L(Train): 0.3499949276447296; L(Test): 0.328428715467453\n",
            "Epoch 805/1000: L(Train): 0.34242576360702515; L(Test): 0.328098326921463\n",
            "Epoch 806/1000: L(Train): 0.3436577320098877; L(Test): 0.3276157081127167\n",
            "Epoch 807/1000: L(Train): 0.34785664081573486; L(Test): 0.3278045356273651\n",
            "Epoch 808/1000: L(Train): 0.3369322717189789; L(Test): 0.3281000554561615\n",
            "Epoch 809/1000: L(Train): 0.3440743088722229; L(Test): 0.3279268741607666\n",
            "Epoch 810/1000: L(Train): 0.34645596146583557; L(Test): 0.32852092385292053\n",
            "Epoch 811/1000: L(Train): 0.35090187191963196; L(Test): 0.32923445105552673\n",
            "Epoch 812/1000: L(Train): 0.344329833984375; L(Test): 0.32804152369499207\n",
            "Epoch 813/1000: L(Train): 0.35147160291671753; L(Test): 0.32846879959106445\n",
            "Epoch 814/1000: L(Train): 0.3437475562095642; L(Test): 0.32789182662963867\n",
            "Epoch 815/1000: L(Train): 0.3388749063014984; L(Test): 0.32807958126068115\n",
            "Epoch 816/1000: L(Train): 0.350838303565979; L(Test): 0.3285990059375763\n",
            "Epoch 817/1000: L(Train): 0.3447591960430145; L(Test): 0.3281373977661133\n",
            "Epoch 818/1000: L(Train): 0.3464137017726898; L(Test): 0.32772397994995117\n",
            "Epoch 819/1000: L(Train): 0.34800854325294495; L(Test): 0.32837384939193726\n",
            "Epoch 820/1000: L(Train): 0.3446663022041321; L(Test): 0.32848629355430603\n",
            "Epoch 821/1000: L(Train): 0.3520202040672302; L(Test): 0.3290186822414398\n",
            "Epoch 822/1000: L(Train): 0.34023579955101013; L(Test): 0.32994380593299866\n",
            "Epoch 823/1000: L(Train): 0.34894853830337524; L(Test): 0.32906225323677063\n",
            "Epoch 824/1000: L(Train): 0.33827826380729675; L(Test): 0.32900869846343994\n",
            "Epoch 825/1000: L(Train): 0.34790387749671936; L(Test): 0.3299224376678467\n",
            "Epoch 826/1000: L(Train): 0.36133652925491333; L(Test): 0.328563392162323\n",
            "Epoch 827/1000: L(Train): 0.3461573123931885; L(Test): 0.3299897611141205\n",
            "Epoch 828/1000: L(Train): 0.33844560384750366; L(Test): 0.33020827174186707\n",
            "Epoch 829/1000: L(Train): 0.3503018915653229; L(Test): 0.3287551999092102\n",
            "Epoch 830/1000: L(Train): 0.340812087059021; L(Test): 0.32890328764915466\n",
            "Epoch 831/1000: L(Train): 0.35445520281791687; L(Test): 0.32780224084854126\n",
            "Epoch 832/1000: L(Train): 0.3432711660861969; L(Test): 0.3281650245189667\n",
            "Epoch 833/1000: L(Train): 0.33926886320114136; L(Test): 0.32830336689949036\n",
            "Epoch 834/1000: L(Train): 0.3471882939338684; L(Test): 0.32769346237182617\n",
            "Epoch 835/1000: L(Train): 0.3449723720550537; L(Test): 0.32814139127731323\n",
            "Epoch 836/1000: L(Train): 0.3380572199821472; L(Test): 0.3281175196170807\n",
            "Epoch 837/1000: L(Train): 0.35004985332489014; L(Test): 0.3275284171104431\n",
            "Epoch 838/1000: L(Train): 0.337993860244751; L(Test): 0.32798677682876587\n",
            "Epoch 839/1000: L(Train): 0.3435123562812805; L(Test): 0.3283160328865051\n",
            "Epoch 840/1000: L(Train): 0.3468683660030365; L(Test): 0.3275653123855591\n",
            "Epoch 841/1000: L(Train): 0.34553730487823486; L(Test): 0.3271501064300537\n",
            "Epoch 842/1000: L(Train): 0.34164807200431824; L(Test): 0.32732143998146057\n",
            "Epoch 843/1000: L(Train): 0.34477177262306213; L(Test): 0.3279322683811188\n",
            "Epoch 844/1000: L(Train): 0.3455124795436859; L(Test): 0.327208548784256\n",
            "Epoch 845/1000: L(Train): 0.3380276560783386; L(Test): 0.3267367482185364\n",
            "Epoch 846/1000: L(Train): 0.3374062776565552; L(Test): 0.32740306854248047\n",
            "Epoch 847/1000: L(Train): 0.3416474759578705; L(Test): 0.32622191309928894\n",
            "Epoch 848/1000: L(Train): 0.3411129415035248; L(Test): 0.3264598250389099\n",
            "Epoch 849/1000: L(Train): 0.3543424606323242; L(Test): 0.3273948132991791\n",
            "Epoch 850/1000: L(Train): 0.34848952293395996; L(Test): 0.3280338644981384\n",
            "Epoch 851/1000: L(Train): 0.34279900789260864; L(Test): 0.32808616757392883\n",
            "Epoch 852/1000: L(Train): 0.3521408140659332; L(Test): 0.32702183723449707\n",
            "Epoch 853/1000: L(Train): 0.3456800580024719; L(Test): 0.3270135223865509\n",
            "Epoch 854/1000: L(Train): 0.352700799703598; L(Test): 0.3276943266391754\n",
            "Epoch 855/1000: L(Train): 0.3403699994087219; L(Test): 0.3281787037849426\n",
            "Epoch 856/1000: L(Train): 0.3431433141231537; L(Test): 0.32784372568130493\n",
            "Epoch 857/1000: L(Train): 0.34448251128196716; L(Test): 0.3282659351825714\n",
            "Epoch 858/1000: L(Train): 0.3416992723941803; L(Test): 0.32802578806877136\n",
            "Epoch 859/1000: L(Train): 0.3498571217060089; L(Test): 0.32733869552612305\n",
            "Epoch 860/1000: L(Train): 0.3449627161026001; L(Test): 0.3267681300640106\n",
            "Epoch 861/1000: L(Train): 0.3445361256599426; L(Test): 0.3271576464176178\n",
            "Epoch 862/1000: L(Train): 0.3456021845340729; L(Test): 0.3279353976249695\n",
            "Epoch 863/1000: L(Train): 0.34137865900993347; L(Test): 0.32695767283439636\n",
            "Epoch 864/1000: L(Train): 0.33519354462623596; L(Test): 0.3257285952568054\n",
            "Epoch 865/1000: L(Train): 0.342428594827652; L(Test): 0.3256320059299469\n",
            "Epoch 866/1000: L(Train): 0.3398725986480713; L(Test): 0.3261750638484955\n",
            "Epoch 867/1000: L(Train): 0.34689784049987793; L(Test): 0.3275977373123169\n",
            "Epoch 868/1000: L(Train): 0.3501787483692169; L(Test): 0.32716962695121765\n",
            "Epoch 869/1000: L(Train): 0.34536150097846985; L(Test): 0.3256503641605377\n",
            "Epoch 870/1000: L(Train): 0.348924845457077; L(Test): 0.3258030414581299\n",
            "Epoch 871/1000: L(Train): 0.3317421078681946; L(Test): 0.32616719603538513\n",
            "Epoch 872/1000: L(Train): 0.3456811308860779; L(Test): 0.32567620277404785\n",
            "Epoch 873/1000: L(Train): 0.34024542570114136; L(Test): 0.32567816972732544\n",
            "Epoch 874/1000: L(Train): 0.34264543652534485; L(Test): 0.32503437995910645\n",
            "Epoch 875/1000: L(Train): 0.3562983274459839; L(Test): 0.3254452049732208\n",
            "Epoch 876/1000: L(Train): 0.34169110655784607; L(Test): 0.3252617120742798\n",
            "Epoch 877/1000: L(Train): 0.34162020683288574; L(Test): 0.3251728117465973\n",
            "Epoch 878/1000: L(Train): 0.34209465980529785; L(Test): 0.32544979453086853\n",
            "Epoch 879/1000: L(Train): 0.3455677926540375; L(Test): 0.3256223201751709\n",
            "Epoch 880/1000: L(Train): 0.3480161726474762; L(Test): 0.3264177143573761\n",
            "Epoch 881/1000: L(Train): 0.345939964056015; L(Test): 0.32515019178390503\n",
            "Epoch 882/1000: L(Train): 0.3533603847026825; L(Test): 0.3253665566444397\n",
            "Epoch 883/1000: L(Train): 0.3470523953437805; L(Test): 0.3253650367259979\n",
            "Epoch 884/1000: L(Train): 0.34335049986839294; L(Test): 0.32495930790901184\n",
            "Epoch 885/1000: L(Train): 0.3521176874637604; L(Test): 0.32549774646759033\n",
            "Epoch 886/1000: L(Train): 0.336708128452301; L(Test): 0.3244081139564514\n",
            "Epoch 887/1000: L(Train): 0.3372237980365753; L(Test): 0.32437068223953247\n",
            "Epoch 888/1000: L(Train): 0.3361489772796631; L(Test): 0.32417774200439453\n",
            "Epoch 889/1000: L(Train): 0.34614887833595276; L(Test): 0.32409751415252686\n",
            "Epoch 890/1000: L(Train): 0.3442952036857605; L(Test): 0.3239031732082367\n",
            "Epoch 891/1000: L(Train): 0.3454556167125702; L(Test): 0.3243538439273834\n",
            "Epoch 892/1000: L(Train): 0.3408069312572479; L(Test): 0.32463112473487854\n",
            "Epoch 893/1000: L(Train): 0.3378355801105499; L(Test): 0.32468122243881226\n",
            "Epoch 894/1000: L(Train): 0.3477780222892761; L(Test): 0.32480356097221375\n",
            "Epoch 895/1000: L(Train): 0.33304333686828613; L(Test): 0.3253421485424042\n",
            "Epoch 896/1000: L(Train): 0.3355558514595032; L(Test): 0.3240588903427124\n",
            "Epoch 897/1000: L(Train): 0.33930760622024536; L(Test): 0.32463979721069336\n",
            "Epoch 898/1000: L(Train): 0.3493853807449341; L(Test): 0.3245047628879547\n",
            "Epoch 899/1000: L(Train): 0.3470996618270874; L(Test): 0.3244318962097168\n",
            "Epoch 900/1000: L(Train): 0.3426157534122467; L(Test): 0.3263719975948334\n",
            "Epoch 901/1000: L(Train): 0.3410486876964569; L(Test): 0.3257797956466675\n",
            "Epoch 902/1000: L(Train): 0.33500680327415466; L(Test): 0.3257238566875458\n",
            "Epoch 903/1000: L(Train): 0.3435978889465332; L(Test): 0.3258289098739624\n",
            "Epoch 904/1000: L(Train): 0.3428858518600464; L(Test): 0.324491947889328\n",
            "Epoch 905/1000: L(Train): 0.34451398253440857; L(Test): 0.32532167434692383\n",
            "Epoch 906/1000: L(Train): 0.3452008068561554; L(Test): 0.3252161741256714\n",
            "Epoch 907/1000: L(Train): 0.34922513365745544; L(Test): 0.32477253675460815\n",
            "Epoch 908/1000: L(Train): 0.3414764404296875; L(Test): 0.32490265369415283\n",
            "Epoch 909/1000: L(Train): 0.3365376889705658; L(Test): 0.3252749741077423\n",
            "Epoch 910/1000: L(Train): 0.3519662916660309; L(Test): 0.3254043757915497\n",
            "Epoch 911/1000: L(Train): 0.34275925159454346; L(Test): 0.3257831335067749\n",
            "Epoch 912/1000: L(Train): 0.3329349756240845; L(Test): 0.32566767930984497\n",
            "Epoch 913/1000: L(Train): 0.34845608472824097; L(Test): 0.32597801089286804\n",
            "Epoch 914/1000: L(Train): 0.33266904950141907; L(Test): 0.32620906829833984\n",
            "Epoch 915/1000: L(Train): 0.34202760457992554; L(Test): 0.32575902342796326\n",
            "Epoch 916/1000: L(Train): 0.3454890847206116; L(Test): 0.3258619010448456\n",
            "Epoch 917/1000: L(Train): 0.343362420797348; L(Test): 0.3248467445373535\n",
            "Epoch 918/1000: L(Train): 0.3332149088382721; L(Test): 0.32428157329559326\n",
            "Epoch 919/1000: L(Train): 0.34938153624534607; L(Test): 0.3249153196811676\n",
            "Epoch 920/1000: L(Train): 0.34707075357437134; L(Test): 0.32473865151405334\n",
            "Epoch 921/1000: L(Train): 0.33531054854393005; L(Test): 0.3250868320465088\n",
            "Epoch 922/1000: L(Train): 0.3441358208656311; L(Test): 0.3253372311592102\n",
            "Epoch 923/1000: L(Train): 0.3477650284767151; L(Test): 0.32509681582450867\n",
            "Epoch 924/1000: L(Train): 0.34772345423698425; L(Test): 0.3248962461948395\n",
            "Epoch 925/1000: L(Train): 0.3419767916202545; L(Test): 0.3247314691543579\n",
            "Epoch 926/1000: L(Train): 0.3473966121673584; L(Test): 0.3239887058734894\n",
            "Epoch 927/1000: L(Train): 0.3383898437023163; L(Test): 0.3240452706813812\n",
            "Epoch 928/1000: L(Train): 0.34764623641967773; L(Test): 0.3240719437599182\n",
            "Epoch 929/1000: L(Train): 0.3461915850639343; L(Test): 0.32419008016586304\n",
            "Epoch 930/1000: L(Train): 0.3557758927345276; L(Test): 0.3241289258003235\n",
            "Epoch 931/1000: L(Train): 0.338140606880188; L(Test): 0.32394498586654663\n",
            "Epoch 932/1000: L(Train): 0.341581791639328; L(Test): 0.3236570954322815\n",
            "Epoch 933/1000: L(Train): 0.3350386619567871; L(Test): 0.3231901228427887\n",
            "Epoch 934/1000: L(Train): 0.3375343382358551; L(Test): 0.32418978214263916\n",
            "Epoch 935/1000: L(Train): 0.340243399143219; L(Test): 0.32533273100852966\n",
            "Epoch 936/1000: L(Train): 0.34633728861808777; L(Test): 0.32510510087013245\n",
            "Epoch 937/1000: L(Train): 0.3489823043346405; L(Test): 0.32528021931648254\n",
            "Epoch 938/1000: L(Train): 0.3437691330909729; L(Test): 0.32486000657081604\n",
            "Epoch 939/1000: L(Train): 0.34242209792137146; L(Test): 0.3234018087387085\n",
            "Epoch 940/1000: L(Train): 0.3401837944984436; L(Test): 0.3233673572540283\n",
            "Epoch 941/1000: L(Train): 0.34020355343818665; L(Test): 0.32329922914505005\n",
            "Epoch 942/1000: L(Train): 0.3488695025444031; L(Test): 0.32447549700737\n",
            "Epoch 943/1000: L(Train): 0.3378080725669861; L(Test): 0.32465407252311707\n",
            "Epoch 944/1000: L(Train): 0.33947834372520447; L(Test): 0.32427260279655457\n",
            "Epoch 945/1000: L(Train): 0.3467814028263092; L(Test): 0.32318806648254395\n",
            "Epoch 946/1000: L(Train): 0.339976966381073; L(Test): 0.3242069482803345\n",
            "Epoch 947/1000: L(Train): 0.34230363368988037; L(Test): 0.3235286772251129\n",
            "Epoch 948/1000: L(Train): 0.3437480926513672; L(Test): 0.3240504264831543\n",
            "Epoch 949/1000: L(Train): 0.3514254093170166; L(Test): 0.3241705298423767\n",
            "Epoch 950/1000: L(Train): 0.3464622497558594; L(Test): 0.3235253095626831\n",
            "Epoch 951/1000: L(Train): 0.3405514359474182; L(Test): 0.32300007343292236\n",
            "Epoch 952/1000: L(Train): 0.34397873282432556; L(Test): 0.32358768582344055\n",
            "Epoch 953/1000: L(Train): 0.35055550932884216; L(Test): 0.3236960470676422\n",
            "Epoch 954/1000: L(Train): 0.3423444628715515; L(Test): 0.3251679539680481\n",
            "Epoch 955/1000: L(Train): 0.3370184302330017; L(Test): 0.32435959577560425\n",
            "Epoch 956/1000: L(Train): 0.33894339203834534; L(Test): 0.32380586862564087\n",
            "Epoch 957/1000: L(Train): 0.347062885761261; L(Test): 0.3249625265598297\n",
            "Epoch 958/1000: L(Train): 0.3460632860660553; L(Test): 0.3250714838504791\n",
            "Epoch 959/1000: L(Train): 0.3411068022251129; L(Test): 0.32626453042030334\n",
            "Epoch 960/1000: L(Train): 0.3369581699371338; L(Test): 0.32697033882141113\n",
            "Epoch 961/1000: L(Train): 0.3458433151245117; L(Test): 0.3256951868534088\n",
            "Epoch 962/1000: L(Train): 0.3464331030845642; L(Test): 0.3256318271160126\n",
            "Epoch 963/1000: L(Train): 0.3444357216358185; L(Test): 0.326037734746933\n",
            "Epoch 964/1000: L(Train): 0.3532452881336212; L(Test): 0.3253701627254486\n",
            "Epoch 965/1000: L(Train): 0.3489716947078705; L(Test): 0.32512205839157104\n",
            "Epoch 966/1000: L(Train): 0.3437310457229614; L(Test): 0.3254047632217407\n",
            "Epoch 967/1000: L(Train): 0.3593394160270691; L(Test): 0.32502639293670654\n",
            "Epoch 968/1000: L(Train): 0.3379548192024231; L(Test): 0.3255646824836731\n",
            "Epoch 969/1000: L(Train): 0.3494308292865753; L(Test): 0.32508692145347595\n",
            "Epoch 970/1000: L(Train): 0.3391982316970825; L(Test): 0.3239888846874237\n",
            "Epoch 971/1000: L(Train): 0.3391355276107788; L(Test): 0.3239823579788208\n",
            "Epoch 972/1000: L(Train): 0.3439141511917114; L(Test): 0.32474929094314575\n",
            "Epoch 973/1000: L(Train): 0.3403055965900421; L(Test): 0.3245336711406708\n",
            "Epoch 974/1000: L(Train): 0.33874836564064026; L(Test): 0.32451024651527405\n",
            "Epoch 975/1000: L(Train): 0.33871591091156006; L(Test): 0.3250328004360199\n",
            "Epoch 976/1000: L(Train): 0.33615952730178833; L(Test): 0.32550063729286194\n",
            "Epoch 977/1000: L(Train): 0.3425236642360687; L(Test): 0.32533562183380127\n",
            "Epoch 978/1000: L(Train): 0.3407481014728546; L(Test): 0.32403889298439026\n",
            "Epoch 979/1000: L(Train): 0.3462366759777069; L(Test): 0.3233509957790375\n",
            "Epoch 980/1000: L(Train): 0.3338681757450104; L(Test): 0.32476478815078735\n",
            "Epoch 981/1000: L(Train): 0.34829479455947876; L(Test): 0.323915034532547\n",
            "Epoch 982/1000: L(Train): 0.34067675471305847; L(Test): 0.3233281373977661\n",
            "Epoch 983/1000: L(Train): 0.3472699224948883; L(Test): 0.324468195438385\n",
            "Epoch 984/1000: L(Train): 0.34626731276512146; L(Test): 0.3231860101222992\n",
            "Epoch 985/1000: L(Train): 0.3300434947013855; L(Test): 0.3229750692844391\n",
            "Epoch 986/1000: L(Train): 0.34767210483551025; L(Test): 0.3229353129863739\n",
            "Epoch 987/1000: L(Train): 0.3357410132884979; L(Test): 0.32439500093460083\n",
            "Epoch 988/1000: L(Train): 0.3488524556159973; L(Test): 0.32326558232307434\n",
            "Epoch 989/1000: L(Train): 0.34113091230392456; L(Test): 0.32272833585739136\n",
            "Epoch 990/1000: L(Train): 0.34138527512550354; L(Test): 0.3243338167667389\n",
            "Epoch 991/1000: L(Train): 0.34027472138404846; L(Test): 0.32399439811706543\n",
            "Epoch 992/1000: L(Train): 0.333987295627594; L(Test): 0.32360920310020447\n",
            "Epoch 993/1000: L(Train): 0.3429918587207794; L(Test): 0.32358479499816895\n",
            "Epoch 994/1000: L(Train): 0.34077560901641846; L(Test): 0.3244014084339142\n",
            "Epoch 995/1000: L(Train): 0.3457890748977661; L(Test): 0.32447725534439087\n",
            "Epoch 996/1000: L(Train): 0.34307554364204407; L(Test): 0.3246075510978699\n",
            "Epoch 997/1000: L(Train): 0.3518047332763672; L(Test): 0.32630327343940735\n",
            "Epoch 998/1000: L(Train): 0.3428173065185547; L(Test): 0.32564616203308105\n",
            "Epoch 999/1000: L(Train): 0.34145087003707886; L(Test): 0.323529988527298\n",
            "Epoch 1000/1000: L(Train): 0.33362677693367004; L(Test): 0.3246023654937744\n"
          ]
        }
      ],
      "source": [
        "epochs = 1000\n",
        "\n",
        "gru = GRU(n_actions=n_actions, additional_inputs=1).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gru.parameters(), lr=0.01)\n",
        "\n",
        "gru = training(\n",
        "    gru=gru,\n",
        "    optimizer=optimizer,\n",
        "    dataset_train=dataset,\n",
        "    dataset_test=dataset,\n",
        "    epochs=epochs,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_gru = '../../weinhardt2025/params/ganesh2024a/gru_ganesh2024a.pkl'\n",
        "# torch.save(gru.state_dict(), path_gru)\n",
        "gru_agent = setup_agent_gru(path_gru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot SPICE against benchmark models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "value_reward_chosen[t+1] = -0.933 1 + 0.008 value_reward_chosen[t] + 0.07 contr_diff + 1.015 reward + 0.0 reward^2 \n",
            "value_reward_not_chosen[t+1] = -0.234 1 + 1.0 value_reward_not_chosen[t] \n",
            "value_choice[t+1] = 1.0 value_choice[t] \n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 8 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plotting\n",
        "participant_id = 7\n",
        "\n",
        "estimator.print_spice_model(participant_id)\n",
        "\n",
        "agents = {\n",
        "    # add baseline agent here\n",
        "    'rnn': estimator.rnn_agent,\n",
        "    'spice': estimator.spice_agent,\n",
        "    'benchmark': gru_agent,\n",
        "}\n",
        "\n",
        "fig, axs = plot_session(agents, dataset.xs[participant_id])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "spice",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
