{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf7jlYw4NA0v",
        "outputId": "0969ca34-675d-422e-cbfb-7387d9bcd8ad"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/whyhardt/SPICE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oXIbg826NS5i",
        "outputId": "3825864a-cb2d-4ad5-f2e5-79a4e81dfc3e"
      },
      "outputs": [],
      "source": [
        "# !pip install -e SPICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f0uVlABYznR5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from spice import SpiceEstimator, SpiceConfig, csv_to_dataset, BaseRNN, plot_session\n",
        "\n",
        "# For custom RNN\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load the data first with the `csv_to_dataset` method. This method returns a `SpiceDataset` object which we can use right away "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of dataset: torch.Size([1176, 25, 8])\n",
            "Number of participants: 98\n",
            "Number of actions in dataset: 2\n",
            "Number of additional inputs: 1\n"
          ]
        }
      ],
      "source": [
        "# Load your data\n",
        "dataset = csv_to_dataset(\n",
        "    file = '../data/ganesh2024a/ganesh2024a_choice.csv',\n",
        "    df_participant_id='subjID',\n",
        "    df_choice='chose_high',\n",
        "    df_feedback='reward',\n",
        "    df_block='blocks',\n",
        "    additional_inputs=['contrast_difference'],\n",
        "    timeshift_additional_inputs=[-1],\n",
        "    )\n",
        "\n",
        "# structure of dataset:\n",
        "# dataset has two main attributes: xs -> inputs; ys -> targets (next action)\n",
        "# shape: (n_participants*n_blocks*n_experiments, n_timesteps, features)\n",
        "# features are (n_actions * action, n_actions * reward, n_additional_inputs * additional_input, block_number, experiment_id, participant_id)\n",
        "\n",
        "# in order to set up the participant embedding we have to compute the number of unique participants in our data \n",
        "# to get the number of participants n_participants we do:\n",
        "n_participants = len(dataset.xs[..., -1].unique())\n",
        "\n",
        "print(f\"Shape of dataset: {dataset.xs.shape}\")\n",
        "print(f\"Number of participants: {n_participants}\")\n",
        "n_actions = dataset.ys.shape[-1]\n",
        "print(f\"Number of actions in dataset: {n_actions}\")\n",
        "print(f\"Number of additional inputs: {dataset.xs.shape[-1]-2*n_actions-3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPICE Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to define the configuration for SPICE with a `SpiceConfig` object.\n",
        "\n",
        "The `SpiceConfig` takes as arguments \n",
        "1. `library_setup (dict)`: Defining the variable names of each module.\n",
        "2. `memory_state (dict)`: Defining the memory state variables and their initial values.\n",
        "3. `states_in_logit (list)`: Defining which of the memory state variables are used later for the logit computation. This is necessary for some background processes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spice_config = SpiceConfig(\n",
        "    library_setup={\n",
        "        'value_reward_chosen': ['contr_diff', 'reward'],\n",
        "        'value_reward_not_chosen': ['contr_diff'],\n",
        "        'value_choice': ['contr_diff', 'choice'],\n",
        "    },\n",
        "    \n",
        "    memory_state={\n",
        "            'value_reward': 0.,\n",
        "            'value_choice': 0.,\n",
        "        }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we are going to define the SPICE model which is a child of the `BaseRNN` and `torch.nn.Module` class and takes as required arguments:\n",
        "1. `spice_config (SpiceConfig)`: previously defined SpiceConfig object\n",
        "2. `n_actions (int)`: number of possible actions in your dataset (including non-displayed ones if applicable).\n",
        "3. `n_participants (int)`: number of participants in your dataset.\n",
        "\n",
        "As usual for a `torch.nn.Module` we have to define at least the `__init__` method and the `forward` method.\n",
        "The `forward` method gets called when computing a forward pass through the model and takes as inputs `(inputs (SpiceDataset.xs), prev_state (dict, default: None), batch_first (bool, default: False))` and returns `(logits (torch.Tensor, shape: (n_participants*n_blocks*n_experiments, timesteps, n_actions)), updated_state (dict))`. Two necessary method calls inside the forward pass are:\n",
        "1. `self.init_forward_pass(inputs, prev_state, batch_first) -> SpiceSignals`: returns a `SpiceSignals` object which carries all relevant information already processed.\n",
        "2. `self.post_forward_pass(SpiceSignals, batch_first) -> SpiceSignals`: does some re-arranging of the logits to adhere to `batch_first`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0kOR2Qgz0FZ"
      },
      "outputs": [],
      "source": [
        "class SPICERNN(BaseRNN):\n",
        "    \n",
        "    def __init__(self, spice_config, **kwargs):\n",
        "        super().__init__(spice_config=spice_config, **kwargs)\n",
        "        \n",
        "        # participant embedding\n",
        "        self.participant_embedding = self.setup_embedding(num_embeddings=self.n_participants, embedding_size=self.embedding_size, dropout=0.)\n",
        "        \n",
        "        # set up the submodules\n",
        "        self.setup_module(key_module='value_reward_chosen', input_size=2+self.embedding_size)\n",
        "        self.setup_module(key_module='value_reward_not_chosen', input_size=1+self.embedding_size)\n",
        "        self.setup_module(key_module='value_choice', input_size=2+self.embedding_size)\n",
        "        \n",
        "    def forward(self, inputs, prev_state, batch_first=False):\n",
        "        \n",
        "        spice_signals = self.init_forward_pass(inputs, prev_state, batch_first)\n",
        "        \n",
        "        contr_diffs = spice_signals.additional_inputs.repeat(1, 1, self.n_actions)\n",
        "        rewards_chosen = (spice_signals.actions * spice_signals.rewards).sum(dim=-1, keepdim=True).repeat(1, 1, self.n_actions)\n",
        "        \n",
        "        # time-invariant participant features\n",
        "        participant_embeddings = self.participant_embedding(spice_signals.participant_ids)\n",
        "        \n",
        "        for timestep in spice_signals.timesteps:\n",
        "            \n",
        "            # update chosen value\n",
        "            self.call_module(\n",
        "                key_module='value_reward_chosen',\n",
        "                key_state='value_reward',\n",
        "                action_mask=spice_signals.actions[timestep],\n",
        "                inputs=(contr_diffs[timestep], rewards_chosen[timestep]),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "            )\n",
        "            \n",
        "            # update not chosen value\n",
        "            self.call_module(\n",
        "                key_module='value_reward_not_chosen',\n",
        "                key_state='value_reward',\n",
        "                action_mask=1-spice_signals.actions[timestep],\n",
        "                inputs=(contr_diffs[timestep]),  # add input rewards_chosen[timestep] for counterfactual updating (adjust in config as well)\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "            )\n",
        "            \n",
        "            # same for choice values\n",
        "            self.call_module(\n",
        "                key_module='value_choice',\n",
        "                key_state='value_choice',\n",
        "                action_mask=spice_signals.actions[timestep],\n",
        "                inputs=(contr_diffs[timestep], spice_signals.actions[timestep]),\n",
        "                participant_index=spice_signals.participant_ids,\n",
        "                participant_embedding=participant_embeddings,\n",
        "            )\n",
        "            \n",
        "            spice_signals.logits[timestep] = self.state['value_reward'] + self.state['value_choice']\n",
        "            \n",
        "        spice_signals = self.post_forward_pass(spice_signals, batch_first)\n",
        "        \n",
        "        return spice_signals.logits, self.get_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's setup now the `SpiceEstimator` object and fit it to the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_spice = '../params/ganesh2024a/spice_ganesh2024a.pkl'\n",
        "\n",
        "estimator = SpiceEstimator(\n",
        "        # model paramaeters\n",
        "        rnn_class=SPICERNN,\n",
        "        spice_config=spice_config,\n",
        "        n_actions=2,\n",
        "        n_participants=n_participants,\n",
        "        \n",
        "        # rnn training parameters\n",
        "        epochs=1000,\n",
        "        warmup_steps=200,\n",
        "        learning_rate=0.01,\n",
        "        \n",
        "        # sindy fitting parameters\n",
        "        sindy_weight=0.1,\n",
        "        sindy_pruning_threshold=0.05,\n",
        "        sindy_pruning_frequency=1,\n",
        "        sindy_pruning_terms=1,\n",
        "        sindy_pruning_patience=100,\n",
        "        sindy_epochs=1000,\n",
        "        sindy_l2_lambda=0.0001,\n",
        "        sindy_library_polynomial_degree=2,\n",
        "        sindy_ensemble_size=1,\n",
        "        \n",
        "        # additional generalization parameters\n",
        "        batch_size=1024,\n",
        "        bagging=True,\n",
        "        scheduler=True,\n",
        "        \n",
        "        verbose=True,\n",
        "        save_path_spice=path_spice,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "3EnmDiUMWq6e",
        "outputId": "e53b1bbd-4173-4d2c-bcdc-15832bc31bd7"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nStarting training on {estimator.device}...\")\n",
        "print(\"=\" * 80)\n",
        "estimator.fit(dataset.xs, dataset.ys, dataset.xs, dataset.ys)\n",
        "# estimator.load_spice(args.model)\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Print example SPICE model for first participant\n",
        "print(\"\\nExample SPICE model (participant 0):\")\n",
        "print(\"-\" * 80)\n",
        "estimator.print_spice_model(participant_id=0)\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "estimator.load_spice(path_spice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRU for benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('../..')\n",
        "from weinhardt2025.benchmarking.benchmarking_gru import GRU, training, setup_agent_gru\n",
        "\n",
        "path_gru = '../../weinhardt2025/params/ganesh2024a/gru_ganesh2024a.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10000: L(Train): 0.689823567867279; L(Test): 0.6534026861190796\n",
            "Epoch 2/10000: L(Train): 0.6544772982597351; L(Test): 0.6174305081367493\n",
            "Epoch 3/10000: L(Train): 0.6187154650688171; L(Test): 0.5790481567382812\n",
            "Epoch 4/10000: L(Train): 0.5809915661811829; L(Test): 0.5414983034133911\n",
            "Epoch 5/10000: L(Train): 0.5447678565979004; L(Test): 0.5073610544204712\n",
            "Epoch 6/10000: L(Train): 0.5105915665626526; L(Test): 0.47670066356658936\n",
            "Epoch 7/10000: L(Train): 0.4810183644294739; L(Test): 0.45386821031570435\n",
            "Epoch 8/10000: L(Train): 0.4579528570175171; L(Test): 0.4428096413612366\n",
            "Epoch 9/10000: L(Train): 0.44717276096343994; L(Test): 0.44129398465156555\n",
            "Epoch 10/10000: L(Train): 0.44429340958595276; L(Test): 0.44515877962112427\n",
            "Epoch 11/10000: L(Train): 0.4482787847518921; L(Test): 0.4515182673931122\n",
            "Epoch 12/10000: L(Train): 0.4546673595905304; L(Test): 0.45708611607551575\n",
            "Epoch 13/10000: L(Train): 0.45963090658187866; L(Test): 0.45915669202804565\n",
            "Epoch 14/10000: L(Train): 0.46249815821647644; L(Test): 0.45741936564445496\n",
            "Epoch 15/10000: L(Train): 0.4606766402721405; L(Test): 0.45321187376976013\n",
            "Epoch 16/10000: L(Train): 0.45683008432388306; L(Test): 0.4479069709777832\n",
            "Epoch 17/10000: L(Train): 0.4492292106151581; L(Test): 0.44237685203552246\n",
            "Epoch 18/10000: L(Train): 0.445979505777359; L(Test): 0.43726107478141785\n",
            "Epoch 19/10000: L(Train): 0.44056233763694763; L(Test): 0.433254212141037\n",
            "Epoch 20/10000: L(Train): 0.4360989034175873; L(Test): 0.430866003036499\n",
            "Epoch 21/10000: L(Train): 0.43454796075820923; L(Test): 0.43008872866630554\n",
            "Epoch 22/10000: L(Train): 0.4348967671394348; L(Test): 0.4303857982158661\n",
            "Epoch 23/10000: L(Train): 0.43409472703933716; L(Test): 0.43105757236480713\n",
            "Epoch 24/10000: L(Train): 0.4354858696460724; L(Test): 0.431581050157547\n",
            "Epoch 25/10000: L(Train): 0.4359937012195587; L(Test): 0.43162232637405396\n",
            "Epoch 26/10000: L(Train): 0.4360108971595764; L(Test): 0.4309709668159485\n",
            "Epoch 27/10000: L(Train): 0.43503302335739136; L(Test): 0.4296141266822815\n",
            "Epoch 28/10000: L(Train): 0.4337638318538666; L(Test): 0.4277181029319763\n",
            "Epoch 29/10000: L(Train): 0.4321925938129425; L(Test): 0.4256114959716797\n",
            "Epoch 30/10000: L(Train): 0.4288569390773773; L(Test): 0.4236410856246948\n",
            "Epoch 31/10000: L(Train): 0.4268842041492462; L(Test): 0.42204999923706055\n",
            "Epoch 32/10000: L(Train): 0.4248008131980896; L(Test): 0.42093580961227417\n",
            "Epoch 33/10000: L(Train): 0.42442238330841064; L(Test): 0.4202607274055481\n",
            "Epoch 34/10000: L(Train): 0.4240039885044098; L(Test): 0.41993448138237\n",
            "Epoch 35/10000: L(Train): 0.4245629906654358; L(Test): 0.41982129216194153\n",
            "Epoch 36/10000: L(Train): 0.42367345094680786; L(Test): 0.41975387930870056\n",
            "Epoch 37/10000: L(Train): 0.42299970984458923; L(Test): 0.41958287358283997\n",
            "Epoch 38/10000: L(Train): 0.42300859093666077; L(Test): 0.41921669244766235\n",
            "Epoch 39/10000: L(Train): 0.4229820966720581; L(Test): 0.41864848136901855\n",
            "Epoch 40/10000: L(Train): 0.4214973747730255; L(Test): 0.4179457724094391\n",
            "Epoch 41/10000: L(Train): 0.4225967526435852; L(Test): 0.4172012507915497\n",
            "Epoch 42/10000: L(Train): 0.4216570556163788; L(Test): 0.41652384400367737\n",
            "Epoch 43/10000: L(Train): 0.4198254644870758; L(Test): 0.41599881649017334\n",
            "Epoch 44/10000: L(Train): 0.42001622915267944; L(Test): 0.41566064953804016\n",
            "Epoch 45/10000: L(Train): 0.41937872767448425; L(Test): 0.4155024290084839\n",
            "Epoch 46/10000: L(Train): 0.4188055992126465; L(Test): 0.41546329855918884\n",
            "Epoch 47/10000: L(Train): 0.4188539683818817; L(Test): 0.4154468774795532\n",
            "Epoch 48/10000: L(Train): 0.419680118560791; L(Test): 0.41536229848861694\n",
            "Epoch 49/10000: L(Train): 0.4185224175453186; L(Test): 0.41516244411468506\n",
            "Epoch 50/10000: L(Train): 0.4188229739665985; L(Test): 0.41487181186676025\n",
            "Epoch 51/10000: L(Train): 0.4187013804912567; L(Test): 0.414545476436615\n",
            "Epoch 52/10000: L(Train): 0.4184724688529968; L(Test): 0.4142599403858185\n",
            "Epoch 53/10000: L(Train): 0.4181078374385834; L(Test): 0.4140528440475464\n",
            "Epoch 54/10000: L(Train): 0.4173505902290344; L(Test): 0.41392233967781067\n",
            "Epoch 55/10000: L(Train): 0.4171893298625946; L(Test): 0.4138445258140564\n",
            "Epoch 56/10000: L(Train): 0.417284220457077; L(Test): 0.4137844443321228\n",
            "Epoch 57/10000: L(Train): 0.4173378050327301; L(Test): 0.413707971572876\n",
            "Epoch 58/10000: L(Train): 0.41730543971061707; L(Test): 0.413589745759964\n",
            "Epoch 59/10000: L(Train): 0.4177500009536743; L(Test): 0.41342252492904663\n",
            "Epoch 60/10000: L(Train): 0.4178048074245453; L(Test): 0.41321179270744324\n",
            "Epoch 61/10000: L(Train): 0.4162939786911011; L(Test): 0.41298866271972656\n",
            "Epoch 62/10000: L(Train): 0.4161697328090668; L(Test): 0.4127865135669708\n",
            "Epoch 63/10000: L(Train): 0.41607263684272766; L(Test): 0.41262802481651306\n",
            "Epoch 64/10000: L(Train): 0.41551515460014343; L(Test): 0.41250693798065186\n",
            "Epoch 65/10000: L(Train): 0.416532039642334; L(Test): 0.41240182518959045\n",
            "Epoch 66/10000: L(Train): 0.415676474571228; L(Test): 0.412286639213562\n",
            "Epoch 67/10000: L(Train): 0.41578409075737; L(Test): 0.4121459424495697\n",
            "Epoch 68/10000: L(Train): 0.41658487915992737; L(Test): 0.41198650002479553\n",
            "Epoch 69/10000: L(Train): 0.4148414731025696; L(Test): 0.4118116796016693\n",
            "Epoch 70/10000: L(Train): 0.4143855571746826; L(Test): 0.41163650155067444\n",
            "Epoch 71/10000: L(Train): 0.41440296173095703; L(Test): 0.4114777147769928\n",
            "Epoch 72/10000: L(Train): 0.4147583246231079; L(Test): 0.41134321689605713\n",
            "Epoch 73/10000: L(Train): 0.4147375822067261; L(Test): 0.41121476888656616\n",
            "Epoch 74/10000: L(Train): 0.4141482710838318; L(Test): 0.4110855758190155\n",
            "Epoch 75/10000: L(Train): 0.4146009385585785; L(Test): 0.41094353795051575\n",
            "Epoch 76/10000: L(Train): 0.414562851190567; L(Test): 0.41079050302505493\n",
            "Epoch 77/10000: L(Train): 0.4140767455101013; L(Test): 0.4106261730194092\n",
            "Epoch 78/10000: L(Train): 0.41382288932800293; L(Test): 0.4104612469673157\n",
            "Epoch 79/10000: L(Train): 0.41463547945022583; L(Test): 0.41030004620552063\n",
            "Epoch 80/10000: L(Train): 0.4136801064014435; L(Test): 0.4101465940475464\n",
            "Epoch 81/10000: L(Train): 0.41387811303138733; L(Test): 0.4099990129470825\n",
            "Epoch 82/10000: L(Train): 0.41320106387138367; L(Test): 0.4098515808582306\n",
            "Epoch 83/10000: L(Train): 0.41250523924827576; L(Test): 0.4097042977809906\n",
            "Epoch 84/10000: L(Train): 0.41367506980895996; L(Test): 0.40955454111099243\n",
            "Epoch 85/10000: L(Train): 0.41196396946907043; L(Test): 0.40939801931381226\n",
            "Epoch 86/10000: L(Train): 0.41216668486595154; L(Test): 0.4092438220977783\n",
            "Epoch 87/10000: L(Train): 0.4124584496021271; L(Test): 0.4090949594974518\n",
            "Epoch 88/10000: L(Train): 0.4135534465312958; L(Test): 0.40895184874534607\n",
            "Epoch 89/10000: L(Train): 0.41233590245246887; L(Test): 0.40880632400512695\n",
            "Epoch 90/10000: L(Train): 0.41148608922958374; L(Test): 0.40865349769592285\n",
            "Epoch 91/10000: L(Train): 0.41111600399017334; L(Test): 0.4084934592247009\n",
            "Epoch 92/10000: L(Train): 0.4127551019191742; L(Test): 0.4083319306373596\n",
            "Epoch 93/10000: L(Train): 0.4114265441894531; L(Test): 0.4081708490848541\n",
            "Epoch 94/10000: L(Train): 0.41189104318618774; L(Test): 0.40801265835762024\n",
            "Epoch 95/10000: L(Train): 0.4115607738494873; L(Test): 0.40785545110702515\n",
            "Epoch 96/10000: L(Train): 0.41186636686325073; L(Test): 0.40769293904304504\n",
            "Epoch 97/10000: L(Train): 0.4105529487133026; L(Test): 0.4075258672237396\n",
            "Epoch 98/10000: L(Train): 0.41095981001853943; L(Test): 0.4073581099510193\n",
            "Epoch 99/10000: L(Train): 0.41086122393608093; L(Test): 0.4071892201900482\n",
            "Epoch 100/10000: L(Train): 0.4105015993118286; L(Test): 0.4070183336734772\n",
            "Epoch 101/10000: L(Train): 0.41131725907325745; L(Test): 0.4068499505519867\n",
            "Epoch 102/10000: L(Train): 0.4096109867095947; L(Test): 0.4066828489303589\n",
            "Epoch 103/10000: L(Train): 0.41002699732780457; L(Test): 0.4065121114253998\n",
            "Epoch 104/10000: L(Train): 0.41054779291152954; L(Test): 0.4063398241996765\n",
            "Epoch 105/10000: L(Train): 0.4102153182029724; L(Test): 0.40616244077682495\n",
            "Epoch 106/10000: L(Train): 0.4095700979232788; L(Test): 0.40598437190055847\n",
            "Epoch 107/10000: L(Train): 0.4086277484893799; L(Test): 0.4058065116405487\n",
            "Epoch 108/10000: L(Train): 0.4088332951068878; L(Test): 0.40562912821769714\n",
            "Epoch 109/10000: L(Train): 0.4090425372123718; L(Test): 0.4054528772830963\n",
            "Epoch 110/10000: L(Train): 0.40905797481536865; L(Test): 0.4052796959877014\n",
            "Epoch 111/10000: L(Train): 0.40890607237815857; L(Test): 0.40510648488998413\n",
            "Epoch 112/10000: L(Train): 0.40846964716911316; L(Test): 0.4049282670021057\n",
            "Epoch 113/10000: L(Train): 0.40859150886535645; L(Test): 0.40476471185684204\n",
            "Epoch 114/10000: L(Train): 0.4082544445991516; L(Test): 0.40461018681526184\n",
            "Epoch 115/10000: L(Train): 0.4080697298049927; L(Test): 0.40446189045906067\n",
            "Epoch 116/10000: L(Train): 0.40768295526504517; L(Test): 0.40429165959358215\n",
            "Epoch 117/10000: L(Train): 0.4075164496898651; L(Test): 0.40410691499710083\n",
            "Epoch 118/10000: L(Train): 0.40732526779174805; L(Test): 0.4039219319820404\n",
            "Epoch 119/10000: L(Train): 0.40857377648353577; L(Test): 0.4037579298019409\n",
            "Epoch 120/10000: L(Train): 0.40788599848747253; L(Test): 0.4036041498184204\n",
            "Epoch 121/10000: L(Train): 0.40642309188842773; L(Test): 0.40345335006713867\n",
            "Epoch 122/10000: L(Train): 0.406568706035614; L(Test): 0.40330010652542114\n",
            "Epoch 123/10000: L(Train): 0.4067888557910919; L(Test): 0.403145432472229\n",
            "Epoch 124/10000: L(Train): 0.406416118144989; L(Test): 0.40300264954566956\n",
            "Epoch 125/10000: L(Train): 0.405915766954422; L(Test): 0.40286847949028015\n",
            "Epoch 126/10000: L(Train): 0.40726739168167114; L(Test): 0.40273579955101013\n",
            "Epoch 127/10000: L(Train): 0.4071290194988251; L(Test): 0.40259620547294617\n",
            "Epoch 128/10000: L(Train): 0.40684187412261963; L(Test): 0.40244409441947937\n",
            "Epoch 129/10000: L(Train): 0.40581777691841125; L(Test): 0.40229302644729614\n",
            "Epoch 130/10000: L(Train): 0.40568992495536804; L(Test): 0.4021623134613037\n",
            "Epoch 131/10000: L(Train): 0.4052184820175171; L(Test): 0.4020328223705292\n",
            "Epoch 132/10000: L(Train): 0.4054090976715088; L(Test): 0.40191972255706787\n",
            "Epoch 133/10000: L(Train): 0.4051109850406647; L(Test): 0.4018116593360901\n",
            "Epoch 134/10000: L(Train): 0.4059378206729889; L(Test): 0.40166202187538147\n",
            "Epoch 135/10000: L(Train): 0.4058109223842621; L(Test): 0.40153399109840393\n",
            "Epoch 136/10000: L(Train): 0.4056171178817749; L(Test): 0.4014206826686859\n",
            "Epoch 137/10000: L(Train): 0.40490391850471497; L(Test): 0.4012804925441742\n",
            "Epoch 138/10000: L(Train): 0.4050058126449585; L(Test): 0.40116816759109497\n",
            "Epoch 139/10000: L(Train): 0.4047148823738098; L(Test): 0.40107840299606323\n",
            "Epoch 140/10000: L(Train): 0.40453603863716125; L(Test): 0.4009280800819397\n",
            "Epoch 141/10000: L(Train): 0.404695987701416; L(Test): 0.40079450607299805\n",
            "Epoch 142/10000: L(Train): 0.4052894711494446; L(Test): 0.4006882309913635\n",
            "Epoch 143/10000: L(Train): 0.40361401438713074; L(Test): 0.40058356523513794\n",
            "Epoch 144/10000: L(Train): 0.4037415683269501; L(Test): 0.4004942774772644\n",
            "Epoch 145/10000: L(Train): 0.40451911091804504; L(Test): 0.4004129469394684\n",
            "Epoch 146/10000: L(Train): 0.40533438324928284; L(Test): 0.4003085196018219\n",
            "Epoch 147/10000: L(Train): 0.4044126272201538; L(Test): 0.4001917541027069\n",
            "Epoch 148/10000: L(Train): 0.404419869184494; L(Test): 0.4000903367996216\n",
            "Epoch 149/10000: L(Train): 0.4041963219642639; L(Test): 0.3999863862991333\n",
            "Epoch 150/10000: L(Train): 0.404257208108902; L(Test): 0.399894654750824\n",
            "Epoch 151/10000: L(Train): 0.40372347831726074; L(Test): 0.3998252749443054\n",
            "Epoch 152/10000: L(Train): 0.40417832136154175; L(Test): 0.3997381031513214\n",
            "Epoch 153/10000: L(Train): 0.40337610244750977; L(Test): 0.39966142177581787\n",
            "Epoch 154/10000: L(Train): 0.4039716422557831; L(Test): 0.3995843529701233\n",
            "Epoch 155/10000: L(Train): 0.4033317267894745; L(Test): 0.399504691362381\n",
            "Epoch 156/10000: L(Train): 0.4035753607749939; L(Test): 0.39944472908973694\n",
            "Epoch 157/10000: L(Train): 0.40411660075187683; L(Test): 0.39937642216682434\n",
            "Epoch 158/10000: L(Train): 0.4033278822898865; L(Test): 0.3992621898651123\n",
            "Epoch 159/10000: L(Train): 0.4030543267726898; L(Test): 0.3991955816745758\n",
            "Epoch 160/10000: L(Train): 0.40310168266296387; L(Test): 0.39911967515945435\n",
            "Epoch 161/10000: L(Train): 0.40363776683807373; L(Test): 0.3990411162376404\n",
            "Epoch 162/10000: L(Train): 0.40288254618644714; L(Test): 0.39901003241539\n",
            "Epoch 163/10000: L(Train): 0.4027639329433441; L(Test): 0.39893004298210144\n",
            "Epoch 164/10000: L(Train): 0.40233251452445984; L(Test): 0.39887893199920654\n",
            "Epoch 165/10000: L(Train): 0.403372198343277; L(Test): 0.39883315563201904\n",
            "Epoch 166/10000: L(Train): 0.40299922227859497; L(Test): 0.39873746037483215\n",
            "Epoch 167/10000: L(Train): 0.40160995721817017; L(Test): 0.3986755609512329\n",
            "Epoch 168/10000: L(Train): 0.4022461473941803; L(Test): 0.398613303899765\n",
            "Epoch 169/10000: L(Train): 0.40258553624153137; L(Test): 0.39852631092071533\n",
            "Epoch 170/10000: L(Train): 0.4016045033931732; L(Test): 0.3984600007534027\n",
            "Epoch 171/10000: L(Train): 0.4017091989517212; L(Test): 0.3984009325504303\n",
            "Epoch 172/10000: L(Train): 0.40201476216316223; L(Test): 0.39833417534828186\n",
            "Epoch 173/10000: L(Train): 0.4022946059703827; L(Test): 0.39828234910964966\n",
            "Epoch 174/10000: L(Train): 0.40156713128089905; L(Test): 0.3982482850551605\n",
            "Epoch 175/10000: L(Train): 0.4026840329170227; L(Test): 0.39821183681488037\n",
            "Epoch 176/10000: L(Train): 0.4025571942329407; L(Test): 0.3981527090072632\n",
            "Epoch 177/10000: L(Train): 0.4023202955722809; L(Test): 0.3980848789215088\n",
            "Epoch 178/10000: L(Train): 0.40176934003829956; L(Test): 0.39802342653274536\n",
            "Epoch 179/10000: L(Train): 0.4016401171684265; L(Test): 0.39797621965408325\n",
            "Epoch 180/10000: L(Train): 0.4018493592739105; L(Test): 0.39791983366012573\n",
            "Epoch 181/10000: L(Train): 0.40157991647720337; L(Test): 0.3978714048862457\n",
            "Epoch 182/10000: L(Train): 0.4017752707004547; L(Test): 0.3978133797645569\n",
            "Epoch 183/10000: L(Train): 0.4012739062309265; L(Test): 0.39775583148002625\n",
            "Epoch 184/10000: L(Train): 0.4017590582370758; L(Test): 0.397706001996994\n",
            "Epoch 185/10000: L(Train): 0.4022567868232727; L(Test): 0.3976512849330902\n",
            "Epoch 186/10000: L(Train): 0.40167713165283203; L(Test): 0.3976041078567505\n",
            "Epoch 187/10000: L(Train): 0.40165475010871887; L(Test): 0.39755532145500183\n",
            "Epoch 188/10000: L(Train): 0.4010106027126312; L(Test): 0.39751288294792175\n",
            "Epoch 189/10000: L(Train): 0.4011163115501404; L(Test): 0.3974732458591461\n",
            "Epoch 190/10000: L(Train): 0.40140077471733093; L(Test): 0.3974302113056183\n",
            "Epoch 191/10000: L(Train): 0.4009026885032654; L(Test): 0.39739519357681274\n",
            "Epoch 192/10000: L(Train): 0.4012678563594818; L(Test): 0.3973519802093506\n",
            "Epoch 193/10000: L(Train): 0.4013783931732178; L(Test): 0.39729639887809753\n",
            "Epoch 194/10000: L(Train): 0.40112191438674927; L(Test): 0.39727404713630676\n",
            "Epoch 195/10000: L(Train): 0.4018309414386749; L(Test): 0.39721164107322693\n",
            "Epoch 196/10000: L(Train): 0.4010927677154541; L(Test): 0.39718323945999146\n",
            "Epoch 197/10000: L(Train): 0.4013376832008362; L(Test): 0.3971702456474304\n",
            "Epoch 198/10000: L(Train): 0.40162137150764465; L(Test): 0.39713144302368164\n",
            "Epoch 199/10000: L(Train): 0.40042465925216675; L(Test): 0.39710676670074463\n",
            "Epoch 200/10000: L(Train): 0.4008234441280365; L(Test): 0.3970717489719391\n",
            "Epoch 201/10000: L(Train): 0.4008648991584778; L(Test): 0.3969913721084595\n",
            "Epoch 202/10000: L(Train): 0.40087538957595825; L(Test): 0.3970157206058502\n",
            "Epoch 203/10000: L(Train): 0.40059658885002136; L(Test): 0.3969445526599884\n",
            "Epoch 204/10000: L(Train): 0.4006011486053467; L(Test): 0.3969060480594635\n",
            "Epoch 205/10000: L(Train): 0.4007245600223541; L(Test): 0.396880179643631\n",
            "Epoch 206/10000: L(Train): 0.400532066822052; L(Test): 0.39682671427726746\n",
            "Epoch 207/10000: L(Train): 0.40127941966056824; L(Test): 0.3968254625797272\n",
            "Epoch 208/10000: L(Train): 0.4015529453754425; L(Test): 0.3967268466949463\n",
            "Epoch 209/10000: L(Train): 0.40143343806266785; L(Test): 0.39668944478034973\n",
            "Epoch 210/10000: L(Train): 0.4002450704574585; L(Test): 0.39666640758514404\n",
            "Epoch 211/10000: L(Train): 0.400840699672699; L(Test): 0.39659833908081055\n",
            "Epoch 212/10000: L(Train): 0.40126845240592957; L(Test): 0.39655256271362305\n",
            "Epoch 213/10000: L(Train): 0.4005093276500702; L(Test): 0.3965238630771637\n",
            "Epoch 214/10000: L(Train): 0.4007742702960968; L(Test): 0.39650702476501465\n",
            "Epoch 215/10000: L(Train): 0.4002695679664612; L(Test): 0.39647573232650757\n",
            "Epoch 216/10000: L(Train): 0.39980146288871765; L(Test): 0.3964284658432007\n",
            "Epoch 217/10000: L(Train): 0.3993011713027954; L(Test): 0.3963834345340729\n",
            "Epoch 218/10000: L(Train): 0.4002319574356079; L(Test): 0.39632517099380493\n",
            "Epoch 219/10000: L(Train): 0.39896687865257263; L(Test): 0.3962746262550354\n",
            "Epoch 220/10000: L(Train): 0.400073379278183; L(Test): 0.39625078439712524\n",
            "Epoch 221/10000: L(Train): 0.4006071090698242; L(Test): 0.39621517062187195\n",
            "Epoch 222/10000: L(Train): 0.40008091926574707; L(Test): 0.39621230959892273\n",
            "Epoch 223/10000: L(Train): 0.4003225862979889; L(Test): 0.3961199223995209\n",
            "Epoch 224/10000: L(Train): 0.4002682566642761; L(Test): 0.39611566066741943\n",
            "Epoch 225/10000: L(Train): 0.3994572162628174; L(Test): 0.39608240127563477\n",
            "Epoch 226/10000: L(Train): 0.3996480107307434; L(Test): 0.3960423171520233\n",
            "Epoch 227/10000: L(Train): 0.3997708857059479; L(Test): 0.3960232436656952\n",
            "Epoch 228/10000: L(Train): 0.399850994348526; L(Test): 0.3959214389324188\n",
            "Epoch 229/10000: L(Train): 0.39959999918937683; L(Test): 0.39594748616218567\n",
            "Epoch 230/10000: L(Train): 0.4004555940628052; L(Test): 0.3958403468132019\n",
            "Epoch 231/10000: L(Train): 0.39959099888801575; L(Test): 0.3957897424697876\n",
            "Epoch 232/10000: L(Train): 0.39943236112594604; L(Test): 0.3957522213459015\n",
            "Epoch 233/10000: L(Train): 0.40020495653152466; L(Test): 0.39569371938705444\n",
            "Epoch 234/10000: L(Train): 0.39926350116729736; L(Test): 0.3956792652606964\n",
            "Epoch 235/10000: L(Train): 0.4000738561153412; L(Test): 0.3956184387207031\n",
            "Epoch 236/10000: L(Train): 0.39950641989707947; L(Test): 0.39560022950172424\n",
            "Epoch 237/10000: L(Train): 0.3999526798725128; L(Test): 0.3955407738685608\n",
            "Epoch 238/10000: L(Train): 0.39900296926498413; L(Test): 0.3954905867576599\n",
            "Epoch 239/10000: L(Train): 0.3987256586551666; L(Test): 0.3954589366912842\n",
            "Epoch 240/10000: L(Train): 0.39925774931907654; L(Test): 0.3954312205314636\n",
            "Epoch 241/10000: L(Train): 0.39956551790237427; L(Test): 0.3953947126865387\n",
            "Epoch 242/10000: L(Train): 0.3985058665275574; L(Test): 0.39540353417396545\n",
            "Epoch 243/10000: L(Train): 0.39890003204345703; L(Test): 0.395329087972641\n",
            "Epoch 244/10000: L(Train): 0.39827027916908264; L(Test): 0.3953220844268799\n",
            "Epoch 245/10000: L(Train): 0.399969220161438; L(Test): 0.39523983001708984\n",
            "Epoch 246/10000: L(Train): 0.3994542956352234; L(Test): 0.3951915204524994\n",
            "Epoch 247/10000: L(Train): 0.39923715591430664; L(Test): 0.3951495289802551\n",
            "Epoch 248/10000: L(Train): 0.3993985652923584; L(Test): 0.3951092064380646\n",
            "Epoch 249/10000: L(Train): 0.39832937717437744; L(Test): 0.3950934112071991\n",
            "Epoch 250/10000: L(Train): 0.39881545305252075; L(Test): 0.39502811431884766\n",
            "Epoch 251/10000: L(Train): 0.3985278010368347; L(Test): 0.39502787590026855\n",
            "Epoch 252/10000: L(Train): 0.39887747168540955; L(Test): 0.3949657380580902\n",
            "Epoch 253/10000: L(Train): 0.39899277687072754; L(Test): 0.394940048456192\n",
            "Epoch 254/10000: L(Train): 0.3995649516582489; L(Test): 0.3948996961116791\n",
            "Epoch 255/10000: L(Train): 0.39793747663497925; L(Test): 0.39486995339393616\n",
            "Epoch 256/10000: L(Train): 0.39867478609085083; L(Test): 0.3948715031147003\n",
            "Epoch 257/10000: L(Train): 0.39923810958862305; L(Test): 0.3948191702365875\n",
            "Epoch 258/10000: L(Train): 0.39813825488090515; L(Test): 0.39482975006103516\n",
            "Epoch 259/10000: L(Train): 0.3989847004413605; L(Test): 0.3947579860687256\n",
            "Epoch 260/10000: L(Train): 0.3992266356945038; L(Test): 0.3947507441043854\n",
            "Epoch 261/10000: L(Train): 0.3988608717918396; L(Test): 0.394755482673645\n",
            "Epoch 262/10000: L(Train): 0.3979344666004181; L(Test): 0.3946705162525177\n",
            "Epoch 263/10000: L(Train): 0.3978346884250641; L(Test): 0.39461633563041687\n",
            "Epoch 264/10000: L(Train): 0.3989381194114685; L(Test): 0.3945446014404297\n",
            "Epoch 265/10000: L(Train): 0.39804205298423767; L(Test): 0.3945556879043579\n",
            "Epoch 266/10000: L(Train): 0.3984072804450989; L(Test): 0.3945152163505554\n",
            "Epoch 267/10000: L(Train): 0.39819571375846863; L(Test): 0.39449432492256165\n",
            "Epoch 268/10000: L(Train): 0.39855414628982544; L(Test): 0.39442452788352966\n",
            "Epoch 269/10000: L(Train): 0.3982597887516022; L(Test): 0.39442527294158936\n",
            "Epoch 270/10000: L(Train): 0.3978883624076843; L(Test): 0.39439254999160767\n",
            "Epoch 271/10000: L(Train): 0.39751729369163513; L(Test): 0.3943275809288025\n",
            "Epoch 272/10000: L(Train): 0.39788180589675903; L(Test): 0.3942999839782715\n",
            "Epoch 273/10000: L(Train): 0.39831098914146423; L(Test): 0.39422816038131714\n",
            "Epoch 274/10000: L(Train): 0.39772334694862366; L(Test): 0.39428460597991943\n",
            "Epoch 275/10000: L(Train): 0.3981992304325104; L(Test): 0.39419397711753845\n",
            "Epoch 276/10000: L(Train): 0.3987877666950226; L(Test): 0.3942249119281769\n",
            "Epoch 277/10000: L(Train): 0.39782771468162537; L(Test): 0.39418327808380127\n",
            "Epoch 278/10000: L(Train): 0.3990914225578308; L(Test): 0.3942663371562958\n",
            "Epoch 279/10000: L(Train): 0.3975767195224762; L(Test): 0.3941061198711395\n",
            "Epoch 280/10000: L(Train): 0.3977585732936859; L(Test): 0.39400044083595276\n",
            "Epoch 281/10000: L(Train): 0.3978606164455414; L(Test): 0.39397668838500977\n",
            "Epoch 282/10000: L(Train): 0.3973936438560486; L(Test): 0.3938907980918884\n",
            "Epoch 283/10000: L(Train): 0.3977794051170349; L(Test): 0.39392319321632385\n",
            "Epoch 284/10000: L(Train): 0.39837950468063354; L(Test): 0.3938601016998291\n",
            "Epoch 285/10000: L(Train): 0.3977583646774292; L(Test): 0.3938661217689514\n",
            "Epoch 286/10000: L(Train): 0.3974106013774872; L(Test): 0.3937881290912628\n",
            "Epoch 287/10000: L(Train): 0.3972795307636261; L(Test): 0.39379560947418213\n",
            "Epoch 288/10000: L(Train): 0.39865872263908386; L(Test): 0.3936794102191925\n",
            "Epoch 289/10000: L(Train): 0.3979407548904419; L(Test): 0.393657386302948\n",
            "Epoch 290/10000: L(Train): 0.39870306849479675; L(Test): 0.3936249911785126\n",
            "Epoch 291/10000: L(Train): 0.3973255157470703; L(Test): 0.39356938004493713\n",
            "Epoch 292/10000: L(Train): 0.39758017659187317; L(Test): 0.3935568928718567\n",
            "Epoch 293/10000: L(Train): 0.397832453250885; L(Test): 0.39346766471862793\n",
            "Epoch 294/10000: L(Train): 0.3979887366294861; L(Test): 0.39348652958869934\n",
            "Epoch 295/10000: L(Train): 0.39723339676856995; L(Test): 0.3934454321861267\n",
            "Epoch 296/10000: L(Train): 0.39684975147247314; L(Test): 0.3933466970920563\n",
            "Epoch 297/10000: L(Train): 0.3975391983985901; L(Test): 0.39329293370246887\n",
            "Epoch 298/10000: L(Train): 0.39743277430534363; L(Test): 0.39327406883239746\n",
            "Epoch 299/10000: L(Train): 0.39748212695121765; L(Test): 0.3932776153087616\n",
            "Epoch 300/10000: L(Train): 0.3972039222717285; L(Test): 0.39323100447654724\n",
            "Epoch 301/10000: L(Train): 0.3975617587566376; L(Test): 0.3931984305381775\n",
            "Epoch 302/10000: L(Train): 0.39711037278175354; L(Test): 0.39315107464790344\n",
            "Epoch 303/10000: L(Train): 0.3968011140823364; L(Test): 0.3931955397129059\n",
            "Epoch 304/10000: L(Train): 0.3975163698196411; L(Test): 0.39301007986068726\n",
            "Epoch 305/10000: L(Train): 0.3980131149291992; L(Test): 0.3930788040161133\n",
            "Epoch 306/10000: L(Train): 0.39718806743621826; L(Test): 0.3929792046546936\n",
            "Epoch 307/10000: L(Train): 0.39740315079689026; L(Test): 0.39292141795158386\n",
            "Epoch 308/10000: L(Train): 0.3966085612773895; L(Test): 0.3929004371166229\n",
            "Epoch 309/10000: L(Train): 0.3971785306930542; L(Test): 0.39290526509284973\n",
            "Epoch 310/10000: L(Train): 0.39748626947402954; L(Test): 0.39294537901878357\n",
            "Epoch 311/10000: L(Train): 0.3975794017314911; L(Test): 0.3928292691707611\n",
            "Epoch 312/10000: L(Train): 0.39622578024864197; L(Test): 0.3927397131919861\n",
            "Epoch 313/10000: L(Train): 0.39631563425064087; L(Test): 0.3926704525947571\n",
            "Epoch 314/10000: L(Train): 0.39683055877685547; L(Test): 0.39256101846694946\n",
            "Epoch 315/10000: L(Train): 0.39575695991516113; L(Test): 0.39255818724632263\n",
            "Epoch 316/10000: L(Train): 0.39690276980400085; L(Test): 0.39253294467926025\n",
            "Epoch 317/10000: L(Train): 0.3971802294254303; L(Test): 0.392485111951828\n",
            "Epoch 318/10000: L(Train): 0.397316038608551; L(Test): 0.39242181181907654\n",
            "Epoch 319/10000: L(Train): 0.39596566557884216; L(Test): 0.3924475312232971\n",
            "Epoch 320/10000: L(Train): 0.3963775038719177; L(Test): 0.39232030510902405\n",
            "Epoch 321/10000: L(Train): 0.39629682898521423; L(Test): 0.3923264145851135\n",
            "Epoch 322/10000: L(Train): 0.3972424566745758; L(Test): 0.3922373652458191\n",
            "Epoch 323/10000: L(Train): 0.39719653129577637; L(Test): 0.3922480046749115\n",
            "Epoch 324/10000: L(Train): 0.3964182436466217; L(Test): 0.3921948969364166\n",
            "Epoch 325/10000: L(Train): 0.396984726190567; L(Test): 0.3921641409397125\n",
            "Epoch 326/10000: L(Train): 0.39662572741508484; L(Test): 0.39218994975090027\n",
            "Epoch 327/10000: L(Train): 0.3962632119655609; L(Test): 0.3921184241771698\n",
            "Epoch 328/10000: L(Train): 0.39665287733078003; L(Test): 0.39200547337532043\n",
            "Epoch 329/10000: L(Train): 0.3959851861000061; L(Test): 0.3919820487499237\n",
            "Epoch 330/10000: L(Train): 0.3956703245639801; L(Test): 0.39201653003692627\n",
            "Epoch 331/10000: L(Train): 0.3962385952472687; L(Test): 0.39190736413002014\n",
            "Epoch 332/10000: L(Train): 0.39668625593185425; L(Test): 0.39185819029808044\n",
            "Epoch 333/10000: L(Train): 0.3957136869430542; L(Test): 0.39189764857292175\n",
            "Epoch 334/10000: L(Train): 0.39584457874298096; L(Test): 0.391849547624588\n",
            "Epoch 335/10000: L(Train): 0.3960198163986206; L(Test): 0.3918488621711731\n",
            "Epoch 336/10000: L(Train): 0.3957303762435913; L(Test): 0.39166632294654846\n",
            "Epoch 337/10000: L(Train): 0.39565157890319824; L(Test): 0.39164671301841736\n",
            "Epoch 338/10000: L(Train): 0.39588841795921326; L(Test): 0.39156851172447205\n",
            "Epoch 339/10000: L(Train): 0.3951457440853119; L(Test): 0.3915209174156189\n",
            "Epoch 340/10000: L(Train): 0.394818514585495; L(Test): 0.3914915919303894\n",
            "Epoch 341/10000: L(Train): 0.39566126465797424; L(Test): 0.3914758861064911\n",
            "Epoch 342/10000: L(Train): 0.39614421129226685; L(Test): 0.3914474844932556\n",
            "Epoch 343/10000: L(Train): 0.39567768573760986; L(Test): 0.3914312720298767\n",
            "Epoch 344/10000: L(Train): 0.39572083950042725; L(Test): 0.39143306016921997\n",
            "Epoch 345/10000: L(Train): 0.39567622542381287; L(Test): 0.3913712203502655\n",
            "Epoch 346/10000: L(Train): 0.3952312767505646; L(Test): 0.3913131356239319\n",
            "Epoch 347/10000: L(Train): 0.3959575891494751; L(Test): 0.3912738561630249\n",
            "Epoch 348/10000: L(Train): 0.3951285779476166; L(Test): 0.39119938015937805\n",
            "Epoch 349/10000: L(Train): 0.3960753381252289; L(Test): 0.39115574955940247\n",
            "Epoch 350/10000: L(Train): 0.3951314389705658; L(Test): 0.39112725853919983\n",
            "Epoch 351/10000: L(Train): 0.39472076296806335; L(Test): 0.39111456274986267\n",
            "Epoch 352/10000: L(Train): 0.39583560824394226; L(Test): 0.39106351137161255\n",
            "Epoch 353/10000: L(Train): 0.39539504051208496; L(Test): 0.3910233974456787\n",
            "Epoch 354/10000: L(Train): 0.3949624001979828; L(Test): 0.3910510241985321\n",
            "Epoch 355/10000: L(Train): 0.39528724551200867; L(Test): 0.39104175567626953\n",
            "Epoch 356/10000: L(Train): 0.39490994811058044; L(Test): 0.39086803793907166\n",
            "Epoch 357/10000: L(Train): 0.3951358497142792; L(Test): 0.3909723162651062\n",
            "Epoch 358/10000: L(Train): 0.3956020176410675; L(Test): 0.39074572920799255\n",
            "Epoch 359/10000: L(Train): 0.39512911438941956; L(Test): 0.39077305793762207\n",
            "Epoch 360/10000: L(Train): 0.39486417174339294; L(Test): 0.39062246680259705\n",
            "Epoch 361/10000: L(Train): 0.3946845233440399; L(Test): 0.39071938395500183\n",
            "Epoch 362/10000: L(Train): 0.395957350730896; L(Test): 0.3905200958251953\n",
            "Epoch 363/10000: L(Train): 0.39504367113113403; L(Test): 0.3906177282333374\n",
            "Epoch 364/10000: L(Train): 0.3953622579574585; L(Test): 0.3905256986618042\n",
            "Epoch 365/10000: L(Train): 0.39448121190071106; L(Test): 0.3906310796737671\n",
            "Epoch 366/10000: L(Train): 0.3944038152694702; L(Test): 0.39050376415252686\n",
            "Epoch 367/10000: L(Train): 0.39497706294059753; L(Test): 0.39046546816825867\n",
            "Epoch 368/10000: L(Train): 0.39474278688430786; L(Test): 0.39056113362312317\n",
            "Epoch 369/10000: L(Train): 0.39437615871429443; L(Test): 0.39043956995010376\n",
            "Epoch 370/10000: L(Train): 0.39464831352233887; L(Test): 0.39049777388572693\n",
            "Epoch 371/10000: L(Train): 0.3941686451435089; L(Test): 0.39045053720474243\n",
            "Epoch 372/10000: L(Train): 0.3945671021938324; L(Test): 0.39058464765548706\n",
            "Epoch 373/10000: L(Train): 0.3956380784511566; L(Test): 0.39035671949386597\n",
            "Epoch 374/10000: L(Train): 0.3946126401424408; L(Test): 0.3903247117996216\n",
            "Epoch 375/10000: L(Train): 0.3949557840824127; L(Test): 0.39050573110580444\n",
            "Epoch 376/10000: L(Train): 0.3951827883720398; L(Test): 0.39022207260131836\n",
            "Epoch 377/10000: L(Train): 0.3947758674621582; L(Test): 0.3901616334915161\n",
            "Epoch 378/10000: L(Train): 0.3957906663417816; L(Test): 0.3900652229785919\n",
            "Epoch 379/10000: L(Train): 0.3940839469432831; L(Test): 0.39014747738838196\n",
            "Epoch 380/10000: L(Train): 0.3952086269855499; L(Test): 0.3900189697742462\n",
            "Epoch 381/10000: L(Train): 0.3950616121292114; L(Test): 0.3900298774242401\n",
            "Epoch 382/10000: L(Train): 0.39507317543029785; L(Test): 0.38999316096305847\n",
            "Epoch 383/10000: L(Train): 0.39466989040374756; L(Test): 0.389945924282074\n",
            "Epoch 384/10000: L(Train): 0.3942636251449585; L(Test): 0.39013537764549255\n",
            "Epoch 385/10000: L(Train): 0.3951365351676941; L(Test): 0.38978660106658936\n",
            "Epoch 386/10000: L(Train): 0.39386290311813354; L(Test): 0.389936625957489\n",
            "Epoch 387/10000: L(Train): 0.39347007870674133; L(Test): 0.38985398411750793\n",
            "Epoch 388/10000: L(Train): 0.39456242322921753; L(Test): 0.38993892073631287\n",
            "Epoch 389/10000: L(Train): 0.3947446942329407; L(Test): 0.3896183669567108\n",
            "Epoch 390/10000: L(Train): 0.3933456540107727; L(Test): 0.38969430327415466\n",
            "Epoch 391/10000: L(Train): 0.39492666721343994; L(Test): 0.3896479904651642\n",
            "Epoch 392/10000: L(Train): 0.39494088292121887; L(Test): 0.38956135511398315\n",
            "Epoch 393/10000: L(Train): 0.3945987820625305; L(Test): 0.38948747515678406\n",
            "Epoch 394/10000: L(Train): 0.3933752775192261; L(Test): 0.38942477107048035\n",
            "Epoch 395/10000: L(Train): 0.39363202452659607; L(Test): 0.38937169313430786\n",
            "Epoch 396/10000: L(Train): 0.39434537291526794; L(Test): 0.38932904601097107\n",
            "Epoch 397/10000: L(Train): 0.39289921522140503; L(Test): 0.38931336998939514\n",
            "Epoch 398/10000: L(Train): 0.3945973813533783; L(Test): 0.38923022150993347\n",
            "Epoch 399/10000: L(Train): 0.3937453627586365; L(Test): 0.38928982615470886\n",
            "Epoch 400/10000: L(Train): 0.39267098903656006; L(Test): 0.3892042934894562\n",
            "Epoch 401/10000: L(Train): 0.39427295327186584; L(Test): 0.38911810517311096\n",
            "Epoch 402/10000: L(Train): 0.3940878212451935; L(Test): 0.3891383707523346\n",
            "Epoch 403/10000: L(Train): 0.3940127193927765; L(Test): 0.3891077935695648\n",
            "Epoch 404/10000: L(Train): 0.39339521527290344; L(Test): 0.38902029395103455\n",
            "Epoch 405/10000: L(Train): 0.393768310546875; L(Test): 0.3889648914337158\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m      5\u001b[39m optimizer = torch.optim.Adam(gru.parameters(), lr=\u001b[32m0.01\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m gru = \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgru\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m torch.save(gru.state_dict(), path_gru)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrained GRU parameters saved to \u001b[39m\u001b[33m\"\u001b[39m + path_gru)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/SPICE/weinhardt2025/aux/../../weinhardt2025/benchmarking/benchmarking_gru.py:77\u001b[39m, in \u001b[36mtraining\u001b[39m\u001b[34m(gru, optimizer, dataset_train, dataset_test, epochs, batch_size, criterion, device)\u001b[39m\n\u001b[32m     68\u001b[39m optimizer.zero_grad()\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# random_indexes = torch.randint(dataset_train.xs.shape[0], (1, dataset_train.xs.shape[0]))[0]\u001b[39;00m\n\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# xs_train = dataset_train.xs[random_indexes].to(device)\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# ys_train = dataset_train.ys[random_indexes].to(device)\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# xs_test = dataset_test.xs.to(device)\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# ys_test = dataset_test.ys.to(device)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spice/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spice/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spice/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/spice/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "epochs = 10000\n",
        "\n",
        "gru = GRU(n_actions=n_actions, additional_inputs=1).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gru.parameters(), lr=0.01)\n",
        "\n",
        "gru = training(\n",
        "    gru=gru,\n",
        "    optimizer=optimizer,\n",
        "    dataset_train=dataset,\n",
        "    dataset_test=dataset,\n",
        "    epochs=epochs,\n",
        "    )\n",
        "\n",
        "torch.save(gru.state_dict(), path_gru)\n",
        "print(\"Trained GRU parameters saved to \" + path_gru)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gru_agent = setup_agent_gru(path_gru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot SPICE against benchmark models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plotting\n",
        "participant_id = 7\n",
        "\n",
        "estimator.print_spice_model(participant_id)\n",
        "\n",
        "agents = {\n",
        "    # add baseline agent here\n",
        "    'rnn': estimator.rnn_agent,\n",
        "    'spice': estimator.spice_agent,\n",
        "    'gru': gru_agent,\n",
        "}\n",
        "\n",
        "fig, axs = plot_session(agents, dataset.xs[participant_id])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "spice",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
