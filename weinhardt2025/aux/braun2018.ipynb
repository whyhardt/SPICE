{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tf7jlYw4NA0v",
    "outputId": "0969ca34-675d-422e-cbfb-7387d9bcd8ad"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/whyhardt/SPICE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oXIbg826NS5i",
    "outputId": "3825864a-cb2d-4ad5-f2e5-79a4e81dfc3e"
   },
   "outputs": [],
   "source": [
    "# !pip install -e SPICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f0uVlABYznR5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from spice import SpiceEstimator, SpiceConfig, convert_dataset, BaseRNN, plot_session, split_data_along_sessiondim\n",
    "\n",
    "# For custom RNN\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data first with the `convert_dataset` method. This method returns a `SpiceDataset` object which we can use right away "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueWarning: Values from choice column (transition) had to be converted from str to int. The mapping is sorted alphabetically: ((0, 'Rep'), (1, 'Switch'))\n",
      "Shape of dataset: torch.Size([870, 1137, 8])\n",
      "Number of participants: 73\n",
      "Number of actions in dataset: 2\n",
      "Number of additional inputs: 1\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "file = '../data/braun2018/braun2018.csv'\n",
    "dataset = convert_dataset(\n",
    "    file = file,\n",
    "    df_participant_id='subject',\n",
    "    df_choice='transition',\n",
    "    df_reward='difference',  # NOTE: please verify: seems like 'pvc' is repeat_reward and 'pvo' is switch_reward; In that case difference = switch_reward - repeat_reward\n",
    "    df_block='block',\n",
    "    additional_inputs=['rt'],\n",
    "    timeshift_additional_inputs=False,\n",
    "    )\n",
    "\n",
    "# structure of dataset:\n",
    "# dataset has two main attributes: xs -> inputs; ys -> targets (next action)\n",
    "# shape: (n_participants*n_blocks*n_experiments, n_timesteps, features)\n",
    "# features are (n_actions * action, n_actions * reward, n_additional_inputs * additional_input, block_number, experiment_id, participant_id)\n",
    "\n",
    "# in order to set up the participant embedding we have to compute the number of unique participants in our data \n",
    "# to get the number of participants n_participants we do:\n",
    "n_participants = len(dataset.xs[..., -1].unique())\n",
    "n_actions = dataset.ys.shape[-1]\n",
    "n_additional_inputs = dataset.xs.shape[-1] - 2*n_actions - 3  # shape[-1] are total number of features - n_actions - reward_per_action (again n_actions) - block number - experiment_id - participant_id -> remaining additional_inputs\n",
    "\n",
    "# timeshift the reward difference signal to be used as an stimulus\n",
    "difference = dataset.xs[..., n_actions:n_actions*2]\n",
    "dataset.xs = torch.concat((\n",
    "    dataset.xs[:, :-1, :n_actions],\n",
    "    difference[:, 1:],\n",
    "    dataset.xs[:, :-1, 2*n_actions:],\n",
    "), dim=-1)\n",
    "\n",
    "# normalize additional inputs\n",
    "additional_inputs = dataset.xs[..., 2*n_actions:-3]\n",
    "mins = additional_inputs.nan_to_num(0).min(dim=0, keepdim=True)[0].min(dim=1, keepdim=True)[0]\n",
    "maxs = additional_inputs.nan_to_num(0).max(dim=0, keepdim=True)[0].max(dim=1, keepdim=True)[0]\n",
    "dataset.xs[..., 2*n_actions:-3] = (additional_inputs - mins) / (maxs - mins)\n",
    "\n",
    "print(f\"Shape of dataset: {dataset.xs.shape}\")\n",
    "print(f\"Number of participants: {n_participants}\")\n",
    "print(f\"Number of actions in dataset: {n_actions}\")\n",
    "print(f\"Number of additional inputs: {dataset.xs.shape[-1]-2*n_actions-3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118.18275862068965 62.09490272247916\n",
      "New max sequence length is 242\n"
     ]
    }
   ],
   "source": [
    "# seems like there are a few blocks that are way longer than the average block\n",
    "# maybe limiting the block length to a given size would help\n",
    "\n",
    "n_trials = np.zeros(dataset.xs.shape[0])\n",
    "for index, block in enumerate(dataset.xs):\n",
    "    n_trials[index] = block.shape[0]-block[:, 0].isnan().sum()\n",
    "    \n",
    "mean, std = n_trials.mean(), n_trials.std()\n",
    "print(mean, std)\n",
    "\n",
    "# would be okay to limit until ca 200 trials per block\n",
    "dataset.xs, dataset.ys = dataset.xs[:, :int(mean+2*std)], dataset.ys[:, :int(mean+1.5*std)]\n",
    "\n",
    "print(f\"New max sequence length is {dataset.xs.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sessions: 652\n",
      "Number of test sessions: 218\n"
     ]
    }
   ],
   "source": [
    "dataset_train, dataset_test = split_data_along_sessiondim(dataset, list_test_sessions=[2, 4, 6])\n",
    "\n",
    "print(f\"Number of training sessions: {dataset_train.xs.shape[0]}\")\n",
    "print(f\"Number of test sessions: {dataset_test.xs.shape[0]}\")\n",
    "\n",
    "dataset_tuple = dataset_train.xs, dataset_train.ys, dataset_test.xs, dataset_test.ys, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   nan, 0.6000],\n",
       "        [0.4000,    nan],\n",
       "        [0.4000,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5500],\n",
       "        [0.5000,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5000],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4500,    nan],\n",
       "        [   nan, 0.5000],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4500,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5500],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4000,    nan],\n",
       "        [   nan, 0.5000],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5500],\n",
       "        [0.5000,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4500,    nan],\n",
       "        [   nan, 0.5000],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4500,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5500],\n",
       "        [   nan, 0.5000],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5000],\n",
       "        [   nan, 0.5500],\n",
       "        [   nan, 0.5000],\n",
       "        [   nan, 0.5500],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5500],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5500],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4000,    nan],\n",
       "        [0.4000,    nan],\n",
       "        [   nan, 0.5000],\n",
       "        [   nan, 0.5000],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4000,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4000,    nan],\n",
       "        [0.4500,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5500],\n",
       "        [   nan, 0.5000],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4500,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5500],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4000,    nan],\n",
       "        [0.4000,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5500],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4000,    nan],\n",
       "        [0.4500,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5500],\n",
       "        [0.5000,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.5000,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5500],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4000,    nan],\n",
       "        [0.4000,    nan],\n",
       "        [   nan, 0.5000],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4000,    nan],\n",
       "        [0.4500,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4000,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5500],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.5000],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4500,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [0.4000,    nan],\n",
       "        [0.4500,    nan],\n",
       "        [0.5000,    nan],\n",
       "        [   nan, 0.6000],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan],\n",
       "        [   nan,    nan]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.xs[0, :, n_actions:2*n_actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPICE Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define the configuration for SPICE with a `SpiceConfig` object.\n",
    "\n",
    "The `SpiceConfig` takes as arguments \n",
    "1. `library_setup (dict)`: Defining the variable names of each module.\n",
    "2. `memory_state (dict)`: Defining the memory state variables and their initial values.\n",
    "3. `states_in_logit (list)`: Defining which of the memory state variables are used later for the logit computation. This is necessary for some background processes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "spice_config = SpiceConfig(\n",
    "    library_setup={\n",
    "        'value_reward_diff': ['reward_diff'],\n",
    "        'value_persistance': ['repeat'],\n",
    "        'wm_rt': ['repeat', 'switch', 'wm_rt[t-1]', 'wm_rt[t-2]', 'wm_rt[t-3]', 'wm_rt[t-4]']\n",
    "    },\n",
    "    \n",
    "    memory_state={\n",
    "            'value_reward_diff': 0.,\n",
    "            'value_persistance': 0.,\n",
    "            'value_wm_rt': 0.,\n",
    "            'wm_rt[t-1]': 1.,\n",
    "            'wm_rt[t-2]': 1.,\n",
    "            'wm_rt[t-3]': 1.,\n",
    "            'wm_rt[t-4]': 1.,\n",
    "        },\n",
    "    \n",
    "    states_in_logit=['value_reward_diff', 'value_persistance', 'value_wm_rt'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are going to define the SPICE model which is a child of the `BaseRNN` and `torch.nn.Module` class and takes as required arguments:\n",
    "1. `spice_config (SpiceConfig)`: previously defined SpiceConfig object\n",
    "2. `n_actions (int)`: number of possible actions in your dataset (including non-displayed ones if applicable).\n",
    "3. `n_participants (int)`: number of participants in your dataset.\n",
    "\n",
    "As usual for a `torch.nn.Module` we have to define at least the `__init__` method and the `forward` method.\n",
    "The `forward` method gets called when computing a forward pass through the model and takes as inputs `(inputs (SpiceDataset.xs), prev_state (dict, default: None), batch_first (bool, default: False))` and returns `(logits (torch.Tensor, shape: (n_participants*n_blocks*n_experiments, timesteps, n_actions)), updated_state (dict))`. Two necessary method calls inside the forward pass are:\n",
    "1. `self.init_forward_pass(inputs, prev_state, batch_first) -> SpiceSignals`: returns a `SpiceSignals` object which carries all relevant information already processed.\n",
    "2. `self.post_forward_pass(SpiceSignals, batch_first) -> SpiceSignals`: does some re-arranging of the logits to adhere to `batch_first`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "z0kOR2Qgz0FZ"
   },
   "outputs": [],
   "source": [
    "class SPICERNN(BaseRNN):\n",
    "    \n",
    "    def __init__(self, spice_config, **kwargs):\n",
    "        super().__init__(spice_config=spice_config, **kwargs)\n",
    "        \n",
    "        # participant embedding\n",
    "        self.participant_embedding = self.setup_embedding(num_embeddings=n_participants, embedding_size=self.embedding_size, dropout=0.)\n",
    "                \n",
    "        # set up the submodules\n",
    "        self.setup_module(key_module='value_reward_diff', input_size=1+self.embedding_size)  # 6 terms\n",
    "        self.setup_module(key_module='value_persistance', input_size=1+self.embedding_size)  # 6 terms\n",
    "        self.setup_module(key_module='wm_rt', input_size=6+self.embedding_size)  # 36 terms\n",
    "        \n",
    "    def forward(self, inputs, prev_state, batch_first=False):\n",
    "        \n",
    "        spice_signals = self.init_forward_pass(inputs, prev_state, batch_first)\n",
    "        \n",
    "        repeat = spice_signals.actions[..., 0].unsqueeze(-1).repeat(1, 1, self.n_actions)  # indexing come from value conversion in convert_dataset (alphabetical); see ValueWarning in previous cell;\n",
    "        switch = spice_signals.actions[..., 1].unsqueeze(-1).repeat(1, 1, self.n_actions)\n",
    "        rt = spice_signals.additional_inputs[..., 0].unsqueeze(-1).repeat(1, 1, self.n_actions)\n",
    "        \n",
    "        # time-invariant participant features\n",
    "        participant_embeddings = self.participant_embedding(spice_signals.participant_ids)\n",
    "        # mask_repeat = torch.tensor((1,0)).reshape(1, self.n_actions).repeat(spice_signals.actions.shape[1], 1)  # (1, 0) comes from the mapping in convert_dataset (0: repeat, 1: switch)\n",
    "        # any value updates will be applied only to the action value of switching; the repeat value will serve as an anchor since there's no uncertainty about the reward outcomes \n",
    "        mask_switch = torch.tensor((0,1)).reshape(1, self.n_actions).repeat(spice_signals.actions.shape[1], 1)\n",
    "        \n",
    "        for timestep in spice_signals.timesteps:\n",
    "            \n",
    "            # update reward value\n",
    "            self.call_module(\n",
    "                key_module='value_reward_diff',\n",
    "                key_state='value_reward_diff',\n",
    "                action_mask=mask_switch,\n",
    "                inputs=(\n",
    "                    spice_signals.rewards[timestep],\n",
    "                    ),\n",
    "                participant_index=spice_signals.participant_ids,\n",
    "                participant_embedding=participant_embeddings,\n",
    "            )\n",
    "            \n",
    "            # update working memory representation of last RTs\n",
    "            self.call_module(\n",
    "                key_module='wm_rt',\n",
    "                key_state='value_wm_rt',\n",
    "                action_mask=mask_switch,\n",
    "                inputs=(\n",
    "                    repeat[timestep],\n",
    "                    switch[timestep],\n",
    "                    self.state['wm_rt[t-1]'],\n",
    "                    self.state['wm_rt[t-2]'],\n",
    "                    self.state['wm_rt[t-3]'],\n",
    "                    self.state['wm_rt[t-4]'],\n",
    "                    ),\n",
    "                participant_index=spice_signals.participant_ids,\n",
    "                participant_embedding=participant_embeddings,\n",
    "            )\n",
    "            \n",
    "            # update repeat value\n",
    "            self.call_module(\n",
    "                key_module='value_persistance',\n",
    "                key_state='value_persistance',\n",
    "                action_mask=mask_switch,\n",
    "                inputs=(\n",
    "                    repeat[timestep],\n",
    "                    ),\n",
    "                participant_index=spice_signals.participant_ids,\n",
    "                participant_embedding=participant_embeddings,\n",
    "            )\n",
    "            \n",
    "            # update working memory of RTs\n",
    "            self.state['wm_rt[t-1]'], self.state['wm_rt[t-2]'], self.state['wm_rt[t-3]'], self.state['wm_rt[t-4]'] = rt[timestep], self.state['wm_rt[t-1]'], self.state['wm_rt[t-2]'], self.state['wm_rt[t-3]']\n",
    "            \n",
    "            # transform logits from item-space to action-space\n",
    "            spice_signals.logits[timestep] = self.state['value_reward_diff'] + self.state['value_persistance'] + self.state['value_wm_rt']\n",
    "            \n",
    "        spice_signals = self.post_forward_pass(spice_signals, batch_first)\n",
    "        \n",
    "        return spice_signals.logits, self.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup now the `SpiceEstimator` object and fit it to the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_spice = '../params/braun2018/spice_braun2018.pkl'\n",
    "estimator = SpiceEstimator(\n",
    "        # model paramaeters\n",
    "        rnn_class=SPICERNN,\n",
    "        spice_config=spice_config,\n",
    "        n_actions=2,\n",
    "        n_participants=n_participants,\n",
    "        n_experiments=1,\n",
    "        \n",
    "        # rnn training parameters\n",
    "        epochs=400,\n",
    "        warmup_steps=100,\n",
    "        learning_rate=0.01,\n",
    "        \n",
    "        # sindy fitting parameters\n",
    "        sindy_weight=0.1,\n",
    "        sindy_threshold=0.05,\n",
    "        sindy_threshold_frequency=1,\n",
    "        sindy_threshold_terms=1,\n",
    "        sindy_cutoff_patience=100,\n",
    "        sindy_epochs=1000,\n",
    "        sindy_alpha=0.0001,\n",
    "        sindy_library_polynomial_degree=2,\n",
    "        sindy_ensemble_size=1,\n",
    "        \n",
    "        # additional generalization parameters\n",
    "        batch_size=1024,\n",
    "        bagging=True,\n",
    "        scheduler=True,\n",
    "        \n",
    "        verbose=True,\n",
    "        save_path_spice=path_spice,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "3EnmDiUMWq6e",
    "outputId": "e53b1bbd-4173-4d2c-bcdc-15832bc31bd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training on cpu...\n",
      "================================================================================\n",
      "\n",
      "Training the RNN...\n",
      "================================================================================\n",
      "Epoch 1/400 --- L(Train): 3.2192521 --- L(Val, RNN): 2.2975204 --- L(Val, SINDy): 2.4543304 --- Time: 9.69s; --- Convergence: 1.15e+00; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.0 1 + 1.0 value_reward_diff[t] + -0.0 reward_diff + 0.001 value_reward_diff^2 + 0.001 value_reward_diff*reward_diff + 0.001 reward_diff^2 \n",
      "value_persistance[t+1] = -0.002 1 + 0.995 value_persistance[t] + 0.008 repeat + 0.003 value_persistance^2 + -0.007 value_persistance*repeat + 0.006 repeat^2 \n",
      "wm_rt[t+1] = 0.009 1 + 1.009 wm_rt[t] + 0.01 repeat + 0.009 switch + 0.007 wm_rt[t-1] + 0.009 wm_rt[t-2] + 0.009 wm_rt[t-3] + 0.007 wm_rt[t-4] + 0.01 wm_rt^2 + 0.009 wm_rt*repeat + 0.011 wm_rt*switch + 0.01 wm_rt*wm_rt[t-1] + 0.01 wm_rt*wm_rt[t-2] + 0.01 wm_rt*wm_rt[t-3] + 0.01 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.001 repeat*switch + 0.006 repeat*wm_rt[t-1] + 0.005 repeat*wm_rt[t-2] + 0.008 repeat*wm_rt[t-3] + 0.005 repeat*wm_rt[t-4] + 0.01 switch^2 + 0.006 switch*wm_rt[t-1] + 0.007 switch*wm_rt[t-2] + 0.007 switch*wm_rt[t-3] + 0.007 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + 0.003 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + 0.005 wm_rt[t-2]*wm_rt[t-3] + 0.003 wm_rt[t-2]*wm_rt[t-4] + 0.004 wm_rt[t-3]^2 + 0.004 wm_rt[t-3]*wm_rt[t-4] + 0.007 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 2/400 --- L(Train): 2.3004529 --- L(Val, RNN): 1.8761017 --- L(Val, SINDy): 2.9345992 --- Time: 3.85s; --- Convergence: 7.85e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.001 1 + 1.0 value_reward_diff[t] + 0.001 reward_diff + 0.001 value_reward_diff^2 + 0.001 value_reward_diff*reward_diff + 0.001 reward_diff^2 \n",
      "value_persistance[t+1] = -0.009 1 + 0.988 value_persistance[t] + 0.015 repeat + 0.01 value_persistance^2 + -0.015 value_persistance*repeat + 0.014 repeat^2 \n",
      "wm_rt[t+1] = 0.017 1 + 1.017 wm_rt[t] + 0.018 repeat + 0.017 switch + 0.015 wm_rt[t-1] + 0.016 wm_rt[t-2] + 0.017 wm_rt[t-3] + 0.014 wm_rt[t-4] + 0.018 wm_rt^2 + 0.017 wm_rt*repeat + 0.019 wm_rt*switch + 0.018 wm_rt*wm_rt[t-1] + 0.018 wm_rt*wm_rt[t-2] + 0.017 wm_rt*wm_rt[t-3] + 0.017 wm_rt*wm_rt[t-4] + 0.015 repeat^2 + -0.001 repeat*switch + 0.013 repeat*wm_rt[t-1] + 0.012 repeat*wm_rt[t-2] + 0.015 repeat*wm_rt[t-3] + 0.013 repeat*wm_rt[t-4] + 0.018 switch^2 + 0.014 switch*wm_rt[t-1] + 0.015 switch*wm_rt[t-2] + 0.014 switch*wm_rt[t-3] + 0.015 switch*wm_rt[t-4] + 0.008 wm_rt[t-1]^2 + 0.009 wm_rt[t-1]*wm_rt[t-2] + 0.009 wm_rt[t-1]*wm_rt[t-3] + 0.009 wm_rt[t-1]*wm_rt[t-4] + 0.009 wm_rt[t-2]^2 + 0.012 wm_rt[t-2]*wm_rt[t-3] + 0.01 wm_rt[t-2]*wm_rt[t-4] + 0.011 wm_rt[t-3]^2 + 0.011 wm_rt[t-3]*wm_rt[t-4] + 0.014 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 3/400 --- L(Train): 1.8924663 --- L(Val, RNN): 1.5766522 --- L(Val, SINDy): 3.0350394 --- Time: 3.05s; --- Convergence: 5.42e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.005 1 + 0.999 value_reward_diff[t] + 0.003 reward_diff + 0.001 value_reward_diff^2 + 0.0 value_reward_diff*reward_diff + 0.003 reward_diff^2 \n",
      "value_persistance[t+1] = -0.016 1 + 0.99 value_persistance[t] + 0.023 repeat + 0.018 value_persistance^2 + -0.022 value_persistance*repeat + 0.021 repeat^2 \n",
      "wm_rt[t+1] = 0.025 1 + 1.025 wm_rt[t] + 0.026 repeat + 0.025 switch + 0.023 wm_rt[t-1] + 0.024 wm_rt[t-2] + 0.025 wm_rt[t-3] + 0.022 wm_rt[t-4] + 0.026 wm_rt^2 + 0.025 wm_rt*repeat + 0.027 wm_rt*switch + 0.026 wm_rt*wm_rt[t-1] + 0.025 wm_rt*wm_rt[t-2] + 0.025 wm_rt*wm_rt[t-3] + 0.025 wm_rt*wm_rt[t-4] + 0.023 repeat^2 + -0.001 repeat*switch + 0.021 repeat*wm_rt[t-1] + 0.02 repeat*wm_rt[t-2] + 0.023 repeat*wm_rt[t-3] + 0.021 repeat*wm_rt[t-4] + 0.025 switch^2 + 0.022 switch*wm_rt[t-1] + 0.023 switch*wm_rt[t-2] + 0.022 switch*wm_rt[t-3] + 0.023 switch*wm_rt[t-4] + 0.015 wm_rt[t-1]^2 + 0.016 wm_rt[t-1]*wm_rt[t-2] + 0.016 wm_rt[t-1]*wm_rt[t-3] + 0.016 wm_rt[t-1]*wm_rt[t-4] + 0.014 wm_rt[t-2]^2 + 0.016 wm_rt[t-2]*wm_rt[t-3] + 0.014 wm_rt[t-2]*wm_rt[t-4] + 0.014 wm_rt[t-3]^2 + 0.013 wm_rt[t-3]*wm_rt[t-4] + 0.021 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 4/400 --- L(Train): 1.5947180 --- L(Val, RNN): 1.2418032 --- L(Val, SINDy): 2.8280308 --- Time: 3.67s; --- Convergence: 4.39e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.001 1 + 1.003 value_reward_diff[t] + -0.002 reward_diff + -0.0 value_reward_diff^2 + 0.001 value_reward_diff*reward_diff + -0.002 reward_diff^2 \n",
      "value_persistance[t+1] = -0.024 1 + 0.996 value_persistance[t] + 0.031 repeat + 0.021 value_persistance^2 + -0.031 value_persistance*repeat + 0.03 repeat^2 \n",
      "wm_rt[t+1] = 0.033 1 + 1.033 wm_rt[t] + 0.034 repeat + 0.033 switch + 0.031 wm_rt[t-1] + 0.033 wm_rt[t-2] + 0.033 wm_rt[t-3] + 0.031 wm_rt[t-4] + 0.034 wm_rt^2 + 0.034 wm_rt*repeat + 0.035 wm_rt*switch + 0.034 wm_rt*wm_rt[t-1] + 0.034 wm_rt*wm_rt[t-2] + 0.034 wm_rt*wm_rt[t-3] + 0.034 wm_rt*wm_rt[t-4] + 0.032 repeat^2 + -0.0 repeat*switch + 0.029 repeat*wm_rt[t-1] + 0.029 repeat*wm_rt[t-2] + 0.032 repeat*wm_rt[t-3] + 0.029 repeat*wm_rt[t-4] + 0.034 switch^2 + 0.03 switch*wm_rt[t-1] + 0.031 switch*wm_rt[t-2] + 0.03 switch*wm_rt[t-3] + 0.031 switch*wm_rt[t-4] + 0.011 wm_rt[t-1]^2 + 0.012 wm_rt[t-1]*wm_rt[t-2] + 0.012 wm_rt[t-1]*wm_rt[t-3] + 0.012 wm_rt[t-1]*wm_rt[t-4] + 0.009 wm_rt[t-2]^2 + 0.011 wm_rt[t-2]*wm_rt[t-3] + 0.009 wm_rt[t-2]*wm_rt[t-4] + 0.008 wm_rt[t-3]^2 + 0.007 wm_rt[t-3]*wm_rt[t-4] + 0.017 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 5/400 --- L(Train): 1.2786441 --- L(Val, RNN): 1.1466286 --- L(Val, SINDy): 2.3967373 --- Time: 4.13s; --- Convergence: 2.67e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.003 1 + 1.003 value_reward_diff[t] + -0.003 reward_diff + -0.0 value_reward_diff^2 + 0.001 value_reward_diff*reward_diff + -0.003 reward_diff^2 \n",
      "value_persistance[t+1] = -0.033 1 + 1.003 value_persistance[t] + 0.04 repeat + 0.018 value_persistance^2 + -0.04 value_persistance*repeat + 0.039 repeat^2 \n",
      "wm_rt[t+1] = 0.04 1 + 1.04 wm_rt[t] + 0.041 repeat + 0.04 switch + 0.034 wm_rt[t-1] + 0.035 wm_rt[t-2] + 0.035 wm_rt[t-3] + 0.034 wm_rt[t-4] + 0.041 wm_rt^2 + 0.041 wm_rt*repeat + 0.043 wm_rt*switch + 0.042 wm_rt*wm_rt[t-1] + 0.041 wm_rt*wm_rt[t-2] + 0.041 wm_rt*wm_rt[t-3] + 0.041 wm_rt*wm_rt[t-4] + 0.038 repeat^2 + -0.0 repeat*switch + 0.036 repeat*wm_rt[t-1] + 0.034 repeat*wm_rt[t-2] + 0.036 repeat*wm_rt[t-3] + 0.035 repeat*wm_rt[t-4] + 0.041 switch^2 + 0.03 switch*wm_rt[t-1] + 0.03 switch*wm_rt[t-2] + 0.029 switch*wm_rt[t-3] + 0.031 switch*wm_rt[t-4] + 0.005 wm_rt[t-1]^2 + 0.006 wm_rt[t-1]*wm_rt[t-2] + 0.006 wm_rt[t-1]*wm_rt[t-3] + 0.006 wm_rt[t-1]*wm_rt[t-4] + 0.003 wm_rt[t-2]^2 + 0.005 wm_rt[t-2]*wm_rt[t-3] + 0.003 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.011 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 6/400 --- L(Train): 1.1592292 --- L(Val, RNN): 1.0919710 --- L(Val, SINDy): 2.1816299 --- Time: 3.76s; --- Convergence: 1.61e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.002 1 + 1.001 value_reward_diff[t] + -0.001 reward_diff + 0.0 value_reward_diff^2 + -0.001 value_reward_diff*reward_diff + -0.001 reward_diff^2 \n",
      "value_persistance[t+1] = -0.04 1 + 1.008 value_persistance[t] + 0.049 repeat + 0.019 value_persistance^2 + -0.048 value_persistance*repeat + 0.047 repeat^2 \n",
      "wm_rt[t+1] = 0.046 1 + 1.047 wm_rt[t] + 0.047 repeat + 0.046 switch + 0.035 wm_rt[t-1] + 0.035 wm_rt[t-2] + 0.035 wm_rt[t-3] + 0.035 wm_rt[t-4] + 0.048 wm_rt^2 + 0.047 wm_rt*repeat + 0.049 wm_rt*switch + 0.048 wm_rt*wm_rt[t-1] + 0.048 wm_rt*wm_rt[t-2] + 0.048 wm_rt*wm_rt[t-3] + 0.048 wm_rt*wm_rt[t-4] + 0.044 repeat^2 + 0.0 repeat*switch + 0.041 repeat*wm_rt[t-1] + 0.039 repeat*wm_rt[t-2] + 0.04 repeat*wm_rt[t-3] + 0.04 repeat*wm_rt[t-4] + 0.047 switch^2 + 0.027 switch*wm_rt[t-1] + 0.027 switch*wm_rt[t-2] + 0.027 switch*wm_rt[t-3] + 0.03 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.004 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.004 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.006 wm_rt[t-3]*wm_rt[t-4] + 0.004 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 7/400 --- L(Train): 1.1440002 --- L(Val, RNN): 0.9455224 --- L(Val, SINDy): 2.1311951 --- Time: 3.32s; --- Convergence: 1.54e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.0 1 + 0.997 value_reward_diff[t] + 0.003 reward_diff + 0.002 value_reward_diff^2 + -0.005 value_reward_diff*reward_diff + 0.003 reward_diff^2 \n",
      "value_persistance[t+1] = -0.046 1 + 1.008 value_persistance[t] + 0.055 repeat + 0.023 value_persistance^2 + -0.056 value_persistance*repeat + 0.054 repeat^2 \n",
      "wm_rt[t+1] = 0.051 1 + 1.052 wm_rt[t] + 0.05 repeat + 0.051 switch + 0.034 wm_rt[t-1] + 0.033 wm_rt[t-2] + 0.033 wm_rt[t-3] + 0.035 wm_rt[t-4] + 0.053 wm_rt^2 + 0.053 wm_rt*repeat + 0.054 wm_rt*switch + 0.053 wm_rt*wm_rt[t-1] + 0.053 wm_rt*wm_rt[t-2] + 0.053 wm_rt*wm_rt[t-3] + 0.053 wm_rt*wm_rt[t-4] + 0.048 repeat^2 + 0.001 repeat*switch + 0.045 repeat*wm_rt[t-1] + 0.042 repeat*wm_rt[t-2] + 0.041 repeat*wm_rt[t-3] + 0.043 repeat*wm_rt[t-4] + 0.052 switch^2 + 0.023 switch*wm_rt[t-1] + 0.023 switch*wm_rt[t-2] + 0.022 switch*wm_rt[t-3] + 0.027 switch*wm_rt[t-4] + -0.01 wm_rt[t-1]^2 + -0.009 wm_rt[t-1]*wm_rt[t-2] + -0.009 wm_rt[t-1]*wm_rt[t-3] + -0.009 wm_rt[t-1]*wm_rt[t-4] + -0.012 wm_rt[t-2]^2 + -0.01 wm_rt[t-2]*wm_rt[t-3] + -0.012 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + -0.014 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 8/400 --- L(Train): 0.9918094 --- L(Val, RNN): 0.8512116 --- L(Val, SINDy): 2.1131985 --- Time: 3.72s; --- Convergence: 1.24e-01; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.0 1 + 0.996 value_reward_diff[t] + 0.006 reward_diff + 0.002 value_reward_diff^2 + -0.008 value_reward_diff*reward_diff + 0.006 reward_diff^2 \n",
      "value_persistance[t+1] = -0.048 1 + 1.006 value_persistance[t] + 0.059 repeat + 0.029 value_persistance^2 + -0.061 value_persistance*repeat + 0.057 repeat^2 \n",
      "wm_rt[t+1] = 0.055 1 + 1.057 wm_rt[t] + 0.052 repeat + 0.057 switch + 0.03 wm_rt[t-1] + 0.029 wm_rt[t-2] + 0.028 wm_rt[t-3] + 0.033 wm_rt[t-4] + 0.058 wm_rt^2 + 0.057 wm_rt*repeat + 0.059 wm_rt*switch + 0.058 wm_rt*wm_rt[t-1] + 0.058 wm_rt*wm_rt[t-2] + 0.058 wm_rt*wm_rt[t-3] + 0.057 wm_rt*wm_rt[t-4] + 0.049 repeat^2 + 0.001 repeat*switch + 0.047 repeat*wm_rt[t-1] + 0.042 repeat*wm_rt[t-2] + 0.04 repeat*wm_rt[t-3] + 0.046 repeat*wm_rt[t-4] + 0.058 switch^2 + 0.016 switch*wm_rt[t-1] + 0.016 switch*wm_rt[t-2] + 0.016 switch*wm_rt[t-3] + 0.022 switch*wm_rt[t-4] + -0.018 wm_rt[t-1]^2 + -0.017 wm_rt[t-1]*wm_rt[t-2] + -0.017 wm_rt[t-1]*wm_rt[t-3] + -0.018 wm_rt[t-1]*wm_rt[t-4] + -0.021 wm_rt[t-2]^2 + -0.019 wm_rt[t-2]*wm_rt[t-3] + -0.021 wm_rt[t-2]*wm_rt[t-4] + -0.022 wm_rt[t-3]^2 + -0.023 wm_rt[t-3]*wm_rt[t-4] + -0.012 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 9/400 --- L(Train): 0.8895550 --- L(Val, RNN): 0.8331535 --- L(Val, SINDy): 2.0140088 --- Time: 3.28s; --- Convergence: 7.10e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.003 1 + 0.996 value_reward_diff[t] + 0.008 reward_diff + 0.002 value_reward_diff^2 + -0.01 value_reward_diff*reward_diff + 0.008 reward_diff^2 \n",
      "value_persistance[t+1] = -0.048 1 + 1.002 value_persistance[t] + 0.057 repeat + 0.035 value_persistance^2 + -0.064 value_persistance*repeat + 0.056 repeat^2 \n",
      "wm_rt[t+1] = 0.058 1 + 1.061 wm_rt[t] + 0.052 repeat + 0.062 switch + 0.024 wm_rt[t-1] + 0.024 wm_rt[t-2] + 0.023 wm_rt[t-3] + 0.03 wm_rt[t-4] + 0.062 wm_rt^2 + 0.061 wm_rt*repeat + 0.063 wm_rt*switch + 0.062 wm_rt*wm_rt[t-1] + 0.062 wm_rt*wm_rt[t-2] + 0.062 wm_rt*wm_rt[t-3] + 0.061 wm_rt*wm_rt[t-4] + 0.049 repeat^2 + 0.001 repeat*switch + 0.046 repeat*wm_rt[t-1] + 0.042 repeat*wm_rt[t-2] + 0.037 repeat*wm_rt[t-3] + 0.05 repeat*wm_rt[t-4] + 0.063 switch^2 + 0.009 switch*wm_rt[t-1] + 0.009 switch*wm_rt[t-2] + 0.009 switch*wm_rt[t-3] + 0.016 switch*wm_rt[t-4] + -0.027 wm_rt[t-1]^2 + -0.026 wm_rt[t-1]*wm_rt[t-2] + -0.026 wm_rt[t-1]*wm_rt[t-3] + -0.027 wm_rt[t-1]*wm_rt[t-4] + -0.03 wm_rt[t-2]^2 + -0.028 wm_rt[t-2]*wm_rt[t-3] + -0.03 wm_rt[t-2]*wm_rt[t-4] + -0.031 wm_rt[t-3]^2 + -0.032 wm_rt[t-3]*wm_rt[t-4] + -0.02 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 10/400 --- L(Train): 0.8803985 --- L(Val, RNN): 0.7598358 --- L(Val, SINDy): 1.9274217 --- Time: 3.44s; --- Convergence: 7.22e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.004 1 + 0.996 value_reward_diff[t] + 0.012 reward_diff + 0.002 value_reward_diff^2 + -0.014 value_reward_diff*reward_diff + 0.011 reward_diff^2 \n",
      "value_persistance[t+1] = -0.045 1 + 0.996 value_persistance[t] + 0.054 repeat + 0.042 value_persistance^2 + -0.066 value_persistance*repeat + 0.052 repeat^2 \n",
      "wm_rt[t+1] = 0.061 1 + 1.064 wm_rt[t] + 0.049 repeat + 0.067 switch + 0.018 wm_rt[t-1] + 0.017 wm_rt[t-2] + 0.016 wm_rt[t-3] + 0.027 wm_rt[t-4] + 0.066 wm_rt^2 + 0.064 wm_rt*repeat + 0.066 wm_rt*switch + 0.065 wm_rt*wm_rt[t-1] + 0.065 wm_rt*wm_rt[t-2] + 0.064 wm_rt*wm_rt[t-3] + 0.063 wm_rt*wm_rt[t-4] + 0.046 repeat^2 + 0.001 repeat*switch + 0.044 repeat*wm_rt[t-1] + 0.041 repeat*wm_rt[t-2] + 0.034 repeat*wm_rt[t-3] + 0.053 repeat*wm_rt[t-4] + 0.068 switch^2 + 0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + 0.01 switch*wm_rt[t-4] + -0.036 wm_rt[t-1]^2 + -0.036 wm_rt[t-1]*wm_rt[t-2] + -0.035 wm_rt[t-1]*wm_rt[t-3] + -0.036 wm_rt[t-1]*wm_rt[t-4] + -0.039 wm_rt[t-2]^2 + -0.037 wm_rt[t-2]*wm_rt[t-3] + -0.039 wm_rt[t-2]*wm_rt[t-4] + -0.04 wm_rt[t-3]^2 + -0.041 wm_rt[t-3]*wm_rt[t-4] + -0.027 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 11/400 --- L(Train): 0.8216498 --- L(Val, RNN): 0.7553343 --- L(Val, SINDy): 1.8270999 --- Time: 3.91s; --- Convergence: 3.83e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.005 1 + 0.994 value_reward_diff[t] + 0.017 reward_diff + 0.002 value_reward_diff^2 + -0.018 value_reward_diff*reward_diff + 0.015 reward_diff^2 \n",
      "value_persistance[t+1] = -0.041 1 + 0.99 value_persistance[t] + 0.049 repeat + 0.049 value_persistance^2 + -0.067 value_persistance*repeat + 0.047 repeat^2 \n",
      "wm_rt[t+1] = 0.062 1 + 1.067 wm_rt[t] + 0.046 repeat + 0.072 switch + 0.01 wm_rt[t-1] + 0.01 wm_rt[t-2] + 0.009 wm_rt[t-3] + 0.024 wm_rt[t-4] + 0.069 wm_rt^2 + 0.066 wm_rt*repeat + 0.068 wm_rt*switch + 0.067 wm_rt*wm_rt[t-1] + 0.066 wm_rt*wm_rt[t-2] + 0.066 wm_rt*wm_rt[t-3] + 0.063 wm_rt*wm_rt[t-4] + 0.043 repeat^2 + 0.0 repeat*switch + 0.041 repeat*wm_rt[t-1] + 0.038 repeat*wm_rt[t-2] + 0.03 repeat*wm_rt[t-3] + 0.057 repeat*wm_rt[t-4] + 0.073 switch^2 + -0.007 switch*wm_rt[t-1] + -0.007 switch*wm_rt[t-2] + -0.006 switch*wm_rt[t-3] + 0.003 switch*wm_rt[t-4] + -0.046 wm_rt[t-1]^2 + -0.045 wm_rt[t-1]*wm_rt[t-2] + -0.045 wm_rt[t-1]*wm_rt[t-3] + -0.045 wm_rt[t-1]*wm_rt[t-4] + -0.048 wm_rt[t-2]^2 + -0.046 wm_rt[t-2]*wm_rt[t-3] + -0.048 wm_rt[t-2]*wm_rt[t-4] + -0.049 wm_rt[t-3]^2 + -0.05 wm_rt[t-3]*wm_rt[t-4] + -0.033 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 12/400 --- L(Train): 0.7937397 --- L(Val, RNN): 0.7996480 --- L(Val, SINDy): 1.8050348 --- Time: 3.86s; --- Convergence: 4.13e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.004 1 + 0.992 value_reward_diff[t] + 0.021 reward_diff + 0.003 value_reward_diff^2 + -0.021 value_reward_diff*reward_diff + 0.02 reward_diff^2 \n",
      "value_persistance[t+1] = -0.037 1 + 0.985 value_persistance[t] + 0.042 repeat + 0.054 value_persistance^2 + -0.072 value_persistance*repeat + 0.04 repeat^2 \n",
      "wm_rt[t+1] = 0.064 1 + 1.069 wm_rt[t] + 0.042 repeat + 0.077 switch + 0.003 wm_rt[t-1] + 0.004 wm_rt[t-2] + 0.003 wm_rt[t-3] + 0.022 wm_rt[t-4] + 0.072 wm_rt^2 + 0.068 wm_rt*repeat + 0.07 wm_rt*switch + 0.069 wm_rt*wm_rt[t-1] + 0.068 wm_rt*wm_rt[t-2] + 0.067 wm_rt*wm_rt[t-3] + 0.064 wm_rt*wm_rt[t-4] + 0.039 repeat^2 + 0.0 repeat*switch + 0.037 repeat*wm_rt[t-1] + 0.035 repeat*wm_rt[t-2] + 0.026 repeat*wm_rt[t-3] + 0.062 repeat*wm_rt[t-4] + 0.078 switch^2 + -0.015 switch*wm_rt[t-1] + -0.014 switch*wm_rt[t-2] + -0.014 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.055 wm_rt[t-1]^2 + -0.054 wm_rt[t-1]*wm_rt[t-2] + -0.054 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.057 wm_rt[t-2]^2 + -0.055 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.057 wm_rt[t-3]^2 + -0.058 wm_rt[t-3]*wm_rt[t-4] + -0.038 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 13/400 --- L(Train): 0.8081124 --- L(Val, RNN): 0.7451150 --- L(Val, SINDy): 1.7926190 --- Time: 3.42s; --- Convergence: 4.79e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.004 1 + 0.989 value_reward_diff[t] + 0.025 reward_diff + 0.004 value_reward_diff^2 + -0.025 value_reward_diff*reward_diff + 0.023 reward_diff^2 \n",
      "value_persistance[t+1] = -0.035 1 + 0.978 value_persistance[t] + 0.035 repeat + 0.061 value_persistance^2 + -0.078 value_persistance*repeat + 0.033 repeat^2 \n",
      "wm_rt[t+1] = 0.065 1 + 1.071 wm_rt[t] + 0.039 repeat + 0.082 switch + -0.004 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.003 wm_rt[t-3] + 0.02 wm_rt[t-4] + 0.075 wm_rt^2 + 0.069 wm_rt*repeat + 0.071 wm_rt*switch + 0.07 wm_rt*wm_rt[t-1] + 0.069 wm_rt*wm_rt[t-2] + 0.068 wm_rt*wm_rt[t-3] + 0.064 wm_rt*wm_rt[t-4] + 0.036 repeat^2 + 0.0 repeat*switch + 0.034 repeat*wm_rt[t-1] + 0.032 repeat*wm_rt[t-2] + 0.023 repeat*wm_rt[t-3] + 0.066 repeat*wm_rt[t-4] + 0.082 switch^2 + -0.023 switch*wm_rt[t-1] + -0.022 switch*wm_rt[t-2] + -0.02 switch*wm_rt[t-3] + -0.007 switch*wm_rt[t-4] + -0.063 wm_rt[t-1]^2 + -0.062 wm_rt[t-1]*wm_rt[t-2] + -0.062 wm_rt[t-1]*wm_rt[t-3] + -0.063 wm_rt[t-1]*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.063 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.065 wm_rt[t-3]^2 + -0.066 wm_rt[t-3]*wm_rt[t-4] + -0.042 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 14/400 --- L(Train): 0.7758739 --- L(Val, RNN): 0.7466490 --- L(Val, SINDy): 1.7099636 --- Time: 3.89s; --- Convergence: 2.47e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.006 1 + 0.987 value_reward_diff[t] + 0.024 reward_diff + 0.004 value_reward_diff^2 + -0.027 value_reward_diff*reward_diff + 0.023 reward_diff^2 \n",
      "value_persistance[t+1] = -0.038 1 + 0.973 value_persistance[t] + 0.028 repeat + 0.067 value_persistance^2 + -0.083 value_persistance*repeat + 0.027 repeat^2 \n",
      "wm_rt[t+1] = 0.065 1 + 1.072 wm_rt[t] + 0.034 repeat + 0.085 switch + -0.011 wm_rt[t-1] + -0.009 wm_rt[t-2] + -0.008 wm_rt[t-3] + 0.02 wm_rt[t-4] + 0.077 wm_rt^2 + 0.069 wm_rt*repeat + 0.072 wm_rt*switch + 0.071 wm_rt*wm_rt[t-1] + 0.069 wm_rt*wm_rt[t-2] + 0.067 wm_rt*wm_rt[t-3] + 0.063 wm_rt*wm_rt[t-4] + 0.032 repeat^2 + 0.0 repeat*switch + 0.03 repeat*wm_rt[t-1] + 0.029 repeat*wm_rt[t-2] + 0.022 repeat*wm_rt[t-3] + 0.072 repeat*wm_rt[t-4] + 0.086 switch^2 + -0.031 switch*wm_rt[t-1] + -0.029 switch*wm_rt[t-2] + -0.027 switch*wm_rt[t-3] + -0.012 switch*wm_rt[t-4] + -0.072 wm_rt[t-1]^2 + -0.071 wm_rt[t-1]*wm_rt[t-2] + -0.071 wm_rt[t-1]*wm_rt[t-3] + -0.072 wm_rt[t-1]*wm_rt[t-4] + -0.074 wm_rt[t-2]^2 + -0.072 wm_rt[t-2]*wm_rt[t-3] + -0.074 wm_rt[t-2]*wm_rt[t-4] + -0.071 wm_rt[t-3]^2 + -0.072 wm_rt[t-3]*wm_rt[t-4] + -0.041 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 15/400 --- L(Train): 0.7704555 --- L(Val, RNN): 0.7382501 --- L(Val, SINDy): 1.5734096 --- Time: 4.37s; --- Convergence: 1.66e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.005 1 + 0.983 value_reward_diff[t] + 0.022 reward_diff + 0.005 value_reward_diff^2 + -0.029 value_reward_diff*reward_diff + 0.02 reward_diff^2 \n",
      "value_persistance[t+1] = -0.044 1 + 0.966 value_persistance[t] + 0.021 repeat + 0.073 value_persistance^2 + -0.09 value_persistance*repeat + 0.02 repeat^2 \n",
      "wm_rt[t+1] = 0.064 1 + 1.073 wm_rt[t] + 0.029 repeat + 0.087 switch + -0.019 wm_rt[t-1] + -0.017 wm_rt[t-2] + -0.013 wm_rt[t-3] + 0.023 wm_rt[t-4] + 0.08 wm_rt^2 + 0.069 wm_rt*repeat + 0.071 wm_rt*switch + 0.07 wm_rt*wm_rt[t-1] + 0.068 wm_rt*wm_rt[t-2] + 0.065 wm_rt*wm_rt[t-3] + 0.06 wm_rt*wm_rt[t-4] + 0.026 repeat^2 + -0.0 repeat*switch + 0.025 repeat*wm_rt[t-1] + 0.026 repeat*wm_rt[t-2] + 0.023 repeat*wm_rt[t-3] + 0.08 repeat*wm_rt[t-4] + 0.088 switch^2 + -0.04 switch*wm_rt[t-1] + -0.038 switch*wm_rt[t-2] + -0.034 switch*wm_rt[t-3] + -0.015 switch*wm_rt[t-4] + -0.082 wm_rt[t-1]^2 + -0.081 wm_rt[t-1]*wm_rt[t-2] + -0.08 wm_rt[t-1]*wm_rt[t-3] + -0.081 wm_rt[t-1]*wm_rt[t-4] + -0.082 wm_rt[t-2]^2 + -0.08 wm_rt[t-2]*wm_rt[t-3] + -0.082 wm_rt[t-2]*wm_rt[t-4] + -0.075 wm_rt[t-3]^2 + -0.076 wm_rt[t-3]*wm_rt[t-4] + -0.037 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 16/400 --- L(Train): 0.7647824 --- L(Val, RNN): 0.6840575 --- L(Val, SINDy): 1.4639091 --- Time: 4.04s; --- Convergence: 3.54e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.003 1 + 0.978 value_reward_diff[t] + 0.019 reward_diff + 0.005 value_reward_diff^2 + -0.032 value_reward_diff*reward_diff + 0.018 reward_diff^2 \n",
      "value_persistance[t+1] = -0.05 1 + 0.959 value_persistance[t] + 0.014 repeat + 0.08 value_persistance^2 + -0.097 value_persistance*repeat + 0.012 repeat^2 \n",
      "wm_rt[t+1] = 0.063 1 + 1.073 wm_rt[t] + 0.023 repeat + 0.089 switch + -0.028 wm_rt[t-1] + -0.024 wm_rt[t-2] + -0.017 wm_rt[t-3] + 0.027 wm_rt[t-4] + 0.082 wm_rt^2 + 0.068 wm_rt*repeat + 0.07 wm_rt*switch + 0.069 wm_rt*wm_rt[t-1] + 0.067 wm_rt*wm_rt[t-2] + 0.062 wm_rt*wm_rt[t-3] + 0.056 wm_rt*wm_rt[t-4] + 0.02 repeat^2 + -0.0 repeat*switch + 0.019 repeat*wm_rt[t-1] + 0.021 repeat*wm_rt[t-2] + 0.024 repeat*wm_rt[t-3] + 0.088 repeat*wm_rt[t-4] + 0.09 switch^2 + -0.049 switch*wm_rt[t-1] + -0.046 switch*wm_rt[t-2] + -0.04 switch*wm_rt[t-3] + -0.015 switch*wm_rt[t-4] + -0.091 wm_rt[t-1]^2 + -0.09 wm_rt[t-1]*wm_rt[t-2] + -0.09 wm_rt[t-1]*wm_rt[t-3] + -0.09 wm_rt[t-1]*wm_rt[t-4] + -0.091 wm_rt[t-2]^2 + -0.089 wm_rt[t-2]*wm_rt[t-3] + -0.091 wm_rt[t-2]*wm_rt[t-4] + -0.078 wm_rt[t-3]^2 + -0.079 wm_rt[t-3]*wm_rt[t-4] + -0.032 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 17/400 --- L(Train): 0.6849612 --- L(Val, RNN): 0.6751532 --- L(Val, SINDy): 1.3896936 --- Time: 4.01s; --- Convergence: 2.21e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.001 1 + 0.974 value_reward_diff[t] + 0.017 reward_diff + 0.006 value_reward_diff^2 + -0.034 value_reward_diff*reward_diff + 0.016 reward_diff^2 \n",
      "value_persistance[t+1] = -0.055 1 + 0.953 value_persistance[t] + 0.007 repeat + 0.086 value_persistance^2 + -0.103 value_persistance*repeat + 0.006 repeat^2 \n",
      "wm_rt[t+1] = 0.062 1 + 1.073 wm_rt[t] + 0.018 repeat + 0.091 switch + -0.035 wm_rt[t-1] + -0.031 wm_rt[t-2] + -0.021 wm_rt[t-3] + 0.032 wm_rt[t-4] + 0.084 wm_rt^2 + 0.067 wm_rt*repeat + 0.069 wm_rt*switch + 0.069 wm_rt*wm_rt[t-1] + 0.065 wm_rt*wm_rt[t-2] + 0.06 wm_rt*wm_rt[t-3] + 0.052 wm_rt*wm_rt[t-4] + 0.015 repeat^2 + -0.0 repeat*switch + 0.014 repeat*wm_rt[t-1] + 0.017 repeat*wm_rt[t-2] + 0.025 repeat*wm_rt[t-3] + 0.094 repeat*wm_rt[t-4] + 0.091 switch^2 + -0.057 switch*wm_rt[t-1] + -0.054 switch*wm_rt[t-2] + -0.045 switch*wm_rt[t-3] + -0.015 switch*wm_rt[t-4] + -0.099 wm_rt[t-1]^2 + -0.098 wm_rt[t-1]*wm_rt[t-2] + -0.098 wm_rt[t-1]*wm_rt[t-3] + -0.099 wm_rt[t-1]*wm_rt[t-4] + -0.099 wm_rt[t-2]^2 + -0.097 wm_rt[t-2]*wm_rt[t-3] + -0.099 wm_rt[t-2]*wm_rt[t-4] + -0.08 wm_rt[t-3]^2 + -0.081 wm_rt[t-3]*wm_rt[t-4] + -0.027 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 18/400 --- L(Train): 0.6775205 --- L(Val, RNN): 0.6692858 --- L(Val, SINDy): 1.3666399 --- Time: 3.35s; --- Convergence: 1.40e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.002 1 + 0.968 value_reward_diff[t] + 0.02 reward_diff + 0.005 value_reward_diff^2 + -0.036 value_reward_diff*reward_diff + 0.019 reward_diff^2 \n",
      "value_persistance[t+1] = -0.057 1 + 0.947 value_persistance[t] + 0.001 repeat + 0.091 value_persistance^2 + -0.108 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.06 1 + 1.073 wm_rt[t] + 0.012 repeat + 0.091 switch + -0.043 wm_rt[t-1] + -0.038 wm_rt[t-2] + -0.023 wm_rt[t-3] + 0.038 wm_rt[t-4] + 0.085 wm_rt^2 + 0.065 wm_rt*repeat + 0.068 wm_rt*switch + 0.067 wm_rt*wm_rt[t-1] + 0.064 wm_rt*wm_rt[t-2] + 0.056 wm_rt*wm_rt[t-3] + 0.048 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + -0.0 repeat*switch + 0.008 repeat*wm_rt[t-1] + 0.012 repeat*wm_rt[t-2] + 0.027 repeat*wm_rt[t-3] + 0.102 repeat*wm_rt[t-4] + 0.092 switch^2 + -0.066 switch*wm_rt[t-1] + -0.061 switch*wm_rt[t-2] + -0.049 switch*wm_rt[t-3] + -0.012 switch*wm_rt[t-4] + -0.108 wm_rt[t-1]^2 + -0.107 wm_rt[t-1]*wm_rt[t-2] + -0.107 wm_rt[t-1]*wm_rt[t-3] + -0.107 wm_rt[t-1]*wm_rt[t-4] + -0.107 wm_rt[t-2]^2 + -0.104 wm_rt[t-2]*wm_rt[t-3] + -0.106 wm_rt[t-2]*wm_rt[t-4] + -0.08 wm_rt[t-3]^2 + -0.08 wm_rt[t-3]*wm_rt[t-4] + -0.02 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 19/400 --- L(Train): 0.6889175 --- L(Val, RNN): 0.6609261 --- L(Val, SINDy): 1.3243734 --- Time: 3.80s; --- Convergence: 1.12e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.003 1 + 0.961 value_reward_diff[t] + 0.026 reward_diff + 0.004 value_reward_diff^2 + -0.035 value_reward_diff*reward_diff + 0.025 reward_diff^2 \n",
      "value_persistance[t+1] = -0.056 1 + 0.942 value_persistance[t] + -0.005 repeat + 0.096 value_persistance^2 + -0.114 value_persistance*repeat + -0.006 repeat^2 \n",
      "wm_rt[t+1] = 0.058 1 + 1.073 wm_rt[t] + 0.004 repeat + 0.094 switch + -0.051 wm_rt[t-1] + -0.045 wm_rt[t-2] + -0.024 wm_rt[t-3] + 0.045 wm_rt[t-4] + 0.087 wm_rt^2 + 0.063 wm_rt*repeat + 0.066 wm_rt*switch + 0.066 wm_rt*wm_rt[t-1] + 0.062 wm_rt*wm_rt[t-2] + 0.052 wm_rt*wm_rt[t-3] + 0.042 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.006 repeat*wm_rt[t-2] + 0.028 repeat*wm_rt[t-3] + 0.11 repeat*wm_rt[t-4] + 0.094 switch^2 + -0.073 switch*wm_rt[t-1] + -0.067 switch*wm_rt[t-2] + -0.051 switch*wm_rt[t-3] + -0.007 switch*wm_rt[t-4] + -0.116 wm_rt[t-1]^2 + -0.115 wm_rt[t-1]*wm_rt[t-2] + -0.115 wm_rt[t-1]*wm_rt[t-3] + -0.115 wm_rt[t-1]*wm_rt[t-4] + -0.114 wm_rt[t-2]^2 + -0.112 wm_rt[t-2]*wm_rt[t-3] + -0.114 wm_rt[t-2]*wm_rt[t-4] + -0.078 wm_rt[t-3]^2 + -0.078 wm_rt[t-3]*wm_rt[t-4] + -0.012 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 20/400 --- L(Train): 0.6689113 --- L(Val, RNN): 0.6483844 --- L(Val, SINDy): 1.3165537 --- Time: 3.77s; --- Convergence: 1.19e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.002 1 + 0.953 value_reward_diff[t] + 0.033 reward_diff + 0.001 value_reward_diff^2 + -0.033 value_reward_diff*reward_diff + 0.032 reward_diff^2 \n",
      "value_persistance[t+1] = -0.053 1 + 0.938 value_persistance[t] + -0.01 repeat + 0.101 value_persistance^2 + -0.118 value_persistance*repeat + -0.012 repeat^2 \n",
      "wm_rt[t+1] = 0.054 1 + 1.072 wm_rt[t] + -0.004 repeat + 0.097 switch + -0.06 wm_rt[t-1] + -0.052 wm_rt[t-2] + -0.024 wm_rt[t-3] + 0.054 wm_rt[t-4] + 0.088 wm_rt^2 + 0.061 wm_rt*repeat + 0.064 wm_rt*switch + 0.064 wm_rt*wm_rt[t-1] + 0.059 wm_rt*wm_rt[t-2] + 0.047 wm_rt*wm_rt[t-3] + 0.035 wm_rt*wm_rt[t-4] + -0.007 repeat^2 + -0.0 repeat*switch + -0.008 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.027 repeat*wm_rt[t-3] + 0.119 repeat*wm_rt[t-4] + 0.098 switch^2 + -0.081 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-2] + -0.051 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.124 wm_rt[t-1]^2 + -0.123 wm_rt[t-1]*wm_rt[t-2] + -0.123 wm_rt[t-1]*wm_rt[t-3] + -0.123 wm_rt[t-1]*wm_rt[t-4] + -0.121 wm_rt[t-2]^2 + -0.118 wm_rt[t-2]*wm_rt[t-3] + -0.12 wm_rt[t-2]*wm_rt[t-4] + -0.074 wm_rt[t-3]^2 + -0.074 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 21/400 --- L(Train): 0.6612769 --- L(Val, RNN): 0.6359918 --- L(Val, SINDy): 1.2848414 --- Time: 3.54s; --- Convergence: 1.21e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.003 1 + 0.944 value_reward_diff[t] + 0.04 reward_diff + -0.004 value_reward_diff^2 + -0.031 value_reward_diff*reward_diff + 0.039 reward_diff^2 \n",
      "value_persistance[t+1] = -0.049 1 + 0.933 value_persistance[t] + -0.015 repeat + 0.105 value_persistance^2 + -0.122 value_persistance*repeat + -0.017 repeat^2 \n",
      "wm_rt[t+1] = 0.051 1 + 1.072 wm_rt[t] + -0.013 repeat + 0.101 switch + -0.068 wm_rt[t-1] + -0.057 wm_rt[t-2] + -0.023 wm_rt[t-3] + 0.063 wm_rt[t-4] + 0.089 wm_rt^2 + 0.059 wm_rt*repeat + 0.062 wm_rt*switch + 0.062 wm_rt*wm_rt[t-1] + 0.057 wm_rt*wm_rt[t-2] + 0.042 wm_rt*wm_rt[t-3] + 0.028 wm_rt*wm_rt[t-4] + -0.016 repeat^2 + -0.0 repeat*switch + -0.017 repeat*wm_rt[t-1] + -0.009 repeat*wm_rt[t-2] + 0.025 repeat*wm_rt[t-3] + 0.128 repeat*wm_rt[t-4] + 0.102 switch^2 + -0.087 switch*wm_rt[t-1] + -0.076 switch*wm_rt[t-2] + -0.049 switch*wm_rt[t-3] + 0.008 switch*wm_rt[t-4] + -0.131 wm_rt[t-1]^2 + -0.13 wm_rt[t-1]*wm_rt[t-2] + -0.13 wm_rt[t-1]*wm_rt[t-3] + -0.13 wm_rt[t-1]*wm_rt[t-4] + -0.126 wm_rt[t-2]^2 + -0.124 wm_rt[t-2]*wm_rt[t-3] + -0.125 wm_rt[t-2]*wm_rt[t-4] + -0.069 wm_rt[t-3]^2 + -0.069 wm_rt[t-3]*wm_rt[t-4] + 0.006 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 22/400 --- L(Train): 0.6426970 --- L(Val, RNN): 0.6256317 --- L(Val, SINDy): 1.1614727 --- Time: 3.28s; --- Convergence: 1.12e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.002 1 + 0.935 value_reward_diff[t] + 0.047 reward_diff + -0.01 value_reward_diff^2 + -0.032 value_reward_diff*reward_diff + 0.046 reward_diff^2 \n",
      "value_persistance[t+1] = -0.043 1 + 0.929 value_persistance[t] + -0.02 repeat + 0.109 value_persistance^2 + -0.126 value_persistance*repeat + -0.021 repeat^2 \n",
      "wm_rt[t+1] = 0.046 1 + 1.071 wm_rt[t] + -0.022 repeat + 0.104 switch + -0.075 wm_rt[t-1] + -0.063 wm_rt[t-2] + -0.021 wm_rt[t-3] + 0.073 wm_rt[t-4] + 0.09 wm_rt^2 + 0.057 wm_rt*repeat + 0.059 wm_rt*switch + 0.06 wm_rt*wm_rt[t-1] + 0.054 wm_rt*wm_rt[t-2] + 0.037 wm_rt*wm_rt[t-3] + 0.02 wm_rt*wm_rt[t-4] + -0.025 repeat^2 + 0.0 repeat*switch + -0.025 repeat*wm_rt[t-1] + -0.018 repeat*wm_rt[t-2] + 0.023 repeat*wm_rt[t-3] + 0.138 repeat*wm_rt[t-4] + 0.105 switch^2 + -0.093 switch*wm_rt[t-1] + -0.079 switch*wm_rt[t-2] + -0.046 switch*wm_rt[t-3] + 0.017 switch*wm_rt[t-4] + -0.137 wm_rt[t-1]^2 + -0.136 wm_rt[t-1]*wm_rt[t-2] + -0.135 wm_rt[t-1]*wm_rt[t-3] + -0.135 wm_rt[t-1]*wm_rt[t-4] + -0.131 wm_rt[t-2]^2 + -0.128 wm_rt[t-2]*wm_rt[t-3] + -0.13 wm_rt[t-2]*wm_rt[t-4] + -0.063 wm_rt[t-3]^2 + -0.062 wm_rt[t-3]*wm_rt[t-4] + 0.016 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 23/400 --- L(Train): 0.6411841 --- L(Val, RNN): 0.6149352 --- L(Val, SINDy): 1.1055174 --- Time: 3.89s; --- Convergence: 1.10e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.0 1 + 0.926 value_reward_diff[t] + 0.052 reward_diff + -0.016 value_reward_diff^2 + -0.036 value_reward_diff*reward_diff + 0.051 reward_diff^2 \n",
      "value_persistance[t+1] = -0.038 1 + 0.926 value_persistance[t] + -0.024 repeat + 0.112 value_persistance^2 + -0.129 value_persistance*repeat + -0.025 repeat^2 \n",
      "wm_rt[t+1] = 0.042 1 + 1.07 wm_rt[t] + -0.03 repeat + 0.105 switch + -0.082 wm_rt[t-1] + -0.067 wm_rt[t-2] + -0.02 wm_rt[t-3] + 0.083 wm_rt[t-4] + 0.092 wm_rt^2 + 0.056 wm_rt*repeat + 0.057 wm_rt*switch + 0.059 wm_rt*wm_rt[t-1] + 0.052 wm_rt*wm_rt[t-2] + 0.033 wm_rt*wm_rt[t-3] + 0.012 wm_rt*wm_rt[t-4] + -0.033 repeat^2 + 0.0 repeat*switch + -0.034 repeat*wm_rt[t-1] + -0.026 repeat*wm_rt[t-2] + 0.019 repeat*wm_rt[t-3] + 0.148 repeat*wm_rt[t-4] + 0.106 switch^2 + -0.097 switch*wm_rt[t-1] + -0.081 switch*wm_rt[t-2] + -0.042 switch*wm_rt[t-3] + 0.026 switch*wm_rt[t-4] + -0.141 wm_rt[t-1]^2 + -0.14 wm_rt[t-1]*wm_rt[t-2] + -0.14 wm_rt[t-1]*wm_rt[t-3] + -0.139 wm_rt[t-1]*wm_rt[t-4] + -0.135 wm_rt[t-2]^2 + -0.132 wm_rt[t-2]*wm_rt[t-3] + -0.133 wm_rt[t-2]*wm_rt[t-4] + -0.057 wm_rt[t-3]^2 + -0.056 wm_rt[t-3]*wm_rt[t-4] + 0.026 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 24/400 --- L(Train): 0.6353185 --- L(Val, RNN): 0.6046350 --- L(Val, SINDy): 1.0568224 --- Time: 3.52s; --- Convergence: 1.06e-02; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.003 1 + 0.916 value_reward_diff[t] + 0.055 reward_diff + -0.023 value_reward_diff^2 + -0.042 value_reward_diff*reward_diff + 0.054 reward_diff^2 \n",
      "value_persistance[t+1] = -0.033 1 + 0.922 value_persistance[t] + -0.028 repeat + 0.116 value_persistance^2 + -0.133 value_persistance*repeat + -0.029 repeat^2 \n",
      "wm_rt[t+1] = 0.036 1 + 1.07 wm_rt[t] + -0.037 repeat + 0.103 switch + -0.087 wm_rt[t-1] + -0.072 wm_rt[t-2] + -0.018 wm_rt[t-3] + 0.093 wm_rt[t-4] + 0.093 wm_rt^2 + 0.054 wm_rt*repeat + 0.055 wm_rt*switch + 0.057 wm_rt*wm_rt[t-1] + 0.05 wm_rt*wm_rt[t-2] + 0.029 wm_rt*wm_rt[t-3] + 0.004 wm_rt*wm_rt[t-4] + -0.04 repeat^2 + 0.0 repeat*switch + -0.041 repeat*wm_rt[t-1] + -0.033 repeat*wm_rt[t-2] + 0.016 repeat*wm_rt[t-3] + 0.159 repeat*wm_rt[t-4] + 0.104 switch^2 + -0.101 switch*wm_rt[t-1] + -0.084 switch*wm_rt[t-2] + -0.039 switch*wm_rt[t-3] + 0.034 switch*wm_rt[t-4] + -0.144 wm_rt[t-1]^2 + -0.143 wm_rt[t-1]*wm_rt[t-2] + -0.142 wm_rt[t-1]*wm_rt[t-3] + -0.142 wm_rt[t-1]*wm_rt[t-4] + -0.138 wm_rt[t-2]^2 + -0.135 wm_rt[t-2]*wm_rt[t-3] + -0.136 wm_rt[t-2]*wm_rt[t-4] + -0.051 wm_rt[t-3]^2 + -0.049 wm_rt[t-3]*wm_rt[t-4] + 0.037 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 25/400 --- L(Train): 0.6073613 --- L(Val, RNN): 0.5983245 --- L(Val, SINDy): 1.0117751 --- Time: 3.70s; --- Convergence: 8.47e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.009 1 + 0.908 value_reward_diff[t] + 0.058 reward_diff + -0.031 value_reward_diff^2 + -0.047 value_reward_diff*reward_diff + 0.057 reward_diff^2 \n",
      "value_persistance[t+1] = -0.028 1 + 0.919 value_persistance[t] + -0.031 repeat + 0.118 value_persistance^2 + -0.135 value_persistance*repeat + -0.033 repeat^2 \n",
      "wm_rt[t+1] = 0.031 1 + 1.07 wm_rt[t] + -0.043 repeat + 0.1 switch + -0.091 wm_rt[t-1] + -0.076 wm_rt[t-2] + -0.016 wm_rt[t-3] + 0.104 wm_rt[t-4] + 0.093 wm_rt^2 + 0.052 wm_rt*repeat + 0.054 wm_rt*switch + 0.056 wm_rt*wm_rt[t-1] + 0.049 wm_rt*wm_rt[t-2] + 0.025 wm_rt*wm_rt[t-3] + -0.004 wm_rt*wm_rt[t-4] + -0.046 repeat^2 + 0.0 repeat*switch + -0.047 repeat*wm_rt[t-1] + -0.041 repeat*wm_rt[t-2] + 0.013 repeat*wm_rt[t-3] + 0.169 repeat*wm_rt[t-4] + 0.101 switch^2 + -0.104 switch*wm_rt[t-1] + -0.086 switch*wm_rt[t-2] + -0.036 switch*wm_rt[t-3] + 0.043 switch*wm_rt[t-4] + -0.145 wm_rt[t-1]^2 + -0.144 wm_rt[t-1]*wm_rt[t-2] + -0.144 wm_rt[t-1]*wm_rt[t-3] + -0.143 wm_rt[t-1]*wm_rt[t-4] + -0.141 wm_rt[t-2]^2 + -0.138 wm_rt[t-2]*wm_rt[t-3] + -0.138 wm_rt[t-2]*wm_rt[t-4] + -0.045 wm_rt[t-3]^2 + -0.043 wm_rt[t-3]*wm_rt[t-4] + 0.048 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 26/400 --- L(Train): 0.5942088 --- L(Val, RNN): 0.5893853 --- L(Val, SINDy): 0.9624159 --- Time: 4.30s; --- Convergence: 8.71e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.016 1 + 0.902 value_reward_diff[t] + 0.063 reward_diff + -0.039 value_reward_diff^2 + -0.051 value_reward_diff*reward_diff + 0.063 reward_diff^2 \n",
      "value_persistance[t+1] = -0.027 1 + 0.917 value_persistance[t] + -0.035 repeat + 0.121 value_persistance^2 + -0.138 value_persistance*repeat + -0.036 repeat^2 \n",
      "wm_rt[t+1] = 0.025 1 + 1.069 wm_rt[t] + -0.048 repeat + 0.095 switch + -0.094 wm_rt[t-1] + -0.081 wm_rt[t-2] + -0.016 wm_rt[t-3] + 0.114 wm_rt[t-4] + 0.094 wm_rt^2 + 0.051 wm_rt*repeat + 0.052 wm_rt*switch + 0.054 wm_rt*wm_rt[t-1] + 0.048 wm_rt*wm_rt[t-2] + 0.022 wm_rt*wm_rt[t-3] + -0.012 wm_rt*wm_rt[t-4] + -0.051 repeat^2 + 0.0 repeat*switch + -0.052 repeat*wm_rt[t-1] + -0.047 repeat*wm_rt[t-2] + 0.009 repeat*wm_rt[t-3] + 0.18 repeat*wm_rt[t-4] + 0.095 switch^2 + -0.105 switch*wm_rt[t-1] + -0.088 switch*wm_rt[t-2] + -0.034 switch*wm_rt[t-3] + 0.052 switch*wm_rt[t-4] + -0.145 wm_rt[t-1]^2 + -0.144 wm_rt[t-1]*wm_rt[t-2] + -0.143 wm_rt[t-1]*wm_rt[t-3] + -0.142 wm_rt[t-1]*wm_rt[t-4] + -0.143 wm_rt[t-2]^2 + -0.14 wm_rt[t-2]*wm_rt[t-3] + -0.14 wm_rt[t-2]*wm_rt[t-4] + -0.04 wm_rt[t-3]^2 + -0.038 wm_rt[t-3]*wm_rt[t-4] + 0.059 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 27/400 --- L(Train): 0.6043874 --- L(Val, RNN): 0.5794702 --- L(Val, SINDy): 0.9549012 --- Time: 3.74s; --- Convergence: 9.31e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.024 1 + 0.895 value_reward_diff[t] + 0.07 reward_diff + -0.048 value_reward_diff^2 + -0.055 value_reward_diff*reward_diff + 0.07 reward_diff^2 \n",
      "value_persistance[t+1] = -0.027 1 + 0.914 value_persistance[t] + -0.038 repeat + 0.123 value_persistance^2 + -0.14 value_persistance*repeat + -0.04 repeat^2 \n",
      "wm_rt[t+1] = 0.02 1 + 1.069 wm_rt[t] + -0.051 repeat + 0.088 switch + -0.096 wm_rt[t-1] + -0.085 wm_rt[t-2] + -0.017 wm_rt[t-3] + 0.125 wm_rt[t-4] + 0.095 wm_rt^2 + 0.049 wm_rt*repeat + 0.051 wm_rt*switch + 0.053 wm_rt*wm_rt[t-1] + 0.048 wm_rt*wm_rt[t-2] + 0.02 wm_rt*wm_rt[t-3] + -0.02 wm_rt*wm_rt[t-4] + -0.053 repeat^2 + -0.0 repeat*switch + -0.055 repeat*wm_rt[t-1] + -0.052 repeat*wm_rt[t-2] + 0.006 repeat*wm_rt[t-3] + 0.192 repeat*wm_rt[t-4] + 0.089 switch^2 + -0.106 switch*wm_rt[t-1] + -0.092 switch*wm_rt[t-2] + -0.033 switch*wm_rt[t-3] + 0.06 switch*wm_rt[t-4] + -0.143 wm_rt[t-1]^2 + -0.142 wm_rt[t-1]*wm_rt[t-2] + -0.141 wm_rt[t-1]*wm_rt[t-3] + -0.14 wm_rt[t-1]*wm_rt[t-4] + -0.145 wm_rt[t-2]^2 + -0.142 wm_rt[t-2]*wm_rt[t-3] + -0.142 wm_rt[t-2]*wm_rt[t-4] + -0.037 wm_rt[t-3]^2 + -0.034 wm_rt[t-3]*wm_rt[t-4] + 0.07 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 28/400 --- L(Train): 0.5982463 --- L(Val, RNN): 0.5692678 --- L(Val, SINDy): 0.8822682 --- Time: 4.26s; --- Convergence: 9.76e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.033 1 + 0.892 value_reward_diff[t] + 0.076 reward_diff + -0.056 value_reward_diff^2 + -0.061 value_reward_diff*reward_diff + 0.077 reward_diff^2 \n",
      "value_persistance[t+1] = -0.029 1 + 0.912 value_persistance[t] + -0.042 repeat + 0.126 value_persistance^2 + -0.142 value_persistance*repeat + -0.043 repeat^2 \n",
      "wm_rt[t+1] = 0.014 1 + 1.068 wm_rt[t] + -0.052 repeat + 0.08 switch + -0.097 wm_rt[t-1] + -0.089 wm_rt[t-2] + -0.019 wm_rt[t-3] + 0.136 wm_rt[t-4] + 0.096 wm_rt^2 + 0.048 wm_rt*repeat + 0.05 wm_rt*switch + 0.052 wm_rt*wm_rt[t-1] + 0.047 wm_rt*wm_rt[t-2] + 0.018 wm_rt*wm_rt[t-3] + -0.028 wm_rt*wm_rt[t-4] + -0.055 repeat^2 + -0.0 repeat*switch + -0.057 repeat*wm_rt[t-1] + -0.057 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.203 repeat*wm_rt[t-4] + 0.081 switch^2 + -0.106 switch*wm_rt[t-1] + -0.095 switch*wm_rt[t-2] + -0.034 switch*wm_rt[t-3] + 0.068 switch*wm_rt[t-4] + -0.14 wm_rt[t-1]^2 + -0.139 wm_rt[t-1]*wm_rt[t-2] + -0.138 wm_rt[t-1]*wm_rt[t-3] + -0.136 wm_rt[t-1]*wm_rt[t-4] + -0.147 wm_rt[t-2]^2 + -0.144 wm_rt[t-2]*wm_rt[t-3] + -0.143 wm_rt[t-2]*wm_rt[t-4] + -0.034 wm_rt[t-3]^2 + -0.031 wm_rt[t-3]*wm_rt[t-4] + 0.081 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 29/400 --- L(Train): 0.5671510 --- L(Val, RNN): 0.5590783 --- L(Val, SINDy): 0.8464975 --- Time: 3.79s; --- Convergence: 9.97e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.043 1 + 0.886 value_reward_diff[t] + 0.077 reward_diff + -0.064 value_reward_diff^2 + -0.068 value_reward_diff*reward_diff + 0.078 reward_diff^2 \n",
      "value_persistance[t+1] = -0.034 1 + 0.91 value_persistance[t] + -0.045 repeat + 0.127 value_persistance^2 + -0.144 value_persistance*repeat + -0.047 repeat^2 \n",
      "wm_rt[t+1] = 0.008 1 + 1.068 wm_rt[t] + -0.052 repeat + 0.071 switch + -0.096 wm_rt[t-1] + -0.094 wm_rt[t-2] + -0.022 wm_rt[t-3] + 0.147 wm_rt[t-4] + 0.096 wm_rt^2 + 0.047 wm_rt*repeat + 0.049 wm_rt*switch + 0.051 wm_rt*wm_rt[t-1] + 0.048 wm_rt*wm_rt[t-2] + 0.018 wm_rt*wm_rt[t-3] + -0.036 wm_rt*wm_rt[t-4] + -0.054 repeat^2 + -0.0 repeat*switch + -0.057 repeat*wm_rt[t-1] + -0.06 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.215 repeat*wm_rt[t-4] + 0.072 switch^2 + -0.105 switch*wm_rt[t-1] + -0.099 switch*wm_rt[t-2] + -0.036 switch*wm_rt[t-3] + 0.076 switch*wm_rt[t-4] + -0.136 wm_rt[t-1]^2 + -0.135 wm_rt[t-1]*wm_rt[t-2] + -0.134 wm_rt[t-1]*wm_rt[t-3] + -0.132 wm_rt[t-1]*wm_rt[t-4] + -0.15 wm_rt[t-2]^2 + -0.147 wm_rt[t-2]*wm_rt[t-3] + -0.145 wm_rt[t-2]*wm_rt[t-4] + -0.033 wm_rt[t-3]^2 + -0.029 wm_rt[t-3]*wm_rt[t-4] + 0.092 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 30/400 --- L(Train): 0.5733282 --- L(Val, RNN): 0.5507431 --- L(Val, SINDy): 0.9281009 --- Time: 5.09s; --- Convergence: 9.15e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.052 1 + 0.879 value_reward_diff[t] + 0.074 reward_diff + -0.073 value_reward_diff^2 + -0.076 value_reward_diff*reward_diff + 0.075 reward_diff^2 \n",
      "value_persistance[t+1] = -0.04 1 + 0.909 value_persistance[t] + -0.049 repeat + 0.129 value_persistance^2 + -0.146 value_persistance*repeat + -0.05 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.068 wm_rt[t] + -0.05 repeat + 0.062 switch + -0.093 wm_rt[t-1] + -0.098 wm_rt[t-2] + -0.026 wm_rt[t-3] + 0.159 wm_rt[t-4] + 0.097 wm_rt^2 + 0.046 wm_rt*repeat + 0.048 wm_rt*switch + 0.05 wm_rt*wm_rt[t-1] + 0.049 wm_rt*wm_rt[t-2] + 0.018 wm_rt*wm_rt[t-3] + -0.044 wm_rt*wm_rt[t-4] + -0.053 repeat^2 + -0.0 repeat*switch + -0.056 repeat*wm_rt[t-1] + -0.064 repeat*wm_rt[t-2] + -0.007 repeat*wm_rt[t-3] + 0.227 repeat*wm_rt[t-4] + 0.063 switch^2 + -0.102 switch*wm_rt[t-1] + -0.103 switch*wm_rt[t-2] + -0.038 switch*wm_rt[t-3] + 0.084 switch*wm_rt[t-4] + -0.13 wm_rt[t-1]^2 + -0.129 wm_rt[t-1]*wm_rt[t-2] + -0.128 wm_rt[t-1]*wm_rt[t-3] + -0.126 wm_rt[t-1]*wm_rt[t-4] + -0.152 wm_rt[t-2]^2 + -0.149 wm_rt[t-2]*wm_rt[t-3] + -0.147 wm_rt[t-2]*wm_rt[t-4] + -0.033 wm_rt[t-3]^2 + -0.028 wm_rt[t-3]*wm_rt[t-4] + 0.103 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 31/400 --- L(Train): 0.5731513 --- L(Val, RNN): 0.5427547 --- L(Val, SINDy): 0.9611480 --- Time: 3.67s; --- Convergence: 8.57e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.062 1 + 0.87 value_reward_diff[t] + 0.07 reward_diff + -0.083 value_reward_diff^2 + -0.085 value_reward_diff*reward_diff + 0.072 reward_diff^2 \n",
      "value_persistance[t+1] = -0.048 1 + 0.907 value_persistance[t] + -0.052 repeat + 0.13 value_persistance^2 + -0.147 value_persistance*repeat + -0.054 repeat^2 \n",
      "wm_rt[t+1] = -0.006 1 + 1.067 wm_rt[t] + -0.048 repeat + 0.052 switch + -0.09 wm_rt[t-1] + -0.103 wm_rt[t-2] + -0.03 wm_rt[t-3] + 0.17 wm_rt[t-4] + 0.097 wm_rt^2 + 0.045 wm_rt*repeat + 0.047 wm_rt*switch + 0.049 wm_rt*wm_rt[t-1] + 0.05 wm_rt*wm_rt[t-2] + 0.018 wm_rt*wm_rt[t-3] + -0.052 wm_rt*wm_rt[t-4] + -0.051 repeat^2 + 0.0 repeat*switch + -0.055 repeat*wm_rt[t-1] + -0.067 repeat*wm_rt[t-2] + -0.012 repeat*wm_rt[t-3] + 0.238 repeat*wm_rt[t-4] + 0.053 switch^2 + -0.098 switch*wm_rt[t-1] + -0.108 switch*wm_rt[t-2] + -0.041 switch*wm_rt[t-3] + 0.093 switch*wm_rt[t-4] + -0.123 wm_rt[t-1]^2 + -0.122 wm_rt[t-1]*wm_rt[t-2] + -0.121 wm_rt[t-1]*wm_rt[t-3] + -0.119 wm_rt[t-1]*wm_rt[t-4] + -0.155 wm_rt[t-2]^2 + -0.152 wm_rt[t-2]*wm_rt[t-3] + -0.149 wm_rt[t-2]*wm_rt[t-4] + -0.033 wm_rt[t-3]^2 + -0.028 wm_rt[t-3]*wm_rt[t-4] + 0.115 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 32/400 --- L(Train): 0.5594453 --- L(Val, RNN): 0.5350260 --- L(Val, SINDy): 0.9758645 --- Time: 4.22s; --- Convergence: 8.15e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.072 1 + 0.862 value_reward_diff[t] + 0.068 reward_diff + -0.093 value_reward_diff^2 + -0.094 value_reward_diff*reward_diff + 0.071 reward_diff^2 \n",
      "value_persistance[t+1] = -0.056 1 + 0.906 value_persistance[t] + -0.055 repeat + 0.132 value_persistance^2 + -0.148 value_persistance*repeat + -0.057 repeat^2 \n",
      "wm_rt[t+1] = -0.013 1 + 1.067 wm_rt[t] + -0.045 repeat + 0.042 switch + -0.085 wm_rt[t-1] + -0.107 wm_rt[t-2] + -0.035 wm_rt[t-3] + 0.182 wm_rt[t-4] + 0.098 wm_rt^2 + 0.044 wm_rt*repeat + 0.046 wm_rt*switch + 0.048 wm_rt*wm_rt[t-1] + 0.051 wm_rt*wm_rt[t-2] + 0.019 wm_rt*wm_rt[t-3] + -0.059 wm_rt*wm_rt[t-4] + -0.048 repeat^2 + 0.0 repeat*switch + -0.053 repeat*wm_rt[t-1] + -0.071 repeat*wm_rt[t-2] + -0.019 repeat*wm_rt[t-3] + 0.249 repeat*wm_rt[t-4] + 0.043 switch^2 + -0.093 switch*wm_rt[t-1] + -0.111 switch*wm_rt[t-2] + -0.043 switch*wm_rt[t-3] + 0.102 switch*wm_rt[t-4] + -0.116 wm_rt[t-1]^2 + -0.115 wm_rt[t-1]*wm_rt[t-2] + -0.114 wm_rt[t-1]*wm_rt[t-3] + -0.111 wm_rt[t-1]*wm_rt[t-4] + -0.157 wm_rt[t-2]^2 + -0.154 wm_rt[t-2]*wm_rt[t-3] + -0.151 wm_rt[t-2]*wm_rt[t-4] + -0.034 wm_rt[t-3]^2 + -0.028 wm_rt[t-3]*wm_rt[t-4] + 0.126 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 33/400 --- L(Train): 0.5414032 --- L(Val, RNN): 0.5277186 --- L(Val, SINDy): 1.0041955 --- Time: 4.01s; --- Convergence: 7.73e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.082 1 + 0.853 value_reward_diff[t] + 0.07 reward_diff + -0.104 value_reward_diff^2 + -0.099 value_reward_diff*reward_diff + 0.073 reward_diff^2 \n",
      "value_persistance[t+1] = -0.065 1 + 0.905 value_persistance[t] + -0.058 repeat + 0.133 value_persistance^2 + -0.15 value_persistance*repeat + -0.059 repeat^2 \n",
      "wm_rt[t+1] = -0.019 1 + 1.066 wm_rt[t] + -0.042 repeat + 0.033 switch + -0.08 wm_rt[t-1] + -0.111 wm_rt[t-2] + -0.039 wm_rt[t-3] + 0.193 wm_rt[t-4] + 0.098 wm_rt^2 + 0.043 wm_rt*repeat + 0.045 wm_rt*switch + 0.048 wm_rt*wm_rt[t-1] + 0.053 wm_rt*wm_rt[t-2] + 0.02 wm_rt*wm_rt[t-3] + -0.066 wm_rt*wm_rt[t-4] + -0.045 repeat^2 + 0.0 repeat*switch + -0.051 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-2] + -0.028 repeat*wm_rt[t-3] + 0.26 repeat*wm_rt[t-4] + 0.034 switch^2 + -0.088 switch*wm_rt[t-1] + -0.113 switch*wm_rt[t-2] + -0.043 switch*wm_rt[t-3] + 0.111 switch*wm_rt[t-4] + -0.108 wm_rt[t-1]^2 + -0.107 wm_rt[t-1]*wm_rt[t-2] + -0.106 wm_rt[t-1]*wm_rt[t-3] + -0.103 wm_rt[t-1]*wm_rt[t-4] + -0.16 wm_rt[t-2]^2 + -0.157 wm_rt[t-2]*wm_rt[t-3] + -0.153 wm_rt[t-2]*wm_rt[t-4] + -0.035 wm_rt[t-3]^2 + -0.028 wm_rt[t-3]*wm_rt[t-4] + 0.138 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 34/400 --- L(Train): 0.5410597 --- L(Val, RNN): 0.5215622 --- L(Val, SINDy): 1.0862093 --- Time: 4.19s; --- Convergence: 6.94e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.087 1 + 0.843 value_reward_diff[t] + 0.075 reward_diff + -0.114 value_reward_diff^2 + -0.101 value_reward_diff*reward_diff + 0.078 reward_diff^2 \n",
      "value_persistance[t+1] = -0.074 1 + 0.904 value_persistance[t] + -0.06 repeat + 0.133 value_persistance^2 + -0.151 value_persistance*repeat + -0.061 repeat^2 \n",
      "wm_rt[t+1] = -0.026 1 + 1.066 wm_rt[t] + -0.039 repeat + 0.025 switch + -0.075 wm_rt[t-1] + -0.115 wm_rt[t-2] + -0.043 wm_rt[t-3] + 0.204 wm_rt[t-4] + 0.098 wm_rt^2 + 0.042 wm_rt*repeat + 0.044 wm_rt*switch + 0.047 wm_rt*wm_rt[t-1] + 0.055 wm_rt*wm_rt[t-2] + 0.021 wm_rt*wm_rt[t-3] + -0.073 wm_rt*wm_rt[t-4] + -0.042 repeat^2 + 0.0 repeat*switch + -0.048 repeat*wm_rt[t-1] + -0.084 repeat*wm_rt[t-2] + -0.037 repeat*wm_rt[t-3] + 0.27 repeat*wm_rt[t-4] + 0.025 switch^2 + -0.082 switch*wm_rt[t-1] + -0.114 switch*wm_rt[t-2] + -0.042 switch*wm_rt[t-3] + 0.121 switch*wm_rt[t-4] + -0.1 wm_rt[t-1]^2 + -0.099 wm_rt[t-1]*wm_rt[t-2] + -0.098 wm_rt[t-1]*wm_rt[t-3] + -0.095 wm_rt[t-1]*wm_rt[t-4] + -0.163 wm_rt[t-2]^2 + -0.16 wm_rt[t-2]*wm_rt[t-3] + -0.155 wm_rt[t-2]*wm_rt[t-4] + -0.036 wm_rt[t-3]^2 + -0.028 wm_rt[t-3]*wm_rt[t-4] + 0.148 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 35/400 --- L(Train): 0.5387267 --- L(Val, RNN): 0.5164880 --- L(Val, SINDy): 1.0864505 --- Time: 4.04s; --- Convergence: 6.01e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.091 1 + 0.832 value_reward_diff[t] + 0.082 reward_diff + -0.126 value_reward_diff^2 + -0.098 value_reward_diff*reward_diff + 0.086 reward_diff^2 \n",
      "value_persistance[t+1] = -0.084 1 + 0.903 value_persistance[t] + -0.061 repeat + 0.134 value_persistance^2 + -0.152 value_persistance*repeat + -0.063 repeat^2 \n",
      "wm_rt[t+1] = -0.033 1 + 1.065 wm_rt[t] + -0.036 repeat + 0.017 switch + -0.069 wm_rt[t-1] + -0.119 wm_rt[t-2] + -0.047 wm_rt[t-3] + 0.215 wm_rt[t-4] + 0.099 wm_rt^2 + 0.041 wm_rt*repeat + 0.043 wm_rt*switch + 0.046 wm_rt*wm_rt[t-1] + 0.057 wm_rt*wm_rt[t-2] + 0.022 wm_rt*wm_rt[t-3] + -0.079 wm_rt*wm_rt[t-4] + -0.039 repeat^2 + -0.0 repeat*switch + -0.047 repeat*wm_rt[t-1] + -0.092 repeat*wm_rt[t-2] + -0.048 repeat*wm_rt[t-3] + 0.279 repeat*wm_rt[t-4] + 0.018 switch^2 + -0.075 switch*wm_rt[t-1] + -0.114 switch*wm_rt[t-2] + -0.04 switch*wm_rt[t-3] + 0.131 switch*wm_rt[t-4] + -0.091 wm_rt[t-1]^2 + -0.09 wm_rt[t-1]*wm_rt[t-2] + -0.09 wm_rt[t-1]*wm_rt[t-3] + -0.086 wm_rt[t-1]*wm_rt[t-4] + -0.166 wm_rt[t-2]^2 + -0.163 wm_rt[t-2]*wm_rt[t-3] + -0.158 wm_rt[t-2]*wm_rt[t-4] + -0.037 wm_rt[t-3]^2 + -0.029 wm_rt[t-3]*wm_rt[t-4] + 0.159 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 36/400 --- L(Train): 0.5278634 --- L(Val, RNN): 0.5116169 --- L(Val, SINDy): 1.0755494 --- Time: 3.62s; --- Convergence: 5.44e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.095 1 + 0.822 value_reward_diff[t] + 0.091 reward_diff + -0.137 value_reward_diff^2 + -0.094 value_reward_diff*reward_diff + 0.095 reward_diff^2 \n",
      "value_persistance[t+1] = -0.092 1 + 0.902 value_persistance[t] + -0.063 repeat + 0.135 value_persistance^2 + -0.153 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = -0.039 1 + 1.065 wm_rt[t] + -0.034 repeat + 0.01 switch + -0.063 wm_rt[t-1] + -0.122 wm_rt[t-2] + -0.051 wm_rt[t-3] + 0.225 wm_rt[t-4] + 0.099 wm_rt^2 + 0.041 wm_rt*repeat + 0.042 wm_rt*switch + 0.046 wm_rt*wm_rt[t-1] + 0.059 wm_rt*wm_rt[t-2] + 0.024 wm_rt*wm_rt[t-3] + -0.085 wm_rt*wm_rt[t-4] + -0.037 repeat^2 + -0.0 repeat*switch + -0.045 repeat*wm_rt[t-1] + -0.1 repeat*wm_rt[t-2] + -0.058 repeat*wm_rt[t-3] + 0.287 repeat*wm_rt[t-4] + 0.011 switch^2 + -0.068 switch*wm_rt[t-1] + -0.114 switch*wm_rt[t-2] + -0.038 switch*wm_rt[t-3] + 0.14 switch*wm_rt[t-4] + -0.083 wm_rt[t-1]^2 + -0.083 wm_rt[t-1]*wm_rt[t-2] + -0.082 wm_rt[t-1]*wm_rt[t-3] + -0.078 wm_rt[t-1]*wm_rt[t-4] + -0.169 wm_rt[t-2]^2 + -0.166 wm_rt[t-2]*wm_rt[t-3] + -0.161 wm_rt[t-2]*wm_rt[t-4] + -0.039 wm_rt[t-3]^2 + -0.03 wm_rt[t-3]*wm_rt[t-4] + 0.169 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 37/400 --- L(Train): 0.5204298 --- L(Val, RNN): 0.5054974 --- L(Val, SINDy): 1.0485463 --- Time: 3.64s; --- Convergence: 5.78e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.102 1 + 0.813 value_reward_diff[t] + 0.101 reward_diff + -0.148 value_reward_diff^2 + -0.088 value_reward_diff*reward_diff + 0.105 reward_diff^2 \n",
      "value_persistance[t+1] = -0.101 1 + 0.902 value_persistance[t] + -0.064 repeat + 0.135 value_persistance^2 + -0.154 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = -0.045 1 + 1.064 wm_rt[t] + -0.032 repeat + 0.006 switch + -0.057 wm_rt[t-1] + -0.126 wm_rt[t-2] + -0.056 wm_rt[t-3] + 0.234 wm_rt[t-4] + 0.099 wm_rt^2 + 0.04 wm_rt*repeat + 0.042 wm_rt*switch + 0.045 wm_rt*wm_rt[t-1] + 0.062 wm_rt*wm_rt[t-2] + 0.026 wm_rt*wm_rt[t-3] + -0.09 wm_rt*wm_rt[t-4] + -0.034 repeat^2 + -0.0 repeat*switch + -0.044 repeat*wm_rt[t-1] + -0.108 repeat*wm_rt[t-2] + -0.069 repeat*wm_rt[t-3] + 0.294 repeat*wm_rt[t-4] + 0.006 switch^2 + -0.061 switch*wm_rt[t-1] + -0.114 switch*wm_rt[t-2] + -0.035 switch*wm_rt[t-3] + 0.15 switch*wm_rt[t-4] + -0.075 wm_rt[t-1]^2 + -0.075 wm_rt[t-1]*wm_rt[t-2] + -0.074 wm_rt[t-1]*wm_rt[t-3] + -0.07 wm_rt[t-1]*wm_rt[t-4] + -0.174 wm_rt[t-2]^2 + -0.171 wm_rt[t-2]*wm_rt[t-3] + -0.165 wm_rt[t-2]*wm_rt[t-4] + -0.043 wm_rt[t-3]^2 + -0.033 wm_rt[t-3]*wm_rt[t-4] + 0.178 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 38/400 --- L(Train): 0.5162535 --- L(Val, RNN): 0.4982383 --- L(Val, SINDy): 1.0696669 --- Time: 3.78s; --- Convergence: 6.52e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.109 1 + 0.803 value_reward_diff[t] + 0.111 reward_diff + -0.16 value_reward_diff^2 + -0.079 value_reward_diff*reward_diff + 0.115 reward_diff^2 \n",
      "value_persistance[t+1] = -0.11 1 + 0.901 value_persistance[t] + -0.064 repeat + 0.136 value_persistance^2 + -0.155 value_persistance*repeat + -0.066 repeat^2 \n",
      "wm_rt[t+1] = -0.05 1 + 1.064 wm_rt[t] + -0.03 repeat + 0.003 switch + -0.051 wm_rt[t-1] + -0.13 wm_rt[t-2] + -0.061 wm_rt[t-3] + 0.243 wm_rt[t-4] + 0.1 wm_rt^2 + 0.039 wm_rt*repeat + 0.041 wm_rt*switch + 0.044 wm_rt*wm_rt[t-1] + 0.064 wm_rt*wm_rt[t-2] + 0.028 wm_rt*wm_rt[t-3] + -0.095 wm_rt*wm_rt[t-4] + -0.033 repeat^2 + -0.0 repeat*switch + -0.043 repeat*wm_rt[t-1] + -0.118 repeat*wm_rt[t-2] + -0.081 repeat*wm_rt[t-3] + 0.3 repeat*wm_rt[t-4] + 0.004 switch^2 + -0.053 switch*wm_rt[t-1] + -0.114 switch*wm_rt[t-2] + -0.032 switch*wm_rt[t-3] + 0.16 switch*wm_rt[t-4] + -0.068 wm_rt[t-1]^2 + -0.067 wm_rt[t-1]*wm_rt[t-2] + -0.066 wm_rt[t-1]*wm_rt[t-3] + -0.062 wm_rt[t-1]*wm_rt[t-4] + -0.18 wm_rt[t-2]^2 + -0.177 wm_rt[t-2]*wm_rt[t-3] + -0.17 wm_rt[t-2]*wm_rt[t-4] + -0.047 wm_rt[t-3]^2 + -0.037 wm_rt[t-3]*wm_rt[t-4] + 0.187 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 39/400 --- L(Train): 0.5158396 --- L(Val, RNN): 0.4902280 --- L(Val, SINDy): 1.1003716 --- Time: 3.91s; --- Convergence: 7.26e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.116 1 + 0.792 value_reward_diff[t] + 0.123 reward_diff + -0.172 value_reward_diff^2 + -0.07 value_reward_diff*reward_diff + 0.127 reward_diff^2 \n",
      "value_persistance[t+1] = -0.117 1 + 0.901 value_persistance[t] + -0.064 repeat + 0.136 value_persistance^2 + -0.156 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = -0.055 1 + 1.064 wm_rt[t] + -0.029 repeat + 0.003 switch + -0.045 wm_rt[t-1] + -0.135 wm_rt[t-2] + -0.067 wm_rt[t-3] + 0.252 wm_rt[t-4] + 0.1 wm_rt^2 + 0.039 wm_rt*repeat + 0.041 wm_rt*switch + 0.044 wm_rt*wm_rt[t-1] + 0.067 wm_rt*wm_rt[t-2] + 0.03 wm_rt*wm_rt[t-3] + -0.1 wm_rt*wm_rt[t-4] + -0.031 repeat^2 + 0.0 repeat*switch + -0.044 repeat*wm_rt[t-1] + -0.127 repeat*wm_rt[t-2] + -0.093 repeat*wm_rt[t-3] + 0.303 repeat*wm_rt[t-4] + 0.004 switch^2 + -0.046 switch*wm_rt[t-1] + -0.114 switch*wm_rt[t-2] + -0.027 switch*wm_rt[t-3] + 0.171 switch*wm_rt[t-4] + -0.061 wm_rt[t-1]^2 + -0.06 wm_rt[t-1]*wm_rt[t-2] + -0.06 wm_rt[t-1]*wm_rt[t-3] + -0.055 wm_rt[t-1]*wm_rt[t-4] + -0.187 wm_rt[t-2]^2 + -0.185 wm_rt[t-2]*wm_rt[t-3] + -0.178 wm_rt[t-2]*wm_rt[t-4] + -0.053 wm_rt[t-3]^2 + -0.042 wm_rt[t-3]*wm_rt[t-4] + 0.195 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 40/400 --- L(Train): 0.5032352 --- L(Val, RNN): 0.4809965 --- L(Val, SINDy): 1.0117245 --- Time: 4.72s; --- Convergence: 8.25e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.124 1 + 0.781 value_reward_diff[t] + 0.134 reward_diff + -0.184 value_reward_diff^2 + -0.062 value_reward_diff*reward_diff + 0.138 reward_diff^2 \n",
      "value_persistance[t+1] = -0.125 1 + 0.9 value_persistance[t] + -0.063 repeat + 0.137 value_persistance^2 + -0.157 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = -0.059 1 + 1.063 wm_rt[t] + -0.027 repeat + 0.004 switch + -0.039 wm_rt[t-1] + -0.139 wm_rt[t-2] + -0.073 wm_rt[t-3] + 0.261 wm_rt[t-4] + 0.1 wm_rt^2 + 0.038 wm_rt*repeat + 0.041 wm_rt*switch + 0.044 wm_rt*wm_rt[t-1] + 0.07 wm_rt*wm_rt[t-2] + 0.032 wm_rt*wm_rt[t-3] + -0.104 wm_rt*wm_rt[t-4] + -0.03 repeat^2 + 0.0 repeat*switch + -0.044 repeat*wm_rt[t-1] + -0.136 repeat*wm_rt[t-2] + -0.105 repeat*wm_rt[t-3] + 0.306 repeat*wm_rt[t-4] + 0.005 switch^2 + -0.039 switch*wm_rt[t-1] + -0.115 switch*wm_rt[t-2] + -0.023 switch*wm_rt[t-3] + 0.182 switch*wm_rt[t-4] + -0.055 wm_rt[t-1]^2 + -0.054 wm_rt[t-1]*wm_rt[t-2] + -0.054 wm_rt[t-1]*wm_rt[t-3] + -0.049 wm_rt[t-1]*wm_rt[t-4] + -0.195 wm_rt[t-2]^2 + -0.193 wm_rt[t-2]*wm_rt[t-3] + -0.185 wm_rt[t-2]*wm_rt[t-4] + -0.059 wm_rt[t-3]^2 + -0.048 wm_rt[t-3]*wm_rt[t-4] + 0.203 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 41/400 --- L(Train): 0.4988053 --- L(Val, RNN): 0.4716820 --- L(Val, SINDy): 0.9841402 --- Time: 4.03s; --- Convergence: 8.78e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.131 1 + 0.77 value_reward_diff[t] + 0.144 reward_diff + -0.196 value_reward_diff^2 + -0.054 value_reward_diff*reward_diff + 0.148 reward_diff^2 \n",
      "value_persistance[t+1] = -0.131 1 + 0.9 value_persistance[t] + -0.063 repeat + 0.137 value_persistance^2 + -0.158 value_persistance*repeat + -0.064 repeat^2 \n",
      "wm_rt[t+1] = -0.062 1 + 1.063 wm_rt[t] + -0.027 repeat + 0.007 switch + -0.034 wm_rt[t-1] + -0.143 wm_rt[t-2] + -0.079 wm_rt[t-3] + 0.269 wm_rt[t-4] + 0.1 wm_rt^2 + 0.037 wm_rt*repeat + 0.041 wm_rt*switch + 0.043 wm_rt*wm_rt[t-1] + 0.072 wm_rt*wm_rt[t-2] + 0.034 wm_rt*wm_rt[t-3] + -0.108 wm_rt*wm_rt[t-4] + -0.029 repeat^2 + 0.0 repeat*switch + -0.045 repeat*wm_rt[t-1] + -0.143 repeat*wm_rt[t-2] + -0.117 repeat*wm_rt[t-3] + 0.307 repeat*wm_rt[t-4] + 0.007 switch^2 + -0.032 switch*wm_rt[t-1] + -0.114 switch*wm_rt[t-2] + -0.019 switch*wm_rt[t-3] + 0.193 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.049 wm_rt[t-1]*wm_rt[t-2] + -0.048 wm_rt[t-1]*wm_rt[t-3] + -0.044 wm_rt[t-1]*wm_rt[t-4] + -0.202 wm_rt[t-2]^2 + -0.2 wm_rt[t-2]*wm_rt[t-3] + -0.192 wm_rt[t-2]*wm_rt[t-4] + -0.066 wm_rt[t-3]^2 + -0.054 wm_rt[t-3]*wm_rt[t-4] + 0.21 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 42/400 --- L(Train): 0.4988008 --- L(Val, RNN): 0.4636399 --- L(Val, SINDy): 0.9800327 --- Time: 3.59s; --- Convergence: 8.41e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.134 1 + 0.758 value_reward_diff[t] + 0.153 reward_diff + -0.209 value_reward_diff^2 + -0.049 value_reward_diff*reward_diff + 0.158 reward_diff^2 \n",
      "value_persistance[t+1] = -0.137 1 + 0.899 value_persistance[t] + -0.063 repeat + 0.138 value_persistance^2 + -0.159 value_persistance*repeat + -0.064 repeat^2 \n",
      "wm_rt[t+1] = -0.065 1 + 1.062 wm_rt[t] + -0.026 repeat + 0.01 switch + -0.03 wm_rt[t-1] + -0.146 wm_rt[t-2] + -0.086 wm_rt[t-3] + 0.277 wm_rt[t-4] + 0.1 wm_rt^2 + 0.037 wm_rt*repeat + 0.041 wm_rt*switch + 0.043 wm_rt*wm_rt[t-1] + 0.075 wm_rt*wm_rt[t-2] + 0.037 wm_rt*wm_rt[t-3] + -0.112 wm_rt*wm_rt[t-4] + -0.028 repeat^2 + -0.0 repeat*switch + -0.045 repeat*wm_rt[t-1] + -0.149 repeat*wm_rt[t-2] + -0.129 repeat*wm_rt[t-3] + 0.308 repeat*wm_rt[t-4] + 0.011 switch^2 + -0.026 switch*wm_rt[t-1] + -0.115 switch*wm_rt[t-2] + -0.016 switch*wm_rt[t-3] + 0.204 switch*wm_rt[t-4] + -0.046 wm_rt[t-1]^2 + -0.045 wm_rt[t-1]*wm_rt[t-2] + -0.044 wm_rt[t-1]*wm_rt[t-3] + -0.04 wm_rt[t-1]*wm_rt[t-4] + -0.21 wm_rt[t-2]^2 + -0.208 wm_rt[t-2]*wm_rt[t-3] + -0.2 wm_rt[t-2]*wm_rt[t-4] + -0.074 wm_rt[t-3]^2 + -0.061 wm_rt[t-3]*wm_rt[t-4] + 0.217 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 43/400 --- L(Train): 0.4915317 --- L(Val, RNN): 0.4562699 --- L(Val, SINDy): 0.9563321 --- Time: 4.64s; --- Convergence: 7.89e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.14 1 + 0.747 value_reward_diff[t] + 0.163 reward_diff + -0.222 value_reward_diff^2 + -0.044 value_reward_diff*reward_diff + 0.168 reward_diff^2 \n",
      "value_persistance[t+1] = -0.141 1 + 0.899 value_persistance[t] + -0.063 repeat + 0.139 value_persistance^2 + -0.16 value_persistance*repeat + -0.064 repeat^2 \n",
      "wm_rt[t+1] = -0.066 1 + 1.062 wm_rt[t] + -0.024 repeat + 0.015 switch + -0.026 wm_rt[t-1] + -0.147 wm_rt[t-2] + -0.093 wm_rt[t-3] + 0.285 wm_rt[t-4] + 0.101 wm_rt^2 + 0.036 wm_rt*repeat + 0.041 wm_rt*switch + 0.043 wm_rt*wm_rt[t-1] + 0.077 wm_rt*wm_rt[t-2] + 0.04 wm_rt*wm_rt[t-3] + -0.116 wm_rt*wm_rt[t-4] + -0.027 repeat^2 + -0.0 repeat*switch + -0.046 repeat*wm_rt[t-1] + -0.151 repeat*wm_rt[t-2] + -0.14 repeat*wm_rt[t-3] + 0.309 repeat*wm_rt[t-4] + 0.016 switch^2 + -0.021 switch*wm_rt[t-1] + -0.116 switch*wm_rt[t-2] + -0.014 switch*wm_rt[t-3] + 0.215 switch*wm_rt[t-4] + -0.043 wm_rt[t-1]^2 + -0.043 wm_rt[t-1]*wm_rt[t-2] + -0.042 wm_rt[t-1]*wm_rt[t-3] + -0.037 wm_rt[t-1]*wm_rt[t-4] + -0.217 wm_rt[t-2]^2 + -0.216 wm_rt[t-2]*wm_rt[t-3] + -0.207 wm_rt[t-2]*wm_rt[t-4] + -0.083 wm_rt[t-3]^2 + -0.069 wm_rt[t-3]*wm_rt[t-4] + 0.223 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 44/400 --- L(Train): 0.4889843 --- L(Val, RNN): 0.4490350 --- L(Val, SINDy): 0.9796510 --- Time: 4.26s; --- Convergence: 7.56e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.143 1 + 0.735 value_reward_diff[t] + 0.173 reward_diff + -0.236 value_reward_diff^2 + -0.038 value_reward_diff*reward_diff + 0.178 reward_diff^2 \n",
      "value_persistance[t+1] = -0.145 1 + 0.898 value_persistance[t] + -0.063 repeat + 0.139 value_persistance^2 + -0.16 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = -0.066 1 + 1.062 wm_rt[t] + -0.023 repeat + 0.021 switch + -0.022 wm_rt[t-1] + -0.146 wm_rt[t-2] + -0.098 wm_rt[t-3] + 0.293 wm_rt[t-4] + 0.101 wm_rt^2 + 0.036 wm_rt*repeat + 0.041 wm_rt*switch + 0.043 wm_rt*wm_rt[t-1] + 0.078 wm_rt*wm_rt[t-2] + 0.042 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + -0.025 repeat^2 + 0.0 repeat*switch + -0.046 repeat*wm_rt[t-1] + -0.148 repeat*wm_rt[t-2] + -0.147 repeat*wm_rt[t-3] + 0.309 repeat*wm_rt[t-4] + 0.022 switch^2 + -0.017 switch*wm_rt[t-1] + -0.117 switch*wm_rt[t-2] + -0.013 switch*wm_rt[t-3] + 0.226 switch*wm_rt[t-4] + -0.041 wm_rt[t-1]^2 + -0.041 wm_rt[t-1]*wm_rt[t-2] + -0.04 wm_rt[t-1]*wm_rt[t-3] + -0.035 wm_rt[t-1]*wm_rt[t-4] + -0.222 wm_rt[t-2]^2 + -0.221 wm_rt[t-2]*wm_rt[t-3] + -0.212 wm_rt[t-2]*wm_rt[t-4] + -0.091 wm_rt[t-3]^2 + -0.077 wm_rt[t-3]*wm_rt[t-4] + 0.229 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 45/400 --- L(Train): 0.4871775 --- L(Val, RNN): 0.4421680 --- L(Val, SINDy): 0.9406555 --- Time: 4.80s; --- Convergence: 7.21e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.138 1 + 0.725 value_reward_diff[t] + 0.184 reward_diff + -0.247 value_reward_diff^2 + -0.03 value_reward_diff*reward_diff + 0.189 reward_diff^2 \n",
      "value_persistance[t+1] = -0.148 1 + 0.898 value_persistance[t] + -0.064 repeat + 0.14 value_persistance^2 + -0.16 value_persistance*repeat + -0.066 repeat^2 \n",
      "wm_rt[t+1] = -0.065 1 + 1.062 wm_rt[t] + -0.021 repeat + 0.027 switch + -0.02 wm_rt[t-1] + -0.143 wm_rt[t-2] + -0.104 wm_rt[t-3] + 0.3 wm_rt[t-4] + 0.101 wm_rt^2 + 0.035 wm_rt*repeat + 0.041 wm_rt*switch + 0.043 wm_rt*wm_rt[t-1] + 0.079 wm_rt*wm_rt[t-2] + 0.045 wm_rt*wm_rt[t-3] + -0.123 wm_rt*wm_rt[t-4] + -0.023 repeat^2 + 0.0 repeat*switch + -0.046 repeat*wm_rt[t-1] + -0.144 repeat*wm_rt[t-2] + -0.153 repeat*wm_rt[t-3] + 0.309 repeat*wm_rt[t-4] + 0.028 switch^2 + -0.014 switch*wm_rt[t-1] + -0.119 switch*wm_rt[t-2] + -0.015 switch*wm_rt[t-3] + 0.237 switch*wm_rt[t-4] + -0.041 wm_rt[t-1]^2 + -0.041 wm_rt[t-1]*wm_rt[t-2] + -0.04 wm_rt[t-1]*wm_rt[t-3] + -0.035 wm_rt[t-1]*wm_rt[t-4] + -0.227 wm_rt[t-2]^2 + -0.226 wm_rt[t-2]*wm_rt[t-3] + -0.217 wm_rt[t-2]*wm_rt[t-4] + -0.099 wm_rt[t-3]^2 + -0.085 wm_rt[t-3]*wm_rt[t-4] + 0.235 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 46/400 --- L(Train): 0.4892641 --- L(Val, RNN): 0.4358011 --- L(Val, SINDy): 0.8253942 --- Time: 4.42s; --- Convergence: 6.79e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.129 1 + 0.714 value_reward_diff[t] + 0.195 reward_diff + -0.259 value_reward_diff^2 + -0.02 value_reward_diff*reward_diff + 0.2 reward_diff^2 \n",
      "value_persistance[t+1] = -0.151 1 + 0.897 value_persistance[t] + -0.067 repeat + 0.141 value_persistance^2 + -0.16 value_persistance*repeat + -0.068 repeat^2 \n",
      "wm_rt[t+1] = -0.064 1 + 1.061 wm_rt[t] + -0.018 repeat + 0.034 switch + -0.019 wm_rt[t-1] + -0.14 wm_rt[t-2] + -0.11 wm_rt[t-3] + 0.306 wm_rt[t-4] + 0.101 wm_rt^2 + 0.035 wm_rt*repeat + 0.042 wm_rt*switch + 0.042 wm_rt*wm_rt[t-1] + 0.08 wm_rt*wm_rt[t-2] + 0.047 wm_rt*wm_rt[t-3] + -0.126 wm_rt*wm_rt[t-4] + -0.021 repeat^2 + -0.0 repeat*switch + -0.045 repeat*wm_rt[t-1] + -0.137 repeat*wm_rt[t-2] + -0.158 repeat*wm_rt[t-3] + 0.308 repeat*wm_rt[t-4] + 0.035 switch^2 + -0.013 switch*wm_rt[t-1] + -0.123 switch*wm_rt[t-2] + -0.018 switch*wm_rt[t-3] + 0.246 switch*wm_rt[t-4] + -0.043 wm_rt[t-1]^2 + -0.042 wm_rt[t-1]*wm_rt[t-2] + -0.041 wm_rt[t-1]*wm_rt[t-3] + -0.036 wm_rt[t-1]*wm_rt[t-4] + -0.232 wm_rt[t-2]^2 + -0.23 wm_rt[t-2]*wm_rt[t-3] + -0.221 wm_rt[t-2]*wm_rt[t-4] + -0.109 wm_rt[t-3]^2 + -0.094 wm_rt[t-3]*wm_rt[t-4] + 0.239 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 47/400 --- L(Train): 0.4775108 --- L(Val, RNN): 0.4300346 --- L(Val, SINDy): 0.8031557 --- Time: 3.44s; --- Convergence: 6.28e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.12 1 + 0.702 value_reward_diff[t] + 0.206 reward_diff + -0.271 value_reward_diff^2 + -0.009 value_reward_diff*reward_diff + 0.211 reward_diff^2 \n",
      "value_persistance[t+1] = -0.153 1 + 0.896 value_persistance[t] + -0.07 repeat + 0.141 value_persistance^2 + -0.16 value_persistance*repeat + -0.072 repeat^2 \n",
      "wm_rt[t+1] = -0.062 1 + 1.061 wm_rt[t] + -0.016 repeat + 0.041 switch + -0.018 wm_rt[t-1] + -0.134 wm_rt[t-2] + -0.114 wm_rt[t-3] + 0.311 wm_rt[t-4] + 0.101 wm_rt^2 + 0.035 wm_rt*repeat + 0.042 wm_rt*switch + 0.042 wm_rt*wm_rt[t-1] + 0.08 wm_rt*wm_rt[t-2] + 0.049 wm_rt*wm_rt[t-3] + -0.129 wm_rt*wm_rt[t-4] + -0.019 repeat^2 + -0.0 repeat*switch + -0.045 repeat*wm_rt[t-1] + -0.128 repeat*wm_rt[t-2] + -0.158 repeat*wm_rt[t-3] + 0.307 repeat*wm_rt[t-4] + 0.042 switch^2 + -0.013 switch*wm_rt[t-1] + -0.126 switch*wm_rt[t-2] + -0.023 switch*wm_rt[t-3] + 0.255 switch*wm_rt[t-4] + -0.045 wm_rt[t-1]^2 + -0.044 wm_rt[t-1]*wm_rt[t-2] + -0.044 wm_rt[t-1]*wm_rt[t-3] + -0.038 wm_rt[t-1]*wm_rt[t-4] + -0.232 wm_rt[t-2]^2 + -0.231 wm_rt[t-2]*wm_rt[t-3] + -0.222 wm_rt[t-2]*wm_rt[t-4] + -0.117 wm_rt[t-3]^2 + -0.101 wm_rt[t-3]*wm_rt[t-4] + 0.243 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 48/400 --- L(Train): 0.4774495 --- L(Val, RNN): 0.4239429 --- L(Val, SINDy): 0.7550119 --- Time: 3.37s; --- Convergence: 6.19e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.108 1 + 0.69 value_reward_diff[t] + 0.218 reward_diff + -0.284 value_reward_diff^2 + 0.003 value_reward_diff*reward_diff + 0.223 reward_diff^2 \n",
      "value_persistance[t+1] = -0.154 1 + 0.896 value_persistance[t] + -0.074 repeat + 0.142 value_persistance^2 + -0.159 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = -0.06 1 + 1.061 wm_rt[t] + -0.014 repeat + 0.047 switch + -0.019 wm_rt[t-1] + -0.127 wm_rt[t-2] + -0.118 wm_rt[t-3] + 0.315 wm_rt[t-4] + 0.101 wm_rt^2 + 0.034 wm_rt*repeat + 0.043 wm_rt*switch + 0.042 wm_rt*wm_rt[t-1] + 0.079 wm_rt*wm_rt[t-2] + 0.051 wm_rt*wm_rt[t-3] + -0.13 wm_rt*wm_rt[t-4] + -0.017 repeat^2 + 0.0 repeat*switch + -0.045 repeat*wm_rt[t-1] + -0.118 repeat*wm_rt[t-2] + -0.155 repeat*wm_rt[t-3] + 0.306 repeat*wm_rt[t-4] + 0.048 switch^2 + -0.015 switch*wm_rt[t-1] + -0.131 switch*wm_rt[t-2] + -0.03 switch*wm_rt[t-3] + 0.263 switch*wm_rt[t-4] + -0.048 wm_rt[t-1]^2 + -0.047 wm_rt[t-1]*wm_rt[t-2] + -0.046 wm_rt[t-1]*wm_rt[t-3] + -0.041 wm_rt[t-1]*wm_rt[t-4] + -0.23 wm_rt[t-2]^2 + -0.229 wm_rt[t-2]*wm_rt[t-3] + -0.22 wm_rt[t-2]*wm_rt[t-4] + -0.123 wm_rt[t-3]^2 + -0.107 wm_rt[t-3]*wm_rt[t-4] + 0.245 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 49/400 --- L(Train): 0.4768141 --- L(Val, RNN): 0.4172404 --- L(Val, SINDy): 0.7737536 --- Time: 4.37s; --- Convergence: 6.44e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.096 1 + 0.677 value_reward_diff[t] + 0.231 reward_diff + -0.297 value_reward_diff^2 + 0.015 value_reward_diff*reward_diff + 0.236 reward_diff^2 \n",
      "value_persistance[t+1] = -0.155 1 + 0.895 value_persistance[t] + -0.078 repeat + 0.143 value_persistance^2 + -0.158 value_persistance*repeat + -0.079 repeat^2 \n",
      "wm_rt[t+1] = -0.058 1 + 1.061 wm_rt[t] + -0.012 repeat + 0.053 switch + -0.02 wm_rt[t-1] + -0.119 wm_rt[t-2] + -0.122 wm_rt[t-3] + 0.318 wm_rt[t-4] + 0.101 wm_rt^2 + 0.034 wm_rt*repeat + 0.043 wm_rt*switch + 0.042 wm_rt*wm_rt[t-1] + 0.078 wm_rt*wm_rt[t-2] + 0.053 wm_rt*wm_rt[t-3] + -0.132 wm_rt*wm_rt[t-4] + -0.015 repeat^2 + -0.0 repeat*switch + -0.045 repeat*wm_rt[t-1] + -0.107 repeat*wm_rt[t-2] + -0.151 repeat*wm_rt[t-3] + 0.303 repeat*wm_rt[t-4] + 0.054 switch^2 + -0.016 switch*wm_rt[t-1] + -0.136 switch*wm_rt[t-2] + -0.038 switch*wm_rt[t-3] + 0.269 switch*wm_rt[t-4] + -0.051 wm_rt[t-1]^2 + -0.05 wm_rt[t-1]*wm_rt[t-2] + -0.05 wm_rt[t-1]*wm_rt[t-3] + -0.044 wm_rt[t-1]*wm_rt[t-4] + -0.227 wm_rt[t-2]^2 + -0.227 wm_rt[t-2]*wm_rt[t-3] + -0.217 wm_rt[t-2]*wm_rt[t-4] + -0.13 wm_rt[t-3]^2 + -0.114 wm_rt[t-3]*wm_rt[t-4] + 0.246 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 50/400 --- L(Train): 0.4769364 --- L(Val, RNN): 0.4097547 --- L(Val, SINDy): 0.7443320 --- Time: 3.61s; --- Convergence: 6.96e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.084 1 + 0.665 value_reward_diff[t] + 0.242 reward_diff + -0.307 value_reward_diff^2 + 0.028 value_reward_diff*reward_diff + 0.247 reward_diff^2 \n",
      "value_persistance[t+1] = -0.155 1 + 0.895 value_persistance[t] + -0.082 repeat + 0.144 value_persistance^2 + -0.157 value_persistance*repeat + -0.083 repeat^2 \n",
      "wm_rt[t+1] = -0.057 1 + 1.061 wm_rt[t] + -0.012 repeat + 0.057 switch + -0.023 wm_rt[t-1] + -0.11 wm_rt[t-2] + -0.124 wm_rt[t-3] + 0.319 wm_rt[t-4] + 0.101 wm_rt^2 + 0.034 wm_rt*repeat + 0.043 wm_rt*switch + 0.042 wm_rt*wm_rt[t-1] + 0.076 wm_rt*wm_rt[t-2] + 0.054 wm_rt*wm_rt[t-3] + -0.133 wm_rt*wm_rt[t-4] + -0.014 repeat^2 + 0.0 repeat*switch + -0.046 repeat*wm_rt[t-1] + -0.095 repeat*wm_rt[t-2] + -0.145 repeat*wm_rt[t-3] + 0.299 repeat*wm_rt[t-4] + 0.058 switch^2 + -0.02 switch*wm_rt[t-1] + -0.141 switch*wm_rt[t-2] + -0.047 switch*wm_rt[t-3] + 0.273 switch*wm_rt[t-4] + -0.055 wm_rt[t-1]^2 + -0.053 wm_rt[t-1]*wm_rt[t-2] + -0.053 wm_rt[t-1]*wm_rt[t-3] + -0.048 wm_rt[t-1]*wm_rt[t-4] + -0.222 wm_rt[t-2]^2 + -0.221 wm_rt[t-2]*wm_rt[t-3] + -0.212 wm_rt[t-2]*wm_rt[t-4] + -0.134 wm_rt[t-3]^2 + -0.118 wm_rt[t-3]*wm_rt[t-4] + 0.246 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 51/400 --- L(Train): 0.4577452 --- L(Val, RNN): 0.4023500 --- L(Val, SINDy): 0.7418557 --- Time: 4.10s; --- Convergence: 7.18e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.071 1 + 0.653 value_reward_diff[t] + 0.255 reward_diff + -0.319 value_reward_diff^2 + 0.034 value_reward_diff*reward_diff + 0.259 reward_diff^2 \n",
      "value_persistance[t+1] = -0.156 1 + 0.894 value_persistance[t] + -0.085 repeat + 0.145 value_persistance^2 + -0.157 value_persistance*repeat + -0.087 repeat^2 \n",
      "wm_rt[t+1] = -0.058 1 + 1.061 wm_rt[t] + -0.012 repeat + 0.06 switch + -0.026 wm_rt[t-1] + -0.1 wm_rt[t-2] + -0.126 wm_rt[t-3] + 0.317 wm_rt[t-4] + 0.101 wm_rt^2 + 0.033 wm_rt*repeat + 0.043 wm_rt*switch + 0.041 wm_rt*wm_rt[t-1] + 0.073 wm_rt*wm_rt[t-2] + 0.055 wm_rt*wm_rt[t-3] + -0.132 wm_rt*wm_rt[t-4] + -0.015 repeat^2 + 0.0 repeat*switch + -0.049 repeat*wm_rt[t-1] + -0.082 repeat*wm_rt[t-2] + -0.138 repeat*wm_rt[t-3] + 0.294 repeat*wm_rt[t-4] + 0.06 switch^2 + -0.023 switch*wm_rt[t-1] + -0.148 switch*wm_rt[t-2] + -0.057 switch*wm_rt[t-3] + 0.276 switch*wm_rt[t-4] + -0.058 wm_rt[t-1]^2 + -0.057 wm_rt[t-1]*wm_rt[t-2] + -0.057 wm_rt[t-1]*wm_rt[t-3] + -0.051 wm_rt[t-1]*wm_rt[t-4] + -0.214 wm_rt[t-2]^2 + -0.214 wm_rt[t-2]*wm_rt[t-3] + -0.204 wm_rt[t-2]*wm_rt[t-4] + -0.137 wm_rt[t-3]^2 + -0.121 wm_rt[t-3]*wm_rt[t-4] + 0.244 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 52/400 --- L(Train): 0.4611425 --- L(Val, RNN): 0.3960317 --- L(Val, SINDy): 0.7502479 --- Time: 3.42s; --- Convergence: 6.75e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.057 1 + 0.64 value_reward_diff[t] + 0.268 reward_diff + -0.332 value_reward_diff^2 + 0.034 value_reward_diff*reward_diff + 0.273 reward_diff^2 \n",
      "value_persistance[t+1] = -0.157 1 + 0.893 value_persistance[t] + -0.089 repeat + 0.146 value_persistance^2 + -0.156 value_persistance*repeat + -0.091 repeat^2 \n",
      "wm_rt[t+1] = -0.059 1 + 1.06 wm_rt[t] + -0.013 repeat + 0.061 switch + -0.03 wm_rt[t-1] + -0.089 wm_rt[t-2] + -0.126 wm_rt[t-3] + 0.315 wm_rt[t-4] + 0.101 wm_rt^2 + 0.032 wm_rt*repeat + 0.043 wm_rt*switch + 0.041 wm_rt*wm_rt[t-1] + 0.07 wm_rt*wm_rt[t-2] + 0.056 wm_rt*wm_rt[t-3] + -0.132 wm_rt*wm_rt[t-4] + -0.016 repeat^2 + -0.0 repeat*switch + -0.052 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-2] + -0.128 repeat*wm_rt[t-3] + 0.287 repeat*wm_rt[t-4] + 0.062 switch^2 + -0.027 switch*wm_rt[t-1] + -0.154 switch*wm_rt[t-2] + -0.067 switch*wm_rt[t-3] + 0.278 switch*wm_rt[t-4] + -0.062 wm_rt[t-1]^2 + -0.06 wm_rt[t-1]*wm_rt[t-2] + -0.06 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.205 wm_rt[t-2]^2 + -0.205 wm_rt[t-2]*wm_rt[t-3] + -0.195 wm_rt[t-2]*wm_rt[t-4] + -0.138 wm_rt[t-3]^2 + -0.121 wm_rt[t-3]*wm_rt[t-4] + 0.241 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 53/400 --- L(Train): 0.4581492 --- L(Val, RNN): 0.3909329 --- L(Val, SINDy): 0.8189798 --- Time: 3.51s; --- Convergence: 5.93e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.044 1 + 0.626 value_reward_diff[t] + 0.282 reward_diff + -0.345 value_reward_diff^2 + 0.029 value_reward_diff*reward_diff + 0.287 reward_diff^2 \n",
      "value_persistance[t+1] = -0.159 1 + 0.893 value_persistance[t] + -0.092 repeat + 0.147 value_persistance^2 + -0.155 value_persistance*repeat + -0.094 repeat^2 \n",
      "wm_rt[t+1] = -0.061 1 + 1.06 wm_rt[t] + -0.014 repeat + 0.061 switch + -0.034 wm_rt[t-1] + -0.078 wm_rt[t-2] + -0.125 wm_rt[t-3] + 0.312 wm_rt[t-4] + 0.101 wm_rt^2 + 0.032 wm_rt*repeat + 0.043 wm_rt*switch + 0.04 wm_rt*wm_rt[t-1] + 0.067 wm_rt*wm_rt[t-2] + 0.056 wm_rt*wm_rt[t-3] + -0.131 wm_rt*wm_rt[t-4] + -0.017 repeat^2 + -0.0 repeat*switch + -0.056 repeat*wm_rt[t-1] + -0.056 repeat*wm_rt[t-2] + -0.119 repeat*wm_rt[t-3] + 0.281 repeat*wm_rt[t-4] + 0.062 switch^2 + -0.031 switch*wm_rt[t-1] + -0.161 switch*wm_rt[t-2] + -0.078 switch*wm_rt[t-3] + 0.279 switch*wm_rt[t-4] + -0.064 wm_rt[t-1]^2 + -0.062 wm_rt[t-1]*wm_rt[t-2] + -0.062 wm_rt[t-1]*wm_rt[t-3] + -0.057 wm_rt[t-1]*wm_rt[t-4] + -0.195 wm_rt[t-2]^2 + -0.195 wm_rt[t-2]*wm_rt[t-3] + -0.185 wm_rt[t-2]*wm_rt[t-4] + -0.137 wm_rt[t-3]^2 + -0.12 wm_rt[t-3]*wm_rt[t-4] + 0.238 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 54/400 --- L(Train): 0.4552364 --- L(Val, RNN): 0.3870969 --- L(Val, SINDy): 0.9093314 --- Time: 4.31s; --- Convergence: 4.88e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.03 1 + 0.611 value_reward_diff[t] + 0.296 reward_diff + -0.359 value_reward_diff^2 + 0.022 value_reward_diff*reward_diff + 0.301 reward_diff^2 \n",
      "value_persistance[t+1] = -0.162 1 + 0.892 value_persistance[t] + -0.094 repeat + 0.148 value_persistance^2 + -0.155 value_persistance*repeat + -0.095 repeat^2 \n",
      "wm_rt[t+1] = -0.064 1 + 1.06 wm_rt[t] + -0.015 repeat + 0.059 switch + -0.038 wm_rt[t-1] + -0.067 wm_rt[t-2] + -0.123 wm_rt[t-3] + 0.308 wm_rt[t-4] + 0.101 wm_rt^2 + 0.031 wm_rt*repeat + 0.042 wm_rt*switch + 0.04 wm_rt*wm_rt[t-1] + 0.064 wm_rt*wm_rt[t-2] + 0.056 wm_rt*wm_rt[t-3] + -0.13 wm_rt*wm_rt[t-4] + -0.018 repeat^2 + 0.0 repeat*switch + -0.06 repeat*wm_rt[t-1] + -0.042 repeat*wm_rt[t-2] + -0.108 repeat*wm_rt[t-3] + 0.273 repeat*wm_rt[t-4] + 0.06 switch^2 + -0.035 switch*wm_rt[t-1] + -0.167 switch*wm_rt[t-2] + -0.088 switch*wm_rt[t-3] + 0.28 switch*wm_rt[t-4] + -0.066 wm_rt[t-1]^2 + -0.064 wm_rt[t-1]*wm_rt[t-2] + -0.064 wm_rt[t-1]*wm_rt[t-3] + -0.059 wm_rt[t-1]*wm_rt[t-4] + -0.185 wm_rt[t-2]^2 + -0.185 wm_rt[t-2]*wm_rt[t-3] + -0.175 wm_rt[t-2]*wm_rt[t-4] + -0.135 wm_rt[t-3]^2 + -0.118 wm_rt[t-3]*wm_rt[t-4] + 0.234 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 55/400 --- L(Train): 0.4570052 --- L(Val, RNN): 0.3853231 --- L(Val, SINDy): 0.9395124 --- Time: 3.34s; --- Convergence: 3.33e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.018 1 + 0.596 value_reward_diff[t] + 0.311 reward_diff + -0.374 value_reward_diff^2 + 0.015 value_reward_diff*reward_diff + 0.316 reward_diff^2 \n",
      "value_persistance[t+1] = -0.165 1 + 0.891 value_persistance[t] + -0.094 repeat + 0.149 value_persistance^2 + -0.155 value_persistance*repeat + -0.096 repeat^2 \n",
      "wm_rt[t+1] = -0.067 1 + 1.059 wm_rt[t] + -0.015 repeat + 0.055 switch + -0.041 wm_rt[t-1] + -0.057 wm_rt[t-2] + -0.12 wm_rt[t-3] + 0.302 wm_rt[t-4] + 0.102 wm_rt^2 + 0.031 wm_rt*repeat + 0.041 wm_rt*switch + 0.039 wm_rt*wm_rt[t-1] + 0.061 wm_rt*wm_rt[t-2] + 0.056 wm_rt*wm_rt[t-3] + -0.128 wm_rt*wm_rt[t-4] + -0.018 repeat^2 + 0.0 repeat*switch + -0.064 repeat*wm_rt[t-1] + -0.029 repeat*wm_rt[t-2] + -0.098 repeat*wm_rt[t-3] + 0.263 repeat*wm_rt[t-4] + 0.056 switch^2 + -0.038 switch*wm_rt[t-1] + -0.175 switch*wm_rt[t-2] + -0.098 switch*wm_rt[t-3] + 0.281 switch*wm_rt[t-4] + -0.066 wm_rt[t-1]^2 + -0.064 wm_rt[t-1]*wm_rt[t-2] + -0.064 wm_rt[t-1]*wm_rt[t-3] + -0.059 wm_rt[t-1]*wm_rt[t-4] + -0.175 wm_rt[t-2]^2 + -0.176 wm_rt[t-2]*wm_rt[t-3] + -0.166 wm_rt[t-2]*wm_rt[t-4] + -0.132 wm_rt[t-3]^2 + -0.115 wm_rt[t-3]*wm_rt[t-4] + 0.228 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 56/400 --- L(Train): 0.4551848 --- L(Val, RNN): 0.3857719 --- L(Val, SINDy): 0.9633207 --- Time: 3.38s; --- Convergence: 1.89e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.011 1 + 0.583 value_reward_diff[t] + 0.326 reward_diff + -0.388 value_reward_diff^2 + 0.014 value_reward_diff*reward_diff + 0.331 reward_diff^2 \n",
      "value_persistance[t+1] = -0.168 1 + 0.891 value_persistance[t] + -0.094 repeat + 0.15 value_persistance^2 + -0.156 value_persistance*repeat + -0.096 repeat^2 \n",
      "wm_rt[t+1] = -0.07 1 + 1.059 wm_rt[t] + -0.016 repeat + 0.051 switch + -0.044 wm_rt[t-1] + -0.049 wm_rt[t-2] + -0.117 wm_rt[t-3] + 0.296 wm_rt[t-4] + 0.102 wm_rt^2 + 0.03 wm_rt*repeat + 0.041 wm_rt*switch + 0.038 wm_rt*wm_rt[t-1] + 0.059 wm_rt*wm_rt[t-2] + 0.056 wm_rt*wm_rt[t-3] + -0.126 wm_rt*wm_rt[t-4] + -0.019 repeat^2 + -0.0 repeat*switch + -0.067 repeat*wm_rt[t-1] + -0.018 repeat*wm_rt[t-2] + -0.088 repeat*wm_rt[t-3] + 0.253 repeat*wm_rt[t-4] + 0.052 switch^2 + -0.041 switch*wm_rt[t-1] + -0.183 switch*wm_rt[t-2] + -0.107 switch*wm_rt[t-3] + 0.282 switch*wm_rt[t-4] + -0.066 wm_rt[t-1]^2 + -0.063 wm_rt[t-1]*wm_rt[t-2] + -0.064 wm_rt[t-1]*wm_rt[t-3] + -0.059 wm_rt[t-1]*wm_rt[t-4] + -0.167 wm_rt[t-2]^2 + -0.168 wm_rt[t-2]*wm_rt[t-3] + -0.158 wm_rt[t-2]*wm_rt[t-4] + -0.128 wm_rt[t-3]^2 + -0.111 wm_rt[t-3]*wm_rt[t-4] + 0.222 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 57/400 --- L(Train): 0.4476250 --- L(Val, RNN): 0.3833385 --- L(Val, SINDy): 0.9108508 --- Time: 3.22s; --- Convergence: 2.16e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.015 1 + 0.578 value_reward_diff[t] + 0.341 reward_diff + -0.396 value_reward_diff^2 + 0.02 value_reward_diff*reward_diff + 0.346 reward_diff^2 \n",
      "value_persistance[t+1] = -0.172 1 + 0.89 value_persistance[t] + -0.094 repeat + 0.151 value_persistance^2 + -0.156 value_persistance*repeat + -0.095 repeat^2 \n",
      "wm_rt[t+1] = -0.072 1 + 1.059 wm_rt[t] + -0.015 repeat + 0.047 switch + -0.046 wm_rt[t-1] + -0.041 wm_rt[t-2] + -0.111 wm_rt[t-3] + 0.29 wm_rt[t-4] + 0.102 wm_rt^2 + 0.03 wm_rt*repeat + 0.04 wm_rt*switch + 0.037 wm_rt*wm_rt[t-1] + 0.057 wm_rt*wm_rt[t-2] + 0.055 wm_rt*wm_rt[t-3] + -0.124 wm_rt*wm_rt[t-4] + -0.018 repeat^2 + -0.0 repeat*switch + -0.071 repeat*wm_rt[t-1] + -0.008 repeat*wm_rt[t-2] + -0.079 repeat*wm_rt[t-3] + 0.242 repeat*wm_rt[t-4] + 0.047 switch^2 + -0.042 switch*wm_rt[t-1] + -0.189 switch*wm_rt[t-2] + -0.112 switch*wm_rt[t-3] + 0.284 switch*wm_rt[t-4] + -0.065 wm_rt[t-1]^2 + -0.062 wm_rt[t-1]*wm_rt[t-2] + -0.062 wm_rt[t-1]*wm_rt[t-3] + -0.058 wm_rt[t-1]*wm_rt[t-4] + -0.16 wm_rt[t-2]^2 + -0.16 wm_rt[t-2]*wm_rt[t-3] + -0.151 wm_rt[t-2]*wm_rt[t-4] + -0.122 wm_rt[t-3]^2 + -0.105 wm_rt[t-3]*wm_rt[t-4] + 0.216 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 58/400 --- L(Train): 0.4575387 --- L(Val, RNN): 0.3809516 --- L(Val, SINDy): 0.8449569 --- Time: 3.63s; --- Convergence: 2.27e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.02 1 + 0.573 value_reward_diff[t] + 0.357 reward_diff + -0.403 value_reward_diff^2 + 0.027 value_reward_diff*reward_diff + 0.362 reward_diff^2 \n",
      "value_persistance[t+1] = -0.176 1 + 0.89 value_persistance[t] + -0.093 repeat + 0.153 value_persistance^2 + -0.157 value_persistance*repeat + -0.094 repeat^2 \n",
      "wm_rt[t+1] = -0.073 1 + 1.059 wm_rt[t] + -0.013 repeat + 0.042 switch + -0.047 wm_rt[t-1] + -0.035 wm_rt[t-2] + -0.104 wm_rt[t-3] + 0.284 wm_rt[t-4] + 0.102 wm_rt^2 + 0.029 wm_rt*repeat + 0.039 wm_rt*switch + 0.036 wm_rt*wm_rt[t-1] + 0.056 wm_rt*wm_rt[t-2] + 0.053 wm_rt*wm_rt[t-3] + -0.123 wm_rt*wm_rt[t-4] + -0.016 repeat^2 + -0.0 repeat*switch + -0.073 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.07 repeat*wm_rt[t-3] + 0.231 repeat*wm_rt[t-4] + 0.043 switch^2 + -0.043 switch*wm_rt[t-1] + -0.196 switch*wm_rt[t-2] + -0.117 switch*wm_rt[t-3] + 0.287 switch*wm_rt[t-4] + -0.063 wm_rt[t-1]^2 + -0.059 wm_rt[t-1]*wm_rt[t-2] + -0.06 wm_rt[t-1]*wm_rt[t-3] + -0.056 wm_rt[t-1]*wm_rt[t-4] + -0.155 wm_rt[t-2]^2 + -0.156 wm_rt[t-2]*wm_rt[t-3] + -0.146 wm_rt[t-2]*wm_rt[t-4] + -0.116 wm_rt[t-3]^2 + -0.099 wm_rt[t-3]*wm_rt[t-4] + 0.21 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 59/400 --- L(Train): 0.4551721 --- L(Val, RNN): 0.3818043 --- L(Val, SINDy): 0.7922307 --- Time: 4.16s; --- Convergence: 1.56e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.025 1 + 0.565 value_reward_diff[t] + 0.373 reward_diff + -0.413 value_reward_diff^2 + 0.033 value_reward_diff*reward_diff + 0.378 reward_diff^2 \n",
      "value_persistance[t+1] = -0.18 1 + 0.889 value_persistance[t] + -0.093 repeat + 0.154 value_persistance^2 + -0.157 value_persistance*repeat + -0.094 repeat^2 \n",
      "wm_rt[t+1] = -0.073 1 + 1.059 wm_rt[t] + -0.01 repeat + 0.038 switch + -0.047 wm_rt[t-1] + -0.032 wm_rt[t-2] + -0.098 wm_rt[t-3] + 0.278 wm_rt[t-4] + 0.102 wm_rt^2 + 0.029 wm_rt*repeat + 0.038 wm_rt*switch + 0.036 wm_rt*wm_rt[t-1] + 0.056 wm_rt*wm_rt[t-2] + 0.052 wm_rt*wm_rt[t-3] + -0.121 wm_rt*wm_rt[t-4] + -0.013 repeat^2 + 0.0 repeat*switch + -0.074 repeat*wm_rt[t-1] + 0.007 repeat*wm_rt[t-2] + -0.063 repeat*wm_rt[t-3] + 0.22 repeat*wm_rt[t-4] + 0.038 switch^2 + -0.043 switch*wm_rt[t-1] + -0.204 switch*wm_rt[t-2] + -0.12 switch*wm_rt[t-3] + 0.291 switch*wm_rt[t-4] + -0.06 wm_rt[t-1]^2 + -0.057 wm_rt[t-1]*wm_rt[t-2] + -0.057 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.154 wm_rt[t-2]^2 + -0.155 wm_rt[t-2]*wm_rt[t-3] + -0.145 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-3]^2 + -0.093 wm_rt[t-3]*wm_rt[t-4] + 0.205 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 60/400 --- L(Train): 0.4571072 --- L(Val, RNN): 0.3855954 --- L(Val, SINDy): 0.8410438 --- Time: 3.71s; --- Convergence: 2.68e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.029 1 + 0.557 value_reward_diff[t] + 0.389 reward_diff + -0.422 value_reward_diff^2 + 0.037 value_reward_diff*reward_diff + 0.394 reward_diff^2 \n",
      "value_persistance[t+1] = -0.185 1 + 0.888 value_persistance[t] + -0.093 repeat + 0.155 value_persistance^2 + -0.157 value_persistance*repeat + -0.094 repeat^2 \n",
      "wm_rt[t+1] = -0.072 1 + 1.058 wm_rt[t] + -0.007 repeat + 0.034 switch + -0.047 wm_rt[t-1] + -0.031 wm_rt[t-2] + -0.091 wm_rt[t-3] + 0.273 wm_rt[t-4] + 0.102 wm_rt^2 + 0.029 wm_rt*repeat + 0.037 wm_rt*switch + 0.035 wm_rt*wm_rt[t-1] + 0.057 wm_rt*wm_rt[t-2] + 0.05 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + -0.01 repeat^2 + 0.0 repeat*switch + -0.076 repeat*wm_rt[t-1] + 0.011 repeat*wm_rt[t-2] + -0.057 repeat*wm_rt[t-3] + 0.209 repeat*wm_rt[t-4] + 0.035 switch^2 + -0.043 switch*wm_rt[t-1] + -0.212 switch*wm_rt[t-2] + -0.122 switch*wm_rt[t-3] + 0.296 switch*wm_rt[t-4] + -0.058 wm_rt[t-1]^2 + -0.054 wm_rt[t-1]*wm_rt[t-2] + -0.054 wm_rt[t-1]*wm_rt[t-3] + -0.052 wm_rt[t-1]*wm_rt[t-4] + -0.155 wm_rt[t-2]^2 + -0.155 wm_rt[t-2]*wm_rt[t-3] + -0.146 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-3]^2 + -0.088 wm_rt[t-3]*wm_rt[t-4] + 0.199 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 61/400 --- L(Train): 0.4582118 --- L(Val, RNN): 0.3850655 --- L(Val, SINDy): 0.8332503 --- Time: 4.40s; --- Convergence: 1.60e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.034 1 + 0.549 value_reward_diff[t] + 0.406 reward_diff + -0.432 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 0.41 reward_diff^2 \n",
      "value_persistance[t+1] = -0.188 1 + 0.888 value_persistance[t] + -0.093 repeat + 0.157 value_persistance^2 + -0.158 value_persistance*repeat + -0.095 repeat^2 \n",
      "wm_rt[t+1] = -0.069 1 + 1.058 wm_rt[t] + -0.005 repeat + 0.032 switch + -0.046 wm_rt[t-1] + -0.032 wm_rt[t-2] + -0.083 wm_rt[t-3] + 0.267 wm_rt[t-4] + 0.102 wm_rt^2 + 0.029 wm_rt*repeat + 0.036 wm_rt*switch + 0.035 wm_rt*wm_rt[t-1] + 0.058 wm_rt*wm_rt[t-2] + 0.048 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + -0.008 repeat^2 + 0.0 repeat*switch + -0.078 repeat*wm_rt[t-1] + 0.012 repeat*wm_rt[t-2] + -0.054 repeat*wm_rt[t-3] + 0.196 repeat*wm_rt[t-4] + 0.033 switch^2 + -0.042 switch*wm_rt[t-1] + -0.218 switch*wm_rt[t-2] + -0.119 switch*wm_rt[t-3] + 0.303 switch*wm_rt[t-4] + -0.055 wm_rt[t-1]^2 + -0.051 wm_rt[t-1]*wm_rt[t-2] + -0.052 wm_rt[t-1]*wm_rt[t-3] + -0.05 wm_rt[t-1]*wm_rt[t-4] + -0.158 wm_rt[t-2]^2 + -0.159 wm_rt[t-2]*wm_rt[t-3] + -0.15 wm_rt[t-2]*wm_rt[t-4] + -0.1 wm_rt[t-3]^2 + -0.082 wm_rt[t-3]*wm_rt[t-4] + 0.193 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 62/400 --- L(Train): 0.4485811 --- L(Val, RNN): 0.3834335 --- L(Val, SINDy): 0.8134388 --- Time: 3.50s; --- Convergence: 1.62e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.042 1 + 0.542 value_reward_diff[t] + 0.422 reward_diff + -0.441 value_reward_diff^2 + 0.052 value_reward_diff*reward_diff + 0.427 reward_diff^2 \n",
      "value_persistance[t+1] = -0.19 1 + 0.887 value_persistance[t] + -0.093 repeat + 0.159 value_persistance^2 + -0.158 value_persistance*repeat + -0.095 repeat^2 \n",
      "wm_rt[t+1] = -0.067 1 + 1.058 wm_rt[t] + -0.004 repeat + 0.031 switch + -0.045 wm_rt[t-1] + -0.035 wm_rt[t-2] + -0.076 wm_rt[t-3] + 0.262 wm_rt[t-4] + 0.102 wm_rt^2 + 0.028 wm_rt*repeat + 0.035 wm_rt*switch + 0.034 wm_rt*wm_rt[t-1] + 0.06 wm_rt*wm_rt[t-2] + 0.045 wm_rt*wm_rt[t-3] + -0.118 wm_rt*wm_rt[t-4] + -0.007 repeat^2 + 0.0 repeat*switch + -0.082 repeat*wm_rt[t-1] + 0.011 repeat*wm_rt[t-2] + -0.053 repeat*wm_rt[t-3] + 0.183 repeat*wm_rt[t-4] + 0.032 switch^2 + -0.041 switch*wm_rt[t-1] + -0.223 switch*wm_rt[t-2] + -0.115 switch*wm_rt[t-3] + 0.312 switch*wm_rt[t-4] + -0.052 wm_rt[t-1]^2 + -0.049 wm_rt[t-1]*wm_rt[t-2] + -0.049 wm_rt[t-1]*wm_rt[t-3] + -0.047 wm_rt[t-1]*wm_rt[t-4] + -0.163 wm_rt[t-2]^2 + -0.163 wm_rt[t-2]*wm_rt[t-3] + -0.154 wm_rt[t-2]*wm_rt[t-4] + -0.094 wm_rt[t-3]^2 + -0.077 wm_rt[t-3]*wm_rt[t-4] + 0.187 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 63/400 --- L(Train): 0.4596222 --- L(Val, RNN): 0.3833549 --- L(Val, SINDy): 0.8066503 --- Time: 4.08s; --- Convergence: 8.48e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.05 1 + 0.536 value_reward_diff[t] + 0.438 reward_diff + -0.449 value_reward_diff^2 + 0.062 value_reward_diff*reward_diff + 0.443 reward_diff^2 \n",
      "value_persistance[t+1] = -0.191 1 + 0.885 value_persistance[t] + -0.094 repeat + 0.161 value_persistance^2 + -0.158 value_persistance*repeat + -0.095 repeat^2 \n",
      "wm_rt[t+1] = -0.064 1 + 1.059 wm_rt[t] + -0.004 repeat + 0.031 switch + -0.045 wm_rt[t-1] + -0.038 wm_rt[t-2] + -0.068 wm_rt[t-3] + 0.259 wm_rt[t-4] + 0.102 wm_rt^2 + 0.028 wm_rt*repeat + 0.035 wm_rt*switch + 0.033 wm_rt*wm_rt[t-1] + 0.062 wm_rt*wm_rt[t-2] + 0.043 wm_rt*wm_rt[t-3] + -0.118 wm_rt*wm_rt[t-4] + -0.006 repeat^2 + -0.0 repeat*switch + -0.086 repeat*wm_rt[t-1] + 0.009 repeat*wm_rt[t-2] + -0.053 repeat*wm_rt[t-3] + 0.171 repeat*wm_rt[t-4] + 0.032 switch^2 + -0.039 switch*wm_rt[t-1] + -0.228 switch*wm_rt[t-2] + -0.111 switch*wm_rt[t-3] + 0.32 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.047 wm_rt[t-1]*wm_rt[t-2] + -0.047 wm_rt[t-1]*wm_rt[t-3] + -0.046 wm_rt[t-1]*wm_rt[t-4] + -0.168 wm_rt[t-2]^2 + -0.169 wm_rt[t-2]*wm_rt[t-3] + -0.16 wm_rt[t-2]*wm_rt[t-4] + -0.09 wm_rt[t-3]^2 + -0.071 wm_rt[t-3]*wm_rt[t-4] + 0.183 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 64/400 --- L(Train): 0.4583958 --- L(Val, RNN): 0.3856747 --- L(Val, SINDy): 0.8751161 --- Time: 3.84s; --- Convergence: 1.58e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.059 1 + 0.53 value_reward_diff[t] + 0.455 reward_diff + -0.456 value_reward_diff^2 + 0.074 value_reward_diff*reward_diff + 0.46 reward_diff^2 \n",
      "value_persistance[t+1] = -0.192 1 + 0.884 value_persistance[t] + -0.094 repeat + 0.163 value_persistance^2 + -0.158 value_persistance*repeat + -0.096 repeat^2 \n",
      "wm_rt[t+1] = -0.061 1 + 1.058 wm_rt[t] + -0.005 repeat + 0.031 switch + -0.047 wm_rt[t-1] + -0.042 wm_rt[t-2] + -0.06 wm_rt[t-3] + 0.255 wm_rt[t-4] + 0.102 wm_rt^2 + 0.027 wm_rt*repeat + 0.034 wm_rt*switch + 0.033 wm_rt*wm_rt[t-1] + 0.064 wm_rt*wm_rt[t-2] + 0.04 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + -0.008 repeat^2 + -0.0 repeat*switch + -0.092 repeat*wm_rt[t-1] + 0.006 repeat*wm_rt[t-2] + -0.054 repeat*wm_rt[t-3] + 0.159 repeat*wm_rt[t-4] + 0.032 switch^2 + -0.04 switch*wm_rt[t-1] + -0.232 switch*wm_rt[t-2] + -0.105 switch*wm_rt[t-3] + 0.33 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.047 wm_rt[t-1]*wm_rt[t-2] + -0.047 wm_rt[t-1]*wm_rt[t-3] + -0.046 wm_rt[t-1]*wm_rt[t-4] + -0.175 wm_rt[t-2]^2 + -0.175 wm_rt[t-2]*wm_rt[t-3] + -0.167 wm_rt[t-2]*wm_rt[t-4] + -0.084 wm_rt[t-3]^2 + -0.066 wm_rt[t-3]*wm_rt[t-4] + 0.178 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 65/400 --- L(Train): 0.4533984 --- L(Val, RNN): 0.3904602 --- L(Val, SINDy): 0.8758169 --- Time: 4.13s; --- Convergence: 3.18e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.069 1 + 0.525 value_reward_diff[t] + 0.472 reward_diff + -0.463 value_reward_diff^2 + 0.087 value_reward_diff*reward_diff + 0.477 reward_diff^2 \n",
      "value_persistance[t+1] = -0.195 1 + 0.883 value_persistance[t] + -0.096 repeat + 0.166 value_persistance^2 + -0.158 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.06 1 + 1.058 wm_rt[t] + -0.007 repeat + 0.032 switch + -0.049 wm_rt[t-1] + -0.047 wm_rt[t-2] + -0.052 wm_rt[t-3] + 0.251 wm_rt[t-4] + 0.102 wm_rt^2 + 0.027 wm_rt*repeat + 0.034 wm_rt*switch + 0.032 wm_rt*wm_rt[t-1] + 0.066 wm_rt*wm_rt[t-2] + 0.037 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + -0.01 repeat^2 + -0.0 repeat*switch + -0.099 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.055 repeat*wm_rt[t-3] + 0.146 repeat*wm_rt[t-4] + 0.033 switch^2 + -0.041 switch*wm_rt[t-1] + -0.236 switch*wm_rt[t-2] + -0.099 switch*wm_rt[t-3] + 0.34 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.047 wm_rt[t-1]*wm_rt[t-2] + -0.047 wm_rt[t-1]*wm_rt[t-3] + -0.046 wm_rt[t-1]*wm_rt[t-4] + -0.182 wm_rt[t-2]^2 + -0.182 wm_rt[t-2]*wm_rt[t-3] + -0.174 wm_rt[t-2]*wm_rt[t-4] + -0.078 wm_rt[t-3]^2 + -0.06 wm_rt[t-3]*wm_rt[t-4] + 0.173 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 66/400 --- L(Train): 0.4553326 --- L(Val, RNN): 0.3943214 --- L(Val, SINDy): 0.8085003 --- Time: 3.93s; --- Convergence: 3.52e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.08 1 + 0.523 value_reward_diff[t] + 0.488 reward_diff + -0.468 value_reward_diff^2 + 0.1 value_reward_diff*reward_diff + 0.493 reward_diff^2 \n",
      "value_persistance[t+1] = -0.199 1 + 0.882 value_persistance[t] + -0.097 repeat + 0.168 value_persistance^2 + -0.158 value_persistance*repeat + -0.099 repeat^2 \n",
      "wm_rt[t+1] = -0.06 1 + 1.058 wm_rt[t] + -0.01 repeat + 0.033 switch + -0.052 wm_rt[t-1] + -0.049 wm_rt[t-2] + -0.042 wm_rt[t-3] + 0.248 wm_rt[t-4] + 0.102 wm_rt^2 + 0.026 wm_rt*repeat + 0.033 wm_rt*switch + 0.031 wm_rt*wm_rt[t-1] + 0.068 wm_rt*wm_rt[t-2] + 0.033 wm_rt*wm_rt[t-3] + -0.121 wm_rt*wm_rt[t-4] + -0.013 repeat^2 + -0.0 repeat*switch + -0.106 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.054 repeat*wm_rt[t-3] + 0.135 repeat*wm_rt[t-4] + 0.033 switch^2 + -0.042 switch*wm_rt[t-1] + -0.237 switch*wm_rt[t-2] + -0.093 switch*wm_rt[t-3] + 0.349 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.048 wm_rt[t-1]*wm_rt[t-2] + -0.047 wm_rt[t-1]*wm_rt[t-3] + -0.047 wm_rt[t-1]*wm_rt[t-4] + -0.187 wm_rt[t-2]^2 + -0.187 wm_rt[t-2]*wm_rt[t-3] + -0.179 wm_rt[t-2]*wm_rt[t-4] + -0.07 wm_rt[t-3]^2 + -0.052 wm_rt[t-3]*wm_rt[t-4] + 0.17 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 67/400 --- L(Train): 0.4504419 --- L(Val, RNN): 0.3948522 --- L(Val, SINDy): 0.7777135 --- Time: 4.10s; --- Convergence: 2.03e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.09 1 + 0.52 value_reward_diff[t] + 0.504 reward_diff + -0.472 value_reward_diff^2 + 0.114 value_reward_diff*reward_diff + 0.509 reward_diff^2 \n",
      "value_persistance[t+1] = -0.203 1 + 0.881 value_persistance[t] + -0.097 repeat + 0.17 value_persistance^2 + -0.158 value_persistance*repeat + -0.099 repeat^2 \n",
      "wm_rt[t+1] = -0.06 1 + 1.058 wm_rt[t] + -0.012 repeat + 0.033 switch + -0.055 wm_rt[t-1] + -0.051 wm_rt[t-2] + -0.033 wm_rt[t-3] + 0.245 wm_rt[t-4] + 0.102 wm_rt^2 + 0.025 wm_rt*repeat + 0.033 wm_rt*switch + 0.03 wm_rt*wm_rt[t-1] + 0.069 wm_rt*wm_rt[t-2] + 0.029 wm_rt*wm_rt[t-3] + -0.122 wm_rt*wm_rt[t-4] + -0.015 repeat^2 + 0.0 repeat*switch + -0.113 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.052 repeat*wm_rt[t-3] + 0.125 repeat*wm_rt[t-4] + 0.033 switch^2 + -0.045 switch*wm_rt[t-1] + -0.239 switch*wm_rt[t-2] + -0.089 switch*wm_rt[t-3] + 0.358 switch*wm_rt[t-4] + -0.052 wm_rt[t-1]^2 + -0.049 wm_rt[t-1]*wm_rt[t-2] + -0.048 wm_rt[t-1]*wm_rt[t-3] + -0.048 wm_rt[t-1]*wm_rt[t-4] + -0.191 wm_rt[t-2]^2 + -0.191 wm_rt[t-2]*wm_rt[t-3] + -0.183 wm_rt[t-2]*wm_rt[t-4] + -0.063 wm_rt[t-3]^2 + -0.045 wm_rt[t-3]*wm_rt[t-4] + 0.166 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 68/400 --- L(Train): 0.4631160 --- L(Val, RNN): 0.3994202 --- L(Val, SINDy): 0.7780789 --- Time: 3.98s; --- Convergence: 3.30e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.097 1 + 0.516 value_reward_diff[t] + 0.521 reward_diff + -0.479 value_reward_diff^2 + 0.127 value_reward_diff*reward_diff + 0.526 reward_diff^2 \n",
      "value_persistance[t+1] = -0.206 1 + 0.879 value_persistance[t] + -0.096 repeat + 0.173 value_persistance^2 + -0.158 value_persistance*repeat + -0.098 repeat^2 \n",
      "wm_rt[t+1] = -0.06 1 + 1.058 wm_rt[t] + -0.012 repeat + 0.032 switch + -0.059 wm_rt[t-1] + -0.051 wm_rt[t-2] + -0.026 wm_rt[t-3] + 0.242 wm_rt[t-4] + 0.102 wm_rt^2 + 0.025 wm_rt*repeat + 0.032 wm_rt*switch + 0.03 wm_rt*wm_rt[t-1] + 0.069 wm_rt*wm_rt[t-2] + 0.026 wm_rt*wm_rt[t-3] + -0.123 wm_rt*wm_rt[t-4] + -0.015 repeat^2 + 0.0 repeat*switch + -0.118 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.047 repeat*wm_rt[t-3] + 0.116 repeat*wm_rt[t-4] + 0.033 switch^2 + -0.049 switch*wm_rt[t-1] + -0.241 switch*wm_rt[t-2] + -0.089 switch*wm_rt[t-3] + 0.363 switch*wm_rt[t-4] + -0.053 wm_rt[t-1]^2 + -0.051 wm_rt[t-1]*wm_rt[t-2] + -0.05 wm_rt[t-1]*wm_rt[t-3] + -0.051 wm_rt[t-1]*wm_rt[t-4] + -0.193 wm_rt[t-2]^2 + -0.193 wm_rt[t-2]*wm_rt[t-3] + -0.185 wm_rt[t-2]*wm_rt[t-4] + -0.058 wm_rt[t-3]^2 + -0.039 wm_rt[t-3]*wm_rt[t-4] + 0.163 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 69/400 --- L(Train): 0.4546504 --- L(Val, RNN): 0.4004605 --- L(Val, SINDy): 0.7559572 --- Time: 3.53s; --- Convergence: 2.17e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.105 1 + 0.513 value_reward_diff[t] + 0.536 reward_diff + -0.484 value_reward_diff^2 + 0.14 value_reward_diff*reward_diff + 0.541 reward_diff^2 \n",
      "value_persistance[t+1] = -0.208 1 + 0.878 value_persistance[t] + -0.096 repeat + 0.176 value_persistance^2 + -0.159 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.061 1 + 1.057 wm_rt[t] + -0.012 repeat + 0.031 switch + -0.063 wm_rt[t-1] + -0.049 wm_rt[t-2] + -0.019 wm_rt[t-3] + 0.239 wm_rt[t-4] + 0.102 wm_rt^2 + 0.024 wm_rt*repeat + 0.031 wm_rt*switch + 0.029 wm_rt*wm_rt[t-1] + 0.069 wm_rt*wm_rt[t-2] + 0.023 wm_rt*wm_rt[t-3] + -0.123 wm_rt*wm_rt[t-4] + -0.015 repeat^2 + 0.0 repeat*switch + -0.123 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.042 repeat*wm_rt[t-3] + 0.109 repeat*wm_rt[t-4] + 0.032 switch^2 + -0.052 switch*wm_rt[t-1] + -0.242 switch*wm_rt[t-2] + -0.09 switch*wm_rt[t-3] + 0.368 switch*wm_rt[t-4] + -0.055 wm_rt[t-1]^2 + -0.052 wm_rt[t-1]*wm_rt[t-2] + -0.052 wm_rt[t-1]*wm_rt[t-3] + -0.053 wm_rt[t-1]*wm_rt[t-4] + -0.194 wm_rt[t-2]^2 + -0.194 wm_rt[t-2]*wm_rt[t-3] + -0.186 wm_rt[t-2]*wm_rt[t-4] + -0.053 wm_rt[t-3]^2 + -0.034 wm_rt[t-3]*wm_rt[t-4] + 0.159 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 70/400 --- L(Train): 0.4449927 --- L(Val, RNN): 0.3986887 --- L(Val, SINDy): 0.7563298 --- Time: 3.38s; --- Convergence: 1.97e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.114 1 + 0.513 value_reward_diff[t] + 0.551 reward_diff + -0.487 value_reward_diff^2 + 0.154 value_reward_diff*reward_diff + 0.556 reward_diff^2 \n",
      "value_persistance[t+1] = -0.21 1 + 0.877 value_persistance[t] + -0.095 repeat + 0.179 value_persistance^2 + -0.159 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.061 1 + 1.057 wm_rt[t] + -0.01 repeat + 0.03 switch + -0.066 wm_rt[t-1] + -0.044 wm_rt[t-2] + -0.014 wm_rt[t-3] + 0.236 wm_rt[t-4] + 0.102 wm_rt^2 + 0.023 wm_rt*repeat + 0.031 wm_rt*switch + 0.028 wm_rt*wm_rt[t-1] + 0.068 wm_rt*wm_rt[t-2] + 0.022 wm_rt*wm_rt[t-3] + -0.123 wm_rt*wm_rt[t-4] + -0.013 repeat^2 + -0.0 repeat*switch + -0.126 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.035 repeat*wm_rt[t-3] + 0.103 repeat*wm_rt[t-4] + 0.031 switch^2 + -0.055 switch*wm_rt[t-1] + -0.241 switch*wm_rt[t-2] + -0.094 switch*wm_rt[t-3] + 0.37 switch*wm_rt[t-4] + -0.056 wm_rt[t-1]^2 + -0.053 wm_rt[t-1]*wm_rt[t-2] + -0.052 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.192 wm_rt[t-2]^2 + -0.192 wm_rt[t-2]*wm_rt[t-3] + -0.185 wm_rt[t-2]*wm_rt[t-4] + -0.05 wm_rt[t-3]^2 + -0.031 wm_rt[t-3]*wm_rt[t-4] + 0.155 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 71/400 --- L(Train): 0.4493614 --- L(Val, RNN): 0.4011679 --- L(Val, SINDy): 0.7536920 --- Time: 3.48s; --- Convergence: 2.22e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.121 1 + 0.512 value_reward_diff[t] + 0.565 reward_diff + -0.49 value_reward_diff^2 + 0.167 value_reward_diff*reward_diff + 0.57 reward_diff^2 \n",
      "value_persistance[t+1] = -0.214 1 + 0.875 value_persistance[t] + -0.095 repeat + 0.182 value_persistance^2 + -0.16 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.062 1 + 1.057 wm_rt[t] + -0.008 repeat + 0.029 switch + -0.067 wm_rt[t-1] + -0.037 wm_rt[t-2] + -0.01 wm_rt[t-3] + 0.232 wm_rt[t-4] + 0.102 wm_rt^2 + 0.023 wm_rt*repeat + 0.03 wm_rt*switch + 0.027 wm_rt*wm_rt[t-1] + 0.067 wm_rt*wm_rt[t-2] + 0.021 wm_rt*wm_rt[t-3] + -0.122 wm_rt*wm_rt[t-4] + -0.01 repeat^2 + -0.0 repeat*switch + -0.128 repeat*wm_rt[t-1] + 0.005 repeat*wm_rt[t-2] + -0.027 repeat*wm_rt[t-3] + 0.098 repeat*wm_rt[t-4] + 0.03 switch^2 + -0.058 switch*wm_rt[t-1] + -0.24 switch*wm_rt[t-2] + -0.1 switch*wm_rt[t-3] + 0.371 switch*wm_rt[t-4] + -0.056 wm_rt[t-1]^2 + -0.053 wm_rt[t-1]*wm_rt[t-2] + -0.053 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.19 wm_rt[t-2]^2 + -0.189 wm_rt[t-2]*wm_rt[t-3] + -0.182 wm_rt[t-2]*wm_rt[t-4] + -0.048 wm_rt[t-3]^2 + -0.029 wm_rt[t-3]*wm_rt[t-4] + 0.15 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 72/400 --- L(Train): 0.4552298 --- L(Val, RNN): 0.4050589 --- L(Val, SINDy): 0.7556068 --- Time: 3.90s; --- Convergence: 3.06e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.124 1 + 0.51 value_reward_diff[t] + 0.579 reward_diff + -0.495 value_reward_diff^2 + 0.179 value_reward_diff*reward_diff + 0.584 reward_diff^2 \n",
      "value_persistance[t+1] = -0.218 1 + 0.874 value_persistance[t] + -0.096 repeat + 0.185 value_persistance^2 + -0.16 value_persistance*repeat + -0.098 repeat^2 \n",
      "wm_rt[t+1] = -0.061 1 + 1.056 wm_rt[t] + -0.004 repeat + 0.028 switch + -0.068 wm_rt[t-1] + -0.031 wm_rt[t-2] + -0.008 wm_rt[t-3] + 0.228 wm_rt[t-4] + 0.102 wm_rt^2 + 0.023 wm_rt*repeat + 0.03 wm_rt*switch + 0.027 wm_rt*wm_rt[t-1] + 0.066 wm_rt*wm_rt[t-2] + 0.021 wm_rt*wm_rt[t-3] + -0.121 wm_rt*wm_rt[t-4] + -0.007 repeat^2 + 0.0 repeat*switch + -0.13 repeat*wm_rt[t-1] + 0.01 repeat*wm_rt[t-2] + -0.021 repeat*wm_rt[t-3] + 0.093 repeat*wm_rt[t-4] + 0.029 switch^2 + -0.059 switch*wm_rt[t-1] + -0.237 switch*wm_rt[t-2] + -0.106 switch*wm_rt[t-3] + 0.371 switch*wm_rt[t-4] + -0.056 wm_rt[t-1]^2 + -0.053 wm_rt[t-1]*wm_rt[t-2] + -0.052 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.186 wm_rt[t-2]^2 + -0.186 wm_rt[t-2]*wm_rt[t-3] + -0.179 wm_rt[t-2]*wm_rt[t-4] + -0.049 wm_rt[t-3]^2 + -0.03 wm_rt[t-3]*wm_rt[t-4] + 0.146 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 73/400 --- L(Train): 0.4562529 --- L(Val, RNN): 0.4030057 --- L(Val, SINDy): 0.7548907 --- Time: 4.45s; --- Convergence: 2.56e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.124 1 + 0.506 value_reward_diff[t] + 0.593 reward_diff + -0.502 value_reward_diff^2 + 0.19 value_reward_diff*reward_diff + 0.598 reward_diff^2 \n",
      "value_persistance[t+1] = -0.224 1 + 0.873 value_persistance[t] + -0.098 repeat + 0.187 value_persistance^2 + -0.159 value_persistance*repeat + -0.1 repeat^2 \n",
      "wm_rt[t+1] = -0.06 1 + 1.056 wm_rt[t] + -0.001 repeat + 0.028 switch + -0.068 wm_rt[t-1] + -0.024 wm_rt[t-2] + -0.009 wm_rt[t-3] + 0.225 wm_rt[t-4] + 0.102 wm_rt^2 + 0.022 wm_rt*repeat + 0.029 wm_rt*switch + 0.026 wm_rt*wm_rt[t-1] + 0.066 wm_rt*wm_rt[t-2] + 0.023 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + -0.004 repeat^2 + 0.0 repeat*switch + -0.132 repeat*wm_rt[t-1] + 0.014 repeat*wm_rt[t-2] + -0.015 repeat*wm_rt[t-3] + 0.09 repeat*wm_rt[t-4] + 0.028 switch^2 + -0.059 switch*wm_rt[t-1] + -0.232 switch*wm_rt[t-2] + -0.112 switch*wm_rt[t-3] + 0.37 switch*wm_rt[t-4] + -0.055 wm_rt[t-1]^2 + -0.051 wm_rt[t-1]*wm_rt[t-2] + -0.051 wm_rt[t-1]*wm_rt[t-3] + -0.053 wm_rt[t-1]*wm_rt[t-4] + -0.183 wm_rt[t-2]^2 + -0.182 wm_rt[t-2]*wm_rt[t-3] + -0.175 wm_rt[t-2]*wm_rt[t-4] + -0.051 wm_rt[t-3]^2 + -0.032 wm_rt[t-3]*wm_rt[t-4] + 0.142 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 74/400 --- L(Train): 0.4573754 --- L(Val, RNN): 0.4053447 --- L(Val, SINDy): 0.7718512 --- Time: 3.57s; --- Convergence: 2.45e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.121 1 + 0.5 value_reward_diff[t] + 0.607 reward_diff + -0.51 value_reward_diff^2 + 0.2 value_reward_diff*reward_diff + 0.611 reward_diff^2 \n",
      "value_persistance[t+1] = -0.229 1 + 0.871 value_persistance[t] + -0.099 repeat + 0.191 value_persistance^2 + -0.159 value_persistance*repeat + -0.1 repeat^2 \n",
      "wm_rt[t+1] = -0.06 1 + 1.056 wm_rt[t] + 0.001 repeat + 0.027 switch + -0.068 wm_rt[t-1] + -0.016 wm_rt[t-2] + -0.014 wm_rt[t-3] + 0.222 wm_rt[t-4] + 0.102 wm_rt^2 + 0.022 wm_rt*repeat + 0.029 wm_rt*switch + 0.026 wm_rt*wm_rt[t-1] + 0.065 wm_rt*wm_rt[t-2] + 0.025 wm_rt*wm_rt[t-3] + -0.117 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.0 repeat*switch + -0.136 repeat*wm_rt[t-1] + 0.017 repeat*wm_rt[t-2] + -0.011 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.028 switch^2 + -0.059 switch*wm_rt[t-1] + -0.226 switch*wm_rt[t-2] + -0.12 switch*wm_rt[t-3] + 0.368 switch*wm_rt[t-4] + -0.053 wm_rt[t-1]^2 + -0.05 wm_rt[t-1]*wm_rt[t-2] + -0.05 wm_rt[t-1]*wm_rt[t-3] + -0.052 wm_rt[t-1]*wm_rt[t-4] + -0.179 wm_rt[t-2]^2 + -0.178 wm_rt[t-2]*wm_rt[t-3] + -0.171 wm_rt[t-2]*wm_rt[t-4] + -0.057 wm_rt[t-3]^2 + -0.037 wm_rt[t-3]*wm_rt[t-4] + 0.137 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 75/400 --- L(Train): 0.4603230 --- L(Val, RNN): 0.4055783 --- L(Val, SINDy): 0.7787085 --- Time: 4.61s; --- Convergence: 1.34e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.12 1 + 0.497 value_reward_diff[t] + 0.619 reward_diff + -0.516 value_reward_diff^2 + 0.211 value_reward_diff*reward_diff + 0.624 reward_diff^2 \n",
      "value_persistance[t+1] = -0.233 1 + 0.869 value_persistance[t] + -0.097 repeat + 0.195 value_persistance^2 + -0.16 value_persistance*repeat + -0.099 repeat^2 \n",
      "wm_rt[t+1] = -0.059 1 + 1.056 wm_rt[t] + 0.003 repeat + 0.026 switch + -0.068 wm_rt[t-1] + -0.007 wm_rt[t-2] + -0.014 wm_rt[t-3] + 0.22 wm_rt[t-4] + 0.102 wm_rt^2 + 0.021 wm_rt*repeat + 0.029 wm_rt*switch + 0.025 wm_rt*wm_rt[t-1] + 0.064 wm_rt*wm_rt[t-2] + 0.027 wm_rt*wm_rt[t-3] + -0.116 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.0 repeat*switch + -0.14 repeat*wm_rt[t-1] + 0.02 repeat*wm_rt[t-2] + -0.007 repeat*wm_rt[t-3] + 0.085 repeat*wm_rt[t-4] + 0.027 switch^2 + -0.058 switch*wm_rt[t-1] + -0.216 switch*wm_rt[t-2] + -0.126 switch*wm_rt[t-3] + 0.367 switch*wm_rt[t-4] + -0.051 wm_rt[t-1]^2 + -0.047 wm_rt[t-1]*wm_rt[t-2] + -0.047 wm_rt[t-1]*wm_rt[t-3] + -0.049 wm_rt[t-1]*wm_rt[t-4] + -0.173 wm_rt[t-2]^2 + -0.173 wm_rt[t-2]*wm_rt[t-3] + -0.165 wm_rt[t-2]*wm_rt[t-4] + -0.059 wm_rt[t-3]^2 + -0.039 wm_rt[t-3]*wm_rt[t-4] + 0.134 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 76/400 --- L(Train): 0.4640747 --- L(Val, RNN): 0.4073550 --- L(Val, SINDy): 0.7569658 --- Time: 3.57s; --- Convergence: 1.56e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.117 1 + 0.493 value_reward_diff[t] + 0.632 reward_diff + -0.522 value_reward_diff^2 + 0.22 value_reward_diff*reward_diff + 0.636 reward_diff^2 \n",
      "value_persistance[t+1] = -0.237 1 + 0.867 value_persistance[t] + -0.096 repeat + 0.199 value_persistance^2 + -0.161 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.059 1 + 1.055 wm_rt[t] + 0.003 repeat + 0.025 switch + -0.07 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.016 wm_rt[t-3] + 0.219 wm_rt[t-4] + 0.102 wm_rt^2 + 0.02 wm_rt*repeat + 0.028 wm_rt*switch + 0.024 wm_rt*wm_rt[t-1] + 0.063 wm_rt*wm_rt[t-2] + 0.028 wm_rt*wm_rt[t-3] + -0.115 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.0 repeat*switch + -0.146 repeat*wm_rt[t-1] + 0.021 repeat*wm_rt[t-2] + -0.004 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + 0.026 switch^2 + -0.06 switch*wm_rt[t-1] + -0.208 switch*wm_rt[t-2] + -0.131 switch*wm_rt[t-3] + 0.366 switch*wm_rt[t-4] + -0.051 wm_rt[t-1]^2 + -0.046 wm_rt[t-1]*wm_rt[t-2] + -0.047 wm_rt[t-1]*wm_rt[t-3] + -0.049 wm_rt[t-1]*wm_rt[t-4] + -0.169 wm_rt[t-2]^2 + -0.168 wm_rt[t-2]*wm_rt[t-3] + -0.161 wm_rt[t-2]*wm_rt[t-4] + -0.061 wm_rt[t-3]^2 + -0.041 wm_rt[t-3]*wm_rt[t-4] + 0.132 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 77/400 --- L(Train): 0.4515969 --- L(Val, RNN): 0.4046473 --- L(Val, SINDy): 0.7187915 --- Time: 3.91s; --- Convergence: 2.13e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.113 1 + 0.489 value_reward_diff[t] + 0.643 reward_diff + -0.529 value_reward_diff^2 + 0.229 value_reward_diff*reward_diff + 0.647 reward_diff^2 \n",
      "value_persistance[t+1] = -0.242 1 + 0.865 value_persistance[t] + -0.095 repeat + 0.202 value_persistance^2 + -0.162 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.059 1 + 1.055 wm_rt[t] + 0.002 repeat + 0.025 switch + -0.073 wm_rt[t-1] + 0.007 wm_rt[t-2] + -0.017 wm_rt[t-3] + 0.219 wm_rt[t-4] + 0.102 wm_rt^2 + 0.02 wm_rt*repeat + 0.028 wm_rt*switch + 0.023 wm_rt*wm_rt[t-1] + 0.061 wm_rt*wm_rt[t-2] + 0.029 wm_rt*wm_rt[t-3] + -0.115 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + -0.152 repeat*wm_rt[t-1] + 0.022 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + 0.025 switch^2 + -0.063 switch*wm_rt[t-1] + -0.202 switch*wm_rt[t-2] + -0.137 switch*wm_rt[t-3] + 0.365 switch*wm_rt[t-4] + -0.052 wm_rt[t-1]^2 + -0.048 wm_rt[t-1]*wm_rt[t-2] + -0.048 wm_rt[t-1]*wm_rt[t-3] + -0.051 wm_rt[t-1]*wm_rt[t-4] + -0.166 wm_rt[t-2]^2 + -0.165 wm_rt[t-2]*wm_rt[t-3] + -0.158 wm_rt[t-2]*wm_rt[t-4] + -0.063 wm_rt[t-3]^2 + -0.043 wm_rt[t-3]*wm_rt[t-4] + 0.131 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 78/400 --- L(Train): 0.4602244 --- L(Val, RNN): 0.4088972 --- L(Val, SINDy): 0.7102526 --- Time: 3.67s; --- Convergence: 3.19e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.106 1 + 0.483 value_reward_diff[t] + 0.654 reward_diff + -0.537 value_reward_diff^2 + 0.238 value_reward_diff*reward_diff + 0.658 reward_diff^2 \n",
      "value_persistance[t+1] = -0.249 1 + 0.864 value_persistance[t] + -0.097 repeat + 0.206 value_persistance^2 + -0.161 value_persistance*repeat + -0.099 repeat^2 \n",
      "wm_rt[t+1] = -0.059 1 + 1.055 wm_rt[t] + 0.001 repeat + 0.024 switch + -0.079 wm_rt[t-1] + 0.011 wm_rt[t-2] + -0.016 wm_rt[t-3] + 0.22 wm_rt[t-4] + 0.102 wm_rt^2 + 0.019 wm_rt*repeat + 0.027 wm_rt*switch + 0.022 wm_rt*wm_rt[t-1] + 0.06 wm_rt*wm_rt[t-2] + 0.028 wm_rt*wm_rt[t-3] + -0.116 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.0 repeat*switch + -0.16 repeat*wm_rt[t-1] + 0.021 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + 0.025 switch^2 + -0.069 switch*wm_rt[t-1] + -0.195 switch*wm_rt[t-2] + -0.141 switch*wm_rt[t-3] + 0.364 switch*wm_rt[t-4] + -0.055 wm_rt[t-1]^2 + -0.051 wm_rt[t-1]*wm_rt[t-2] + -0.051 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.165 wm_rt[t-2]^2 + -0.164 wm_rt[t-2]*wm_rt[t-3] + -0.157 wm_rt[t-2]*wm_rt[t-4] + -0.063 wm_rt[t-3]^2 + -0.043 wm_rt[t-3]*wm_rt[t-4] + 0.131 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 79/400 --- L(Train): 0.4617030 --- L(Val, RNN): 0.4024400 --- L(Val, SINDy): 0.7574608 --- Time: 3.24s; --- Convergence: 4.82e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.101 1 + 0.478 value_reward_diff[t] + 0.664 reward_diff + -0.544 value_reward_diff^2 + 0.245 value_reward_diff*reward_diff + 0.668 reward_diff^2 \n",
      "value_persistance[t+1] = -0.253 1 + 0.861 value_persistance[t] + -0.098 repeat + 0.211 value_persistance^2 + -0.161 value_persistance*repeat + -0.099 repeat^2 \n",
      "wm_rt[t+1] = -0.059 1 + 1.055 wm_rt[t] + 0.0 repeat + 0.023 switch + -0.084 wm_rt[t-1] + 0.016 wm_rt[t-2] + -0.012 wm_rt[t-3] + 0.222 wm_rt[t-4] + 0.102 wm_rt^2 + 0.018 wm_rt*repeat + 0.027 wm_rt*switch + 0.021 wm_rt*wm_rt[t-1] + 0.059 wm_rt*wm_rt[t-2] + 0.026 wm_rt*wm_rt[t-3] + -0.118 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.0 repeat*switch + -0.167 repeat*wm_rt[t-1] + 0.02 repeat*wm_rt[t-2] + 0.008 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.024 switch^2 + -0.074 switch*wm_rt[t-1] + -0.187 switch*wm_rt[t-2] + -0.142 switch*wm_rt[t-3] + 0.364 switch*wm_rt[t-4] + -0.058 wm_rt[t-1]^2 + -0.054 wm_rt[t-1]*wm_rt[t-2] + -0.054 wm_rt[t-1]*wm_rt[t-3] + -0.057 wm_rt[t-1]*wm_rt[t-4] + -0.163 wm_rt[t-2]^2 + -0.162 wm_rt[t-2]*wm_rt[t-3] + -0.155 wm_rt[t-2]*wm_rt[t-4] + -0.06 wm_rt[t-3]^2 + -0.04 wm_rt[t-3]*wm_rt[t-4] + 0.132 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 80/400 --- L(Train): 0.4549775 --- L(Val, RNN): 0.4031561 --- L(Val, SINDy): 0.8213280 --- Time: 3.59s; --- Convergence: 2.77e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.093 1 + 0.472 value_reward_diff[t] + 0.674 reward_diff + -0.551 value_reward_diff^2 + 0.252 value_reward_diff*reward_diff + 0.678 reward_diff^2 \n",
      "value_persistance[t+1] = -0.258 1 + 0.859 value_persistance[t] + -0.097 repeat + 0.216 value_persistance^2 + -0.162 value_persistance*repeat + -0.099 repeat^2 \n",
      "wm_rt[t+1] = -0.058 1 + 1.054 wm_rt[t] + 0.001 repeat + 0.022 switch + -0.088 wm_rt[t-1] + 0.019 wm_rt[t-2] + -0.01 wm_rt[t-3] + 0.223 wm_rt[t-4] + 0.102 wm_rt^2 + 0.017 wm_rt*repeat + 0.026 wm_rt*switch + 0.02 wm_rt*wm_rt[t-1] + 0.059 wm_rt*wm_rt[t-2] + 0.025 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.0 repeat*switch + -0.173 repeat*wm_rt[t-1] + 0.017 repeat*wm_rt[t-2] + 0.009 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.022 switch^2 + -0.079 switch*wm_rt[t-1] + -0.18 switch*wm_rt[t-2] + -0.143 switch*wm_rt[t-3] + 0.363 switch*wm_rt[t-4] + -0.061 wm_rt[t-1]^2 + -0.056 wm_rt[t-1]*wm_rt[t-2] + -0.056 wm_rt[t-1]*wm_rt[t-3] + -0.06 wm_rt[t-1]*wm_rt[t-4] + -0.163 wm_rt[t-2]^2 + -0.163 wm_rt[t-2]*wm_rt[t-3] + -0.155 wm_rt[t-2]*wm_rt[t-4] + -0.059 wm_rt[t-3]^2 + -0.039 wm_rt[t-3]*wm_rt[t-4] + 0.132 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 81/400 --- L(Train): 0.4641658 --- L(Val, RNN): 0.4062192 --- L(Val, SINDy): 0.7505035 --- Time: 4.09s; --- Convergence: 2.92e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.085 1 + 0.466 value_reward_diff[t] + 0.684 reward_diff + -0.559 value_reward_diff^2 + 0.258 value_reward_diff*reward_diff + 0.688 reward_diff^2 \n",
      "value_persistance[t+1] = -0.263 1 + 0.857 value_persistance[t] + -0.097 repeat + 0.22 value_persistance^2 + -0.163 value_persistance*repeat + -0.098 repeat^2 \n",
      "wm_rt[t+1] = -0.057 1 + 1.054 wm_rt[t] + 0.001 repeat + 0.021 switch + -0.092 wm_rt[t-1] + 0.021 wm_rt[t-2] + -0.008 wm_rt[t-3] + 0.223 wm_rt[t-4] + 0.102 wm_rt^2 + 0.016 wm_rt*repeat + 0.026 wm_rt*switch + 0.019 wm_rt*wm_rt[t-1] + 0.059 wm_rt*wm_rt[t-2] + 0.024 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.0 repeat*switch + -0.179 repeat*wm_rt[t-1] + 0.013 repeat*wm_rt[t-2] + 0.01 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.021 switch^2 + -0.083 switch*wm_rt[t-1] + -0.173 switch*wm_rt[t-2] + -0.144 switch*wm_rt[t-3] + 0.363 switch*wm_rt[t-4] + -0.062 wm_rt[t-1]^2 + -0.058 wm_rt[t-1]*wm_rt[t-2] + -0.058 wm_rt[t-1]*wm_rt[t-3] + -0.061 wm_rt[t-1]*wm_rt[t-4] + -0.164 wm_rt[t-2]^2 + -0.164 wm_rt[t-2]*wm_rt[t-3] + -0.156 wm_rt[t-2]*wm_rt[t-4] + -0.058 wm_rt[t-3]^2 + -0.038 wm_rt[t-3]*wm_rt[t-4] + 0.131 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 82/400 --- L(Train): 0.4573638 --- L(Val, RNN): 0.4087736 --- L(Val, SINDy): 0.7063137 --- Time: 4.07s; --- Convergence: 2.74e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.08 1 + 0.463 value_reward_diff[t] + 0.692 reward_diff + -0.563 value_reward_diff^2 + 0.263 value_reward_diff*reward_diff + 0.696 reward_diff^2 \n",
      "value_persistance[t+1] = -0.269 1 + 0.855 value_persistance[t] + -0.096 repeat + 0.225 value_persistance^2 + -0.163 value_persistance*repeat + -0.098 repeat^2 \n",
      "wm_rt[t+1] = -0.056 1 + 1.054 wm_rt[t] + 0.001 repeat + 0.02 switch + -0.092 wm_rt[t-1] + 0.027 wm_rt[t-2] + -0.005 wm_rt[t-3] + 0.224 wm_rt[t-4] + 0.102 wm_rt^2 + 0.015 wm_rt*repeat + 0.025 wm_rt*switch + 0.018 wm_rt*wm_rt[t-1] + 0.059 wm_rt*wm_rt[t-2] + 0.023 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.0 repeat*switch + -0.185 repeat*wm_rt[t-1] + 0.008 repeat*wm_rt[t-2] + 0.008 repeat*wm_rt[t-3] + 0.085 repeat*wm_rt[t-4] + 0.021 switch^2 + -0.082 switch*wm_rt[t-1] + -0.162 switch*wm_rt[t-2] + -0.14 switch*wm_rt[t-3] + 0.364 switch*wm_rt[t-4] + -0.06 wm_rt[t-1]^2 + -0.056 wm_rt[t-1]*wm_rt[t-2] + -0.056 wm_rt[t-1]*wm_rt[t-3] + -0.059 wm_rt[t-1]*wm_rt[t-4] + -0.162 wm_rt[t-2]^2 + -0.161 wm_rt[t-2]*wm_rt[t-3] + -0.154 wm_rt[t-2]*wm_rt[t-4] + -0.056 wm_rt[t-3]^2 + -0.036 wm_rt[t-3]*wm_rt[t-4] + 0.131 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 83/400 --- L(Train): 0.4679056 --- L(Val, RNN): 0.4096076 --- L(Val, SINDy): 0.7002758 --- Time: 3.33s; --- Convergence: 1.78e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.074 1 + 0.461 value_reward_diff[t] + 0.7 reward_diff + -0.567 value_reward_diff^2 + 0.268 value_reward_diff*reward_diff + 0.704 reward_diff^2 \n",
      "value_persistance[t+1] = -0.276 1 + 0.852 value_persistance[t] + -0.096 repeat + 0.229 value_persistance^2 + -0.163 value_persistance*repeat + -0.098 repeat^2 \n",
      "wm_rt[t+1] = -0.057 1 + 1.054 wm_rt[t] + -0.001 repeat + 0.02 switch + -0.094 wm_rt[t-1] + 0.028 wm_rt[t-2] + -0.006 wm_rt[t-3] + 0.222 wm_rt[t-4] + 0.102 wm_rt^2 + 0.014 wm_rt*repeat + 0.025 wm_rt*switch + 0.018 wm_rt*wm_rt[t-1] + 0.06 wm_rt*wm_rt[t-2] + 0.024 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + -0.004 repeat^2 + 0.0 repeat*switch + -0.193 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.082 repeat*wm_rt[t-4] + 0.021 switch^2 + -0.083 switch*wm_rt[t-1] + -0.155 switch*wm_rt[t-2] + -0.139 switch*wm_rt[t-3] + 0.363 switch*wm_rt[t-4] + -0.06 wm_rt[t-1]^2 + -0.055 wm_rt[t-1]*wm_rt[t-2] + -0.056 wm_rt[t-1]*wm_rt[t-3] + -0.059 wm_rt[t-1]*wm_rt[t-4] + -0.164 wm_rt[t-2]^2 + -0.163 wm_rt[t-2]*wm_rt[t-3] + -0.156 wm_rt[t-2]*wm_rt[t-4] + -0.058 wm_rt[t-3]^2 + -0.038 wm_rt[t-3]*wm_rt[t-4] + 0.128 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 84/400 --- L(Train): 0.4620641 --- L(Val, RNN): 0.4068850 --- L(Val, SINDy): 0.7851615 --- Time: 3.50s; --- Convergence: 2.25e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.066 1 + 0.457 value_reward_diff[t] + 0.708 reward_diff + -0.572 value_reward_diff^2 + 0.273 value_reward_diff*reward_diff + 0.712 reward_diff^2 \n",
      "value_persistance[t+1] = -0.281 1 + 0.85 value_persistance[t] + -0.095 repeat + 0.235 value_persistance^2 + -0.164 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.058 1 + 1.053 wm_rt[t] + 0.0 repeat + 0.019 switch + -0.095 wm_rt[t-1] + 0.03 wm_rt[t-2] + -0.005 wm_rt[t-3] + 0.218 wm_rt[t-4] + 0.102 wm_rt^2 + 0.013 wm_rt*repeat + 0.025 wm_rt*switch + 0.017 wm_rt*wm_rt[t-1] + 0.061 wm_rt*wm_rt[t-2] + 0.024 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + 0.0 repeat*switch + -0.199 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.003 repeat*wm_rt[t-3] + 0.078 repeat*wm_rt[t-4] + 0.02 switch^2 + -0.084 switch*wm_rt[t-1] + -0.148 switch*wm_rt[t-2] + -0.136 switch*wm_rt[t-3] + 0.362 switch*wm_rt[t-4] + -0.06 wm_rt[t-1]^2 + -0.055 wm_rt[t-1]*wm_rt[t-2] + -0.055 wm_rt[t-1]*wm_rt[t-3] + -0.059 wm_rt[t-1]*wm_rt[t-4] + -0.165 wm_rt[t-2]^2 + -0.164 wm_rt[t-2]*wm_rt[t-3] + -0.158 wm_rt[t-2]*wm_rt[t-4] + -0.058 wm_rt[t-3]^2 + -0.038 wm_rt[t-3]*wm_rt[t-4] + 0.123 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 85/400 --- L(Train): 0.4671314 --- L(Val, RNN): 0.4088248 --- L(Val, SINDy): 0.7702231 --- Time: 3.45s; --- Convergence: 2.10e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.056 1 + 0.453 value_reward_diff[t] + 0.717 reward_diff + -0.58 value_reward_diff^2 + 0.276 value_reward_diff*reward_diff + 0.721 reward_diff^2 \n",
      "value_persistance[t+1] = -0.284 1 + 0.847 value_persistance[t] + -0.095 repeat + 0.24 value_persistance^2 + -0.165 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.058 1 + 1.053 wm_rt[t] + 0.003 repeat + 0.017 switch + -0.095 wm_rt[t-1] + 0.035 wm_rt[t-2] + 0.001 wm_rt[t-3] + 0.217 wm_rt[t-4] + 0.102 wm_rt^2 + 0.013 wm_rt*repeat + 0.024 wm_rt*switch + 0.016 wm_rt*wm_rt[t-1] + 0.061 wm_rt*wm_rt[t-2] + 0.022 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.0 repeat*switch + -0.204 repeat*wm_rt[t-1] + -0.007 repeat*wm_rt[t-2] + -0.003 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + 0.018 switch^2 + -0.083 switch*wm_rt[t-1] + -0.139 switch*wm_rt[t-2] + -0.13 switch*wm_rt[t-3] + 0.362 switch*wm_rt[t-4] + -0.057 wm_rt[t-1]^2 + -0.052 wm_rt[t-1]*wm_rt[t-2] + -0.053 wm_rt[t-1]*wm_rt[t-3] + -0.057 wm_rt[t-1]*wm_rt[t-4] + -0.163 wm_rt[t-2]^2 + -0.162 wm_rt[t-2]*wm_rt[t-3] + -0.155 wm_rt[t-2]*wm_rt[t-4] + -0.053 wm_rt[t-3]^2 + -0.034 wm_rt[t-3]*wm_rt[t-4] + 0.121 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 86/400 --- L(Train): 0.4700819 --- L(Val, RNN): 0.4101718 --- L(Val, SINDy): 0.7790447 --- Time: 3.21s; --- Convergence: 1.72e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.044 1 + 0.448 value_reward_diff[t] + 0.727 reward_diff + -0.589 value_reward_diff^2 + 0.278 value_reward_diff*reward_diff + 0.731 reward_diff^2 \n",
      "value_persistance[t+1] = -0.288 1 + 0.844 value_persistance[t] + -0.098 repeat + 0.246 value_persistance^2 + -0.164 value_persistance*repeat + -0.099 repeat^2 \n",
      "wm_rt[t+1] = -0.057 1 + 1.053 wm_rt[t] + 0.007 repeat + 0.016 switch + -0.098 wm_rt[t-1] + 0.037 wm_rt[t-2] + 0.004 wm_rt[t-3] + 0.215 wm_rt[t-4] + 0.102 wm_rt^2 + 0.012 wm_rt*repeat + 0.023 wm_rt*switch + 0.015 wm_rt*wm_rt[t-1] + 0.061 wm_rt*wm_rt[t-2] + 0.021 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.004 repeat^2 + -0.0 repeat*switch + -0.208 repeat*wm_rt[t-1] + -0.009 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + 0.017 switch^2 + -0.087 switch*wm_rt[t-1] + -0.136 switch*wm_rt[t-2] + -0.13 switch*wm_rt[t-3] + 0.358 switch*wm_rt[t-4] + -0.059 wm_rt[t-1]^2 + -0.054 wm_rt[t-1]*wm_rt[t-2] + -0.055 wm_rt[t-1]*wm_rt[t-3] + -0.059 wm_rt[t-1]*wm_rt[t-4] + -0.164 wm_rt[t-2]^2 + -0.163 wm_rt[t-2]*wm_rt[t-3] + -0.156 wm_rt[t-2]*wm_rt[t-4] + -0.052 wm_rt[t-3]^2 + -0.032 wm_rt[t-3]*wm_rt[t-4] + 0.117 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 87/400 --- L(Train): 0.4648479 --- L(Val, RNN): 0.4124280 --- L(Val, SINDy): 0.7225933 --- Time: 3.66s; --- Convergence: 1.99e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.032 1 + 0.443 value_reward_diff[t] + 0.737 reward_diff + -0.598 value_reward_diff^2 + 0.279 value_reward_diff*reward_diff + 0.741 reward_diff^2 \n",
      "value_persistance[t+1] = -0.297 1 + 0.842 value_persistance[t] + -0.101 repeat + 0.25 value_persistance^2 + -0.163 value_persistance*repeat + -0.103 repeat^2 \n",
      "wm_rt[t+1] = -0.058 1 + 1.052 wm_rt[t] + 0.008 repeat + 0.015 switch + -0.101 wm_rt[t-1] + 0.04 wm_rt[t-2] + 0.006 wm_rt[t-3] + 0.211 wm_rt[t-4] + 0.102 wm_rt^2 + 0.012 wm_rt*repeat + 0.023 wm_rt*switch + 0.014 wm_rt*wm_rt[t-1] + 0.061 wm_rt*wm_rt[t-2] + 0.019 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + -0.0 repeat*switch + -0.214 repeat*wm_rt[t-1] + -0.011 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.074 repeat*wm_rt[t-4] + 0.016 switch^2 + -0.091 switch*wm_rt[t-1] + -0.133 switch*wm_rt[t-2] + -0.129 switch*wm_rt[t-3] + 0.353 switch*wm_rt[t-4] + -0.061 wm_rt[t-1]^2 + -0.057 wm_rt[t-1]*wm_rt[t-2] + -0.057 wm_rt[t-1]*wm_rt[t-3] + -0.061 wm_rt[t-1]*wm_rt[t-4] + -0.165 wm_rt[t-2]^2 + -0.164 wm_rt[t-2]*wm_rt[t-3] + -0.157 wm_rt[t-2]*wm_rt[t-4] + -0.05 wm_rt[t-3]^2 + -0.031 wm_rt[t-3]*wm_rt[t-4] + 0.112 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 88/400 --- L(Train): 0.4717507 --- L(Val, RNN): 0.4084994 --- L(Val, SINDy): 0.7253584 --- Time: 4.19s; --- Convergence: 2.96e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.021 1 + 0.44 value_reward_diff[t] + 0.747 reward_diff + -0.605 value_reward_diff^2 + 0.282 value_reward_diff*reward_diff + 0.751 reward_diff^2 \n",
      "value_persistance[t+1] = -0.304 1 + 0.84 value_persistance[t] + -0.1 repeat + 0.256 value_persistance^2 + -0.164 value_persistance*repeat + -0.101 repeat^2 \n",
      "wm_rt[t+1] = -0.06 1 + 1.052 wm_rt[t] + 0.006 repeat + 0.015 switch + -0.101 wm_rt[t-1] + 0.046 wm_rt[t-2] + 0.013 wm_rt[t-3] + 0.209 wm_rt[t-4] + 0.102 wm_rt^2 + 0.01 wm_rt*repeat + 0.023 wm_rt*switch + 0.013 wm_rt*wm_rt[t-1] + 0.06 wm_rt*wm_rt[t-2] + 0.017 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.003 repeat^2 + -0.0 repeat*switch + -0.222 repeat*wm_rt[t-1] + -0.012 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.074 repeat*wm_rt[t-4] + 0.016 switch^2 + -0.09 switch*wm_rt[t-1] + -0.125 switch*wm_rt[t-2] + -0.125 switch*wm_rt[t-3] + 0.349 switch*wm_rt[t-4] + -0.06 wm_rt[t-1]^2 + -0.055 wm_rt[t-1]*wm_rt[t-2] + -0.055 wm_rt[t-1]*wm_rt[t-3] + -0.06 wm_rt[t-1]*wm_rt[t-4] + -0.161 wm_rt[t-2]^2 + -0.16 wm_rt[t-2]*wm_rt[t-3] + -0.153 wm_rt[t-2]*wm_rt[t-4] + -0.044 wm_rt[t-3]^2 + -0.026 wm_rt[t-3]*wm_rt[t-4] + 0.108 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 89/400 --- L(Train): 0.4759861 --- L(Val, RNN): 0.4094999 --- L(Val, SINDy): 0.7652823 --- Time: 4.14s; --- Convergence: 1.98e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.011 1 + 0.439 value_reward_diff[t] + 0.758 reward_diff + -0.609 value_reward_diff^2 + 0.285 value_reward_diff*reward_diff + 0.761 reward_diff^2 \n",
      "value_persistance[t+1] = -0.313 1 + 0.837 value_persistance[t] + -0.096 repeat + 0.261 value_persistance^2 + -0.166 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.061 1 + 1.051 wm_rt[t] + 0.004 repeat + 0.015 switch + -0.104 wm_rt[t-1] + 0.051 wm_rt[t-2] + 0.015 wm_rt[t-3] + 0.203 wm_rt[t-4] + 0.102 wm_rt^2 + 0.009 wm_rt*repeat + 0.023 wm_rt*switch + 0.012 wm_rt*wm_rt[t-1] + 0.059 wm_rt*wm_rt[t-2] + 0.015 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + -0.229 repeat*wm_rt[t-1] + -0.013 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.074 repeat*wm_rt[t-4] + 0.016 switch^2 + -0.092 switch*wm_rt[t-1] + -0.122 switch*wm_rt[t-2] + -0.124 switch*wm_rt[t-3] + 0.341 switch*wm_rt[t-4] + -0.061 wm_rt[t-1]^2 + -0.055 wm_rt[t-1]*wm_rt[t-2] + -0.056 wm_rt[t-1]*wm_rt[t-3] + -0.061 wm_rt[t-1]*wm_rt[t-4] + -0.16 wm_rt[t-2]^2 + -0.158 wm_rt[t-2]*wm_rt[t-3] + -0.152 wm_rt[t-2]*wm_rt[t-4] + -0.042 wm_rt[t-3]^2 + -0.024 wm_rt[t-3]*wm_rt[t-4] + 0.102 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 90/400 --- L(Train): 0.4711376 --- L(Val, RNN): 0.4112447 --- L(Val, SINDy): 0.8355008 --- Time: 3.34s; --- Convergence: 1.86e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.003 1 + 0.438 value_reward_diff[t] + 0.768 reward_diff + -0.613 value_reward_diff^2 + 0.288 value_reward_diff*reward_diff + 0.772 reward_diff^2 \n",
      "value_persistance[t+1] = -0.321 1 + 0.835 value_persistance[t] + -0.092 repeat + 0.266 value_persistance^2 + -0.168 value_persistance*repeat + -0.094 repeat^2 \n",
      "wm_rt[t+1] = -0.061 1 + 1.051 wm_rt[t] + 0.003 repeat + 0.015 switch + -0.106 wm_rt[t-1] + 0.055 wm_rt[t-2] + 0.019 wm_rt[t-3] + 0.199 wm_rt[t-4] + 0.102 wm_rt^2 + 0.008 wm_rt*repeat + 0.023 wm_rt*switch + 0.012 wm_rt*wm_rt[t-1] + 0.059 wm_rt*wm_rt[t-2] + 0.014 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + -0.235 repeat*wm_rt[t-1] + -0.012 repeat*wm_rt[t-2] + 0.004 repeat*wm_rt[t-3] + 0.074 repeat*wm_rt[t-4] + 0.016 switch^2 + -0.094 switch*wm_rt[t-1] + -0.118 switch*wm_rt[t-2] + -0.124 switch*wm_rt[t-3] + 0.333 switch*wm_rt[t-4] + -0.061 wm_rt[t-1]^2 + -0.056 wm_rt[t-1]*wm_rt[t-2] + -0.056 wm_rt[t-1]*wm_rt[t-3] + -0.061 wm_rt[t-1]*wm_rt[t-4] + -0.158 wm_rt[t-2]^2 + -0.156 wm_rt[t-2]*wm_rt[t-3] + -0.15 wm_rt[t-2]*wm_rt[t-4] + -0.04 wm_rt[t-3]^2 + -0.022 wm_rt[t-3]*wm_rt[t-4] + 0.096 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 91/400 --- L(Train): 0.4841661 --- L(Val, RNN): 0.4120467 --- L(Val, SINDy): 0.7487793 --- Time: 3.95s; --- Convergence: 1.33e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.004 1 + 0.44 value_reward_diff[t] + 0.778 reward_diff + -0.614 value_reward_diff^2 + 0.292 value_reward_diff*reward_diff + 0.781 reward_diff^2 \n",
      "value_persistance[t+1] = -0.324 1 + 0.831 value_persistance[t] + -0.091 repeat + 0.272 value_persistance^2 + -0.169 value_persistance*repeat + -0.093 repeat^2 \n",
      "wm_rt[t+1] = -0.06 1 + 1.051 wm_rt[t] + 0.005 repeat + 0.015 switch + -0.106 wm_rt[t-1] + 0.062 wm_rt[t-2] + 0.024 wm_rt[t-3] + 0.198 wm_rt[t-4] + 0.101 wm_rt^2 + 0.007 wm_rt*repeat + 0.022 wm_rt*switch + 0.011 wm_rt*wm_rt[t-1] + 0.058 wm_rt*wm_rt[t-2] + 0.011 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.0 repeat*switch + -0.24 repeat*wm_rt[t-1] + -0.011 repeat*wm_rt[t-2] + 0.007 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + 0.016 switch^2 + -0.094 switch*wm_rt[t-1] + -0.113 switch*wm_rt[t-2] + -0.121 switch*wm_rt[t-3] + 0.328 switch*wm_rt[t-4] + -0.061 wm_rt[t-1]^2 + -0.055 wm_rt[t-1]*wm_rt[t-2] + -0.056 wm_rt[t-1]*wm_rt[t-3] + -0.061 wm_rt[t-1]*wm_rt[t-4] + -0.155 wm_rt[t-2]^2 + -0.153 wm_rt[t-2]*wm_rt[t-3] + -0.147 wm_rt[t-2]*wm_rt[t-4] + -0.035 wm_rt[t-3]^2 + -0.017 wm_rt[t-3]*wm_rt[t-4] + 0.094 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 92/400 --- L(Train): 0.4763186 --- L(Val, RNN): 0.4129985 --- L(Val, SINDy): 0.7280833 --- Time: 3.43s; --- Convergence: 1.14e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.008 1 + 0.444 value_reward_diff[t] + 0.788 reward_diff + -0.613 value_reward_diff^2 + 0.296 value_reward_diff*reward_diff + 0.791 reward_diff^2 \n",
      "value_persistance[t+1] = -0.33 1 + 0.829 value_persistance[t] + -0.095 repeat + 0.278 value_persistance^2 + -0.167 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.059 1 + 1.05 wm_rt[t] + 0.007 repeat + 0.015 switch + -0.108 wm_rt[t-1] + 0.065 wm_rt[t-2] + 0.027 wm_rt[t-3] + 0.195 wm_rt[t-4] + 0.101 wm_rt^2 + 0.007 wm_rt*repeat + 0.022 wm_rt*switch + 0.01 wm_rt*wm_rt[t-1] + 0.058 wm_rt*wm_rt[t-2] + 0.01 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.004 repeat^2 + -0.0 repeat*switch + -0.243 repeat*wm_rt[t-1] + -0.011 repeat*wm_rt[t-2] + 0.007 repeat*wm_rt[t-3] + 0.077 repeat*wm_rt[t-4] + 0.015 switch^2 + -0.097 switch*wm_rt[t-1] + -0.111 switch*wm_rt[t-2] + -0.12 switch*wm_rt[t-3] + 0.323 switch*wm_rt[t-4] + -0.062 wm_rt[t-1]^2 + -0.056 wm_rt[t-1]*wm_rt[t-2] + -0.057 wm_rt[t-1]*wm_rt[t-3] + -0.062 wm_rt[t-1]*wm_rt[t-4] + -0.155 wm_rt[t-2]^2 + -0.153 wm_rt[t-2]*wm_rt[t-3] + -0.147 wm_rt[t-2]*wm_rt[t-4] + -0.034 wm_rt[t-3]^2 + -0.016 wm_rt[t-3]*wm_rt[t-4] + 0.089 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 93/400 --- L(Train): 0.4706507 --- L(Val, RNN): 0.4130204 --- L(Val, SINDy): 0.8805867 --- Time: 3.37s; --- Convergence: 5.82e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.009 1 + 0.451 value_reward_diff[t] + 0.798 reward_diff + -0.609 value_reward_diff^2 + 0.301 value_reward_diff*reward_diff + 0.801 reward_diff^2 \n",
      "value_persistance[t+1] = -0.34 1 + 0.827 value_persistance[t] + -0.101 repeat + 0.282 value_persistance^2 + -0.164 value_persistance*repeat + -0.103 repeat^2 \n",
      "wm_rt[t+1] = -0.06 1 + 1.05 wm_rt[t] + 0.008 repeat + 0.013 switch + -0.111 wm_rt[t-1] + 0.066 wm_rt[t-2] + 0.027 wm_rt[t-3] + 0.191 wm_rt[t-4] + 0.101 wm_rt^2 + 0.006 wm_rt*repeat + 0.022 wm_rt*switch + 0.009 wm_rt*wm_rt[t-1] + 0.058 wm_rt*wm_rt[t-2] + 0.01 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + -0.0 repeat*switch + -0.248 repeat*wm_rt[t-1] + -0.013 repeat*wm_rt[t-2] + 0.005 repeat*wm_rt[t-3] + 0.077 repeat*wm_rt[t-4] + 0.014 switch^2 + -0.1 switch*wm_rt[t-1] + -0.109 switch*wm_rt[t-2] + -0.12 switch*wm_rt[t-3] + 0.316 switch*wm_rt[t-4] + -0.064 wm_rt[t-1]^2 + -0.058 wm_rt[t-1]*wm_rt[t-2] + -0.058 wm_rt[t-1]*wm_rt[t-3] + -0.064 wm_rt[t-1]*wm_rt[t-4] + -0.156 wm_rt[t-2]^2 + -0.154 wm_rt[t-2]*wm_rt[t-3] + -0.148 wm_rt[t-2]*wm_rt[t-4] + -0.035 wm_rt[t-3]^2 + -0.017 wm_rt[t-3]*wm_rt[t-4] + 0.084 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 94/400 --- L(Train): 0.4748059 --- L(Val, RNN): 0.4125369 --- L(Val, SINDy): 0.7042897 --- Time: 3.89s; --- Convergence: 5.33e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.008 1 + 0.459 value_reward_diff[t] + 0.808 reward_diff + -0.602 value_reward_diff^2 + 0.306 value_reward_diff*reward_diff + 0.811 reward_diff^2 \n",
      "value_persistance[t+1] = -0.338 1 + 0.822 value_persistance[t] + -0.097 repeat + 0.29 value_persistance^2 + -0.168 value_persistance*repeat + -0.098 repeat^2 \n",
      "wm_rt[t+1] = -0.061 1 + 1.049 wm_rt[t] + 0.009 repeat + 0.011 switch + -0.106 wm_rt[t-1] + 0.074 wm_rt[t-2] + 0.033 wm_rt[t-3] + 0.194 wm_rt[t-4] + 0.101 wm_rt^2 + 0.005 wm_rt*repeat + 0.021 wm_rt*switch + 0.008 wm_rt*wm_rt[t-1] + 0.057 wm_rt*wm_rt[t-2] + 0.008 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + 0.0 repeat*switch + -0.253 repeat*wm_rt[t-1] + -0.014 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.079 repeat*wm_rt[t-4] + 0.012 switch^2 + -0.095 switch*wm_rt[t-1] + -0.101 switch*wm_rt[t-2] + -0.113 switch*wm_rt[t-3] + 0.317 switch*wm_rt[t-4] + -0.058 wm_rt[t-1]^2 + -0.052 wm_rt[t-1]*wm_rt[t-2] + -0.052 wm_rt[t-1]*wm_rt[t-3] + -0.058 wm_rt[t-1]*wm_rt[t-4] + -0.15 wm_rt[t-2]^2 + -0.147 wm_rt[t-2]*wm_rt[t-3] + -0.142 wm_rt[t-2]*wm_rt[t-4] + -0.029 wm_rt[t-3]^2 + -0.012 wm_rt[t-3]*wm_rt[t-4] + 0.086 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 95/400 --- L(Train): 0.4812925 --- L(Val, RNN): 0.4120530 --- L(Val, SINDy): 0.7853367 --- Time: 3.52s; --- Convergence: 5.08e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.007 1 + 0.468 value_reward_diff[t] + 0.819 reward_diff + -0.596 value_reward_diff^2 + 0.31 value_reward_diff*reward_diff + 0.821 reward_diff^2 \n",
      "value_persistance[t+1] = -0.341 1 + 0.819 value_persistance[t] + -0.095 repeat + 0.295 value_persistance^2 + -0.169 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.062 1 + 1.049 wm_rt[t] + 0.01 repeat + 0.01 switch + -0.106 wm_rt[t-1] + 0.076 wm_rt[t-2] + 0.033 wm_rt[t-3] + 0.192 wm_rt[t-4] + 0.101 wm_rt^2 + 0.004 wm_rt*repeat + 0.021 wm_rt*switch + 0.007 wm_rt*wm_rt[t-1] + 0.058 wm_rt*wm_rt[t-2] + 0.007 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + 0.0 repeat*switch + -0.259 repeat*wm_rt[t-1] + -0.018 repeat*wm_rt[t-2] + -0.005 repeat*wm_rt[t-3] + 0.077 repeat*wm_rt[t-4] + 0.011 switch^2 + -0.096 switch*wm_rt[t-1] + -0.099 switch*wm_rt[t-2] + -0.111 switch*wm_rt[t-3] + 0.315 switch*wm_rt[t-4] + -0.058 wm_rt[t-1]^2 + -0.052 wm_rt[t-1]*wm_rt[t-2] + -0.053 wm_rt[t-1]*wm_rt[t-3] + -0.058 wm_rt[t-1]*wm_rt[t-4] + -0.15 wm_rt[t-2]^2 + -0.148 wm_rt[t-2]*wm_rt[t-3] + -0.142 wm_rt[t-2]*wm_rt[t-4] + -0.029 wm_rt[t-3]^2 + -0.012 wm_rt[t-3]*wm_rt[t-4] + 0.083 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 96/400 --- L(Train): 0.4709285 --- L(Val, RNN): 0.4129357 --- L(Val, SINDy): 0.8150395 --- Time: 3.82s; --- Convergence: 6.96e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.01 1 + 0.474 value_reward_diff[t] + 0.831 reward_diff + -0.593 value_reward_diff^2 + 0.312 value_reward_diff*reward_diff + 0.833 reward_diff^2 \n",
      "value_persistance[t+1] = -0.351 1 + 0.82 value_persistance[t] + -0.099 repeat + 0.293 value_persistance^2 + -0.166 value_persistance*repeat + -0.1 repeat^2 \n",
      "wm_rt[t+1] = -0.06 1 + 1.049 wm_rt[t] + 0.01 repeat + 0.011 switch + -0.112 wm_rt[t-1] + 0.073 wm_rt[t-2] + 0.029 wm_rt[t-3] + 0.186 wm_rt[t-4] + 0.101 wm_rt^2 + 0.003 wm_rt*repeat + 0.021 wm_rt*switch + 0.007 wm_rt*wm_rt[t-1] + 0.06 wm_rt*wm_rt[t-2] + 0.008 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.0 repeat*switch + -0.265 repeat*wm_rt[t-1] + -0.024 repeat*wm_rt[t-2] + -0.014 repeat*wm_rt[t-3] + 0.072 repeat*wm_rt[t-4] + 0.012 switch^2 + -0.102 switch*wm_rt[t-1] + -0.101 switch*wm_rt[t-2] + -0.112 switch*wm_rt[t-3] + 0.309 switch*wm_rt[t-4] + -0.063 wm_rt[t-1]^2 + -0.057 wm_rt[t-1]*wm_rt[t-2] + -0.058 wm_rt[t-1]*wm_rt[t-3] + -0.064 wm_rt[t-1]*wm_rt[t-4] + -0.155 wm_rt[t-2]^2 + -0.153 wm_rt[t-2]*wm_rt[t-3] + -0.147 wm_rt[t-2]*wm_rt[t-4] + -0.034 wm_rt[t-3]^2 + -0.017 wm_rt[t-3]*wm_rt[t-4] + 0.077 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 97/400 --- L(Train): 0.4861830 --- L(Val, RNN): 0.4119150 --- L(Val, SINDy): 0.7493600 --- Time: 4.04s; --- Convergence: 8.58e-04; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.015 1 + 0.48 value_reward_diff[t] + 0.844 reward_diff + -0.592 value_reward_diff^2 + 0.311 value_reward_diff*reward_diff + 0.846 reward_diff^2 \n",
      "value_persistance[t+1] = -0.359 1 + 0.821 value_persistance[t] + -0.099 repeat + 0.292 value_persistance^2 + -0.166 value_persistance*repeat + -0.1 repeat^2 \n",
      "wm_rt[t+1] = -0.058 1 + 1.049 wm_rt[t] + 0.009 repeat + 0.013 switch + -0.111 wm_rt[t-1] + 0.077 wm_rt[t-2] + 0.034 wm_rt[t-3] + 0.187 wm_rt[t-4] + 0.101 wm_rt^2 + 0.001 wm_rt*repeat + 0.02 wm_rt*switch + 0.006 wm_rt*wm_rt[t-1] + 0.058 wm_rt*wm_rt[t-2] + 0.005 wm_rt*wm_rt[t-3] + -0.121 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.0 repeat*switch + -0.271 repeat*wm_rt[t-1] + -0.024 repeat*wm_rt[t-2] + -0.016 repeat*wm_rt[t-3] + 0.07 repeat*wm_rt[t-4] + 0.014 switch^2 + -0.101 switch*wm_rt[t-1] + -0.097 switch*wm_rt[t-2] + -0.107 switch*wm_rt[t-3] + 0.311 switch*wm_rt[t-4] + -0.062 wm_rt[t-1]^2 + -0.056 wm_rt[t-1]*wm_rt[t-2] + -0.056 wm_rt[t-1]*wm_rt[t-3] + -0.062 wm_rt[t-1]*wm_rt[t-4] + -0.152 wm_rt[t-2]^2 + -0.15 wm_rt[t-2]*wm_rt[t-3] + -0.144 wm_rt[t-2]*wm_rt[t-4] + -0.03 wm_rt[t-3]^2 + -0.013 wm_rt[t-3]*wm_rt[t-4] + 0.077 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 98/400 --- L(Train): 0.4780309 --- L(Val, RNN): 0.4132369 --- L(Val, SINDy): 0.7860463 --- Time: 3.87s; --- Convergence: 1.09e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.02 1 + 0.484 value_reward_diff[t] + 0.856 reward_diff + -0.593 value_reward_diff^2 + 0.309 value_reward_diff*reward_diff + 0.858 reward_diff^2 \n",
      "value_persistance[t+1] = -0.359 1 + 0.817 value_persistance[t] + -0.093 repeat + 0.298 value_persistance^2 + -0.171 value_persistance*repeat + -0.095 repeat^2 \n",
      "wm_rt[t+1] = -0.058 1 + 1.048 wm_rt[t] + 0.008 repeat + 0.012 switch + -0.11 wm_rt[t-1] + 0.082 wm_rt[t-2] + 0.039 wm_rt[t-3] + 0.189 wm_rt[t-4] + 0.101 wm_rt^2 + -0.0 wm_rt*repeat + 0.02 wm_rt*switch + 0.005 wm_rt*wm_rt[t-1] + 0.055 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.124 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + 0.0 repeat*switch + -0.277 repeat*wm_rt[t-1] + -0.021 repeat*wm_rt[t-2] + -0.017 repeat*wm_rt[t-3] + 0.071 repeat*wm_rt[t-4] + 0.013 switch^2 + -0.1 switch*wm_rt[t-1] + -0.092 switch*wm_rt[t-2] + -0.101 switch*wm_rt[t-3] + 0.312 switch*wm_rt[t-4] + -0.06 wm_rt[t-1]^2 + -0.054 wm_rt[t-1]*wm_rt[t-2] + -0.055 wm_rt[t-1]*wm_rt[t-3] + -0.061 wm_rt[t-1]*wm_rt[t-4] + -0.148 wm_rt[t-2]^2 + -0.146 wm_rt[t-2]*wm_rt[t-3] + -0.14 wm_rt[t-2]*wm_rt[t-4] + -0.025 wm_rt[t-3]^2 + -0.008 wm_rt[t-3]*wm_rt[t-4] + 0.078 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 99/400 --- L(Train): 0.4827545 --- L(Val, RNN): 0.4155856 --- L(Val, SINDy): 0.7962847 --- Time: 3.76s; --- Convergence: 1.72e-03; LR: 1.00e-02; Metric: inf; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.027 1 + 0.487 value_reward_diff[t] + 0.869 reward_diff + -0.594 value_reward_diff^2 + 0.305 value_reward_diff*reward_diff + 0.87 reward_diff^2 \n",
      "value_persistance[t+1] = -0.357 1 + 0.812 value_persistance[t] + -0.089 repeat + 0.305 value_persistance^2 + -0.175 value_persistance*repeat + -0.091 repeat^2 \n",
      "wm_rt[t+1] = -0.061 1 + 1.047 wm_rt[t] + 0.008 repeat + 0.009 switch + -0.112 wm_rt[t-1] + 0.085 wm_rt[t-2] + 0.04 wm_rt[t-3] + 0.188 wm_rt[t-4] + 0.101 wm_rt^2 + -0.002 wm_rt*repeat + 0.019 wm_rt*switch + 0.004 wm_rt*wm_rt[t-1] + 0.053 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.125 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + 0.0 repeat*switch + -0.282 repeat*wm_rt[t-1] + -0.018 repeat*wm_rt[t-2] + -0.018 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + 0.009 switch^2 + -0.102 switch*wm_rt[t-1] + -0.092 switch*wm_rt[t-2] + -0.1 switch*wm_rt[t-3] + 0.309 switch*wm_rt[t-4] + -0.062 wm_rt[t-1]^2 + -0.056 wm_rt[t-1]*wm_rt[t-2] + -0.057 wm_rt[t-1]*wm_rt[t-3] + -0.063 wm_rt[t-1]*wm_rt[t-4] + -0.147 wm_rt[t-2]^2 + -0.144 wm_rt[t-2]*wm_rt[t-3] + -0.139 wm_rt[t-2]*wm_rt[t-4] + -0.025 wm_rt[t-3]^2 + -0.008 wm_rt[t-3]*wm_rt[t-4] + 0.076 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\n",
      ">>> Warmup complete (epoch 100). Reset optimizer state for 3 SINDy parameters (fresh start at full regularization strength).\n",
      "\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 100/400 --- L(Train): 0.4850138 --- L(Val, RNN): 0.4181865 --- L(Val, SINDy): 0.8319465 --- Time: 4.06s; --- Convergence: 2.16e-03; LR: 1.00e-02; Metric: 0.4181865; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.031 1 + 0.492 value_reward_diff[t] + 0.88 reward_diff + -0.594 value_reward_diff^2 + 0.299 value_reward_diff*reward_diff + 0.882 reward_diff^2 \n",
      "value_persistance[t+1] = -0.357 1 + 0.807 value_persistance[t] + -0.093 repeat + 0.31 value_persistance^2 + -0.172 value_persistance*repeat + -0.094 repeat^2 \n",
      "wm_rt[t+1] = -0.061 1 + 1.047 wm_rt[t] + 0.01 repeat + 0.007 switch + -0.116 wm_rt[t-1] + 0.086 wm_rt[t-2] + 0.039 wm_rt[t-3] + 0.185 wm_rt[t-4] + 0.101 wm_rt^2 + -0.003 wm_rt*repeat + 0.018 wm_rt*switch + 0.003 wm_rt*wm_rt[t-1] + 0.051 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.125 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.0 repeat*switch + -0.285 repeat*wm_rt[t-1] + -0.014 repeat*wm_rt[t-2] + -0.02 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + 0.008 switch^2 + -0.107 switch*wm_rt[t-1] + -0.093 switch*wm_rt[t-2] + -0.101 switch*wm_rt[t-3] + 0.303 switch*wm_rt[t-4] + -0.066 wm_rt[t-1]^2 + -0.059 wm_rt[t-1]*wm_rt[t-2] + -0.06 wm_rt[t-1]*wm_rt[t-3] + -0.066 wm_rt[t-1]*wm_rt[t-4] + -0.147 wm_rt[t-2]^2 + -0.144 wm_rt[t-2]*wm_rt[t-3] + -0.139 wm_rt[t-2]*wm_rt[t-4] + -0.026 wm_rt[t-3]^2 + -0.009 wm_rt[t-3]*wm_rt[t-4] + 0.073 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 1, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 101/400 --- L(Train): 0.4852248 --- L(Val, RNN): 0.4182378 --- L(Val, SINDy): 1.1192037 --- Time: 4.27s; --- Convergence: 1.11e-03; LR: 1.00e-02; Metric: 0.4181865; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.021 1 + 0.502 value_reward_diff[t] + 0.87 reward_diff + -0.584 value_reward_diff^2 + 0.289 value_reward_diff*reward_diff + 0.872 reward_diff^2 \n",
      "value_persistance[t+1] = -0.347 1 + 0.797 value_persistance[t] + -0.103 repeat + 0.32 value_persistance^2 + -0.162 value_persistance*repeat + -0.104 repeat^2 \n",
      "wm_rt[t+1] = -0.051 1 + 1.057 wm_rt[t] + 0.0 repeat + 0.017 switch + -0.106 wm_rt[t-1] + 0.096 wm_rt[t-2] + 0.049 wm_rt[t-3] + 0.195 wm_rt[t-4] + 0.091 wm_rt^2 + -0.013 wm_rt*repeat + 0.028 wm_rt*switch + 0.013 wm_rt*wm_rt[t-1] + 0.041 wm_rt*wm_rt[t-2] + -0.013 wm_rt*wm_rt[t-3] + -0.135 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + 0.0 repeat*switch + -0.275 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.029 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.018 switch^2 + -0.097 switch*wm_rt[t-1] + -0.083 switch*wm_rt[t-2] + -0.091 switch*wm_rt[t-3] + 0.313 switch*wm_rt[t-4] + -0.056 wm_rt[t-1]^2 + -0.049 wm_rt[t-1]*wm_rt[t-2] + -0.05 wm_rt[t-1]*wm_rt[t-3] + -0.056 wm_rt[t-1]*wm_rt[t-4] + -0.137 wm_rt[t-2]^2 + -0.134 wm_rt[t-2]*wm_rt[t-3] + -0.129 wm_rt[t-2]*wm_rt[t-4] + -0.016 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.083 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 2, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 2, 2, 1, 2, 0, 2, 2, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 102/400 --- L(Train): 0.4907758 --- L(Val, RNN): 0.4174051 --- L(Val, SINDy): 0.8314175 --- Time: 4.49s; --- Convergence: 9.69e-04; LR: 1.00e-02; Metric: 0.4174051; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.016 1 + 0.508 value_reward_diff[t] + 0.867 reward_diff + -0.578 value_reward_diff^2 + 0.279 value_reward_diff*reward_diff + 0.868 reward_diff^2 \n",
      "value_persistance[t+1] = -0.354 1 + 0.804 value_persistance[t] + -0.113 repeat + 0.314 value_persistance^2 + -0.152 value_persistance*repeat + -0.114 repeat^2 \n",
      "wm_rt[t+1] = -0.057 1 + 1.05 wm_rt[t] + 0.007 repeat + 0.011 switch + -0.111 wm_rt[t-1] + 0.09 wm_rt[t-2] + 0.043 wm_rt[t-3] + 0.188 wm_rt[t-4] + 0.084 wm_rt^2 + -0.007 wm_rt*repeat + 0.022 wm_rt*switch + 0.008 wm_rt*wm_rt[t-1] + 0.047 wm_rt*wm_rt[t-2] + -0.005 wm_rt*wm_rt[t-3] + -0.128 wm_rt*wm_rt[t-4] + 0.004 repeat^2 + 0.0 repeat*switch + -0.268 repeat*wm_rt[t-1] + -0.009 repeat*wm_rt[t-2] + -0.038 repeat*wm_rt[t-3] + 0.08 repeat*wm_rt[t-4] + 0.011 switch^2 + -0.102 switch*wm_rt[t-1] + -0.089 switch*wm_rt[t-2] + -0.097 switch*wm_rt[t-3] + 0.306 switch*wm_rt[t-4] + -0.062 wm_rt[t-1]^2 + -0.055 wm_rt[t-1]*wm_rt[t-2] + -0.056 wm_rt[t-1]*wm_rt[t-3] + -0.062 wm_rt[t-1]*wm_rt[t-4] + -0.142 wm_rt[t-2]^2 + -0.14 wm_rt[t-2]*wm_rt[t-3] + -0.135 wm_rt[t-2]*wm_rt[t-4] + -0.022 wm_rt[t-3]^2 + -0.006 wm_rt[t-3]*wm_rt[t-4] + 0.076 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 3, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 1, 3, 3, 0, 0, 3, 0, 0, 3, 3, 3, 2, 3, 0, 3, 3, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 103/400 --- L(Train): 0.4811572 --- L(Val, RNN): 0.4164426 --- L(Val, SINDy): 0.8842148 --- Time: 3.88s; --- Convergence: 9.66e-04; LR: 1.00e-02; Metric: 0.4164426; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.016 1 + 0.51 value_reward_diff[t] + 0.87 reward_diff + -0.578 value_reward_diff^2 + 0.269 value_reward_diff*reward_diff + 0.871 reward_diff^2 \n",
      "value_persistance[t+1] = -0.357 1 + 0.806 value_persistance[t] + -0.111 repeat + 0.312 value_persistance^2 + -0.153 value_persistance*repeat + -0.113 repeat^2 \n",
      "wm_rt[t+1] = -0.063 1 + 1.043 wm_rt[t] + 0.015 repeat + 0.004 switch + -0.117 wm_rt[t-1] + 0.086 wm_rt[t-2] + 0.037 wm_rt[t-3] + 0.182 wm_rt[t-4] + 0.077 wm_rt^2 + -0.0 wm_rt*repeat + 0.016 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.05 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.122 wm_rt*wm_rt[t-4] + 0.012 repeat^2 + 0.0 repeat*switch + -0.259 repeat*wm_rt[t-1] + -0.007 repeat*wm_rt[t-2] + -0.042 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + 0.005 switch^2 + -0.108 switch*wm_rt[t-1] + -0.094 switch*wm_rt[t-2] + -0.103 switch*wm_rt[t-3] + 0.3 switch*wm_rt[t-4] + -0.067 wm_rt[t-1]^2 + -0.06 wm_rt[t-1]*wm_rt[t-2] + -0.061 wm_rt[t-1]*wm_rt[t-3] + -0.067 wm_rt[t-1]*wm_rt[t-4] + -0.147 wm_rt[t-2]^2 + -0.145 wm_rt[t-2]*wm_rt[t-3] + -0.139 wm_rt[t-2]*wm_rt[t-4] + -0.028 wm_rt[t-3]^2 + -0.011 wm_rt[t-3]*wm_rt[t-4] + 0.07 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 4, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 2, 4, 4, 0, 0, 4, 0, 0, 4, 4, 4, 3, 4, 0, 4, 4, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 104/400 --- L(Train): 0.4812475 --- L(Val, RNN): 0.4160808 --- L(Val, SINDy): 0.9383555 --- Time: 3.72s; --- Convergence: 6.64e-04; LR: 1.00e-02; Metric: 0.4160808; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.02 1 + 0.509 value_reward_diff[t] + 0.876 reward_diff + -0.582 value_reward_diff^2 + 0.26 value_reward_diff*reward_diff + 0.876 reward_diff^2 \n",
      "value_persistance[t+1] = -0.353 1 + 0.801 value_persistance[t] + -0.106 repeat + 0.317 value_persistance^2 + -0.159 value_persistance*repeat + -0.107 repeat^2 \n",
      "wm_rt[t+1] = -0.062 1 + 1.045 wm_rt[t] + 0.022 repeat + 0.003 switch + -0.117 wm_rt[t-1] + 0.087 wm_rt[t-2] + 0.037 wm_rt[t-3] + 0.181 wm_rt[t-4] + 0.078 wm_rt^2 + 0.004 wm_rt*repeat + 0.014 wm_rt*switch + 0.004 wm_rt*wm_rt[t-1] + 0.047 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.122 wm_rt*wm_rt[t-4] + 0.02 repeat^2 + -0.0 repeat*switch + -0.252 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.039 repeat*wm_rt[t-3] + 0.078 repeat*wm_rt[t-4] + 0.003 switch^2 + -0.108 switch*wm_rt[t-1] + -0.094 switch*wm_rt[t-2] + -0.104 switch*wm_rt[t-3] + 0.298 switch*wm_rt[t-4] + -0.067 wm_rt[t-1]^2 + -0.061 wm_rt[t-1]*wm_rt[t-2] + -0.061 wm_rt[t-1]*wm_rt[t-3] + -0.068 wm_rt[t-1]*wm_rt[t-4] + -0.146 wm_rt[t-2]^2 + -0.144 wm_rt[t-2]*wm_rt[t-3] + -0.138 wm_rt[t-2]*wm_rt[t-4] + -0.028 wm_rt[t-3]^2 + -0.011 wm_rt[t-3]*wm_rt[t-4] + 0.068 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 5, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 3, 5, 5, 0, 0, 5, 0, 0, 5, 5, 5, 4, 5, 0, 5, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 105/400 --- L(Train): 0.4793609 --- L(Val, RNN): 0.4162717 --- L(Val, SINDy): 1.0010490 --- Time: 3.55s; --- Convergence: 4.27e-04; LR: 1.00e-02; Metric: 0.4160808; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.025 1 + 0.507 value_reward_diff[t] + 0.883 reward_diff + -0.587 value_reward_diff^2 + 0.25 value_reward_diff*reward_diff + 0.884 reward_diff^2 \n",
      "value_persistance[t+1] = -0.347 1 + 0.795 value_persistance[t] + -0.099 repeat + 0.323 value_persistance^2 + -0.165 value_persistance*repeat + -0.101 repeat^2 \n",
      "wm_rt[t+1] = -0.059 1 + 1.048 wm_rt[t] + 0.02 repeat + 0.004 switch + -0.115 wm_rt[t-1] + 0.091 wm_rt[t-2] + 0.04 wm_rt[t-3] + 0.182 wm_rt[t-4] + 0.081 wm_rt^2 + 0.0 wm_rt*repeat + 0.016 wm_rt*switch + 0.003 wm_rt*wm_rt[t-1] + 0.042 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.124 wm_rt*wm_rt[t-4] + 0.017 repeat^2 + -0.0 repeat*switch + -0.255 repeat*wm_rt[t-1] + 0.005 repeat*wm_rt[t-2] + -0.035 repeat*wm_rt[t-3] + 0.08 repeat*wm_rt[t-4] + 0.005 switch^2 + -0.106 switch*wm_rt[t-1] + -0.091 switch*wm_rt[t-2] + -0.102 switch*wm_rt[t-3] + 0.298 switch*wm_rt[t-4] + -0.065 wm_rt[t-1]^2 + -0.058 wm_rt[t-1]*wm_rt[t-2] + -0.059 wm_rt[t-1]*wm_rt[t-3] + -0.065 wm_rt[t-1]*wm_rt[t-4] + -0.142 wm_rt[t-2]^2 + -0.14 wm_rt[t-2]*wm_rt[t-3] + -0.134 wm_rt[t-2]*wm_rt[t-4] + -0.026 wm_rt[t-3]^2 + -0.009 wm_rt[t-3]*wm_rt[t-4] + 0.069 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 6, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 4, 6, 6, 0, 0, 6, 0, 0, 6, 6, 6, 5, 6, 0, 6, 6, 0, 6, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 106/400 --- L(Train): 0.4780425 --- L(Val, RNN): 0.4160297 --- L(Val, SINDy): 1.0402868 --- Time: 3.68s; --- Convergence: 3.35e-04; LR: 1.00e-02; Metric: 0.4160297; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.031 1 + 0.503 value_reward_diff[t] + 0.891 reward_diff + -0.593 value_reward_diff^2 + 0.241 value_reward_diff*reward_diff + 0.891 reward_diff^2 \n",
      "value_persistance[t+1] = -0.343 1 + 0.791 value_persistance[t] + -0.094 repeat + 0.327 value_persistance^2 + -0.17 value_persistance*repeat + -0.096 repeat^2 \n",
      "wm_rt[t+1] = -0.056 1 + 1.051 wm_rt[t] + 0.016 repeat + 0.006 switch + -0.112 wm_rt[t-1] + 0.094 wm_rt[t-2] + 0.042 wm_rt[t-3] + 0.183 wm_rt[t-4] + 0.085 wm_rt^2 + -0.005 wm_rt*repeat + 0.018 wm_rt*switch + -0.0 wm_rt*wm_rt[t-1] + 0.037 wm_rt*wm_rt[t-2] + -0.005 wm_rt*wm_rt[t-3] + -0.126 wm_rt*wm_rt[t-4] + 0.013 repeat^2 + -0.0 repeat*switch + -0.259 repeat*wm_rt[t-1] + 0.01 repeat*wm_rt[t-2] + -0.033 repeat*wm_rt[t-3] + 0.079 repeat*wm_rt[t-4] + 0.007 switch^2 + -0.103 switch*wm_rt[t-1] + -0.088 switch*wm_rt[t-2] + -0.099 switch*wm_rt[t-3] + 0.3 switch*wm_rt[t-4] + -0.062 wm_rt[t-1]^2 + -0.055 wm_rt[t-1]*wm_rt[t-2] + -0.056 wm_rt[t-1]*wm_rt[t-3] + -0.063 wm_rt[t-1]*wm_rt[t-4] + -0.138 wm_rt[t-2]^2 + -0.136 wm_rt[t-2]*wm_rt[t-3] + -0.131 wm_rt[t-2]*wm_rt[t-4] + -0.023 wm_rt[t-3]^2 + -0.006 wm_rt[t-3]*wm_rt[t-4] + 0.07 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 7, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 7, 7, 0, 0, 7, 0, 0, 7, 7, 7, 6, 7, 0, 7, 7, 0, 7, 7, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 107/400 --- L(Train): 0.4721246 --- L(Val, RNN): 0.4164520 --- L(Val, SINDy): 1.0568013 --- Time: 4.00s; --- Convergence: 3.78e-04; LR: 1.00e-02; Metric: 0.4160297; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.037 1 + 0.5 value_reward_diff[t] + 0.899 reward_diff + -0.599 value_reward_diff^2 + 0.231 value_reward_diff*reward_diff + 0.899 reward_diff^2 \n",
      "value_persistance[t+1] = -0.345 1 + 0.792 value_persistance[t] + -0.093 repeat + 0.326 value_persistance^2 + -0.171 value_persistance*repeat + -0.095 repeat^2 \n",
      "wm_rt[t+1] = -0.053 1 + 1.054 wm_rt[t] + 0.01 repeat + 0.009 switch + -0.11 wm_rt[t-1] + 0.097 wm_rt[t-2] + 0.044 wm_rt[t-3] + 0.183 wm_rt[t-4] + 0.088 wm_rt^2 + -0.011 wm_rt*repeat + 0.021 wm_rt*switch + -0.005 wm_rt*wm_rt[t-1] + 0.032 wm_rt*wm_rt[t-2] + -0.007 wm_rt*wm_rt[t-3] + -0.128 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.0 repeat*switch + -0.265 repeat*wm_rt[t-1] + 0.014 repeat*wm_rt[t-2] + -0.032 repeat*wm_rt[t-3] + 0.077 repeat*wm_rt[t-4] + 0.009 switch^2 + -0.101 switch*wm_rt[t-1] + -0.085 switch*wm_rt[t-2] + -0.098 switch*wm_rt[t-3] + 0.3 switch*wm_rt[t-4] + -0.06 wm_rt[t-1]^2 + -0.053 wm_rt[t-1]*wm_rt[t-2] + -0.054 wm_rt[t-1]*wm_rt[t-3] + -0.06 wm_rt[t-1]*wm_rt[t-4] + -0.135 wm_rt[t-2]^2 + -0.133 wm_rt[t-2]*wm_rt[t-3] + -0.128 wm_rt[t-2]*wm_rt[t-4] + -0.021 wm_rt[t-3]^2 + -0.005 wm_rt[t-3]*wm_rt[t-4] + 0.07 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 8, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 8, 8, 0, 0, 8, 0, 0, 8, 8, 8, 7, 8, 0, 8, 8, 0, 8, 8, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 108/400 --- L(Train): 0.4702058 --- L(Val, RNN): 0.4170568 --- L(Val, SINDy): 0.9809694 --- Time: 3.77s; --- Convergence: 4.92e-04; LR: 1.00e-02; Metric: 0.4160297; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.042 1 + 0.498 value_reward_diff[t] + 0.908 reward_diff + -0.604 value_reward_diff^2 + 0.223 value_reward_diff*reward_diff + 0.908 reward_diff^2 \n",
      "value_persistance[t+1] = -0.348 1 + 0.795 value_persistance[t] + -0.094 repeat + 0.322 value_persistance^2 + -0.17 value_persistance*repeat + -0.096 repeat^2 \n",
      "wm_rt[t+1] = -0.052 1 + 1.056 wm_rt[t] + 0.005 repeat + 0.009 switch + -0.109 wm_rt[t-1] + 0.099 wm_rt[t-2] + 0.044 wm_rt[t-3] + 0.181 wm_rt[t-4] + 0.089 wm_rt^2 + -0.016 wm_rt*repeat + 0.021 wm_rt*switch + -0.01 wm_rt*wm_rt[t-1] + 0.029 wm_rt*wm_rt[t-2] + -0.007 wm_rt*wm_rt[t-3] + -0.128 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.0 repeat*switch + -0.27 repeat*wm_rt[t-1] + 0.016 repeat*wm_rt[t-2] + -0.033 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + 0.01 switch^2 + -0.1 switch*wm_rt[t-1] + -0.083 switch*wm_rt[t-2] + -0.097 switch*wm_rt[t-3] + 0.299 switch*wm_rt[t-4] + -0.059 wm_rt[t-1]^2 + -0.052 wm_rt[t-1]*wm_rt[t-2] + -0.053 wm_rt[t-1]*wm_rt[t-3] + -0.059 wm_rt[t-1]*wm_rt[t-4] + -0.133 wm_rt[t-2]^2 + -0.131 wm_rt[t-2]*wm_rt[t-3] + -0.126 wm_rt[t-2]*wm_rt[t-4] + -0.021 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + 0.068 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 9, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 9, 9, 0, 0, 9, 0, 0, 9, 9, 9, 8, 9, 0, 9, 9, 0, 9, 9, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 109/400 --- L(Train): 0.4680434 --- L(Val, RNN): 0.4171794 --- L(Val, SINDy): 0.9434949 --- Time: 3.98s; --- Convergence: 3.07e-04; LR: 1.00e-02; Metric: 0.4160297; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.047 1 + 0.497 value_reward_diff[t] + 0.915 reward_diff + -0.608 value_reward_diff^2 + 0.215 value_reward_diff*reward_diff + 0.915 reward_diff^2 \n",
      "value_persistance[t+1] = -0.352 1 + 0.798 value_persistance[t] + -0.096 repeat + 0.319 value_persistance^2 + -0.168 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.051 1 + 1.057 wm_rt[t] + 0.001 repeat + 0.009 switch + -0.109 wm_rt[t-1] + 0.101 wm_rt[t-2] + 0.044 wm_rt[t-3] + 0.179 wm_rt[t-4] + 0.09 wm_rt^2 + -0.02 wm_rt*repeat + 0.022 wm_rt*switch + -0.015 wm_rt*wm_rt[t-1] + 0.027 wm_rt*wm_rt[t-2] + -0.007 wm_rt*wm_rt[t-3] + -0.127 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + -0.273 repeat*wm_rt[t-1] + 0.017 repeat*wm_rt[t-2] + -0.034 repeat*wm_rt[t-3] + 0.069 repeat*wm_rt[t-4] + 0.01 switch^2 + -0.1 switch*wm_rt[t-1] + -0.082 switch*wm_rt[t-2] + -0.097 switch*wm_rt[t-3] + 0.297 switch*wm_rt[t-4] + -0.058 wm_rt[t-1]^2 + -0.052 wm_rt[t-1]*wm_rt[t-2] + -0.053 wm_rt[t-1]*wm_rt[t-3] + -0.059 wm_rt[t-1]*wm_rt[t-4] + -0.132 wm_rt[t-2]^2 + -0.13 wm_rt[t-2]*wm_rt[t-3] + -0.124 wm_rt[t-2]*wm_rt[t-4] + -0.021 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + 0.066 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 10, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 10, 10, 0, 0, 10, 0, 0, 10, 10, 10, 9, 10, 0, 10, 10, 0, 10, 10, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 110/400 --- L(Train): 0.4653208 --- L(Val, RNN): 0.4165216 --- L(Val, SINDy): 0.8655344 --- Time: 3.23s; --- Convergence: 4.82e-04; LR: 1.00e-02; Metric: 0.4160297; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.051 1 + 0.498 value_reward_diff[t] + 0.924 reward_diff + -0.61 value_reward_diff^2 + 0.208 value_reward_diff*reward_diff + 0.924 reward_diff^2 \n",
      "value_persistance[t+1] = -0.354 1 + 0.801 value_persistance[t] + -0.097 repeat + 0.317 value_persistance^2 + -0.167 value_persistance*repeat + -0.098 repeat^2 \n",
      "wm_rt[t+1] = -0.051 1 + 1.058 wm_rt[t] + 0.002 repeat + 0.007 switch + -0.11 wm_rt[t-1] + 0.101 wm_rt[t-2] + 0.043 wm_rt[t-3] + 0.176 wm_rt[t-4] + 0.091 wm_rt^2 + -0.021 wm_rt*repeat + 0.02 wm_rt*switch + -0.016 wm_rt*wm_rt[t-1] + 0.026 wm_rt*wm_rt[t-2] + -0.006 wm_rt*wm_rt[t-3] + -0.125 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + -0.273 repeat*wm_rt[t-1] + 0.018 repeat*wm_rt[t-2] + -0.035 repeat*wm_rt[t-3] + 0.065 repeat*wm_rt[t-4] + 0.008 switch^2 + -0.101 switch*wm_rt[t-1] + -0.083 switch*wm_rt[t-2] + -0.098 switch*wm_rt[t-3] + 0.294 switch*wm_rt[t-4] + -0.059 wm_rt[t-1]^2 + -0.052 wm_rt[t-1]*wm_rt[t-2] + -0.054 wm_rt[t-1]*wm_rt[t-3] + -0.06 wm_rt[t-1]*wm_rt[t-4] + -0.132 wm_rt[t-2]^2 + -0.13 wm_rt[t-2]*wm_rt[t-3] + -0.124 wm_rt[t-2]*wm_rt[t-4] + -0.022 wm_rt[t-3]^2 + -0.006 wm_rt[t-3]*wm_rt[t-4] + 0.063 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 11, 11, 0, 0, 11, 0, 0, 11, 11, 11, 10, 11, 0, 11, 11, 0, 11, 11, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 111/400 --- L(Train): 0.4709386 --- L(Val, RNN): 0.4158271 --- L(Val, SINDy): 0.8225517 --- Time: 3.65s; --- Convergence: 5.89e-04; LR: 1.00e-02; Metric: 0.4158271; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.5 value_reward_diff[t] + 0.933 reward_diff + -0.61 value_reward_diff^2 + 0.202 value_reward_diff*reward_diff + 0.932 reward_diff^2 \n",
      "value_persistance[t+1] = -0.355 1 + 0.801 value_persistance[t] + -0.096 repeat + 0.316 value_persistance^2 + -0.167 value_persistance*repeat + -0.098 repeat^2 \n",
      "wm_rt[t+1] = -0.05 1 + 1.058 wm_rt[t] + 0.005 repeat + 0.005 switch + -0.111 wm_rt[t-1] + 0.1 wm_rt[t-2] + 0.042 wm_rt[t-3] + 0.173 wm_rt[t-4] + 0.091 wm_rt^2 + -0.019 wm_rt*repeat + 0.018 wm_rt*switch + -0.015 wm_rt*wm_rt[t-1] + 0.025 wm_rt*wm_rt[t-2] + -0.004 wm_rt*wm_rt[t-3] + -0.123 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.0 repeat*switch + -0.27 repeat*wm_rt[t-1] + 0.019 repeat*wm_rt[t-2] + -0.036 repeat*wm_rt[t-3] + 0.064 repeat*wm_rt[t-4] + 0.005 switch^2 + -0.102 switch*wm_rt[t-1] + -0.083 switch*wm_rt[t-2] + -0.1 switch*wm_rt[t-3] + 0.291 switch*wm_rt[t-4] + -0.061 wm_rt[t-1]^2 + -0.054 wm_rt[t-1]*wm_rt[t-2] + -0.055 wm_rt[t-1]*wm_rt[t-3] + -0.061 wm_rt[t-1]*wm_rt[t-4] + -0.132 wm_rt[t-2]^2 + -0.13 wm_rt[t-2]*wm_rt[t-3] + -0.125 wm_rt[t-2]*wm_rt[t-4] + -0.024 wm_rt[t-3]^2 + -0.007 wm_rt[t-3]*wm_rt[t-4] + 0.059 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 12, 12, 0, 0, 12, 0, 0, 12, 12, 12, 11, 12, 0, 12, 12, 0, 12, 12, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 12, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 112/400 --- L(Train): 0.4592325 --- L(Val, RNN): 0.4152767 --- L(Val, SINDy): 0.8014114 --- Time: 3.72s; --- Convergence: 5.69e-04; LR: 1.00e-02; Metric: 0.4152767; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.503 value_reward_diff[t] + 0.941 reward_diff + -0.611 value_reward_diff^2 + 0.197 value_reward_diff*reward_diff + 0.941 reward_diff^2 \n",
      "value_persistance[t+1] = -0.355 1 + 0.8 value_persistance[t] + -0.095 repeat + 0.316 value_persistance^2 + -0.168 value_persistance*repeat + -0.097 repeat^2 \n",
      "wm_rt[t+1] = -0.05 1 + 1.058 wm_rt[t] + 0.008 repeat + 0.002 switch + -0.112 wm_rt[t-1] + 0.101 wm_rt[t-2] + 0.041 wm_rt[t-3] + 0.17 wm_rt[t-4] + 0.09 wm_rt^2 + -0.016 wm_rt*repeat + 0.015 wm_rt*switch + -0.013 wm_rt*wm_rt[t-1] + 0.024 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.122 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + 0.0 repeat*switch + -0.267 repeat*wm_rt[t-1] + 0.021 repeat*wm_rt[t-2] + -0.034 repeat*wm_rt[t-3] + 0.064 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.103 switch*wm_rt[t-1] + -0.084 switch*wm_rt[t-2] + -0.102 switch*wm_rt[t-3] + 0.288 switch*wm_rt[t-4] + -0.061 wm_rt[t-1]^2 + -0.054 wm_rt[t-1]*wm_rt[t-2] + -0.056 wm_rt[t-1]*wm_rt[t-3] + -0.062 wm_rt[t-1]*wm_rt[t-4] + -0.132 wm_rt[t-2]^2 + -0.13 wm_rt[t-2]*wm_rt[t-3] + -0.125 wm_rt[t-2]*wm_rt[t-4] + -0.024 wm_rt[t-3]^2 + -0.008 wm_rt[t-3]*wm_rt[t-4] + 0.057 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 13, 13, 0, 0, 13, 0, 0, 13, 13, 13, 12, 13, 0, 13, 13, 0, 13, 13, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 13, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 113/400 --- L(Train): 0.4559807 --- L(Val, RNN): 0.4156301 --- L(Val, SINDy): 0.7812074 --- Time: 3.30s; --- Convergence: 4.61e-04; LR: 1.00e-02; Metric: 0.4152767; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.506 value_reward_diff[t] + 0.951 reward_diff + -0.61 value_reward_diff^2 + 0.192 value_reward_diff*reward_diff + 0.95 reward_diff^2 \n",
      "value_persistance[t+1] = -0.354 1 + 0.799 value_persistance[t] + -0.094 repeat + 0.317 value_persistance^2 + -0.169 value_persistance*repeat + -0.096 repeat^2 \n",
      "wm_rt[t+1] = -0.051 1 + 1.057 wm_rt[t] + 0.012 repeat + -0.001 switch + -0.111 wm_rt[t-1] + 0.102 wm_rt[t-2] + 0.041 wm_rt[t-3] + 0.169 wm_rt[t-4] + 0.089 wm_rt^2 + -0.014 wm_rt*repeat + 0.013 wm_rt*switch + -0.011 wm_rt*wm_rt[t-1] + 0.023 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.121 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + -0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + 0.023 repeat*wm_rt[t-2] + -0.033 repeat*wm_rt[t-3] + 0.066 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.103 switch*wm_rt[t-1] + -0.083 switch*wm_rt[t-2] + -0.102 switch*wm_rt[t-3] + 0.285 switch*wm_rt[t-4] + -0.061 wm_rt[t-1]^2 + -0.054 wm_rt[t-1]*wm_rt[t-2] + -0.056 wm_rt[t-1]*wm_rt[t-3] + -0.062 wm_rt[t-1]*wm_rt[t-4] + -0.132 wm_rt[t-2]^2 + -0.129 wm_rt[t-2]*wm_rt[t-3] + -0.124 wm_rt[t-2]*wm_rt[t-4] + -0.025 wm_rt[t-3]^2 + -0.008 wm_rt[t-3]*wm_rt[t-4] + 0.055 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 14, 14, 0, 0, 14, 0, 0, 14, 14, 14, 13, 14, 0, 14, 14, 0, 14, 14, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 14, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 114/400 --- L(Train): 0.4561963 --- L(Val, RNN): 0.4153333 --- L(Val, SINDy): 0.8218862 --- Time: 3.42s; --- Convergence: 3.79e-04; LR: 1.00e-02; Metric: 0.4152767; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.06 1 + 0.509 value_reward_diff[t] + 0.96 reward_diff + -0.609 value_reward_diff^2 + 0.189 value_reward_diff*reward_diff + 0.959 reward_diff^2 \n",
      "value_persistance[t+1] = -0.354 1 + 0.799 value_persistance[t] + -0.093 repeat + 0.318 value_persistance^2 + -0.17 value_persistance*repeat + -0.095 repeat^2 \n",
      "wm_rt[t+1] = -0.051 1 + 1.056 wm_rt[t] + 0.015 repeat + -0.002 switch + -0.11 wm_rt[t-1] + 0.103 wm_rt[t-2] + 0.041 wm_rt[t-3] + 0.168 wm_rt[t-4] + 0.088 wm_rt^2 + -0.012 wm_rt*repeat + 0.011 wm_rt*switch + -0.008 wm_rt*wm_rt[t-1] + 0.021 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.121 wm_rt*wm_rt[t-4] + 0.012 repeat^2 + -0.0 repeat*switch + -0.261 repeat*wm_rt[t-1] + 0.025 repeat*wm_rt[t-2] + -0.031 repeat*wm_rt[t-3] + 0.068 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.103 switch*wm_rt[t-1] + -0.082 switch*wm_rt[t-2] + -0.102 switch*wm_rt[t-3] + 0.284 switch*wm_rt[t-4] + -0.061 wm_rt[t-1]^2 + -0.054 wm_rt[t-1]*wm_rt[t-2] + -0.055 wm_rt[t-1]*wm_rt[t-3] + -0.061 wm_rt[t-1]*wm_rt[t-4] + -0.13 wm_rt[t-2]^2 + -0.128 wm_rt[t-2]*wm_rt[t-3] + -0.123 wm_rt[t-2]*wm_rt[t-4] + -0.024 wm_rt[t-3]^2 + -0.008 wm_rt[t-3]*wm_rt[t-4] + 0.054 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 15, 15, 0, 0, 15, 0, 0, 15, 15, 15, 14, 15, 0, 15, 15, 0, 15, 15, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 15, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 115/400 --- L(Train): 0.4629917 --- L(Val, RNN): 0.4146662 --- L(Val, SINDy): 0.9167295 --- Time: 3.31s; --- Convergence: 5.23e-04; LR: 1.00e-02; Metric: 0.4146662; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.06 1 + 0.514 value_reward_diff[t] + 0.969 reward_diff + -0.606 value_reward_diff^2 + 0.186 value_reward_diff*reward_diff + 0.969 reward_diff^2 \n",
      "value_persistance[t+1] = -0.355 1 + 0.799 value_persistance[t] + -0.093 repeat + 0.317 value_persistance^2 + -0.17 value_persistance*repeat + -0.095 repeat^2 \n",
      "wm_rt[t+1] = -0.05 1 + 1.056 wm_rt[t] + 0.016 repeat + -0.002 switch + -0.108 wm_rt[t-1] + 0.105 wm_rt[t-2] + 0.043 wm_rt[t-3] + 0.168 wm_rt[t-4] + 0.087 wm_rt^2 + -0.011 wm_rt*repeat + 0.012 wm_rt*switch + -0.005 wm_rt*wm_rt[t-1] + 0.019 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.121 wm_rt*wm_rt[t-4] + 0.014 repeat^2 + -0.0 repeat*switch + -0.259 repeat*wm_rt[t-1] + 0.026 repeat*wm_rt[t-2] + -0.03 repeat*wm_rt[t-3] + 0.069 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.101 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-2] + -0.101 switch*wm_rt[t-3] + 0.283 switch*wm_rt[t-4] + -0.059 wm_rt[t-1]^2 + -0.052 wm_rt[t-1]*wm_rt[t-2] + -0.053 wm_rt[t-1]*wm_rt[t-3] + -0.06 wm_rt[t-1]*wm_rt[t-4] + -0.128 wm_rt[t-2]^2 + -0.126 wm_rt[t-2]*wm_rt[t-3] + -0.121 wm_rt[t-2]*wm_rt[t-4] + -0.023 wm_rt[t-3]^2 + -0.007 wm_rt[t-3]*wm_rt[t-4] + 0.053 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 16, 16, 0, 0, 16, 0, 0, 16, 16, 16, 15, 16, 0, 16, 16, 0, 16, 16, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 16, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 116/400 --- L(Train): 0.4538769 --- L(Val, RNN): 0.4138710 --- L(Val, SINDy): 0.9527596 --- Time: 3.89s; --- Convergence: 6.59e-04; LR: 1.00e-02; Metric: 0.4138710; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.519 value_reward_diff[t] + 0.979 reward_diff + -0.603 value_reward_diff^2 + 0.184 value_reward_diff*reward_diff + 0.978 reward_diff^2 \n",
      "value_persistance[t+1] = -0.356 1 + 0.8 value_persistance[t] + -0.094 repeat + 0.317 value_persistance^2 + -0.169 value_persistance*repeat + -0.095 repeat^2 \n",
      "wm_rt[t+1] = -0.05 1 + 1.056 wm_rt[t] + 0.017 repeat + -0.001 switch + -0.106 wm_rt[t-1] + 0.108 wm_rt[t-2] + 0.044 wm_rt[t-3] + 0.168 wm_rt[t-4] + 0.087 wm_rt^2 + -0.011 wm_rt*repeat + 0.013 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.017 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.121 wm_rt*wm_rt[t-4] + 0.014 repeat^2 + -0.0 repeat*switch + -0.259 repeat*wm_rt[t-1] + 0.026 repeat*wm_rt[t-2] + -0.031 repeat*wm_rt[t-3] + 0.069 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.099 switch*wm_rt[t-1] + -0.077 switch*wm_rt[t-2] + -0.099 switch*wm_rt[t-3] + 0.283 switch*wm_rt[t-4] + -0.057 wm_rt[t-1]^2 + -0.05 wm_rt[t-1]*wm_rt[t-2] + -0.051 wm_rt[t-1]*wm_rt[t-3] + -0.058 wm_rt[t-1]*wm_rt[t-4] + -0.126 wm_rt[t-2]^2 + -0.124 wm_rt[t-2]*wm_rt[t-3] + -0.119 wm_rt[t-2]*wm_rt[t-4] + -0.022 wm_rt[t-3]^2 + -0.006 wm_rt[t-3]*wm_rt[t-4] + 0.053 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 1, 0, 17, 17, 0, 0, 17, 0, 0, 17, 17, 17, 16, 17, 0, 17, 17, 0, 17, 17, 0, 17, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 17, 17, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 117/400 --- L(Train): 0.4479436 --- L(Val, RNN): 0.4143949 --- L(Val, SINDy): 0.9749411 --- Time: 3.56s; --- Convergence: 5.92e-04; LR: 1.00e-02; Metric: 0.4138710; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.523 value_reward_diff[t] + 0.989 reward_diff + -0.6 value_reward_diff^2 + 0.182 value_reward_diff*reward_diff + 0.988 reward_diff^2 \n",
      "value_persistance[t+1] = -0.357 1 + 0.8 value_persistance[t] + -0.094 repeat + 0.316 value_persistance^2 + -0.168 value_persistance*repeat + -0.096 repeat^2 \n",
      "wm_rt[t+1] = -0.05 1 + 1.055 wm_rt[t] + 0.016 repeat + 0.001 switch + -0.104 wm_rt[t-1] + 0.109 wm_rt[t-2] + 0.045 wm_rt[t-3] + 0.167 wm_rt[t-4] + 0.086 wm_rt^2 + -0.013 wm_rt*repeat + 0.015 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.016 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.121 wm_rt*wm_rt[t-4] + 0.013 repeat^2 + -0.0 repeat*switch + -0.26 repeat*wm_rt[t-1] + 0.024 repeat*wm_rt[t-2] + -0.033 repeat*wm_rt[t-3] + 0.069 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.098 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-2] + -0.098 switch*wm_rt[t-3] + 0.282 switch*wm_rt[t-4] + -0.055 wm_rt[t-1]^2 + -0.048 wm_rt[t-1]*wm_rt[t-2] + -0.049 wm_rt[t-1]*wm_rt[t-3] + -0.056 wm_rt[t-1]*wm_rt[t-4] + -0.124 wm_rt[t-2]^2 + -0.122 wm_rt[t-2]*wm_rt[t-3] + -0.117 wm_rt[t-2]*wm_rt[t-4] + -0.021 wm_rt[t-3]^2 + -0.005 wm_rt[t-3]*wm_rt[t-4] + 0.053 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 2, 0, 18, 18, 0, 0, 18, 0, 0, 18, 18, 18, 17, 18, 0, 18, 18, 0, 18, 18, 0, 18, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 18, 18, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 118/400 --- L(Train): 0.4522632 --- L(Val, RNN): 0.4138748 --- L(Val, SINDy): 1.0133086 --- Time: 4.25s; --- Convergence: 5.56e-04; LR: 1.00e-02; Metric: 0.4138710; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.528 value_reward_diff[t] + 0.999 reward_diff + -0.596 value_reward_diff^2 + 0.181 value_reward_diff*reward_diff + 0.998 reward_diff^2 \n",
      "value_persistance[t+1] = -0.358 1 + 0.801 value_persistance[t] + -0.094 repeat + 0.315 value_persistance^2 + -0.168 value_persistance*repeat + -0.096 repeat^2 \n",
      "wm_rt[t+1] = -0.049 1 + 1.055 wm_rt[t] + 0.014 repeat + 0.003 switch + -0.104 wm_rt[t-1] + 0.11 wm_rt[t-2] + 0.044 wm_rt[t-3] + 0.166 wm_rt[t-4] + 0.085 wm_rt^2 + -0.015 wm_rt*repeat + 0.018 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.016 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + 0.011 repeat^2 + -0.0 repeat*switch + -0.262 repeat*wm_rt[t-1] + 0.02 repeat*wm_rt[t-2] + -0.037 repeat*wm_rt[t-3] + 0.066 repeat*wm_rt[t-4] + 0.004 switch^2 + -0.097 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-2] + -0.097 switch*wm_rt[t-3] + 0.281 switch*wm_rt[t-4] + -0.054 wm_rt[t-1]^2 + -0.047 wm_rt[t-1]*wm_rt[t-2] + -0.049 wm_rt[t-1]*wm_rt[t-3] + -0.055 wm_rt[t-1]*wm_rt[t-4] + -0.124 wm_rt[t-2]^2 + -0.122 wm_rt[t-2]*wm_rt[t-3] + -0.117 wm_rt[t-2]*wm_rt[t-4] + -0.022 wm_rt[t-3]^2 + -0.006 wm_rt[t-3]*wm_rt[t-4] + 0.051 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 3, 0, 19, 19, 0, 0, 19, 0, 0, 19, 19, 19, 18, 19, 0, 19, 19, 0, 19, 19, 0, 19, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 19, 19, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 119/400 --- L(Train): 0.4508817 --- L(Val, RNN): 0.4123715 --- L(Val, SINDy): 0.9472281 --- Time: 3.57s; --- Convergence: 1.03e-03; LR: 1.00e-02; Metric: 0.4123715; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.531 value_reward_diff[t] + 1.01 reward_diff + -0.594 value_reward_diff^2 + 0.178 value_reward_diff*reward_diff + 1.009 reward_diff^2 \n",
      "value_persistance[t+1] = -0.358 1 + 0.8 value_persistance[t] + -0.094 repeat + 0.316 value_persistance^2 + -0.169 value_persistance*repeat + -0.095 repeat^2 \n",
      "wm_rt[t+1] = -0.048 1 + 1.056 wm_rt[t] + 0.012 repeat + 0.006 switch + -0.104 wm_rt[t-1] + 0.109 wm_rt[t-2] + 0.043 wm_rt[t-3] + 0.163 wm_rt[t-4] + 0.086 wm_rt^2 + -0.018 wm_rt*repeat + 0.021 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.018 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.117 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + 0.0 repeat*switch + -0.265 repeat*wm_rt[t-1] + 0.015 repeat*wm_rt[t-2] + -0.042 repeat*wm_rt[t-3] + 0.063 repeat*wm_rt[t-4] + 0.006 switch^2 + -0.097 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-2] + -0.098 switch*wm_rt[t-3] + 0.279 switch*wm_rt[t-4] + -0.055 wm_rt[t-1]^2 + -0.048 wm_rt[t-1]*wm_rt[t-2] + -0.05 wm_rt[t-1]*wm_rt[t-3] + -0.056 wm_rt[t-1]*wm_rt[t-4] + -0.125 wm_rt[t-2]^2 + -0.123 wm_rt[t-2]*wm_rt[t-3] + -0.118 wm_rt[t-2]*wm_rt[t-4] + -0.023 wm_rt[t-3]^2 + -0.007 wm_rt[t-3]*wm_rt[t-4] + 0.048 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 4, 0, 20, 20, 0, 0, 20, 0, 0, 20, 20, 20, 19, 20, 0, 20, 20, 0, 20, 20, 0, 20, 0, 0, 0, 0, 0, 4, 3, 0, 0, 0, 0, 20, 20, 1\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 120/400 --- L(Train): 0.4518287 --- L(Val, RNN): 0.4129547 --- L(Val, SINDy): 0.8606857 --- Time: 3.67s; --- Convergence: 8.06e-04; LR: 1.00e-02; Metric: 0.4123715; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.051 1 + 0.533 value_reward_diff[t] + 1.02 reward_diff + -0.592 value_reward_diff^2 + 0.176 value_reward_diff*reward_diff + 1.019 reward_diff^2 \n",
      "value_persistance[t+1] = -0.358 1 + 0.799 value_persistance[t] + -0.093 repeat + 0.316 value_persistance^2 + -0.17 value_persistance*repeat + -0.094 repeat^2 \n",
      "wm_rt[t+1] = -0.046 1 + 1.058 wm_rt[t] + 0.01 repeat + 0.007 switch + -0.106 wm_rt[t-1] + 0.108 wm_rt[t-2] + 0.041 wm_rt[t-3] + 0.16 wm_rt[t-4] + 0.086 wm_rt^2 + -0.02 wm_rt*repeat + 0.023 wm_rt*switch + 0.003 wm_rt*wm_rt[t-1] + 0.019 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.115 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + 0.0 repeat*switch + -0.267 repeat*wm_rt[t-1] + 0.011 repeat*wm_rt[t-2] + -0.045 repeat*wm_rt[t-3] + 0.061 repeat*wm_rt[t-4] + 0.008 switch^2 + -0.099 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-2] + -0.099 switch*wm_rt[t-3] + 0.276 switch*wm_rt[t-4] + -0.057 wm_rt[t-1]^2 + -0.05 wm_rt[t-1]*wm_rt[t-2] + -0.051 wm_rt[t-1]*wm_rt[t-3] + -0.058 wm_rt[t-1]*wm_rt[t-4] + -0.126 wm_rt[t-2]^2 + -0.124 wm_rt[t-2]*wm_rt[t-3] + -0.119 wm_rt[t-2]*wm_rt[t-4] + -0.025 wm_rt[t-3]^2 + -0.009 wm_rt[t-3]*wm_rt[t-4] + 0.045 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 5, 0, 21, 21, 0, 0, 21, 0, 0, 21, 21, 21, 20, 21, 0, 21, 21, 0, 21, 21, 0, 21, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 21, 21, 2\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 121/400 --- L(Train): 0.4481285 --- L(Val, RNN): 0.4105388 --- L(Val, SINDy): 0.8443224 --- Time: 4.08s; --- Convergence: 1.61e-03; LR: 1.00e-02; Metric: 0.4105388; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.049 1 + 0.535 value_reward_diff[t] + 1.03 reward_diff + -0.59 value_reward_diff^2 + 0.174 value_reward_diff*reward_diff + 1.03 reward_diff^2 \n",
      "value_persistance[t+1] = -0.358 1 + 0.799 value_persistance[t] + -0.092 repeat + 0.317 value_persistance^2 + -0.17 value_persistance*repeat + -0.093 repeat^2 \n",
      "wm_rt[t+1] = -0.045 1 + 1.058 wm_rt[t] + 0.008 repeat + 0.008 switch + -0.107 wm_rt[t-1] + 0.108 wm_rt[t-2] + 0.04 wm_rt[t-3] + 0.158 wm_rt[t-4] + 0.087 wm_rt^2 + -0.022 wm_rt*repeat + 0.024 wm_rt*switch + 0.003 wm_rt*wm_rt[t-1] + 0.019 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.114 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + 0.0 repeat*switch + -0.269 repeat*wm_rt[t-1] + 0.008 repeat*wm_rt[t-2] + -0.047 repeat*wm_rt[t-3] + 0.06 repeat*wm_rt[t-4] + 0.008 switch^2 + -0.1 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-2] + -0.1 switch*wm_rt[t-3] + 0.273 switch*wm_rt[t-4] + -0.058 wm_rt[t-1]^2 + -0.051 wm_rt[t-1]*wm_rt[t-2] + -0.052 wm_rt[t-1]*wm_rt[t-3] + -0.059 wm_rt[t-1]*wm_rt[t-4] + -0.127 wm_rt[t-2]^2 + -0.124 wm_rt[t-2]*wm_rt[t-3] + -0.119 wm_rt[t-2]*wm_rt[t-4] + -0.026 wm_rt[t-3]^2 + -0.01 wm_rt[t-3]*wm_rt[t-4] + 0.042 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 1, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 6, 0, 22, 22, 0, 0, 22, 0, 0, 22, 22, 22, 21, 22, 0, 22, 22, 0, 22, 22, 0, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 22, 3\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 122/400 --- L(Train): 0.4475510 --- L(Val, RNN): 0.4109920 --- L(Val, SINDy): 0.7507222 --- Time: 3.88s; --- Convergence: 1.03e-03; LR: 1.00e-02; Metric: 0.4105388; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.047 1 + 0.537 value_reward_diff[t] + 1.04 reward_diff + -0.588 value_reward_diff^2 + 0.172 value_reward_diff*reward_diff + 1.039 reward_diff^2 \n",
      "value_persistance[t+1] = -0.358 1 + 0.798 value_persistance[t] + -0.091 repeat + 0.318 value_persistance^2 + -0.171 value_persistance*repeat + -0.092 repeat^2 \n",
      "wm_rt[t+1] = -0.045 1 + 1.058 wm_rt[t] + 0.007 repeat + 0.008 switch + -0.107 wm_rt[t-1] + 0.108 wm_rt[t-2] + 0.041 wm_rt[t-3] + 0.156 wm_rt[t-4] + 0.086 wm_rt^2 + -0.024 wm_rt*repeat + 0.024 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.018 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.114 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + 0.0 repeat*switch + -0.27 repeat*wm_rt[t-1] + 0.007 repeat*wm_rt[t-2] + -0.047 repeat*wm_rt[t-3] + 0.062 repeat*wm_rt[t-4] + 0.008 switch^2 + -0.1 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-2] + -0.099 switch*wm_rt[t-3] + 0.271 switch*wm_rt[t-4] + -0.058 wm_rt[t-1]^2 + -0.051 wm_rt[t-1]*wm_rt[t-2] + -0.053 wm_rt[t-1]*wm_rt[t-3] + -0.059 wm_rt[t-1]*wm_rt[t-4] + -0.126 wm_rt[t-2]^2 + -0.124 wm_rt[t-2]*wm_rt[t-3] + -0.119 wm_rt[t-2]*wm_rt[t-4] + -0.026 wm_rt[t-3]^2 + -0.01 wm_rt[t-3]*wm_rt[t-4] + 0.041 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 2, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 7, 0, 23, 23, 0, 0, 23, 0, 0, 23, 23, 23, 22, 23, 0, 23, 23, 0, 23, 23, 0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 23, 4\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 123/400 --- L(Train): 0.4550598 --- L(Val, RNN): 0.4115866 --- L(Val, SINDy): 0.8023151 --- Time: 4.00s; --- Convergence: 8.13e-04; LR: 1.00e-02; Metric: 0.4105388; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.045 1 + 0.54 value_reward_diff[t] + 1.05 reward_diff + -0.586 value_reward_diff^2 + 0.17 value_reward_diff*reward_diff + 1.049 reward_diff^2 \n",
      "value_persistance[t+1] = -0.358 1 + 0.798 value_persistance[t] + -0.091 repeat + 0.318 value_persistance^2 + -0.171 value_persistance*repeat + -0.092 repeat^2 \n",
      "wm_rt[t+1] = -0.045 1 + 1.058 wm_rt[t] + 0.008 repeat + 0.006 switch + -0.106 wm_rt[t-1] + 0.11 wm_rt[t-2] + 0.042 wm_rt[t-3] + 0.157 wm_rt[t-4] + 0.086 wm_rt^2 + -0.025 wm_rt*repeat + 0.023 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.015 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.115 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + 0.0 repeat*switch + -0.269 repeat*wm_rt[t-1] + 0.008 repeat*wm_rt[t-2] + -0.045 repeat*wm_rt[t-3] + 0.066 repeat*wm_rt[t-4] + 0.007 switch^2 + -0.099 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-2] + -0.099 switch*wm_rt[t-3] + 0.269 switch*wm_rt[t-4] + -0.058 wm_rt[t-1]^2 + -0.05 wm_rt[t-1]*wm_rt[t-2] + -0.052 wm_rt[t-1]*wm_rt[t-3] + -0.059 wm_rt[t-1]*wm_rt[t-4] + -0.125 wm_rt[t-2]^2 + -0.122 wm_rt[t-2]*wm_rt[t-3] + -0.117 wm_rt[t-2]*wm_rt[t-4] + -0.024 wm_rt[t-3]^2 + -0.008 wm_rt[t-3]*wm_rt[t-4] + 0.041 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 3, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 8, 0, 24, 24, 0, 0, 24, 0, 0, 24, 24, 24, 23, 24, 0, 24, 24, 0, 24, 24, 0, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 24, 5\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 124/400 --- L(Train): 0.4440907 --- L(Val, RNN): 0.4086581 --- L(Val, SINDy): 0.7322860 --- Time: 4.16s; --- Convergence: 1.87e-03; LR: 1.00e-02; Metric: 0.4086581; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.043 1 + 0.542 value_reward_diff[t] + 1.06 reward_diff + -0.585 value_reward_diff^2 + 0.168 value_reward_diff*reward_diff + 1.059 reward_diff^2 \n",
      "value_persistance[t+1] = -0.36 1 + 0.799 value_persistance[t] + -0.091 repeat + 0.317 value_persistance^2 + -0.171 value_persistance*repeat + -0.093 repeat^2 \n",
      "wm_rt[t+1] = -0.044 1 + 1.058 wm_rt[t] + 0.01 repeat + 0.004 switch + -0.105 wm_rt[t-1] + 0.113 wm_rt[t-2] + 0.044 wm_rt[t-3] + 0.157 wm_rt[t-4] + 0.086 wm_rt^2 + -0.024 wm_rt*repeat + 0.021 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.012 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.117 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + 0.0 repeat*switch + -0.268 repeat*wm_rt[t-1] + 0.01 repeat*wm_rt[t-2] + -0.042 repeat*wm_rt[t-3] + 0.071 repeat*wm_rt[t-4] + 0.005 switch^2 + -0.098 switch*wm_rt[t-1] + -0.069 switch*wm_rt[t-2] + -0.097 switch*wm_rt[t-3] + 0.268 switch*wm_rt[t-4] + -0.056 wm_rt[t-1]^2 + -0.049 wm_rt[t-1]*wm_rt[t-2] + -0.05 wm_rt[t-1]*wm_rt[t-3] + -0.057 wm_rt[t-1]*wm_rt[t-4] + -0.122 wm_rt[t-2]^2 + -0.12 wm_rt[t-2]*wm_rt[t-3] + -0.115 wm_rt[t-2]*wm_rt[t-4] + -0.022 wm_rt[t-3]^2 + -0.006 wm_rt[t-3]*wm_rt[t-4] + 0.041 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 4, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 9, 0, 25, 25, 0, 0, 25, 0, 0, 25, 25, 25, 24, 25, 0, 25, 25, 0, 25, 25, 0, 25, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 25, 25, 6\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 125/400 --- L(Train): 0.4415819 --- L(Val, RNN): 0.4083242 --- L(Val, SINDy): 0.7638004 --- Time: 4.32s; --- Convergence: 1.10e-03; LR: 1.00e-02; Metric: 0.4083242; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.042 1 + 0.543 value_reward_diff[t] + 1.069 reward_diff + -0.584 value_reward_diff^2 + 0.165 value_reward_diff*reward_diff + 1.068 reward_diff^2 \n",
      "value_persistance[t+1] = -0.362 1 + 0.8 value_persistance[t] + -0.092 repeat + 0.316 value_persistance^2 + -0.17 value_persistance*repeat + -0.093 repeat^2 \n",
      "wm_rt[t+1] = -0.044 1 + 1.057 wm_rt[t] + 0.012 repeat + 0.002 switch + -0.103 wm_rt[t-1] + 0.115 wm_rt[t-2] + 0.046 wm_rt[t-3] + 0.158 wm_rt[t-4] + 0.085 wm_rt^2 + -0.024 wm_rt*repeat + 0.019 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.009 wm_rt*wm_rt[t-2] + -0.004 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + -0.0 repeat*switch + -0.265 repeat*wm_rt[t-1] + 0.012 repeat*wm_rt[t-2] + -0.039 repeat*wm_rt[t-3] + 0.077 repeat*wm_rt[t-4] + 0.003 switch^2 + -0.097 switch*wm_rt[t-1] + -0.066 switch*wm_rt[t-2] + -0.096 switch*wm_rt[t-3] + 0.267 switch*wm_rt[t-4] + -0.054 wm_rt[t-1]^2 + -0.047 wm_rt[t-1]*wm_rt[t-2] + -0.049 wm_rt[t-1]*wm_rt[t-3] + -0.056 wm_rt[t-1]*wm_rt[t-4] + -0.12 wm_rt[t-2]^2 + -0.117 wm_rt[t-2]*wm_rt[t-3] + -0.112 wm_rt[t-2]*wm_rt[t-4] + -0.02 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + 0.042 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 5, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 10, 0, 26, 26, 0, 0, 26, 0, 0, 26, 26, 26, 25, 26, 0, 26, 26, 0, 26, 26, 0, 26, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 26, 26, 7\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 126/400 --- L(Train): 0.4466566 --- L(Val, RNN): 0.4084653 --- L(Val, SINDy): 0.7781041 --- Time: 4.22s; --- Convergence: 6.22e-04; LR: 1.00e-02; Metric: 0.4083242; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.041 1 + 0.543 value_reward_diff[t] + 1.078 reward_diff + -0.584 value_reward_diff^2 + 0.162 value_reward_diff*reward_diff + 1.077 reward_diff^2 \n",
      "value_persistance[t+1] = -0.363 1 + 0.801 value_persistance[t] + -0.092 repeat + 0.315 value_persistance^2 + -0.17 value_persistance*repeat + -0.093 repeat^2 \n",
      "wm_rt[t+1] = -0.044 1 + 1.057 wm_rt[t] + 0.013 repeat + -0.0 switch + -0.102 wm_rt[t-1] + 0.117 wm_rt[t-2] + 0.048 wm_rt[t-3] + 0.158 wm_rt[t-4] + 0.085 wm_rt^2 + -0.024 wm_rt*repeat + 0.017 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.006 wm_rt*wm_rt[t-2] + -0.006 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.01 repeat^2 + -0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + 0.013 repeat*wm_rt[t-2] + -0.036 repeat*wm_rt[t-3] + 0.081 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.096 switch*wm_rt[t-1] + -0.064 switch*wm_rt[t-2] + -0.095 switch*wm_rt[t-3] + 0.265 switch*wm_rt[t-4] + -0.053 wm_rt[t-1]^2 + -0.046 wm_rt[t-1]*wm_rt[t-2] + -0.047 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.117 wm_rt[t-2]^2 + -0.115 wm_rt[t-2]*wm_rt[t-3] + -0.11 wm_rt[t-2]*wm_rt[t-4] + -0.018 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + 0.042 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 6, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 11, 0, 27, 27, 0, 0, 27, 0, 0, 27, 27, 27, 26, 27, 0, 27, 27, 0, 27, 27, 0, 27, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 27, 27, 8\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 127/400 --- L(Train): 0.4349322 --- L(Val, RNN): 0.4069294 --- L(Val, SINDy): 0.7934223 --- Time: 4.07s; --- Convergence: 1.08e-03; LR: 1.00e-02; Metric: 0.4069294; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.04 1 + 0.543 value_reward_diff[t] + 1.086 reward_diff + -0.584 value_reward_diff^2 + 0.159 value_reward_diff*reward_diff + 1.085 reward_diff^2 \n",
      "value_persistance[t+1] = -0.364 1 + 0.8 value_persistance[t] + -0.091 repeat + 0.315 value_persistance^2 + -0.17 value_persistance*repeat + -0.093 repeat^2 \n",
      "wm_rt[t+1] = -0.043 1 + 1.056 wm_rt[t] + 0.014 repeat + -0.002 switch + -0.101 wm_rt[t-1] + 0.119 wm_rt[t-2] + 0.049 wm_rt[t-3] + 0.158 wm_rt[t-4] + 0.085 wm_rt^2 + -0.024 wm_rt*repeat + 0.015 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.004 wm_rt*wm_rt[t-2] + -0.007 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.012 repeat^2 + -0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + 0.013 repeat*wm_rt[t-2] + -0.035 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.096 switch*wm_rt[t-1] + -0.063 switch*wm_rt[t-2] + -0.094 switch*wm_rt[t-3] + 0.263 switch*wm_rt[t-4] + -0.052 wm_rt[t-1]^2 + -0.045 wm_rt[t-1]*wm_rt[t-2] + -0.047 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.116 wm_rt[t-2]^2 + -0.114 wm_rt[t-2]*wm_rt[t-3] + -0.109 wm_rt[t-2]*wm_rt[t-4] + -0.017 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.042 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 7, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 12, 0, 28, 28, 0, 0, 28, 0, 0, 28, 28, 28, 27, 28, 0, 28, 28, 0, 28, 28, 0, 28, 0, 0, 0, 0, 0, 4, 3, 0, 0, 0, 0, 28, 28, 9\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 128/400 --- L(Train): 0.4354756 --- L(Val, RNN): 0.4088430 --- L(Val, SINDy): 0.8098940 --- Time: 4.37s; --- Convergence: 1.50e-03; LR: 1.00e-02; Metric: 0.4069294; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.04 1 + 0.544 value_reward_diff[t] + 1.094 reward_diff + -0.583 value_reward_diff^2 + 0.157 value_reward_diff*reward_diff + 1.093 reward_diff^2 \n",
      "value_persistance[t+1] = -0.364 1 + 0.799 value_persistance[t] + -0.089 repeat + 0.316 value_persistance^2 + -0.172 value_persistance*repeat + -0.091 repeat^2 \n",
      "wm_rt[t+1] = -0.043 1 + 1.056 wm_rt[t] + 0.014 repeat + -0.003 switch + -0.101 wm_rt[t-1] + 0.119 wm_rt[t-2] + 0.048 wm_rt[t-3] + 0.156 wm_rt[t-4] + 0.084 wm_rt^2 + -0.024 wm_rt*repeat + 0.015 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + -0.007 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.012 repeat^2 + -0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + 0.012 repeat*wm_rt[t-2] + -0.036 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.096 switch*wm_rt[t-1] + -0.062 switch*wm_rt[t-2] + -0.095 switch*wm_rt[t-3] + 0.26 switch*wm_rt[t-4] + -0.053 wm_rt[t-1]^2 + -0.045 wm_rt[t-1]*wm_rt[t-2] + -0.047 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.116 wm_rt[t-2]^2 + -0.113 wm_rt[t-2]*wm_rt[t-3] + -0.108 wm_rt[t-2]*wm_rt[t-4] + -0.018 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.04 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 8, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 13, 0, 29, 29, 0, 0, 29, 0, 0, 29, 29, 29, 28, 29, 0, 29, 29, 0, 29, 29, 0, 29, 0, 0, 0, 0, 0, 5, 4, 0, 0, 0, 0, 29, 29, 10\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 129/400 --- L(Train): 0.4359200 --- L(Val, RNN): 0.4060120 --- L(Val, SINDy): 0.8859826 --- Time: 4.27s; --- Convergence: 2.16e-03; LR: 1.00e-02; Metric: 0.4060120; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.039 1 + 0.544 value_reward_diff[t] + 1.102 reward_diff + -0.583 value_reward_diff^2 + 0.154 value_reward_diff*reward_diff + 1.101 reward_diff^2 \n",
      "value_persistance[t+1] = -0.363 1 + 0.798 value_persistance[t] + -0.088 repeat + 0.317 value_persistance^2 + -0.174 value_persistance*repeat + -0.089 repeat^2 \n",
      "wm_rt[t+1] = -0.043 1 + 1.056 wm_rt[t] + 0.014 repeat + -0.003 switch + -0.101 wm_rt[t-1] + 0.12 wm_rt[t-2] + 0.047 wm_rt[t-3] + 0.154 wm_rt[t-4] + 0.084 wm_rt^2 + -0.025 wm_rt*repeat + 0.015 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + -0.006 wm_rt*wm_rt[t-3] + -0.118 wm_rt*wm_rt[t-4] + 0.011 repeat^2 + -0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + 0.009 repeat*wm_rt[t-2] + -0.038 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.096 switch*wm_rt[t-1] + -0.061 switch*wm_rt[t-2] + -0.095 switch*wm_rt[t-3] + 0.257 switch*wm_rt[t-4] + -0.053 wm_rt[t-1]^2 + -0.046 wm_rt[t-1]*wm_rt[t-2] + -0.047 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.116 wm_rt[t-2]^2 + -0.113 wm_rt[t-2]*wm_rt[t-3] + -0.108 wm_rt[t-2]*wm_rt[t-4] + -0.019 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + 0.037 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 9, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 14, 0, 30, 30, 0, 0, 30, 0, 0, 30, 30, 30, 29, 30, 0, 30, 30, 0, 30, 30, 0, 30, 0, 0, 0, 0, 0, 6, 5, 0, 0, 0, 0, 30, 30, 11\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 130/400 --- L(Train): 0.4395721 --- L(Val, RNN): 0.4039339 --- L(Val, SINDy): 0.8843869 --- Time: 3.83s; --- Convergence: 2.12e-03; LR: 1.00e-02; Metric: 0.4039339; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.04 1 + 0.543 value_reward_diff[t] + 1.11 reward_diff + -0.584 value_reward_diff^2 + 0.151 value_reward_diff*reward_diff + 1.108 reward_diff^2 \n",
      "value_persistance[t+1] = -0.363 1 + 0.797 value_persistance[t] + -0.086 repeat + 0.318 value_persistance^2 + -0.175 value_persistance*repeat + -0.088 repeat^2 \n",
      "wm_rt[t+1] = -0.043 1 + 1.057 wm_rt[t] + 0.013 repeat + -0.002 switch + -0.102 wm_rt[t-1] + 0.12 wm_rt[t-2] + 0.046 wm_rt[t-3] + 0.151 wm_rt[t-4] + 0.083 wm_rt^2 + -0.027 wm_rt*repeat + 0.017 wm_rt*switch + -0.0 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + -0.005 wm_rt*wm_rt[t-3] + -0.117 wm_rt*wm_rt[t-4] + 0.01 repeat^2 + 0.0 repeat*switch + -0.265 repeat*wm_rt[t-1] + 0.007 repeat*wm_rt[t-2] + -0.041 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.097 switch*wm_rt[t-1] + -0.06 switch*wm_rt[t-2] + -0.096 switch*wm_rt[t-3] + 0.255 switch*wm_rt[t-4] + -0.053 wm_rt[t-1]^2 + -0.046 wm_rt[t-1]*wm_rt[t-2] + -0.048 wm_rt[t-1]*wm_rt[t-3] + -0.055 wm_rt[t-1]*wm_rt[t-4] + -0.116 wm_rt[t-2]^2 + -0.113 wm_rt[t-2]*wm_rt[t-3] + -0.108 wm_rt[t-2]*wm_rt[t-4] + -0.02 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + 0.034 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 10, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 15, 0, 31, 31, 0, 0, 31, 0, 0, 31, 31, 31, 30, 31, 0, 31, 31, 0, 31, 31, 0, 31, 0, 0, 0, 0, 0, 7, 6, 0, 0, 0, 0, 31, 31, 12\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 131/400 --- L(Train): 0.4358531 --- L(Val, RNN): 0.4064050 --- L(Val, SINDy): 0.7934620 --- Time: 4.25s; --- Convergence: 2.30e-03; LR: 1.00e-02; Metric: 0.4039339; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.04 1 + 0.542 value_reward_diff[t] + 1.117 reward_diff + -0.586 value_reward_diff^2 + 0.148 value_reward_diff*reward_diff + 1.115 reward_diff^2 \n",
      "value_persistance[t+1] = -0.363 1 + 0.796 value_persistance[t] + -0.085 repeat + 0.319 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.042 1 + 1.057 wm_rt[t] + 0.011 repeat + -0.0 switch + -0.102 wm_rt[t-1] + 0.12 wm_rt[t-2] + 0.046 wm_rt[t-3] + 0.149 wm_rt[t-4] + 0.083 wm_rt^2 + -0.029 wm_rt*repeat + 0.019 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + -0.004 wm_rt*wm_rt[t-3] + -0.115 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + 0.0 repeat*switch + -0.266 repeat*wm_rt[t-1] + 0.004 repeat*wm_rt[t-2] + -0.043 repeat*wm_rt[t-3] + 0.081 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.097 switch*wm_rt[t-1] + -0.059 switch*wm_rt[t-2] + -0.096 switch*wm_rt[t-3] + 0.252 switch*wm_rt[t-4] + -0.053 wm_rt[t-1]^2 + -0.046 wm_rt[t-1]*wm_rt[t-2] + -0.048 wm_rt[t-1]*wm_rt[t-3] + -0.055 wm_rt[t-1]*wm_rt[t-4] + -0.116 wm_rt[t-2]^2 + -0.113 wm_rt[t-2]*wm_rt[t-3] + -0.108 wm_rt[t-2]*wm_rt[t-4] + -0.021 wm_rt[t-3]^2 + -0.005 wm_rt[t-3]*wm_rt[t-4] + 0.032 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 11, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 16, 0, 32, 32, 0, 0, 32, 0, 0, 32, 32, 32, 31, 32, 0, 32, 32, 0, 32, 32, 0, 32, 0, 0, 0, 0, 0, 8, 7, 0, 0, 0, 0, 32, 32, 13\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 132/400 --- L(Train): 0.4243794 --- L(Val, RNN): 0.4030072 --- L(Val, SINDy): 0.7787010 --- Time: 3.33s; --- Convergence: 2.85e-03; LR: 1.00e-02; Metric: 0.4030072; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.04 1 + 0.542 value_reward_diff[t] + 1.125 reward_diff + -0.586 value_reward_diff^2 + 0.145 value_reward_diff*reward_diff + 1.123 reward_diff^2 \n",
      "value_persistance[t+1] = -0.363 1 + 0.796 value_persistance[t] + -0.084 repeat + 0.32 value_persistance^2 + -0.177 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.041 1 + 1.058 wm_rt[t] + 0.01 repeat + 0.002 switch + -0.101 wm_rt[t-1] + 0.121 wm_rt[t-2] + 0.045 wm_rt[t-3] + 0.147 wm_rt[t-4] + 0.083 wm_rt^2 + -0.03 wm_rt*repeat + 0.021 wm_rt*switch + 0.004 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.114 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + 0.0 repeat*switch + -0.267 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.045 repeat*wm_rt[t-3] + 0.08 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.096 switch*wm_rt[t-1] + -0.058 switch*wm_rt[t-2] + -0.095 switch*wm_rt[t-3] + 0.25 switch*wm_rt[t-4] + -0.053 wm_rt[t-1]^2 + -0.046 wm_rt[t-1]*wm_rt[t-2] + -0.047 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.115 wm_rt[t-2]^2 + -0.112 wm_rt[t-2]*wm_rt[t-3] + -0.108 wm_rt[t-2]*wm_rt[t-4] + -0.021 wm_rt[t-3]^2 + -0.006 wm_rt[t-3]*wm_rt[t-4] + 0.03 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 12, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 17, 0, 33, 33, 0, 0, 33, 0, 0, 33, 33, 33, 32, 33, 0, 33, 33, 0, 33, 33, 0, 33, 0, 0, 0, 0, 0, 9, 8, 0, 0, 0, 0, 33, 33, 14\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 133/400 --- L(Train): 0.4282210 --- L(Val, RNN): 0.4032556 --- L(Val, SINDy): 0.7243213 --- Time: 4.10s; --- Convergence: 1.55e-03; LR: 1.00e-02; Metric: 0.4030072; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.04 1 + 0.541 value_reward_diff[t] + 1.132 reward_diff + -0.586 value_reward_diff^2 + 0.143 value_reward_diff*reward_diff + 1.13 reward_diff^2 \n",
      "value_persistance[t+1] = -0.363 1 + 0.795 value_persistance[t] + -0.084 repeat + 0.32 value_persistance^2 + -0.177 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.04 1 + 1.058 wm_rt[t] + 0.009 repeat + 0.003 switch + -0.1 wm_rt[t-1] + 0.122 wm_rt[t-2] + 0.046 wm_rt[t-3] + 0.146 wm_rt[t-4] + 0.083 wm_rt^2 + -0.032 wm_rt*repeat + 0.023 wm_rt*switch + 0.006 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.114 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + 0.0 repeat*switch + -0.268 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.047 repeat*wm_rt[t-3] + 0.08 repeat*wm_rt[t-4] + 0.004 switch^2 + -0.095 switch*wm_rt[t-1] + -0.056 switch*wm_rt[t-2] + -0.095 switch*wm_rt[t-3] + 0.249 switch*wm_rt[t-4] + -0.052 wm_rt[t-1]^2 + -0.045 wm_rt[t-1]*wm_rt[t-2] + -0.047 wm_rt[t-1]*wm_rt[t-3] + -0.054 wm_rt[t-1]*wm_rt[t-4] + -0.114 wm_rt[t-2]^2 + -0.111 wm_rt[t-2]*wm_rt[t-3] + -0.106 wm_rt[t-2]*wm_rt[t-4] + -0.021 wm_rt[t-3]^2 + -0.006 wm_rt[t-3]*wm_rt[t-4] + 0.028 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 13, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 18, 0, 34, 34, 0, 0, 34, 0, 0, 34, 34, 34, 33, 34, 0, 34, 34, 0, 34, 34, 0, 34, 0, 0, 0, 0, 0, 10, 9, 0, 0, 0, 0, 34, 34, 15\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 134/400 --- L(Train): 0.4360246 --- L(Val, RNN): 0.4047153 --- L(Val, SINDy): 0.7145732 --- Time: 3.76s; --- Convergence: 1.50e-03; LR: 1.00e-02; Metric: 0.4030072; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.04 1 + 0.541 value_reward_diff[t] + 1.14 reward_diff + -0.587 value_reward_diff^2 + 0.141 value_reward_diff*reward_diff + 1.137 reward_diff^2 \n",
      "value_persistance[t+1] = -0.364 1 + 0.795 value_persistance[t] + -0.084 repeat + 0.321 value_persistance^2 + -0.178 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.039 1 + 1.058 wm_rt[t] + 0.008 repeat + 0.005 switch + -0.099 wm_rt[t-1] + 0.124 wm_rt[t-2] + 0.046 wm_rt[t-3] + 0.145 wm_rt[t-4] + 0.083 wm_rt^2 + -0.033 wm_rt*repeat + 0.025 wm_rt*switch + 0.008 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.114 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.0 repeat*switch + -0.269 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.048 repeat*wm_rt[t-3] + 0.08 repeat*wm_rt[t-4] + 0.006 switch^2 + -0.094 switch*wm_rt[t-1] + -0.054 switch*wm_rt[t-2] + -0.094 switch*wm_rt[t-3] + 0.247 switch*wm_rt[t-4] + -0.051 wm_rt[t-1]^2 + -0.044 wm_rt[t-1]*wm_rt[t-2] + -0.046 wm_rt[t-1]*wm_rt[t-3] + -0.053 wm_rt[t-1]*wm_rt[t-4] + -0.112 wm_rt[t-2]^2 + -0.11 wm_rt[t-2]*wm_rt[t-3] + -0.105 wm_rt[t-2]*wm_rt[t-4] + -0.021 wm_rt[t-3]^2 + -0.005 wm_rt[t-3]*wm_rt[t-4] + 0.027 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 14, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 19, 0, 35, 35, 0, 0, 35, 0, 0, 35, 35, 35, 34, 35, 0, 35, 35, 0, 35, 35, 0, 35, 0, 0, 0, 0, 0, 11, 10, 0, 0, 0, 0, 35, 35, 16\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 135/400 --- L(Train): 0.4307046 --- L(Val, RNN): 0.4009095 --- L(Val, SINDy): 0.7241721 --- Time: 3.56s; --- Convergence: 2.65e-03; LR: 1.00e-02; Metric: 0.4009095; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.042 1 + 0.538 value_reward_diff[t] + 1.147 reward_diff + -0.59 value_reward_diff^2 + 0.138 value_reward_diff*reward_diff + 1.144 reward_diff^2 \n",
      "value_persistance[t+1] = -0.364 1 + 0.794 value_persistance[t] + -0.084 repeat + 0.321 value_persistance^2 + -0.177 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.04 1 + 1.057 wm_rt[t] + 0.008 repeat + 0.005 switch + -0.098 wm_rt[t-1] + 0.126 wm_rt[t-2] + 0.047 wm_rt[t-3] + 0.144 wm_rt[t-4] + 0.082 wm_rt^2 + -0.034 wm_rt*repeat + 0.025 wm_rt*switch + 0.009 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.004 wm_rt*wm_rt[t-3] + -0.114 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.0 repeat*switch + -0.269 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.048 repeat*wm_rt[t-3] + 0.081 repeat*wm_rt[t-4] + 0.006 switch^2 + -0.094 switch*wm_rt[t-1] + -0.052 switch*wm_rt[t-2] + -0.093 switch*wm_rt[t-3] + 0.245 switch*wm_rt[t-4] + -0.051 wm_rt[t-1]^2 + -0.043 wm_rt[t-1]*wm_rt[t-2] + -0.045 wm_rt[t-1]*wm_rt[t-3] + -0.052 wm_rt[t-1]*wm_rt[t-4] + -0.111 wm_rt[t-2]^2 + -0.108 wm_rt[t-2]*wm_rt[t-3] + -0.103 wm_rt[t-2]*wm_rt[t-4] + -0.02 wm_rt[t-3]^2 + -0.005 wm_rt[t-3]*wm_rt[t-4] + 0.026 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 15, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 20, 0, 36, 36, 0, 0, 36, 0, 0, 36, 36, 36, 35, 36, 0, 36, 36, 0, 36, 36, 0, 36, 0, 0, 0, 0, 0, 12, 11, 0, 0, 0, 0, 36, 36, 17\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 136/400 --- L(Train): 0.4318321 --- L(Val, RNN): 0.4019838 --- L(Val, SINDy): 0.7514923 --- Time: 3.43s; --- Convergence: 1.86e-03; LR: 1.00e-02; Metric: 0.4009095; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.043 1 + 0.536 value_reward_diff[t] + 1.154 reward_diff + -0.593 value_reward_diff^2 + 0.135 value_reward_diff*reward_diff + 1.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.365 1 + 0.794 value_persistance[t] + -0.085 repeat + 0.321 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.04 1 + 1.057 wm_rt[t] + 0.009 repeat + 0.005 switch + -0.098 wm_rt[t-1] + 0.127 wm_rt[t-2] + 0.047 wm_rt[t-3] + 0.142 wm_rt[t-4] + 0.082 wm_rt^2 + -0.034 wm_rt*repeat + 0.025 wm_rt*switch + 0.01 wm_rt*wm_rt[t-1] + -0.004 wm_rt*wm_rt[t-2] + -0.004 wm_rt*wm_rt[t-3] + -0.114 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.0 repeat*switch + -0.268 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.048 repeat*wm_rt[t-3] + 0.081 repeat*wm_rt[t-4] + 0.006 switch^2 + -0.093 switch*wm_rt[t-1] + -0.05 switch*wm_rt[t-2] + -0.092 switch*wm_rt[t-3] + 0.243 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.043 wm_rt[t-1]*wm_rt[t-2] + -0.045 wm_rt[t-1]*wm_rt[t-3] + -0.052 wm_rt[t-1]*wm_rt[t-4] + -0.109 wm_rt[t-2]^2 + -0.107 wm_rt[t-2]*wm_rt[t-3] + -0.102 wm_rt[t-2]*wm_rt[t-4] + -0.02 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + 0.025 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 16, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 21, 0, 37, 37, 0, 0, 37, 0, 0, 37, 37, 37, 36, 37, 0, 37, 37, 0, 37, 37, 0, 37, 0, 0, 0, 0, 0, 13, 12, 0, 0, 0, 0, 37, 37, 18\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 137/400 --- L(Train): 0.4278072 --- L(Val, RNN): 0.3999306 --- L(Val, SINDy): 0.7207334 --- Time: 3.17s; --- Convergence: 1.96e-03; LR: 1.00e-02; Metric: 0.3999306; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.043 1 + 0.534 value_reward_diff[t] + 1.161 reward_diff + -0.594 value_reward_diff^2 + 0.133 value_reward_diff*reward_diff + 1.159 reward_diff^2 \n",
      "value_persistance[t+1] = -0.366 1 + 0.794 value_persistance[t] + -0.086 repeat + 0.321 value_persistance^2 + -0.175 value_persistance*repeat + -0.087 repeat^2 \n",
      "wm_rt[t+1] = -0.04 1 + 1.056 wm_rt[t] + 0.01 repeat + 0.004 switch + -0.097 wm_rt[t-1] + 0.129 wm_rt[t-2] + 0.048 wm_rt[t-3] + 0.141 wm_rt[t-4] + 0.081 wm_rt^2 + -0.034 wm_rt*repeat + 0.025 wm_rt*switch + 0.01 wm_rt*wm_rt[t-1] + -0.006 wm_rt*wm_rt[t-2] + -0.005 wm_rt*wm_rt[t-3] + -0.115 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.0 repeat*switch + -0.268 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.048 repeat*wm_rt[t-3] + 0.082 repeat*wm_rt[t-4] + 0.005 switch^2 + -0.093 switch*wm_rt[t-1] + -0.049 switch*wm_rt[t-2] + -0.092 switch*wm_rt[t-3] + 0.241 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.043 wm_rt[t-1]*wm_rt[t-2] + -0.044 wm_rt[t-1]*wm_rt[t-3] + -0.052 wm_rt[t-1]*wm_rt[t-4] + -0.108 wm_rt[t-2]^2 + -0.106 wm_rt[t-2]*wm_rt[t-3] + -0.101 wm_rt[t-2]*wm_rt[t-4] + -0.019 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + 0.023 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 17, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 22, 0, 38, 38, 0, 0, 38, 0, 0, 38, 38, 38, 37, 38, 0, 38, 38, 0, 38, 38, 0, 38, 0, 1, 0, 0, 0, 14, 13, 0, 0, 0, 0, 38, 38, 19\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 138/400 --- L(Train): 0.4326591 --- L(Val, RNN): 0.3983784 --- L(Val, SINDy): 0.7411283 --- Time: 4.26s; --- Convergence: 1.76e-03; LR: 1.00e-02; Metric: 0.3983784; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.044 1 + 0.532 value_reward_diff[t] + 1.169 reward_diff + -0.597 value_reward_diff^2 + 0.131 value_reward_diff*reward_diff + 1.166 reward_diff^2 \n",
      "value_persistance[t+1] = -0.366 1 + 0.794 value_persistance[t] + -0.087 repeat + 0.321 value_persistance^2 + -0.174 value_persistance*repeat + -0.088 repeat^2 \n",
      "wm_rt[t+1] = -0.04 1 + 1.055 wm_rt[t] + 0.01 repeat + 0.003 switch + -0.097 wm_rt[t-1] + 0.13 wm_rt[t-2] + 0.048 wm_rt[t-3] + 0.14 wm_rt[t-4] + 0.08 wm_rt^2 + -0.034 wm_rt*repeat + 0.024 wm_rt*switch + 0.011 wm_rt*wm_rt[t-1] + -0.007 wm_rt*wm_rt[t-2] + -0.005 wm_rt*wm_rt[t-3] + -0.115 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + 0.0 repeat*switch + -0.267 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.048 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + 0.004 switch^2 + -0.093 switch*wm_rt[t-1] + -0.048 switch*wm_rt[t-2] + -0.092 switch*wm_rt[t-3] + 0.239 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.043 wm_rt[t-1]*wm_rt[t-2] + -0.044 wm_rt[t-1]*wm_rt[t-3] + -0.052 wm_rt[t-1]*wm_rt[t-4] + -0.107 wm_rt[t-2]^2 + -0.105 wm_rt[t-2]*wm_rt[t-3] + -0.1 wm_rt[t-2]*wm_rt[t-4] + -0.019 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + 0.022 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 18, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 23, 0, 39, 39, 0, 0, 39, 0, 0, 39, 39, 39, 38, 39, 0, 39, 39, 0, 39, 39, 0, 39, 0, 2, 0, 0, 0, 15, 14, 0, 0, 0, 0, 39, 39, 20\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 139/400 --- L(Train): 0.4208602 --- L(Val, RNN): 0.4001521 --- L(Val, SINDy): 0.7503321 --- Time: 3.94s; --- Convergence: 1.76e-03; LR: 1.00e-02; Metric: 0.3983784; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.045 1 + 0.529 value_reward_diff[t] + 1.177 reward_diff + -0.599 value_reward_diff^2 + 0.129 value_reward_diff*reward_diff + 1.174 reward_diff^2 \n",
      "value_persistance[t+1] = -0.367 1 + 0.794 value_persistance[t] + -0.087 repeat + 0.321 value_persistance^2 + -0.174 value_persistance*repeat + -0.089 repeat^2 \n",
      "wm_rt[t+1] = -0.041 1 + 1.055 wm_rt[t] + 0.011 repeat + 0.002 switch + -0.097 wm_rt[t-1] + 0.131 wm_rt[t-2] + 0.048 wm_rt[t-3] + 0.139 wm_rt[t-4] + 0.079 wm_rt^2 + -0.034 wm_rt*repeat + 0.024 wm_rt*switch + 0.011 wm_rt*wm_rt[t-1] + -0.009 wm_rt*wm_rt[t-2] + -0.006 wm_rt*wm_rt[t-3] + -0.115 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + 0.0 repeat*switch + -0.266 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.047 repeat*wm_rt[t-3] + 0.085 repeat*wm_rt[t-4] + 0.003 switch^2 + -0.093 switch*wm_rt[t-1] + -0.047 switch*wm_rt[t-2] + -0.092 switch*wm_rt[t-3] + 0.237 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.043 wm_rt[t-1]*wm_rt[t-2] + -0.044 wm_rt[t-1]*wm_rt[t-3] + -0.052 wm_rt[t-1]*wm_rt[t-4] + -0.106 wm_rt[t-2]^2 + -0.104 wm_rt[t-2]*wm_rt[t-3] + -0.099 wm_rt[t-2]*wm_rt[t-4] + -0.019 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + 0.02 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 19, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 24, 0, 40, 40, 0, 0, 40, 0, 0, 40, 40, 40, 39, 40, 0, 40, 40, 0, 40, 40, 0, 40, 0, 3, 0, 0, 0, 16, 15, 0, 0, 0, 0, 40, 40, 21\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 140/400 --- L(Train): 0.4285656 --- L(Val, RNN): 0.3966252 --- L(Val, SINDy): 0.7795891 --- Time: 3.46s; --- Convergence: 2.65e-03; LR: 1.00e-02; Metric: 0.3966252; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.045 1 + 0.528 value_reward_diff[t] + 1.185 reward_diff + -0.6 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 1.182 reward_diff^2 \n",
      "value_persistance[t+1] = -0.368 1 + 0.794 value_persistance[t] + -0.088 repeat + 0.321 value_persistance^2 + -0.173 value_persistance*repeat + -0.089 repeat^2 \n",
      "wm_rt[t+1] = -0.041 1 + 1.054 wm_rt[t] + 0.013 repeat + 0.001 switch + -0.097 wm_rt[t-1] + 0.132 wm_rt[t-2] + 0.049 wm_rt[t-3] + 0.137 wm_rt[t-4] + 0.079 wm_rt^2 + -0.034 wm_rt*repeat + 0.023 wm_rt*switch + 0.011 wm_rt*wm_rt[t-1] + -0.011 wm_rt*wm_rt[t-2] + -0.007 wm_rt*wm_rt[t-3] + -0.116 wm_rt*wm_rt[t-4] + 0.01 repeat^2 + 0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.047 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.094 switch*wm_rt[t-1] + -0.045 switch*wm_rt[t-2] + -0.092 switch*wm_rt[t-3] + 0.235 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.043 wm_rt[t-1]*wm_rt[t-2] + -0.044 wm_rt[t-1]*wm_rt[t-3] + -0.052 wm_rt[t-1]*wm_rt[t-4] + -0.105 wm_rt[t-2]^2 + -0.102 wm_rt[t-2]*wm_rt[t-3] + -0.098 wm_rt[t-2]*wm_rt[t-4] + -0.018 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + 0.019 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 20, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 25, 0, 41, 41, 0, 0, 41, 0, 0, 41, 41, 41, 40, 41, 0, 41, 41, 0, 41, 41, 0, 41, 0, 4, 0, 0, 0, 17, 16, 0, 0, 0, 0, 41, 41, 22\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 141/400 --- L(Train): 0.4326550 --- L(Val, RNN): 0.3997782 --- L(Val, SINDy): 0.8137298 --- Time: 3.39s; --- Convergence: 2.90e-03; LR: 1.00e-02; Metric: 0.3966252; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.045 1 + 0.528 value_reward_diff[t] + 1.193 reward_diff + -0.601 value_reward_diff^2 + 0.127 value_reward_diff*reward_diff + 1.189 reward_diff^2 \n",
      "value_persistance[t+1] = -0.368 1 + 0.793 value_persistance[t] + -0.088 repeat + 0.322 value_persistance^2 + -0.173 value_persistance*repeat + -0.09 repeat^2 \n",
      "wm_rt[t+1] = -0.04 1 + 1.055 wm_rt[t] + 0.014 repeat + 0.0 switch + -0.097 wm_rt[t-1] + 0.133 wm_rt[t-2] + 0.049 wm_rt[t-3] + 0.136 wm_rt[t-4] + 0.079 wm_rt^2 + -0.034 wm_rt*repeat + 0.022 wm_rt*switch + 0.012 wm_rt*wm_rt[t-1] + -0.012 wm_rt*wm_rt[t-2] + -0.007 wm_rt*wm_rt[t-3] + -0.116 wm_rt*wm_rt[t-4] + 0.011 repeat^2 + 0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.047 repeat*wm_rt[t-3] + 0.087 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.094 switch*wm_rt[t-1] + -0.044 switch*wm_rt[t-2] + -0.092 switch*wm_rt[t-3] + 0.232 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.043 wm_rt[t-1]*wm_rt[t-2] + -0.044 wm_rt[t-1]*wm_rt[t-3] + -0.052 wm_rt[t-1]*wm_rt[t-4] + -0.104 wm_rt[t-2]^2 + -0.101 wm_rt[t-2]*wm_rt[t-3] + -0.097 wm_rt[t-2]*wm_rt[t-4] + -0.018 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + 0.017 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 21, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 26, 0, 42, 42, 0, 0, 42, 0, 0, 42, 42, 42, 41, 42, 0, 42, 42, 0, 42, 42, 0, 42, 0, 5, 0, 0, 0, 18, 17, 0, 0, 0, 0, 42, 42, 23\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 142/400 --- L(Train): 0.4348595 --- L(Val, RNN): 0.3936742 --- L(Val, SINDy): 0.7875210 --- Time: 4.00s; --- Convergence: 4.50e-03; LR: 1.00e-02; Metric: 0.3936742; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.045 1 + 0.527 value_reward_diff[t] + 1.201 reward_diff + -0.602 value_reward_diff^2 + 0.126 value_reward_diff*reward_diff + 1.197 reward_diff^2 \n",
      "value_persistance[t+1] = -0.368 1 + 0.793 value_persistance[t] + -0.088 repeat + 0.322 value_persistance^2 + -0.173 value_persistance*repeat + -0.089 repeat^2 \n",
      "wm_rt[t+1] = -0.039 1 + 1.055 wm_rt[t] + 0.014 repeat + -0.0 switch + -0.097 wm_rt[t-1] + 0.134 wm_rt[t-2] + 0.049 wm_rt[t-3] + 0.135 wm_rt[t-4] + 0.079 wm_rt^2 + -0.034 wm_rt*repeat + 0.022 wm_rt*switch + 0.013 wm_rt*wm_rt[t-1] + -0.014 wm_rt*wm_rt[t-2] + -0.008 wm_rt*wm_rt[t-3] + -0.117 wm_rt*wm_rt[t-4] + 0.011 repeat^2 + -0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.047 repeat*wm_rt[t-3] + 0.087 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.094 switch*wm_rt[t-1] + -0.043 switch*wm_rt[t-2] + -0.091 switch*wm_rt[t-3] + 0.23 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.042 wm_rt[t-1]*wm_rt[t-2] + -0.044 wm_rt[t-1]*wm_rt[t-3] + -0.052 wm_rt[t-1]*wm_rt[t-4] + -0.103 wm_rt[t-2]^2 + -0.1 wm_rt[t-2]*wm_rt[t-3] + -0.096 wm_rt[t-2]*wm_rt[t-4] + -0.018 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + 0.016 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 22, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 27, 0, 43, 43, 0, 0, 43, 0, 0, 43, 43, 43, 42, 43, 0, 43, 43, 0, 43, 43, 0, 43, 0, 6, 0, 0, 0, 19, 18, 0, 0, 0, 0, 43, 43, 24\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 143/400 --- L(Train): 0.4322817 --- L(Val, RNN): 0.3967802 --- L(Val, SINDy): 0.6712148 --- Time: 3.78s; --- Convergence: 3.80e-03; LR: 1.00e-02; Metric: 0.3936742; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.046 1 + 0.526 value_reward_diff[t] + 1.208 reward_diff + -0.602 value_reward_diff^2 + 0.126 value_reward_diff*reward_diff + 1.205 reward_diff^2 \n",
      "value_persistance[t+1] = -0.368 1 + 0.792 value_persistance[t] + -0.087 repeat + 0.323 value_persistance^2 + -0.174 value_persistance*repeat + -0.089 repeat^2 \n",
      "wm_rt[t+1] = -0.038 1 + 1.056 wm_rt[t] + 0.014 repeat + -0.0 switch + -0.096 wm_rt[t-1] + 0.136 wm_rt[t-2] + 0.049 wm_rt[t-3] + 0.133 wm_rt[t-4] + 0.08 wm_rt^2 + -0.035 wm_rt*repeat + 0.022 wm_rt*switch + 0.013 wm_rt*wm_rt[t-1] + -0.015 wm_rt*wm_rt[t-2] + -0.008 wm_rt*wm_rt[t-3] + -0.117 wm_rt*wm_rt[t-4] + 0.011 repeat^2 + -0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.048 repeat*wm_rt[t-3] + 0.087 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.093 switch*wm_rt[t-1] + -0.041 switch*wm_rt[t-2] + -0.091 switch*wm_rt[t-3] + 0.229 switch*wm_rt[t-4] + -0.05 wm_rt[t-1]^2 + -0.042 wm_rt[t-1]*wm_rt[t-2] + -0.044 wm_rt[t-1]*wm_rt[t-3] + -0.051 wm_rt[t-1]*wm_rt[t-4] + -0.102 wm_rt[t-2]^2 + -0.099 wm_rt[t-2]*wm_rt[t-3] + -0.095 wm_rt[t-2]*wm_rt[t-4] + -0.018 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + 0.014 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 23, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 28, 0, 44, 44, 0, 0, 44, 0, 0, 44, 44, 44, 43, 44, 0, 44, 44, 0, 44, 44, 0, 44, 0, 7, 0, 0, 1, 20, 19, 0, 0, 0, 0, 44, 44, 25\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 144/400 --- L(Train): 0.4165096 --- L(Val, RNN): 0.3969964 --- L(Val, SINDy): 0.6674588 --- Time: 4.10s; --- Convergence: 2.01e-03; LR: 1.00e-02; Metric: 0.3936742; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.046 1 + 0.526 value_reward_diff[t] + 1.217 reward_diff + -0.603 value_reward_diff^2 + 0.126 value_reward_diff*reward_diff + 1.213 reward_diff^2 \n",
      "value_persistance[t+1] = -0.368 1 + 0.79 value_persistance[t] + -0.086 repeat + 0.325 value_persistance^2 + -0.174 value_persistance*repeat + -0.088 repeat^2 \n",
      "wm_rt[t+1] = -0.038 1 + 1.056 wm_rt[t] + 0.013 repeat + 0.0 switch + -0.095 wm_rt[t-1] + 0.137 wm_rt[t-2] + 0.05 wm_rt[t-3] + 0.132 wm_rt[t-4] + 0.079 wm_rt^2 + -0.036 wm_rt*repeat + 0.022 wm_rt*switch + 0.013 wm_rt*wm_rt[t-1] + -0.016 wm_rt*wm_rt[t-2] + -0.009 wm_rt*wm_rt[t-3] + -0.118 wm_rt*wm_rt[t-4] + 0.01 repeat^2 + -0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.049 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.093 switch*wm_rt[t-1] + -0.039 switch*wm_rt[t-2] + -0.09 switch*wm_rt[t-3] + 0.227 switch*wm_rt[t-4] + -0.049 wm_rt[t-1]^2 + -0.041 wm_rt[t-1]*wm_rt[t-2] + -0.043 wm_rt[t-1]*wm_rt[t-3] + -0.05 wm_rt[t-1]*wm_rt[t-4] + -0.1 wm_rt[t-2]^2 + -0.098 wm_rt[t-2]*wm_rt[t-3] + -0.093 wm_rt[t-2]*wm_rt[t-4] + -0.017 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + 0.013 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 24, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 29, 0, 45, 45, 0, 0, 45, 0, 0, 45, 45, 45, 44, 45, 0, 45, 45, 0, 45, 45, 0, 45, 0, 8, 0, 0, 2, 21, 20, 0, 0, 0, 0, 45, 45, 26\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 145/400 --- L(Train): 0.4240955 --- L(Val, RNN): 0.3914802 --- L(Val, SINDy): 0.6589310 --- Time: 3.76s; --- Convergence: 3.76e-03; LR: 1.00e-02; Metric: 0.3914802; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.047 1 + 0.525 value_reward_diff[t] + 1.224 reward_diff + -0.604 value_reward_diff^2 + 0.125 value_reward_diff*reward_diff + 1.221 reward_diff^2 \n",
      "value_persistance[t+1] = -0.367 1 + 0.789 value_persistance[t] + -0.085 repeat + 0.326 value_persistance^2 + -0.175 value_persistance*repeat + -0.087 repeat^2 \n",
      "wm_rt[t+1] = -0.038 1 + 1.055 wm_rt[t] + 0.012 repeat + 0.0 switch + -0.095 wm_rt[t-1] + 0.139 wm_rt[t-2] + 0.05 wm_rt[t-3] + 0.131 wm_rt[t-4] + 0.079 wm_rt^2 + -0.037 wm_rt*repeat + 0.023 wm_rt*switch + 0.012 wm_rt*wm_rt[t-1] + -0.018 wm_rt*wm_rt[t-2] + -0.009 wm_rt*wm_rt[t-3] + -0.118 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + 0.0 repeat*switch + -0.265 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.05 repeat*wm_rt[t-3] + 0.085 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.092 switch*wm_rt[t-1] + -0.038 switch*wm_rt[t-2] + -0.089 switch*wm_rt[t-3] + 0.226 switch*wm_rt[t-4] + -0.048 wm_rt[t-1]^2 + -0.04 wm_rt[t-1]*wm_rt[t-2] + -0.042 wm_rt[t-1]*wm_rt[t-3] + -0.05 wm_rt[t-1]*wm_rt[t-4] + -0.099 wm_rt[t-2]^2 + -0.096 wm_rt[t-2]*wm_rt[t-3] + -0.092 wm_rt[t-2]*wm_rt[t-4] + -0.017 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.012 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 25, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 30, 0, 46, 46, 0, 0, 0, 0, 0, 46, 46, 46, 45, 46, 0, 46, 46, 0, 46, 0, 0, 46, 0, 9, 0, 0, 3, 22, 21, 1, 0, 0, 0, 46, 46, 27\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 146/400 --- L(Train): 0.4149967 --- L(Val, RNN): 0.3938826 --- L(Val, SINDy): 0.6891964 --- Time: 4.21s; --- Convergence: 3.08e-03; LR: 1.00e-02; Metric: 0.3914802; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.048 1 + 0.525 value_reward_diff[t] + 1.232 reward_diff + -0.605 value_reward_diff^2 + 0.126 value_reward_diff*reward_diff + 1.228 reward_diff^2 \n",
      "value_persistance[t+1] = -0.367 1 + 0.788 value_persistance[t] + -0.085 repeat + 0.327 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.038 1 + 1.055 wm_rt[t] + 0.011 repeat + 0.001 switch + -0.094 wm_rt[t-1] + 0.14 wm_rt[t-2] + 0.051 wm_rt[t-3] + 0.129 wm_rt[t-4] + 0.079 wm_rt^2 + -0.038 wm_rt*repeat + 0.023 wm_rt*switch + 0.012 wm_rt*wm_rt[t-1] + -0.019 wm_rt*wm_rt[t-2] + -0.009 wm_rt*wm_rt[t-3] + -0.118 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + 0.0 repeat*switch + -0.266 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.052 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.091 switch*wm_rt[t-1] + -0.036 switch*wm_rt[t-2] + -0.088 switch*wm_rt[t-3] + 0.225 switch*wm_rt[t-4] + -0.047 wm_rt[t-1]^2 + -0.039 wm_rt[t-1]*wm_rt[t-2] + -0.041 wm_rt[t-1]*wm_rt[t-3] + -0.049 wm_rt[t-1]*wm_rt[t-4] + -0.098 wm_rt[t-2]^2 + -0.095 wm_rt[t-2]*wm_rt[t-3] + -0.091 wm_rt[t-2]*wm_rt[t-4] + -0.016 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.01 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 26, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 31, 0, 47, 47, 0, 0, 0, 0, 0, 47, 47, 47, 46, 47, 0, 47, 47, 0, 47, 0, 0, 47, 0, 10, 0, 0, 4, 23, 22, 2, 0, 0, 0, 47, 47, 28\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 147/400 --- L(Train): 0.4234675 --- L(Val, RNN): 0.3939598 --- L(Val, SINDy): 0.7254838 --- Time: 3.86s; --- Convergence: 1.58e-03; LR: 1.00e-02; Metric: 0.3914802; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.048 1 + 0.526 value_reward_diff[t] + 1.24 reward_diff + -0.604 value_reward_diff^2 + 0.127 value_reward_diff*reward_diff + 1.236 reward_diff^2 \n",
      "value_persistance[t+1] = -0.367 1 + 0.787 value_persistance[t] + -0.085 repeat + 0.328 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.038 1 + 1.055 wm_rt[t] + 0.01 repeat + 0.001 switch + -0.093 wm_rt[t-1] + 0.141 wm_rt[t-2] + 0.051 wm_rt[t-3] + 0.128 wm_rt[t-4] + 0.078 wm_rt^2 + -0.04 wm_rt*repeat + 0.024 wm_rt*switch + 0.012 wm_rt*wm_rt[t-1] + -0.02 wm_rt*wm_rt[t-2] + -0.01 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.0 repeat*switch + -0.267 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.053 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.09 switch*wm_rt[t-1] + -0.034 switch*wm_rt[t-2] + -0.087 switch*wm_rt[t-3] + 0.224 switch*wm_rt[t-4] + -0.046 wm_rt[t-1]^2 + -0.039 wm_rt[t-1]*wm_rt[t-2] + -0.041 wm_rt[t-1]*wm_rt[t-3] + -0.048 wm_rt[t-1]*wm_rt[t-4] + -0.096 wm_rt[t-2]^2 + -0.094 wm_rt[t-2]*wm_rt[t-3] + -0.09 wm_rt[t-2]*wm_rt[t-4] + -0.016 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.009 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 27, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 32, 0, 48, 48, 0, 0, 0, 0, 0, 48, 48, 48, 47, 48, 0, 48, 48, 0, 48, 0, 0, 48, 0, 11, 0, 0, 5, 24, 23, 3, 0, 0, 0, 48, 48, 29\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 148/400 --- L(Train): 0.4160900 --- L(Val, RNN): 0.3921203 --- L(Val, SINDy): 0.7503877 --- Time: 3.47s; --- Convergence: 1.71e-03; LR: 1.00e-02; Metric: 0.3914802; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.047 1 + 0.528 value_reward_diff[t] + 1.248 reward_diff + -0.602 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 1.244 reward_diff^2 \n",
      "value_persistance[t+1] = -0.367 1 + 0.787 value_persistance[t] + -0.085 repeat + 0.328 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.037 1 + 1.055 wm_rt[t] + 0.009 repeat + 0.002 switch + -0.092 wm_rt[t-1] + 0.142 wm_rt[t-2] + 0.051 wm_rt[t-3] + 0.127 wm_rt[t-4] + 0.078 wm_rt^2 + -0.041 wm_rt*repeat + 0.024 wm_rt*switch + 0.012 wm_rt*wm_rt[t-1] + -0.021 wm_rt*wm_rt[t-2] + -0.01 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.0 repeat*switch + -0.268 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.055 repeat*wm_rt[t-3] + 0.082 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.09 switch*wm_rt[t-1] + -0.033 switch*wm_rt[t-2] + -0.087 switch*wm_rt[t-3] + 0.222 switch*wm_rt[t-4] + -0.046 wm_rt[t-1]^2 + -0.038 wm_rt[t-1]*wm_rt[t-2] + -0.04 wm_rt[t-1]*wm_rt[t-3] + -0.048 wm_rt[t-1]*wm_rt[t-4] + -0.095 wm_rt[t-2]^2 + -0.093 wm_rt[t-2]*wm_rt[t-3] + -0.088 wm_rt[t-2]*wm_rt[t-4] + -0.016 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.007 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 28, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 33, 0, 49, 49, 0, 0, 0, 0, 0, 49, 49, 49, 48, 49, 0, 49, 49, 0, 49, 0, 0, 49, 0, 12, 0, 0, 6, 25, 24, 4, 0, 0, 0, 49, 49, 30\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 149/400 --- L(Train): 0.4206254 --- L(Val, RNN): 0.3921503 --- L(Val, SINDy): 0.7984413 --- Time: 3.34s; --- Convergence: 8.70e-04; LR: 1.00e-02; Metric: 0.3914802; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.047 1 + 0.529 value_reward_diff[t] + 1.256 reward_diff + -0.601 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 1.252 reward_diff^2 \n",
      "value_persistance[t+1] = -0.368 1 + 0.787 value_persistance[t] + -0.085 repeat + 0.328 value_persistance^2 + -0.175 value_persistance*repeat + -0.087 repeat^2 \n",
      "wm_rt[t+1] = -0.037 1 + 1.055 wm_rt[t] + 0.009 repeat + 0.002 switch + -0.092 wm_rt[t-1] + 0.143 wm_rt[t-2] + 0.051 wm_rt[t-3] + 0.125 wm_rt[t-4] + 0.078 wm_rt^2 + -0.041 wm_rt*repeat + 0.024 wm_rt*switch + 0.013 wm_rt*wm_rt[t-1] + -0.022 wm_rt*wm_rt[t-2] + -0.01 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + 0.0 repeat*switch + -0.267 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.055 repeat*wm_rt[t-3] + 0.082 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.09 switch*wm_rt[t-1] + -0.032 switch*wm_rt[t-2] + -0.086 switch*wm_rt[t-3] + 0.22 switch*wm_rt[t-4] + -0.045 wm_rt[t-1]^2 + -0.038 wm_rt[t-1]*wm_rt[t-2] + -0.04 wm_rt[t-1]*wm_rt[t-3] + -0.047 wm_rt[t-1]*wm_rt[t-4] + -0.095 wm_rt[t-2]^2 + -0.092 wm_rt[t-2]*wm_rt[t-3] + -0.088 wm_rt[t-2]*wm_rt[t-4] + -0.016 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.006 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 29, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 34, 0, 50, 50, 0, 0, 0, 0, 0, 50, 50, 50, 49, 50, 0, 50, 50, 0, 50, 0, 0, 50, 0, 13, 0, 0, 7, 26, 25, 5, 0, 0, 0, 50, 50, 31\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 150/400 --- L(Train): 0.4182616 --- L(Val, RNN): 0.3905860 --- L(Val, SINDy): 0.7718668 --- Time: 4.37s; --- Convergence: 1.22e-03; LR: 1.00e-02; Metric: 0.3905860; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.048 1 + 0.53 value_reward_diff[t] + 1.264 reward_diff + -0.601 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 1.26 reward_diff^2 \n",
      "value_persistance[t+1] = -0.369 1 + 0.787 value_persistance[t] + -0.086 repeat + 0.328 value_persistance^2 + -0.175 value_persistance*repeat + -0.088 repeat^2 \n",
      "wm_rt[t+1] = -0.036 1 + 1.055 wm_rt[t] + 0.01 repeat + 0.001 switch + -0.092 wm_rt[t-1] + 0.144 wm_rt[t-2] + 0.051 wm_rt[t-3] + 0.124 wm_rt[t-4] + 0.078 wm_rt^2 + -0.041 wm_rt*repeat + 0.024 wm_rt*switch + 0.015 wm_rt*wm_rt[t-1] + -0.023 wm_rt*wm_rt[t-2] + -0.01 wm_rt*wm_rt[t-3] + -0.119 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + 0.0 repeat*switch + -0.266 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.056 repeat*wm_rt[t-3] + 0.082 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.09 switch*wm_rt[t-1] + -0.031 switch*wm_rt[t-2] + -0.086 switch*wm_rt[t-3] + 0.218 switch*wm_rt[t-4] + -0.046 wm_rt[t-1]^2 + -0.038 wm_rt[t-1]*wm_rt[t-2] + -0.04 wm_rt[t-1]*wm_rt[t-3] + -0.047 wm_rt[t-1]*wm_rt[t-4] + -0.094 wm_rt[t-2]^2 + -0.091 wm_rt[t-2]*wm_rt[t-3] + -0.087 wm_rt[t-2]*wm_rt[t-4] + -0.016 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.004 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 30, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 35, 0, 51, 51, 0, 0, 0, 0, 0, 51, 51, 51, 50, 51, 0, 51, 51, 0, 51, 0, 0, 51, 0, 14, 0, 0, 8, 27, 26, 6, 0, 0, 0, 51, 51, 32\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 151/400 --- L(Train): 0.4191967 --- L(Val, RNN): 0.3891306 --- L(Val, SINDy): 0.7091107 --- Time: 3.39s; --- Convergence: 1.34e-03; LR: 1.00e-02; Metric: 0.3891306; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.05 1 + 0.529 value_reward_diff[t] + 1.272 reward_diff + -0.603 value_reward_diff^2 + 0.126 value_reward_diff*reward_diff + 1.268 reward_diff^2 \n",
      "value_persistance[t+1] = -0.369 1 + 0.787 value_persistance[t] + -0.087 repeat + 0.328 value_persistance^2 + -0.174 value_persistance*repeat + -0.088 repeat^2 \n",
      "wm_rt[t+1] = -0.036 1 + 1.055 wm_rt[t] + 0.011 repeat + 0.001 switch + -0.092 wm_rt[t-1] + 0.145 wm_rt[t-2] + 0.051 wm_rt[t-3] + 0.122 wm_rt[t-4] + 0.078 wm_rt^2 + -0.04 wm_rt*repeat + 0.025 wm_rt*switch + 0.016 wm_rt*wm_rt[t-1] + -0.023 wm_rt*wm_rt[t-2] + -0.01 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + 0.0 repeat*switch + -0.266 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.057 repeat*wm_rt[t-3] + 0.082 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.09 switch*wm_rt[t-1] + -0.03 switch*wm_rt[t-2] + -0.086 switch*wm_rt[t-3] + 0.216 switch*wm_rt[t-4] + -0.046 wm_rt[t-1]^2 + -0.038 wm_rt[t-1]*wm_rt[t-2] + -0.04 wm_rt[t-1]*wm_rt[t-3] + -0.048 wm_rt[t-1]*wm_rt[t-4] + -0.093 wm_rt[t-2]^2 + -0.091 wm_rt[t-2]*wm_rt[t-3] + -0.087 wm_rt[t-2]*wm_rt[t-4] + -0.016 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 31, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 36, 0, 52, 52, 0, 0, 0, 0, 0, 52, 52, 52, 51, 52, 0, 52, 52, 0, 52, 0, 0, 52, 0, 15, 0, 0, 9, 28, 27, 7, 0, 0, 0, 52, 52, 33\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 152/400 --- L(Train): 0.4153970 --- L(Val, RNN): 0.3910328 --- L(Val, SINDy): 0.6595931 --- Time: 3.29s; --- Convergence: 1.62e-03; LR: 1.00e-02; Metric: 0.3891306; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.051 1 + 0.529 value_reward_diff[t] + 1.28 reward_diff + -0.603 value_reward_diff^2 + 0.125 value_reward_diff*reward_diff + 1.276 reward_diff^2 \n",
      "value_persistance[t+1] = -0.37 1 + 0.787 value_persistance[t] + -0.087 repeat + 0.328 value_persistance^2 + -0.173 value_persistance*repeat + -0.089 repeat^2 \n",
      "wm_rt[t+1] = -0.036 1 + 1.055 wm_rt[t] + 0.011 repeat + 0.001 switch + -0.091 wm_rt[t-1] + 0.146 wm_rt[t-2] + 0.051 wm_rt[t-3] + 0.121 wm_rt[t-4] + 0.077 wm_rt^2 + -0.041 wm_rt*repeat + 0.025 wm_rt*switch + 0.017 wm_rt*wm_rt[t-1] + -0.024 wm_rt*wm_rt[t-2] + -0.011 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + -0.0 repeat*switch + -0.265 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.057 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.09 switch*wm_rt[t-1] + -0.029 switch*wm_rt[t-2] + -0.086 switch*wm_rt[t-3] + 0.214 switch*wm_rt[t-4] + -0.045 wm_rt[t-1]^2 + -0.038 wm_rt[t-1]*wm_rt[t-2] + -0.04 wm_rt[t-1]*wm_rt[t-3] + -0.047 wm_rt[t-1]*wm_rt[t-4] + -0.093 wm_rt[t-2]^2 + -0.09 wm_rt[t-2]*wm_rt[t-3] + -0.086 wm_rt[t-2]*wm_rt[t-4] + -0.016 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 37, 0, 53, 53, 0, 0, 0, 0, 0, 53, 53, 53, 52, 53, 0, 53, 53, 0, 53, 0, 0, 53, 0, 16, 0, 0, 10, 29, 28, 8, 0, 0, 0, 53, 53, 34\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 153/400 --- L(Train): 0.4224961 --- L(Val, RNN): 0.3875906 --- L(Val, SINDy): 0.6174005 --- Time: 3.52s; --- Convergence: 2.53e-03; LR: 1.00e-02; Metric: 0.3875906; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.051 1 + 0.53 value_reward_diff[t] + 1.288 reward_diff + -0.602 value_reward_diff^2 + 0.126 value_reward_diff*reward_diff + 1.284 reward_diff^2 \n",
      "value_persistance[t+1] = -0.371 1 + 0.787 value_persistance[t] + -0.088 repeat + 0.328 value_persistance^2 + -0.173 value_persistance*repeat + -0.089 repeat^2 \n",
      "wm_rt[t+1] = -0.036 1 + 1.055 wm_rt[t] + 0.011 repeat + 0.001 switch + -0.091 wm_rt[t-1] + 0.147 wm_rt[t-2] + 0.052 wm_rt[t-3] + 0.12 wm_rt[t-4] + 0.077 wm_rt^2 + -0.041 wm_rt*repeat + 0.024 wm_rt*switch + 0.018 wm_rt*wm_rt[t-1] + -0.024 wm_rt*wm_rt[t-2] + -0.011 wm_rt*wm_rt[t-3] + -0.12 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + -0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.057 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.089 switch*wm_rt[t-1] + -0.028 switch*wm_rt[t-2] + -0.086 switch*wm_rt[t-3] + 0.213 switch*wm_rt[t-4] + -0.045 wm_rt[t-1]^2 + -0.037 wm_rt[t-1]*wm_rt[t-2] + -0.039 wm_rt[t-1]*wm_rt[t-3] + -0.047 wm_rt[t-1]*wm_rt[t-4] + -0.092 wm_rt[t-2]^2 + -0.089 wm_rt[t-2]*wm_rt[t-3] + -0.085 wm_rt[t-2]*wm_rt[t-4] + -0.016 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 38, 0, 54, 54, 0, 0, 0, 0, 0, 54, 54, 54, 53, 54, 0, 54, 54, 0, 54, 0, 0, 54, 0, 17, 0, 0, 11, 30, 29, 9, 0, 0, 0, 54, 54, 35\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 154/400 --- L(Train): 0.4107786 --- L(Val, RNN): 0.3866762 --- L(Val, SINDy): 0.6113435 --- Time: 3.78s; --- Convergence: 1.72e-03; LR: 1.00e-02; Metric: 0.3866762; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.051 1 + 0.531 value_reward_diff[t] + 1.296 reward_diff + -0.602 value_reward_diff^2 + 0.125 value_reward_diff*reward_diff + 1.292 reward_diff^2 \n",
      "value_persistance[t+1] = -0.372 1 + 0.787 value_persistance[t] + -0.088 repeat + 0.328 value_persistance^2 + -0.173 value_persistance*repeat + -0.089 repeat^2 \n",
      "wm_rt[t+1] = -0.036 1 + 1.054 wm_rt[t] + 0.012 repeat + 0.0 switch + -0.09 wm_rt[t-1] + 0.148 wm_rt[t-2] + 0.052 wm_rt[t-3] + 0.119 wm_rt[t-4] + 0.076 wm_rt^2 + -0.041 wm_rt*repeat + 0.024 wm_rt*switch + 0.018 wm_rt*wm_rt[t-1] + -0.025 wm_rt*wm_rt[t-2] + -0.011 wm_rt*wm_rt[t-3] + -0.121 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + -0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.057 repeat*wm_rt[t-3] + 0.085 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.089 switch*wm_rt[t-1] + -0.027 switch*wm_rt[t-2] + -0.085 switch*wm_rt[t-3] + 0.211 switch*wm_rt[t-4] + -0.044 wm_rt[t-1]^2 + -0.036 wm_rt[t-1]*wm_rt[t-2] + -0.038 wm_rt[t-1]*wm_rt[t-3] + -0.046 wm_rt[t-1]*wm_rt[t-4] + -0.091 wm_rt[t-2]^2 + -0.088 wm_rt[t-2]*wm_rt[t-3] + -0.084 wm_rt[t-2]*wm_rt[t-4] + -0.015 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 39, 0, 55, 55, 0, 0, 0, 0, 0, 55, 55, 55, 54, 55, 0, 55, 55, 0, 55, 0, 0, 55, 0, 18, 0, 0, 12, 31, 30, 10, 0, 0, 0, 55, 55, 36\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 155/400 --- L(Train): 0.4188443 --- L(Val, RNN): 0.3869388 --- L(Val, SINDy): 0.6550677 --- Time: 4.17s; --- Convergence: 9.93e-04; LR: 1.00e-02; Metric: 0.3866762; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.53 value_reward_diff[t] + 1.305 reward_diff + -0.603 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 1.3 reward_diff^2 \n",
      "value_persistance[t+1] = -0.372 1 + 0.787 value_persistance[t] + -0.087 repeat + 0.328 value_persistance^2 + -0.173 value_persistance*repeat + -0.089 repeat^2 \n",
      "wm_rt[t+1] = -0.037 1 + 1.053 wm_rt[t] + 0.012 repeat + -0.0 switch + -0.089 wm_rt[t-1] + 0.149 wm_rt[t-2] + 0.053 wm_rt[t-3] + 0.119 wm_rt[t-4] + 0.075 wm_rt^2 + -0.041 wm_rt*repeat + 0.024 wm_rt*switch + 0.019 wm_rt*wm_rt[t-1] + -0.026 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.121 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + -0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.056 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.088 switch*wm_rt[t-1] + -0.026 switch*wm_rt[t-2] + -0.085 switch*wm_rt[t-3] + 0.21 switch*wm_rt[t-4] + -0.043 wm_rt[t-1]^2 + -0.035 wm_rt[t-1]*wm_rt[t-2] + -0.037 wm_rt[t-1]*wm_rt[t-3] + -0.045 wm_rt[t-1]*wm_rt[t-4] + -0.09 wm_rt[t-2]^2 + -0.087 wm_rt[t-2]*wm_rt[t-3] + -0.083 wm_rt[t-2]*wm_rt[t-4] + -0.015 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 40, 0, 56, 56, 0, 0, 0, 0, 0, 56, 56, 56, 55, 56, 0, 56, 56, 0, 56, 0, 0, 56, 0, 19, 0, 0, 13, 32, 31, 11, 0, 0, 0, 56, 56, 37\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 156/400 --- L(Train): 0.4116341 --- L(Val, RNN): 0.3875466 --- L(Val, SINDy): 0.6911464 --- Time: 3.57s; --- Convergence: 8.00e-04; LR: 1.00e-02; Metric: 0.3866762; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.53 value_reward_diff[t] + 1.313 reward_diff + -0.603 value_reward_diff^2 + 0.122 value_reward_diff*reward_diff + 1.308 reward_diff^2 \n",
      "value_persistance[t+1] = -0.372 1 + 0.786 value_persistance[t] + -0.087 repeat + 0.329 value_persistance^2 + -0.174 value_persistance*repeat + -0.088 repeat^2 \n",
      "wm_rt[t+1] = -0.037 1 + 1.053 wm_rt[t] + 0.012 repeat + 0.0 switch + -0.087 wm_rt[t-1] + 0.151 wm_rt[t-2] + 0.054 wm_rt[t-3] + 0.118 wm_rt[t-4] + 0.075 wm_rt^2 + -0.042 wm_rt*repeat + 0.024 wm_rt*switch + 0.019 wm_rt*wm_rt[t-1] + -0.026 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.122 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + 0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.057 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.087 switch*wm_rt[t-1] + -0.025 switch*wm_rt[t-2] + -0.084 switch*wm_rt[t-3] + 0.209 switch*wm_rt[t-4] + -0.042 wm_rt[t-1]^2 + -0.034 wm_rt[t-1]*wm_rt[t-2] + -0.036 wm_rt[t-1]*wm_rt[t-3] + -0.044 wm_rt[t-1]*wm_rt[t-4] + -0.088 wm_rt[t-2]^2 + -0.086 wm_rt[t-2]*wm_rt[t-3] + -0.082 wm_rt[t-2]*wm_rt[t-4] + -0.014 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 41, 0, 57, 57, 0, 0, 0, 0, 0, 57, 57, 57, 56, 57, 0, 57, 57, 0, 57, 0, 0, 57, 0, 20, 0, 0, 14, 33, 32, 12, 0, 0, 0, 57, 57, 38\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 157/400 --- L(Train): 0.4099859 --- L(Val, RNN): 0.3856432 --- L(Val, SINDy): 0.7188560 --- Time: 3.39s; --- Convergence: 1.35e-03; LR: 1.00e-02; Metric: 0.3856432; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.052 1 + 0.533 value_reward_diff[t] + 1.321 reward_diff + -0.6 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 1.317 reward_diff^2 \n",
      "value_persistance[t+1] = -0.372 1 + 0.785 value_persistance[t] + -0.086 repeat + 0.33 value_persistance^2 + -0.175 value_persistance*repeat + -0.087 repeat^2 \n",
      "wm_rt[t+1] = -0.036 1 + 1.053 wm_rt[t] + 0.012 repeat + 0.0 switch + -0.086 wm_rt[t-1] + 0.152 wm_rt[t-2] + 0.054 wm_rt[t-3] + 0.117 wm_rt[t-4] + 0.075 wm_rt^2 + -0.042 wm_rt*repeat + 0.024 wm_rt*switch + 0.02 wm_rt*wm_rt[t-1] + -0.027 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.122 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + 0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.057 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.086 switch*wm_rt[t-1] + -0.023 switch*wm_rt[t-2] + -0.083 switch*wm_rt[t-3] + 0.207 switch*wm_rt[t-4] + -0.041 wm_rt[t-1]^2 + -0.033 wm_rt[t-1]*wm_rt[t-2] + -0.035 wm_rt[t-1]*wm_rt[t-3] + -0.043 wm_rt[t-1]*wm_rt[t-4] + -0.087 wm_rt[t-2]^2 + -0.085 wm_rt[t-2]*wm_rt[t-3] + -0.081 wm_rt[t-2]*wm_rt[t-4] + -0.014 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.004 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 42, 0, 58, 58, 0, 0, 0, 0, 0, 58, 58, 58, 57, 58, 0, 58, 58, 0, 58, 0, 0, 58, 0, 21, 0, 0, 15, 34, 33, 13, 0, 0, 0, 58, 58, 39\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 158/400 --- L(Train): 0.4131535 --- L(Val, RNN): 0.3835861 --- L(Val, SINDy): 0.7401534 --- Time: 3.32s; --- Convergence: 1.70e-03; LR: 1.00e-02; Metric: 0.3835861; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.052 1 + 0.535 value_reward_diff[t] + 1.33 reward_diff + -0.599 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 1.325 reward_diff^2 \n",
      "value_persistance[t+1] = -0.372 1 + 0.784 value_persistance[t] + -0.085 repeat + 0.331 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.036 1 + 1.053 wm_rt[t] + 0.012 repeat + 0.0 switch + -0.085 wm_rt[t-1] + 0.153 wm_rt[t-2] + 0.054 wm_rt[t-3] + 0.116 wm_rt[t-4] + 0.075 wm_rt^2 + -0.042 wm_rt*repeat + 0.024 wm_rt*switch + 0.021 wm_rt*wm_rt[t-1] + -0.027 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.123 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + 0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.057 repeat*wm_rt[t-3] + 0.087 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.085 switch*wm_rt[t-1] + -0.022 switch*wm_rt[t-2] + -0.083 switch*wm_rt[t-3] + 0.206 switch*wm_rt[t-4] + -0.04 wm_rt[t-1]^2 + -0.032 wm_rt[t-1]*wm_rt[t-2] + -0.034 wm_rt[t-1]*wm_rt[t-3] + -0.042 wm_rt[t-1]*wm_rt[t-4] + -0.087 wm_rt[t-2]^2 + -0.084 wm_rt[t-2]*wm_rt[t-3] + -0.08 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.005 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 43, 0, 59, 59, 0, 0, 0, 0, 0, 59, 59, 59, 58, 59, 0, 59, 59, 0, 59, 0, 0, 59, 0, 22, 0, 0, 16, 35, 34, 14, 0, 0, 0, 59, 59, 40\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 159/400 --- L(Train): 0.4084893 --- L(Val, RNN): 0.3843348 --- L(Val, SINDy): 0.6847957 --- Time: 3.26s; --- Convergence: 1.23e-03; LR: 1.00e-02; Metric: 0.3835861; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.534 value_reward_diff[t] + 1.338 reward_diff + -0.6 value_reward_diff^2 + 0.12 value_reward_diff*reward_diff + 1.333 reward_diff^2 \n",
      "value_persistance[t+1] = -0.372 1 + 0.783 value_persistance[t] + -0.084 repeat + 0.332 value_persistance^2 + -0.177 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.035 1 + 1.053 wm_rt[t] + 0.012 repeat + 0.001 switch + -0.085 wm_rt[t-1] + 0.153 wm_rt[t-2] + 0.054 wm_rt[t-3] + 0.114 wm_rt[t-4] + 0.075 wm_rt^2 + -0.043 wm_rt*repeat + 0.025 wm_rt*switch + 0.022 wm_rt*wm_rt[t-1] + -0.027 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.123 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + 0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.058 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.084 switch*wm_rt[t-1] + -0.022 switch*wm_rt[t-2] + -0.083 switch*wm_rt[t-3] + 0.204 switch*wm_rt[t-4] + -0.039 wm_rt[t-1]^2 + -0.031 wm_rt[t-1]*wm_rt[t-2] + -0.033 wm_rt[t-1]*wm_rt[t-3] + -0.041 wm_rt[t-1]*wm_rt[t-4] + -0.087 wm_rt[t-2]^2 + -0.084 wm_rt[t-2]*wm_rt[t-3] + -0.08 wm_rt[t-2]*wm_rt[t-4] + -0.014 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.007 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 44, 0, 60, 60, 0, 0, 0, 0, 0, 60, 60, 60, 59, 60, 0, 60, 60, 0, 60, 0, 0, 60, 0, 23, 0, 0, 17, 36, 35, 15, 0, 0, 0, 60, 60, 41\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 160/400 --- L(Train): 0.4100769 --- L(Val, RNN): 0.3835670 --- L(Val, SINDy): 0.6739277 --- Time: 3.30s; --- Convergence: 9.97e-04; LR: 1.00e-02; Metric: 0.3835670; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.534 value_reward_diff[t] + 1.345 reward_diff + -0.6 value_reward_diff^2 + 0.118 value_reward_diff*reward_diff + 1.341 reward_diff^2 \n",
      "value_persistance[t+1] = -0.372 1 + 0.782 value_persistance[t] + -0.083 repeat + 0.333 value_persistance^2 + -0.177 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.035 1 + 1.054 wm_rt[t] + 0.011 repeat + 0.001 switch + -0.084 wm_rt[t-1] + 0.153 wm_rt[t-2] + 0.054 wm_rt[t-3] + 0.113 wm_rt[t-4] + 0.075 wm_rt^2 + -0.044 wm_rt*repeat + 0.026 wm_rt*switch + 0.023 wm_rt*wm_rt[t-1] + -0.026 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.123 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + -0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + -0.005 repeat*wm_rt[t-2] + -0.059 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.084 switch*wm_rt[t-1] + -0.021 switch*wm_rt[t-2] + -0.083 switch*wm_rt[t-3] + 0.202 switch*wm_rt[t-4] + -0.039 wm_rt[t-1]^2 + -0.031 wm_rt[t-1]*wm_rt[t-2] + -0.033 wm_rt[t-1]*wm_rt[t-3] + -0.041 wm_rt[t-1]*wm_rt[t-4] + -0.086 wm_rt[t-2]^2 + -0.084 wm_rt[t-2]*wm_rt[t-3] + -0.08 wm_rt[t-2]*wm_rt[t-4] + -0.014 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.009 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 45, 0, 61, 61, 0, 0, 0, 0, 0, 61, 61, 61, 60, 61, 0, 61, 61, 0, 61, 0, 0, 61, 0, 24, 0, 0, 18, 37, 36, 16, 0, 0, 0, 61, 61, 42\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 161/400 --- L(Train): 0.4146639 --- L(Val, RNN): 0.3843598 --- L(Val, SINDy): 0.6221220 --- Time: 3.86s; --- Convergence: 8.95e-04; LR: 1.00e-02; Metric: 0.3835670; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.535 value_reward_diff[t] + 1.353 reward_diff + -0.599 value_reward_diff^2 + 0.118 value_reward_diff*reward_diff + 1.348 reward_diff^2 \n",
      "value_persistance[t+1] = -0.372 1 + 0.782 value_persistance[t] + -0.083 repeat + 0.333 value_persistance^2 + -0.177 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.034 1 + 1.054 wm_rt[t] + 0.011 repeat + 0.001 switch + -0.084 wm_rt[t-1] + 0.154 wm_rt[t-2] + 0.053 wm_rt[t-3] + 0.111 wm_rt[t-4] + 0.075 wm_rt^2 + -0.044 wm_rt*repeat + 0.026 wm_rt*switch + 0.023 wm_rt*wm_rt[t-1] + -0.026 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.123 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + -0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + -0.005 repeat*wm_rt[t-2] + -0.06 repeat*wm_rt[t-3] + 0.085 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.084 switch*wm_rt[t-1] + -0.021 switch*wm_rt[t-2] + -0.083 switch*wm_rt[t-3] + 0.2 switch*wm_rt[t-4] + -0.038 wm_rt[t-1]^2 + -0.03 wm_rt[t-1]*wm_rt[t-2] + -0.033 wm_rt[t-1]*wm_rt[t-3] + -0.041 wm_rt[t-1]*wm_rt[t-4] + -0.086 wm_rt[t-2]^2 + -0.084 wm_rt[t-2]*wm_rt[t-3] + -0.079 wm_rt[t-2]*wm_rt[t-4] + -0.014 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.01 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 46, 0, 62, 62, 0, 0, 0, 0, 0, 62, 62, 62, 61, 62, 0, 62, 62, 0, 62, 0, 0, 62, 0, 25, 0, 0, 19, 38, 37, 17, 0, 0, 0, 62, 62, 43\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 162/400 --- L(Train): 0.4142925 --- L(Val, RNN): 0.3824062 --- L(Val, SINDy): 0.6579200 --- Time: 3.58s; --- Convergence: 1.42e-03; LR: 1.00e-02; Metric: 0.3824062; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.536 value_reward_diff[t] + 1.361 reward_diff + -0.598 value_reward_diff^2 + 0.116 value_reward_diff*reward_diff + 1.356 reward_diff^2 \n",
      "value_persistance[t+1] = -0.372 1 + 0.781 value_persistance[t] + -0.083 repeat + 0.334 value_persistance^2 + -0.177 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.034 1 + 1.053 wm_rt[t] + 0.011 repeat + 0.001 switch + -0.083 wm_rt[t-1] + 0.154 wm_rt[t-2] + 0.053 wm_rt[t-3] + 0.11 wm_rt[t-4] + 0.074 wm_rt^2 + -0.045 wm_rt*repeat + 0.026 wm_rt*switch + 0.023 wm_rt*wm_rt[t-1] + -0.026 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.123 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + -0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.061 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.083 switch*wm_rt[t-1] + -0.02 switch*wm_rt[t-2] + -0.083 switch*wm_rt[t-3] + 0.198 switch*wm_rt[t-4] + -0.037 wm_rt[t-1]^2 + -0.029 wm_rt[t-1]*wm_rt[t-2] + -0.032 wm_rt[t-1]*wm_rt[t-3] + -0.04 wm_rt[t-1]*wm_rt[t-4] + -0.086 wm_rt[t-2]^2 + -0.083 wm_rt[t-2]*wm_rt[t-3] + -0.079 wm_rt[t-2]*wm_rt[t-4] + -0.014 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.012 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 47, 0, 63, 63, 0, 0, 0, 0, 0, 63, 63, 63, 62, 63, 0, 63, 63, 0, 63, 0, 0, 63, 0, 26, 0, 0, 20, 39, 38, 18, 0, 0, 0, 63, 63, 44\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 163/400 --- L(Train): 0.4170209 --- L(Val, RNN): 0.3823252 --- L(Val, SINDy): 0.6991714 --- Time: 4.56s; --- Convergence: 7.53e-04; LR: 1.00e-02; Metric: 0.3823252; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.535 value_reward_diff[t] + 1.369 reward_diff + -0.598 value_reward_diff^2 + 0.113 value_reward_diff*reward_diff + 1.364 reward_diff^2 \n",
      "value_persistance[t+1] = -0.373 1 + 0.781 value_persistance[t] + -0.084 repeat + 0.334 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.034 1 + 1.053 wm_rt[t] + 0.011 repeat + 0.0 switch + -0.081 wm_rt[t-1] + 0.155 wm_rt[t-2] + 0.054 wm_rt[t-3] + 0.109 wm_rt[t-4] + 0.074 wm_rt^2 + -0.045 wm_rt*repeat + 0.025 wm_rt*switch + 0.024 wm_rt*wm_rt[t-1] + -0.026 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.124 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + 0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.061 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.082 switch*wm_rt[t-1] + -0.019 switch*wm_rt[t-2] + -0.082 switch*wm_rt[t-3] + 0.196 switch*wm_rt[t-4] + -0.036 wm_rt[t-1]^2 + -0.028 wm_rt[t-1]*wm_rt[t-2] + -0.03 wm_rt[t-1]*wm_rt[t-3] + -0.038 wm_rt[t-1]*wm_rt[t-4] + -0.085 wm_rt[t-2]^2 + -0.082 wm_rt[t-2]*wm_rt[t-3] + -0.078 wm_rt[t-2]*wm_rt[t-4] + -0.014 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.013 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 48, 0, 64, 64, 0, 0, 0, 0, 0, 64, 64, 64, 63, 64, 0, 64, 64, 0, 64, 0, 0, 64, 0, 27, 0, 0, 21, 40, 39, 19, 0, 0, 0, 64, 64, 45\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 164/400 --- L(Train): 0.4116940 --- L(Val, RNN): 0.3811681 --- L(Val, SINDy): 0.6833735 --- Time: 4.04s; --- Convergence: 9.55e-04; LR: 1.00e-02; Metric: 0.3811681; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.537 value_reward_diff[t] + 1.377 reward_diff + -0.597 value_reward_diff^2 + 0.112 value_reward_diff*reward_diff + 1.372 reward_diff^2 \n",
      "value_persistance[t+1] = -0.374 1 + 0.781 value_persistance[t] + -0.085 repeat + 0.334 value_persistance^2 + -0.175 value_persistance*repeat + -0.087 repeat^2 \n",
      "wm_rt[t+1] = -0.034 1 + 1.053 wm_rt[t] + 0.011 repeat + 0.001 switch + -0.08 wm_rt[t-1] + 0.157 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.108 wm_rt[t-4] + 0.074 wm_rt^2 + -0.045 wm_rt*repeat + 0.025 wm_rt*switch + 0.024 wm_rt*wm_rt[t-1] + -0.026 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.124 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + 0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.061 repeat*wm_rt[t-3] + 0.087 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.08 switch*wm_rt[t-1] + -0.018 switch*wm_rt[t-2] + -0.082 switch*wm_rt[t-3] + 0.195 switch*wm_rt[t-4] + -0.034 wm_rt[t-1]^2 + -0.026 wm_rt[t-1]*wm_rt[t-2] + -0.029 wm_rt[t-1]*wm_rt[t-3] + -0.037 wm_rt[t-1]*wm_rt[t-4] + -0.084 wm_rt[t-2]^2 + -0.081 wm_rt[t-2]*wm_rt[t-3] + -0.077 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.014 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 49, 0, 65, 65, 0, 0, 0, 0, 0, 65, 65, 65, 64, 65, 0, 65, 65, 0, 65, 0, 0, 65, 0, 28, 0, 0, 22, 41, 40, 20, 0, 0, 0, 65, 65, 46\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 165/400 --- L(Train): 0.4062471 --- L(Val, RNN): 0.3798455 --- L(Val, SINDy): 0.6963108 --- Time: 3.90s; --- Convergence: 1.14e-03; LR: 1.00e-02; Metric: 0.3798455; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.539 value_reward_diff[t] + 1.386 reward_diff + -0.595 value_reward_diff^2 + 0.111 value_reward_diff*reward_diff + 1.381 reward_diff^2 \n",
      "value_persistance[t+1] = -0.375 1 + 0.782 value_persistance[t] + -0.087 repeat + 0.333 value_persistance^2 + -0.174 value_persistance*repeat + -0.088 repeat^2 \n",
      "wm_rt[t+1] = -0.033 1 + 1.054 wm_rt[t] + 0.011 repeat + 0.001 switch + -0.078 wm_rt[t-1] + 0.158 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.107 wm_rt[t-4] + 0.074 wm_rt^2 + -0.046 wm_rt*repeat + 0.025 wm_rt*switch + 0.025 wm_rt*wm_rt[t-1] + -0.026 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.125 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + -0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.061 repeat*wm_rt[t-3] + 0.088 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.079 switch*wm_rt[t-1] + -0.017 switch*wm_rt[t-2] + -0.081 switch*wm_rt[t-3] + 0.193 switch*wm_rt[t-4] + -0.033 wm_rt[t-1]^2 + -0.025 wm_rt[t-1]*wm_rt[t-2] + -0.027 wm_rt[t-1]*wm_rt[t-3] + -0.035 wm_rt[t-1]*wm_rt[t-4] + -0.083 wm_rt[t-2]^2 + -0.08 wm_rt[t-2]*wm_rt[t-3] + -0.076 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.015 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 50, 0, 66, 66, 0, 0, 0, 0, 0, 66, 66, 66, 65, 66, 0, 66, 66, 0, 66, 0, 0, 66, 0, 29, 0, 0, 23, 42, 41, 21, 0, 0, 0, 66, 66, 47\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 166/400 --- L(Train): 0.4016048 --- L(Val, RNN): 0.3798852 --- L(Val, SINDy): 0.6660587 --- Time: 3.87s; --- Convergence: 5.89e-04; LR: 1.00e-02; Metric: 0.3798455; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.541 value_reward_diff[t] + 1.395 reward_diff + -0.593 value_reward_diff^2 + 0.11 value_reward_diff*reward_diff + 1.39 reward_diff^2 \n",
      "value_persistance[t+1] = -0.376 1 + 0.782 value_persistance[t] + -0.087 repeat + 0.333 value_persistance^2 + -0.173 value_persistance*repeat + -0.089 repeat^2 \n",
      "wm_rt[t+1] = -0.032 1 + 1.054 wm_rt[t] + 0.011 repeat + 0.0 switch + -0.077 wm_rt[t-1] + 0.159 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.106 wm_rt[t-4] + 0.074 wm_rt^2 + -0.046 wm_rt*repeat + 0.025 wm_rt*switch + 0.026 wm_rt*wm_rt[t-1] + -0.026 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.125 wm_rt*wm_rt[t-4] + 0.009 repeat^2 + -0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.005 repeat*wm_rt[t-2] + -0.062 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.078 switch*wm_rt[t-1] + -0.016 switch*wm_rt[t-2] + -0.081 switch*wm_rt[t-3] + 0.191 switch*wm_rt[t-4] + -0.032 wm_rt[t-1]^2 + -0.024 wm_rt[t-1]*wm_rt[t-2] + -0.026 wm_rt[t-1]*wm_rt[t-3] + -0.034 wm_rt[t-1]*wm_rt[t-4] + -0.082 wm_rt[t-2]^2 + -0.079 wm_rt[t-2]*wm_rt[t-3] + -0.075 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.016 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 51, 0, 67, 67, 0, 0, 0, 0, 0, 67, 67, 67, 66, 67, 0, 67, 67, 0, 67, 0, 0, 67, 0, 30, 0, 0, 24, 43, 42, 22, 0, 0, 0, 67, 67, 48\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 167/400 --- L(Train): 0.4121807 --- L(Val, RNN): 0.3763934 --- L(Val, SINDy): 0.6721455 --- Time: 3.52s; --- Convergence: 2.04e-03; LR: 1.00e-02; Metric: 0.3763934; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.542 value_reward_diff[t] + 1.403 reward_diff + -0.592 value_reward_diff^2 + 0.108 value_reward_diff*reward_diff + 1.398 reward_diff^2 \n",
      "value_persistance[t+1] = -0.376 1 + 0.781 value_persistance[t] + -0.087 repeat + 0.334 value_persistance^2 + -0.173 value_persistance*repeat + -0.089 repeat^2 \n",
      "wm_rt[t+1] = -0.032 1 + 1.054 wm_rt[t] + 0.011 repeat + -0.001 switch + -0.076 wm_rt[t-1] + 0.159 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.104 wm_rt[t-4] + 0.074 wm_rt^2 + -0.046 wm_rt*repeat + 0.025 wm_rt*switch + 0.026 wm_rt*wm_rt[t-1] + -0.026 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.126 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + 0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.062 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.077 switch*wm_rt[t-1] + -0.015 switch*wm_rt[t-2] + -0.081 switch*wm_rt[t-3] + 0.189 switch*wm_rt[t-4] + -0.031 wm_rt[t-1]^2 + -0.023 wm_rt[t-1]*wm_rt[t-2] + -0.025 wm_rt[t-1]*wm_rt[t-3] + -0.033 wm_rt[t-1]*wm_rt[t-4] + -0.082 wm_rt[t-2]^2 + -0.079 wm_rt[t-2]*wm_rt[t-3] + -0.075 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.018 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 52, 0, 68, 68, 0, 0, 0, 0, 0, 68, 68, 68, 67, 68, 0, 68, 68, 0, 68, 0, 0, 68, 0, 31, 0, 0, 25, 44, 43, 23, 0, 0, 0, 68, 68, 49\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 168/400 --- L(Train): 0.4010425 --- L(Val, RNN): 0.3788925 --- L(Val, SINDy): 0.6300461 --- Time: 3.89s; --- Convergence: 2.27e-03; LR: 1.00e-02; Metric: 0.3763934; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.545 value_reward_diff[t] + 1.412 reward_diff + -0.589 value_reward_diff^2 + 0.108 value_reward_diff*reward_diff + 1.407 reward_diff^2 \n",
      "value_persistance[t+1] = -0.376 1 + 0.78 value_persistance[t] + -0.086 repeat + 0.335 value_persistance^2 + -0.174 value_persistance*repeat + -0.088 repeat^2 \n",
      "wm_rt[t+1] = -0.033 1 + 1.054 wm_rt[t] + 0.011 repeat + -0.001 switch + -0.075 wm_rt[t-1] + 0.16 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.103 wm_rt[t-4] + 0.073 wm_rt^2 + -0.047 wm_rt*repeat + 0.025 wm_rt*switch + 0.026 wm_rt*wm_rt[t-1] + -0.026 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.126 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + 0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.063 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.077 switch*wm_rt[t-1] + -0.015 switch*wm_rt[t-2] + -0.081 switch*wm_rt[t-3] + 0.187 switch*wm_rt[t-4] + -0.03 wm_rt[t-1]^2 + -0.022 wm_rt[t-1]*wm_rt[t-2] + -0.024 wm_rt[t-1]*wm_rt[t-3] + -0.033 wm_rt[t-1]*wm_rt[t-4] + -0.081 wm_rt[t-2]^2 + -0.078 wm_rt[t-2]*wm_rt[t-3] + -0.074 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.02 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 53, 0, 69, 69, 0, 0, 0, 0, 0, 69, 69, 69, 68, 69, 0, 69, 69, 0, 69, 0, 0, 69, 0, 32, 0, 0, 26, 45, 44, 24, 0, 0, 0, 69, 69, 50\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 169/400 --- L(Train): 0.4017568 --- L(Val, RNN): 0.3772579 --- L(Val, SINDy): 0.6537501 --- Time: 3.77s; --- Convergence: 1.95e-03; LR: 1.00e-02; Metric: 0.3763934; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.547 value_reward_diff[t] + 1.422 reward_diff + -0.586 value_reward_diff^2 + 0.107 value_reward_diff*reward_diff + 1.416 reward_diff^2 \n",
      "value_persistance[t+1] = -0.376 1 + 0.779 value_persistance[t] + -0.086 repeat + 0.336 value_persistance^2 + -0.175 value_persistance*repeat + -0.087 repeat^2 \n",
      "wm_rt[t+1] = -0.032 1 + 1.054 wm_rt[t] + 0.01 repeat + -0.0 switch + -0.074 wm_rt[t-1] + 0.16 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.101 wm_rt[t-4] + 0.073 wm_rt^2 + -0.047 wm_rt*repeat + 0.025 wm_rt*switch + 0.027 wm_rt*wm_rt[t-1] + -0.026 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.127 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + -0.0 repeat*switch + -0.264 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.064 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.076 switch*wm_rt[t-1] + -0.014 switch*wm_rt[t-2] + -0.081 switch*wm_rt[t-3] + 0.184 switch*wm_rt[t-4] + -0.029 wm_rt[t-1]^2 + -0.021 wm_rt[t-1]*wm_rt[t-2] + -0.024 wm_rt[t-1]*wm_rt[t-3] + -0.032 wm_rt[t-1]*wm_rt[t-4] + -0.081 wm_rt[t-2]^2 + -0.078 wm_rt[t-2]*wm_rt[t-3] + -0.074 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.022 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 54, 0, 70, 70, 0, 0, 0, 0, 0, 70, 70, 70, 69, 70, 0, 70, 70, 0, 70, 0, 0, 70, 0, 33, 0, 0, 27, 46, 45, 25, 0, 0, 0, 70, 70, 51\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 170/400 --- L(Train): 0.4066787 --- L(Val, RNN): 0.3776871 --- L(Val, SINDy): 0.6679533 --- Time: 3.72s; --- Convergence: 1.19e-03; LR: 1.00e-02; Metric: 0.3763934; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.549 value_reward_diff[t] + 1.43 reward_diff + -0.584 value_reward_diff^2 + 0.106 value_reward_diff*reward_diff + 1.425 reward_diff^2 \n",
      "value_persistance[t+1] = -0.377 1 + 0.779 value_persistance[t] + -0.085 repeat + 0.336 value_persistance^2 + -0.175 value_persistance*repeat + -0.087 repeat^2 \n",
      "wm_rt[t+1] = -0.031 1 + 1.054 wm_rt[t] + 0.01 repeat + -0.0 switch + -0.074 wm_rt[t-1] + 0.161 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.099 wm_rt[t-4] + 0.074 wm_rt^2 + -0.048 wm_rt*repeat + 0.026 wm_rt*switch + 0.029 wm_rt*wm_rt[t-1] + -0.025 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.127 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + 0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.065 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.075 switch*wm_rt[t-1] + -0.014 switch*wm_rt[t-2] + -0.081 switch*wm_rt[t-3] + 0.182 switch*wm_rt[t-4] + -0.029 wm_rt[t-1]^2 + -0.021 wm_rt[t-1]*wm_rt[t-2] + -0.023 wm_rt[t-1]*wm_rt[t-3] + -0.032 wm_rt[t-1]*wm_rt[t-4] + -0.081 wm_rt[t-2]^2 + -0.078 wm_rt[t-2]*wm_rt[t-3] + -0.074 wm_rt[t-2]*wm_rt[t-4] + -0.014 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.023 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 55, 0, 71, 71, 0, 0, 0, 0, 0, 71, 71, 71, 70, 71, 0, 71, 71, 0, 71, 0, 0, 71, 0, 34, 0, 0, 28, 47, 46, 26, 0, 0, 0, 71, 71, 52\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 171/400 --- L(Train): 0.4123520 --- L(Val, RNN): 0.3759320 --- L(Val, SINDy): 0.6896464 --- Time: 3.59s; --- Convergence: 1.47e-03; LR: 1.00e-02; Metric: 0.3759320; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.052 1 + 0.551 value_reward_diff[t] + 1.439 reward_diff + -0.581 value_reward_diff^2 + 0.105 value_reward_diff*reward_diff + 1.433 reward_diff^2 \n",
      "value_persistance[t+1] = -0.377 1 + 0.778 value_persistance[t] + -0.085 repeat + 0.337 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.03 1 + 1.055 wm_rt[t] + 0.01 repeat + 0.0 switch + -0.073 wm_rt[t-1] + 0.161 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.098 wm_rt[t-4] + 0.074 wm_rt^2 + -0.048 wm_rt*repeat + 0.026 wm_rt*switch + 0.03 wm_rt*wm_rt[t-1] + -0.025 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.128 wm_rt*wm_rt[t-4] + 0.008 repeat^2 + -0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.065 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.075 switch*wm_rt[t-1] + -0.013 switch*wm_rt[t-2] + -0.081 switch*wm_rt[t-3] + 0.18 switch*wm_rt[t-4] + -0.028 wm_rt[t-1]^2 + -0.02 wm_rt[t-1]*wm_rt[t-2] + -0.022 wm_rt[t-1]*wm_rt[t-3] + -0.031 wm_rt[t-1]*wm_rt[t-4] + -0.08 wm_rt[t-2]^2 + -0.077 wm_rt[t-2]*wm_rt[t-3] + -0.074 wm_rt[t-2]*wm_rt[t-4] + -0.014 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.025 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 56, 0, 72, 72, 0, 0, 0, 0, 0, 72, 72, 72, 71, 72, 0, 72, 72, 0, 72, 0, 0, 72, 0, 35, 0, 0, 29, 48, 47, 27, 0, 0, 0, 72, 72, 53\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 172/400 --- L(Train): 0.4039886 --- L(Val, RNN): 0.3767019 --- L(Val, SINDy): 0.6278457 --- Time: 3.87s; --- Convergence: 1.12e-03; LR: 1.00e-02; Metric: 0.3759320; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.551 value_reward_diff[t] + 1.446 reward_diff + -0.582 value_reward_diff^2 + 0.102 value_reward_diff*reward_diff + 1.441 reward_diff^2 \n",
      "value_persistance[t+1] = -0.378 1 + 0.778 value_persistance[t] + -0.084 repeat + 0.337 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.029 1 + 1.055 wm_rt[t] + 0.01 repeat + 0.001 switch + -0.072 wm_rt[t-1] + 0.162 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.097 wm_rt[t-4] + 0.074 wm_rt^2 + -0.048 wm_rt*repeat + 0.026 wm_rt*switch + 0.031 wm_rt*wm_rt[t-1] + -0.025 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.128 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.074 switch*wm_rt[t-1] + -0.013 switch*wm_rt[t-2] + -0.08 switch*wm_rt[t-3] + 0.179 switch*wm_rt[t-4] + -0.027 wm_rt[t-1]^2 + -0.019 wm_rt[t-1]*wm_rt[t-2] + -0.021 wm_rt[t-1]*wm_rt[t-3] + -0.03 wm_rt[t-1]*wm_rt[t-4] + -0.08 wm_rt[t-2]^2 + -0.077 wm_rt[t-2]*wm_rt[t-3] + -0.073 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.026 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 57, 0, 73, 73, 0, 0, 0, 0, 0, 73, 73, 73, 72, 73, 0, 73, 73, 0, 73, 0, 0, 73, 0, 36, 0, 0, 30, 49, 48, 28, 0, 0, 0, 73, 73, 54\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 173/400 --- L(Train): 0.3983693 --- L(Val, RNN): 0.3741731 --- L(Val, SINDy): 0.6337723 --- Time: 3.86s; --- Convergence: 1.83e-03; LR: 1.00e-02; Metric: 0.3741731; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.55 value_reward_diff[t] + 1.454 reward_diff + -0.583 value_reward_diff^2 + 0.099 value_reward_diff*reward_diff + 1.448 reward_diff^2 \n",
      "value_persistance[t+1] = -0.379 1 + 0.778 value_persistance[t] + -0.084 repeat + 0.337 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.029 1 + 1.055 wm_rt[t] + 0.01 repeat + 0.001 switch + -0.071 wm_rt[t-1] + 0.163 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.096 wm_rt[t-4] + 0.074 wm_rt^2 + -0.049 wm_rt*repeat + 0.026 wm_rt*switch + 0.032 wm_rt*wm_rt[t-1] + -0.025 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.129 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + 0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.073 switch*wm_rt[t-1] + -0.012 switch*wm_rt[t-2] + -0.08 switch*wm_rt[t-3] + 0.178 switch*wm_rt[t-4] + -0.026 wm_rt[t-1]^2 + -0.018 wm_rt[t-1]*wm_rt[t-2] + -0.02 wm_rt[t-1]*wm_rt[t-3] + -0.029 wm_rt[t-1]*wm_rt[t-4] + -0.079 wm_rt[t-2]^2 + -0.076 wm_rt[t-2]*wm_rt[t-3] + -0.072 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.027 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 58, 0, 74, 74, 0, 0, 0, 0, 0, 74, 74, 74, 73, 74, 0, 74, 74, 0, 74, 0, 0, 74, 0, 37, 0, 0, 31, 50, 49, 29, 0, 0, 0, 74, 74, 55\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 174/400 --- L(Train): 0.3989825 --- L(Val, RNN): 0.3752820 --- L(Val, SINDy): 0.6750422 --- Time: 3.62s; --- Convergence: 1.47e-03; LR: 1.00e-02; Metric: 0.3741731; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.552 value_reward_diff[t] + 1.462 reward_diff + -0.58 value_reward_diff^2 + 0.098 value_reward_diff*reward_diff + 1.456 reward_diff^2 \n",
      "value_persistance[t+1] = -0.38 1 + 0.779 value_persistance[t] + -0.085 repeat + 0.337 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.029 1 + 1.055 wm_rt[t] + 0.01 repeat + 0.0 switch + -0.07 wm_rt[t-1] + 0.163 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.095 wm_rt[t-4] + 0.073 wm_rt^2 + -0.049 wm_rt*repeat + 0.026 wm_rt*switch + 0.032 wm_rt*wm_rt[t-1] + -0.025 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.13 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + 0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.072 switch*wm_rt[t-1] + -0.012 switch*wm_rt[t-2] + -0.08 switch*wm_rt[t-3] + 0.176 switch*wm_rt[t-4] + -0.025 wm_rt[t-1]^2 + -0.017 wm_rt[t-1]*wm_rt[t-2] + -0.02 wm_rt[t-1]*wm_rt[t-3] + -0.028 wm_rt[t-1]*wm_rt[t-4] + -0.079 wm_rt[t-2]^2 + -0.076 wm_rt[t-2]*wm_rt[t-3] + -0.072 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.028 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 59, 0, 75, 75, 0, 0, 0, 0, 0, 75, 75, 75, 74, 75, 0, 75, 75, 0, 75, 0, 0, 75, 0, 38, 0, 0, 32, 51, 50, 30, 0, 0, 0, 75, 75, 56\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 175/400 --- L(Train): 0.4017019 --- L(Val, RNN): 0.3740681 --- L(Val, SINDy): 0.7289498 --- Time: 4.24s; --- Convergence: 1.34e-03; LR: 1.00e-02; Metric: 0.3740681; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.553 value_reward_diff[t] + 1.47 reward_diff + -0.579 value_reward_diff^2 + 0.097 value_reward_diff*reward_diff + 1.463 reward_diff^2 \n",
      "value_persistance[t+1] = -0.381 1 + 0.779 value_persistance[t] + -0.085 repeat + 0.336 value_persistance^2 + -0.175 value_persistance*repeat + -0.087 repeat^2 \n",
      "wm_rt[t+1] = -0.029 1 + 1.055 wm_rt[t] + 0.01 repeat + -0.0 switch + -0.069 wm_rt[t-1] + 0.164 wm_rt[t-2] + 0.056 wm_rt[t-3] + 0.094 wm_rt[t-4] + 0.073 wm_rt^2 + -0.049 wm_rt*repeat + 0.026 wm_rt*switch + 0.033 wm_rt*wm_rt[t-1] + -0.025 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.131 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.005 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.09 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.072 switch*wm_rt[t-1] + -0.011 switch*wm_rt[t-2] + -0.08 switch*wm_rt[t-3] + 0.174 switch*wm_rt[t-4] + -0.025 wm_rt[t-1]^2 + -0.017 wm_rt[t-1]*wm_rt[t-2] + -0.019 wm_rt[t-1]*wm_rt[t-3] + -0.027 wm_rt[t-1]*wm_rt[t-4] + -0.078 wm_rt[t-2]^2 + -0.075 wm_rt[t-2]*wm_rt[t-3] + -0.071 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.03 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 60, 0, 76, 76, 0, 0, 0, 0, 0, 76, 76, 76, 75, 76, 0, 76, 76, 0, 76, 0, 0, 76, 0, 39, 0, 0, 33, 52, 51, 31, 0, 0, 0, 76, 76, 57\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 176/400 --- L(Train): 0.3980956 --- L(Val, RNN): 0.3696165 --- L(Val, SINDy): 0.7302115 --- Time: 4.49s; --- Convergence: 2.90e-03; LR: 1.00e-02; Metric: 0.3696165; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.554 value_reward_diff[t] + 1.477 reward_diff + -0.579 value_reward_diff^2 + 0.095 value_reward_diff*reward_diff + 1.471 reward_diff^2 \n",
      "value_persistance[t+1] = -0.382 1 + 0.779 value_persistance[t] + -0.085 repeat + 0.336 value_persistance^2 + -0.175 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.029 1 + 1.055 wm_rt[t] + 0.01 repeat + -0.001 switch + -0.068 wm_rt[t-1] + 0.164 wm_rt[t-2] + 0.056 wm_rt[t-3] + 0.093 wm_rt[t-4] + 0.073 wm_rt^2 + -0.05 wm_rt*repeat + 0.026 wm_rt*switch + 0.034 wm_rt*wm_rt[t-1] + -0.025 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.131 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + 0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.005 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.09 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.071 switch*wm_rt[t-1] + -0.011 switch*wm_rt[t-2] + -0.08 switch*wm_rt[t-3] + 0.173 switch*wm_rt[t-4] + -0.024 wm_rt[t-1]^2 + -0.016 wm_rt[t-1]*wm_rt[t-2] + -0.018 wm_rt[t-1]*wm_rt[t-3] + -0.027 wm_rt[t-1]*wm_rt[t-4] + -0.078 wm_rt[t-2]^2 + -0.075 wm_rt[t-2]*wm_rt[t-3] + -0.071 wm_rt[t-2]*wm_rt[t-4] + -0.013 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.031 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 61, 0, 77, 77, 0, 0, 0, 0, 0, 77, 77, 77, 76, 77, 0, 77, 77, 0, 77, 0, 0, 77, 0, 40, 0, 0, 34, 53, 52, 32, 0, 0, 0, 77, 77, 58\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 177/400 --- L(Train): 0.4080594 --- L(Val, RNN): 0.3759738 --- L(Val, SINDy): 0.7106557 --- Time: 4.30s; --- Convergence: 4.63e-03; LR: 1.00e-02; Metric: 0.3696165; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.553 value_reward_diff[t] + 1.484 reward_diff + -0.579 value_reward_diff^2 + 0.092 value_reward_diff*reward_diff + 1.478 reward_diff^2 \n",
      "value_persistance[t+1] = -0.382 1 + 0.779 value_persistance[t] + -0.084 repeat + 0.337 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.028 1 + 1.055 wm_rt[t] + 0.01 repeat + -0.0 switch + -0.068 wm_rt[t-1] + 0.165 wm_rt[t-2] + 0.056 wm_rt[t-3] + 0.092 wm_rt[t-4] + 0.073 wm_rt^2 + -0.05 wm_rt*repeat + 0.026 wm_rt*switch + 0.035 wm_rt*wm_rt[t-1] + -0.024 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.132 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.0 repeat*switch + -0.262 repeat*wm_rt[t-1] + -0.005 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.09 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.071 switch*wm_rt[t-1] + -0.01 switch*wm_rt[t-2] + -0.08 switch*wm_rt[t-3] + 0.171 switch*wm_rt[t-4] + -0.024 wm_rt[t-1]^2 + -0.016 wm_rt[t-1]*wm_rt[t-2] + -0.018 wm_rt[t-1]*wm_rt[t-3] + -0.027 wm_rt[t-1]*wm_rt[t-4] + -0.077 wm_rt[t-2]^2 + -0.075 wm_rt[t-2]*wm_rt[t-3] + -0.071 wm_rt[t-2]*wm_rt[t-4] + -0.012 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.032 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 62, 0, 78, 78, 0, 0, 0, 0, 0, 78, 78, 78, 77, 78, 0, 78, 78, 0, 78, 0, 0, 78, 0, 41, 0, 0, 35, 54, 53, 33, 0, 0, 0, 78, 78, 59\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 178/400 --- L(Train): 0.4127233 --- L(Val, RNN): 0.3676738 --- L(Val, SINDy): 0.6645154 --- Time: 4.32s; --- Convergence: 6.46e-03; LR: 1.00e-02; Metric: 0.3676738; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.551 value_reward_diff[t] + 1.491 reward_diff + -0.581 value_reward_diff^2 + 0.088 value_reward_diff*reward_diff + 1.485 reward_diff^2 \n",
      "value_persistance[t+1] = -0.383 1 + 0.778 value_persistance[t] + -0.084 repeat + 0.337 value_persistance^2 + -0.176 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.027 1 + 1.056 wm_rt[t] + 0.01 repeat + 0.0 switch + -0.067 wm_rt[t-1] + 0.165 wm_rt[t-2] + 0.056 wm_rt[t-3] + 0.09 wm_rt[t-4] + 0.073 wm_rt^2 + -0.05 wm_rt*repeat + 0.026 wm_rt*switch + 0.036 wm_rt*wm_rt[t-1] + -0.024 wm_rt*wm_rt[t-2] + -0.012 wm_rt*wm_rt[t-3] + -0.133 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.0 repeat*switch + -0.262 repeat*wm_rt[t-1] + -0.005 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.09 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.071 switch*wm_rt[t-1] + -0.01 switch*wm_rt[t-2] + -0.079 switch*wm_rt[t-3] + 0.17 switch*wm_rt[t-4] + -0.023 wm_rt[t-1]^2 + -0.015 wm_rt[t-1]*wm_rt[t-2] + -0.018 wm_rt[t-1]*wm_rt[t-3] + -0.026 wm_rt[t-1]*wm_rt[t-4] + -0.077 wm_rt[t-2]^2 + -0.074 wm_rt[t-2]*wm_rt[t-3] + -0.071 wm_rt[t-2]*wm_rt[t-4] + -0.012 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.033 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 63, 0, 79, 79, 0, 0, 0, 0, 0, 0, 79, 79, 78, 79, 0, 79, 79, 0, 79, 0, 0, 79, 0, 42, 0, 0, 36, 55, 54, 34, 0, 0, 0, 79, 79, 60\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 179/400 --- L(Train): 0.4097760 --- L(Val, RNN): 0.3711953 --- L(Val, SINDy): 0.5815258 --- Time: 4.15s; --- Convergence: 4.99e-03; LR: 1.00e-02; Metric: 0.3676738; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.551 value_reward_diff[t] + 1.498 reward_diff + -0.58 value_reward_diff^2 + 0.087 value_reward_diff*reward_diff + 1.492 reward_diff^2 \n",
      "value_persistance[t+1] = -0.383 1 + 0.777 value_persistance[t] + -0.083 repeat + 0.338 value_persistance^2 + -0.177 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.026 1 + 1.056 wm_rt[t] + 0.01 repeat + 0.0 switch + -0.067 wm_rt[t-1] + 0.166 wm_rt[t-2] + 0.057 wm_rt[t-3] + 0.089 wm_rt[t-4] + 0.073 wm_rt^2 + -0.05 wm_rt*repeat + 0.026 wm_rt*switch + 0.037 wm_rt*wm_rt[t-1] + -0.024 wm_rt*wm_rt[t-2] + -0.013 wm_rt*wm_rt[t-3] + -0.133 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + 0.0 repeat*switch + -0.262 repeat*wm_rt[t-1] + -0.005 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.09 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.071 switch*wm_rt[t-1] + -0.01 switch*wm_rt[t-2] + -0.079 switch*wm_rt[t-3] + 0.168 switch*wm_rt[t-4] + -0.023 wm_rt[t-1]^2 + -0.015 wm_rt[t-1]*wm_rt[t-2] + -0.017 wm_rt[t-1]*wm_rt[t-3] + -0.026 wm_rt[t-1]*wm_rt[t-4] + -0.077 wm_rt[t-2]^2 + -0.074 wm_rt[t-2]*wm_rt[t-3] + -0.07 wm_rt[t-2]*wm_rt[t-4] + -0.012 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.034 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 64, 0, 80, 80, 0, 0, 0, 0, 0, 0, 80, 80, 79, 80, 0, 80, 80, 0, 80, 0, 0, 80, 0, 43, 0, 0, 37, 56, 55, 35, 0, 0, 0, 80, 80, 61\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 180/400 --- L(Train): 0.3979208 --- L(Val, RNN): 0.3730222 --- L(Val, SINDy): 0.5760191 --- Time: 4.13s; --- Convergence: 3.41e-03; LR: 1.00e-02; Metric: 0.3676738; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.55 value_reward_diff[t] + 1.505 reward_diff + -0.581 value_reward_diff^2 + 0.084 value_reward_diff*reward_diff + 1.499 reward_diff^2 \n",
      "value_persistance[t+1] = -0.382 1 + 0.776 value_persistance[t] + -0.083 repeat + 0.339 value_persistance^2 + -0.177 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.026 1 + 1.056 wm_rt[t] + 0.01 repeat + -0.0 switch + -0.066 wm_rt[t-1] + 0.166 wm_rt[t-2] + 0.057 wm_rt[t-3] + 0.089 wm_rt[t-4] + 0.073 wm_rt^2 + -0.051 wm_rt*repeat + 0.026 wm_rt*switch + 0.037 wm_rt*wm_rt[t-1] + -0.024 wm_rt*wm_rt[t-2] + -0.013 wm_rt*wm_rt[t-3] + -0.134 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + 0.0 repeat*switch + -0.262 repeat*wm_rt[t-1] + -0.005 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.09 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.07 switch*wm_rt[t-1] + -0.009 switch*wm_rt[t-2] + -0.079 switch*wm_rt[t-3] + 0.167 switch*wm_rt[t-4] + -0.022 wm_rt[t-1]^2 + -0.014 wm_rt[t-1]*wm_rt[t-2] + -0.017 wm_rt[t-1]*wm_rt[t-3] + -0.025 wm_rt[t-1]*wm_rt[t-4] + -0.077 wm_rt[t-2]^2 + -0.074 wm_rt[t-2]*wm_rt[t-3] + -0.07 wm_rt[t-2]*wm_rt[t-4] + -0.011 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.035 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 65, 0, 81, 81, 0, 0, 0, 0, 0, 0, 81, 81, 80, 81, 0, 81, 81, 0, 81, 0, 0, 81, 0, 44, 0, 0, 38, 57, 56, 36, 0, 0, 0, 81, 81, 62\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 181/400 --- L(Train): 0.4035007 --- L(Val, RNN): 0.3681647 --- L(Val, SINDy): 0.5897274 --- Time: 3.54s; --- Convergence: 4.13e-03; LR: 1.00e-02; Metric: 0.3676738; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.549 value_reward_diff[t] + 1.512 reward_diff + -0.582 value_reward_diff^2 + 0.081 value_reward_diff*reward_diff + 1.505 reward_diff^2 \n",
      "value_persistance[t+1] = -0.382 1 + 0.775 value_persistance[t] + -0.083 repeat + 0.34 value_persistance^2 + -0.177 value_persistance*repeat + -0.084 repeat^2 \n",
      "wm_rt[t+1] = -0.027 1 + 1.055 wm_rt[t] + 0.009 repeat + 0.0 switch + -0.065 wm_rt[t-1] + 0.166 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.088 wm_rt[t-4] + 0.073 wm_rt^2 + -0.052 wm_rt*repeat + 0.026 wm_rt*switch + 0.037 wm_rt*wm_rt[t-1] + -0.024 wm_rt*wm_rt[t-2] + -0.013 wm_rt*wm_rt[t-3] + -0.135 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.069 switch*wm_rt[t-1] + -0.009 switch*wm_rt[t-2] + -0.078 switch*wm_rt[t-3] + 0.166 switch*wm_rt[t-4] + -0.021 wm_rt[t-1]^2 + -0.013 wm_rt[t-1]*wm_rt[t-2] + -0.016 wm_rt[t-1]*wm_rt[t-3] + -0.025 wm_rt[t-1]*wm_rt[t-4] + -0.076 wm_rt[t-2]^2 + -0.073 wm_rt[t-2]*wm_rt[t-3] + -0.07 wm_rt[t-2]*wm_rt[t-4] + -0.011 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.036 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 66, 0, 82, 82, 0, 0, 0, 0, 0, 0, 82, 82, 81, 82, 0, 82, 82, 0, 82, 0, 0, 82, 0, 45, 0, 0, 39, 58, 57, 37, 0, 0, 0, 82, 82, 63\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 182/400 --- L(Train): 0.4069531 --- L(Val, RNN): 0.3704180 --- L(Val, SINDy): 0.5992218 --- Time: 4.28s; --- Convergence: 3.19e-03; LR: 1.00e-02; Metric: 0.3676738; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.553 value_reward_diff[t] + 1.52 reward_diff + -0.577 value_reward_diff^2 + 0.084 value_reward_diff*reward_diff + 1.513 reward_diff^2 \n",
      "value_persistance[t+1] = -0.383 1 + 0.775 value_persistance[t] + -0.083 repeat + 0.34 value_persistance^2 + -0.177 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.026 1 + 1.055 wm_rt[t] + 0.009 repeat + 0.0 switch + -0.065 wm_rt[t-1] + 0.167 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.087 wm_rt[t-4] + 0.073 wm_rt^2 + -0.052 wm_rt*repeat + 0.026 wm_rt*switch + 0.038 wm_rt*wm_rt[t-1] + -0.023 wm_rt*wm_rt[t-2] + -0.013 wm_rt*wm_rt[t-3] + -0.136 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.0 repeat*switch + -0.263 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.069 switch*wm_rt[t-1] + -0.009 switch*wm_rt[t-2] + -0.078 switch*wm_rt[t-3] + 0.165 switch*wm_rt[t-4] + -0.021 wm_rt[t-1]^2 + -0.013 wm_rt[t-1]*wm_rt[t-2] + -0.015 wm_rt[t-1]*wm_rt[t-3] + -0.024 wm_rt[t-1]*wm_rt[t-4] + -0.076 wm_rt[t-2]^2 + -0.073 wm_rt[t-2]*wm_rt[t-3] + -0.07 wm_rt[t-2]*wm_rt[t-4] + -0.011 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.038 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 67, 0, 83, 83, 0, 0, 0, 0, 0, 0, 83, 83, 82, 83, 0, 83, 83, 0, 83, 0, 0, 83, 0, 46, 0, 0, 40, 59, 58, 38, 0, 0, 0, 83, 83, 64\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 183/400 --- L(Train): 0.3916104 --- L(Val, RNN): 0.3696714 --- L(Val, SINDy): 0.6295620 --- Time: 3.55s; --- Convergence: 1.97e-03; LR: 1.00e-02; Metric: 0.3676738; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.557 value_reward_diff[t] + 1.528 reward_diff + -0.572 value_reward_diff^2 + 0.087 value_reward_diff*reward_diff + 1.52 reward_diff^2 \n",
      "value_persistance[t+1] = -0.384 1 + 0.775 value_persistance[t] + -0.085 repeat + 0.34 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.025 1 + 1.056 wm_rt[t] + 0.009 repeat + -0.0 switch + -0.064 wm_rt[t-1] + 0.167 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.085 wm_rt[t-4] + 0.073 wm_rt^2 + -0.052 wm_rt*repeat + 0.026 wm_rt*switch + 0.039 wm_rt*wm_rt[t-1] + -0.023 wm_rt*wm_rt[t-2] + -0.013 wm_rt*wm_rt[t-3] + -0.137 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + 0.0 repeat*switch + -0.262 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.068 switch*wm_rt[t-1] + -0.008 switch*wm_rt[t-2] + -0.077 switch*wm_rt[t-3] + 0.163 switch*wm_rt[t-4] + -0.02 wm_rt[t-1]^2 + -0.012 wm_rt[t-1]*wm_rt[t-2] + -0.015 wm_rt[t-1]*wm_rt[t-3] + -0.023 wm_rt[t-1]*wm_rt[t-4] + -0.076 wm_rt[t-2]^2 + -0.073 wm_rt[t-2]*wm_rt[t-3] + -0.07 wm_rt[t-2]*wm_rt[t-4] + -0.01 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.039 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 68, 0, 84, 84, 0, 0, 0, 0, 0, 0, 84, 84, 83, 84, 0, 84, 84, 0, 84, 0, 0, 84, 0, 47, 0, 0, 41, 60, 59, 39, 0, 0, 0, 84, 84, 65\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 184/400 --- L(Train): 0.3912691 --- L(Val, RNN): 0.3664860 --- L(Val, SINDy): 0.7090639 --- Time: 3.51s; --- Convergence: 2.58e-03; LR: 1.00e-02; Metric: 0.3664860; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.558 value_reward_diff[t] + 1.535 reward_diff + -0.571 value_reward_diff^2 + 0.087 value_reward_diff*reward_diff + 1.527 reward_diff^2 \n",
      "value_persistance[t+1] = -0.385 1 + 0.775 value_persistance[t] + -0.086 repeat + 0.34 value_persistance^2 + -0.174 value_persistance*repeat + -0.087 repeat^2 \n",
      "wm_rt[t+1] = -0.025 1 + 1.056 wm_rt[t] + 0.009 repeat + -0.001 switch + -0.063 wm_rt[t-1] + 0.168 wm_rt[t-2] + 0.059 wm_rt[t-3] + 0.085 wm_rt[t-4] + 0.073 wm_rt^2 + -0.052 wm_rt*repeat + 0.026 wm_rt*switch + 0.04 wm_rt*wm_rt[t-1] + -0.023 wm_rt*wm_rt[t-2] + -0.013 wm_rt*wm_rt[t-3] + -0.138 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + 0.0 repeat*switch + -0.262 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.088 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.068 switch*wm_rt[t-1] + -0.008 switch*wm_rt[t-2] + -0.077 switch*wm_rt[t-3] + 0.162 switch*wm_rt[t-4] + -0.019 wm_rt[t-1]^2 + -0.011 wm_rt[t-1]*wm_rt[t-2] + -0.014 wm_rt[t-1]*wm_rt[t-3] + -0.023 wm_rt[t-1]*wm_rt[t-4] + -0.076 wm_rt[t-2]^2 + -0.073 wm_rt[t-2]*wm_rt[t-3] + -0.069 wm_rt[t-2]*wm_rt[t-4] + -0.01 wm_rt[t-3]^2 + 0.003 wm_rt[t-3]*wm_rt[t-4] + -0.04 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 69, 0, 85, 85, 0, 0, 0, 0, 0, 0, 85, 85, 84, 85, 0, 85, 85, 0, 85, 0, 0, 85, 0, 48, 0, 0, 42, 61, 60, 40, 0, 0, 0, 85, 85, 66\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 185/400 --- L(Train): 0.3923957 --- L(Val, RNN): 0.3674395 --- L(Val, SINDy): 0.7064799 --- Time: 3.58s; --- Convergence: 1.77e-03; LR: 1.00e-02; Metric: 0.3664860; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.558 value_reward_diff[t] + 1.541 reward_diff + -0.57 value_reward_diff^2 + 0.085 value_reward_diff*reward_diff + 1.533 reward_diff^2 \n",
      "value_persistance[t+1] = -0.385 1 + 0.775 value_persistance[t] + -0.086 repeat + 0.341 value_persistance^2 + -0.174 value_persistance*repeat + -0.088 repeat^2 \n",
      "wm_rt[t+1] = -0.025 1 + 1.055 wm_rt[t] + 0.009 repeat + -0.0 switch + -0.062 wm_rt[t-1] + 0.168 wm_rt[t-2] + 0.06 wm_rt[t-3] + 0.084 wm_rt[t-4] + 0.072 wm_rt^2 + -0.053 wm_rt*repeat + 0.026 wm_rt*switch + 0.04 wm_rt*wm_rt[t-1] + -0.023 wm_rt*wm_rt[t-2] + -0.013 wm_rt*wm_rt[t-3] + -0.138 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + 0.0 repeat*switch + -0.262 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.067 repeat*wm_rt[t-3] + 0.088 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.067 switch*wm_rt[t-1] + -0.007 switch*wm_rt[t-2] + -0.076 switch*wm_rt[t-3] + 0.162 switch*wm_rt[t-4] + -0.018 wm_rt[t-1]^2 + -0.01 wm_rt[t-1]*wm_rt[t-2] + -0.013 wm_rt[t-1]*wm_rt[t-3] + -0.021 wm_rt[t-1]*wm_rt[t-4] + -0.075 wm_rt[t-2]^2 + -0.072 wm_rt[t-2]*wm_rt[t-3] + -0.069 wm_rt[t-2]*wm_rt[t-4] + -0.009 wm_rt[t-3]^2 + 0.003 wm_rt[t-3]*wm_rt[t-4] + -0.04 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 70, 0, 86, 86, 0, 0, 0, 0, 0, 0, 86, 86, 85, 86, 0, 86, 86, 0, 86, 0, 0, 86, 0, 49, 0, 0, 43, 62, 61, 41, 0, 0, 0, 86, 86, 67\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 186/400 --- L(Train): 0.3983509 --- L(Val, RNN): 0.3678629 --- L(Val, SINDy): 0.6768837 --- Time: 3.40s; --- Convergence: 1.09e-03; LR: 1.00e-02; Metric: 0.3664860; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.559 value_reward_diff[t] + 1.548 reward_diff + -0.569 value_reward_diff^2 + 0.085 value_reward_diff*reward_diff + 1.54 reward_diff^2 \n",
      "value_persistance[t+1] = -0.385 1 + 0.774 value_persistance[t] + -0.086 repeat + 0.342 value_persistance^2 + -0.174 value_persistance*repeat + -0.087 repeat^2 \n",
      "wm_rt[t+1] = -0.025 1 + 1.055 wm_rt[t] + 0.009 repeat + -0.001 switch + -0.061 wm_rt[t-1] + 0.168 wm_rt[t-2] + 0.06 wm_rt[t-3] + 0.083 wm_rt[t-4] + 0.072 wm_rt^2 + -0.053 wm_rt*repeat + 0.026 wm_rt*switch + 0.041 wm_rt*wm_rt[t-1] + -0.022 wm_rt*wm_rt[t-2] + -0.013 wm_rt*wm_rt[t-3] + -0.139 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.0 repeat*switch + -0.261 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.067 repeat*wm_rt[t-3] + 0.088 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.066 switch*wm_rt[t-1] + -0.007 switch*wm_rt[t-2] + -0.076 switch*wm_rt[t-3] + 0.161 switch*wm_rt[t-4] + -0.017 wm_rt[t-1]^2 + -0.009 wm_rt[t-1]*wm_rt[t-2] + -0.012 wm_rt[t-1]*wm_rt[t-3] + -0.021 wm_rt[t-1]*wm_rt[t-4] + -0.075 wm_rt[t-2]^2 + -0.072 wm_rt[t-2]*wm_rt[t-3] + -0.069 wm_rt[t-2]*wm_rt[t-4] + -0.009 wm_rt[t-3]^2 + 0.004 wm_rt[t-3]*wm_rt[t-4] + -0.041 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 71, 0, 87, 87, 0, 0, 0, 0, 0, 0, 87, 87, 86, 87, 0, 87, 87, 0, 87, 0, 0, 87, 0, 50, 0, 0, 44, 63, 62, 42, 0, 0, 0, 87, 87, 68\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 187/400 --- L(Train): 0.3961034 --- L(Val, RNN): 0.3656048 --- L(Val, SINDy): 0.6231822 --- Time: 3.46s; --- Convergence: 1.68e-03; LR: 1.00e-02; Metric: 0.3656048; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.559 value_reward_diff[t] + 1.555 reward_diff + -0.568 value_reward_diff^2 + 0.085 value_reward_diff*reward_diff + 1.547 reward_diff^2 \n",
      "value_persistance[t+1] = -0.385 1 + 0.772 value_persistance[t] + -0.085 repeat + 0.343 value_persistance^2 + -0.175 value_persistance*repeat + -0.087 repeat^2 \n",
      "wm_rt[t+1] = -0.024 1 + 1.056 wm_rt[t] + 0.01 repeat + -0.001 switch + -0.061 wm_rt[t-1] + 0.168 wm_rt[t-2] + 0.059 wm_rt[t-3] + 0.081 wm_rt[t-4] + 0.072 wm_rt^2 + -0.053 wm_rt*repeat + 0.026 wm_rt*switch + 0.042 wm_rt*wm_rt[t-1] + -0.022 wm_rt*wm_rt[t-2] + -0.013 wm_rt*wm_rt[t-3] + -0.14 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.0 repeat*switch + -0.261 repeat*wm_rt[t-1] + -0.007 repeat*wm_rt[t-2] + -0.067 repeat*wm_rt[t-3] + 0.087 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.066 switch*wm_rt[t-1] + -0.008 switch*wm_rt[t-2] + -0.076 switch*wm_rt[t-3] + 0.159 switch*wm_rt[t-4] + -0.018 wm_rt[t-1]^2 + -0.009 wm_rt[t-1]*wm_rt[t-2] + -0.012 wm_rt[t-1]*wm_rt[t-3] + -0.021 wm_rt[t-1]*wm_rt[t-4] + -0.076 wm_rt[t-2]^2 + -0.073 wm_rt[t-2]*wm_rt[t-3] + -0.069 wm_rt[t-2]*wm_rt[t-4] + -0.01 wm_rt[t-3]^2 + 0.003 wm_rt[t-3]*wm_rt[t-4] + -0.043 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 72, 0, 88, 88, 0, 0, 0, 0, 0, 0, 88, 88, 87, 88, 0, 88, 88, 0, 88, 0, 0, 88, 0, 51, 0, 0, 45, 64, 63, 43, 0, 0, 0, 88, 88, 69\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 188/400 --- L(Train): 0.3959274 --- L(Val, RNN): 0.3654341 --- L(Val, SINDy): 0.6110910 --- Time: 4.08s; --- Convergence: 9.23e-04; LR: 1.00e-02; Metric: 0.3654341; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.558 value_reward_diff[t] + 1.562 reward_diff + -0.568 value_reward_diff^2 + 0.083 value_reward_diff*reward_diff + 1.553 reward_diff^2 \n",
      "value_persistance[t+1] = -0.385 1 + 0.772 value_persistance[t] + -0.085 repeat + 0.343 value_persistance^2 + -0.175 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.023 1 + 1.056 wm_rt[t] + 0.01 repeat + -0.001 switch + -0.061 wm_rt[t-1] + 0.168 wm_rt[t-2] + 0.059 wm_rt[t-3] + 0.08 wm_rt[t-4] + 0.072 wm_rt^2 + -0.053 wm_rt*repeat + 0.026 wm_rt*switch + 0.043 wm_rt*wm_rt[t-1] + -0.022 wm_rt*wm_rt[t-2] + -0.013 wm_rt*wm_rt[t-3] + -0.14 wm_rt*wm_rt[t-4] + 0.007 repeat^2 + -0.0 repeat*switch + -0.261 repeat*wm_rt[t-1] + -0.007 repeat*wm_rt[t-2] + -0.068 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.066 switch*wm_rt[t-1] + -0.008 switch*wm_rt[t-2] + -0.076 switch*wm_rt[t-3] + 0.157 switch*wm_rt[t-4] + -0.017 wm_rt[t-1]^2 + -0.009 wm_rt[t-1]*wm_rt[t-2] + -0.012 wm_rt[t-1]*wm_rt[t-3] + -0.021 wm_rt[t-1]*wm_rt[t-4] + -0.076 wm_rt[t-2]^2 + -0.073 wm_rt[t-2]*wm_rt[t-3] + -0.069 wm_rt[t-2]*wm_rt[t-4] + -0.01 wm_rt[t-3]^2 + 0.003 wm_rt[t-3]*wm_rt[t-4] + -0.045 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 73, 0, 89, 89, 0, 0, 0, 0, 0, 0, 89, 89, 88, 89, 0, 89, 89, 0, 89, 0, 0, 89, 0, 52, 0, 0, 46, 65, 64, 44, 0, 0, 0, 89, 89, 70\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 189/400 --- L(Train): 0.3906195 --- L(Val, RNN): 0.3658337 --- L(Val, SINDy): 0.6066552 --- Time: 3.74s; --- Convergence: 6.62e-04; LR: 1.00e-02; Metric: 0.3654341; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.557 value_reward_diff[t] + 1.568 reward_diff + -0.569 value_reward_diff^2 + 0.081 value_reward_diff*reward_diff + 1.559 reward_diff^2 \n",
      "value_persistance[t+1] = -0.386 1 + 0.772 value_persistance[t] + -0.085 repeat + 0.343 value_persistance^2 + -0.175 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.022 1 + 1.056 wm_rt[t] + 0.009 repeat + -0.001 switch + -0.06 wm_rt[t-1] + 0.168 wm_rt[t-2] + 0.059 wm_rt[t-3] + 0.079 wm_rt[t-4] + 0.072 wm_rt^2 + -0.054 wm_rt*repeat + 0.026 wm_rt*switch + 0.043 wm_rt*wm_rt[t-1] + -0.021 wm_rt*wm_rt[t-2] + -0.013 wm_rt*wm_rt[t-3] + -0.141 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.0 repeat*switch + -0.261 repeat*wm_rt[t-1] + -0.008 repeat*wm_rt[t-2] + -0.068 repeat*wm_rt[t-3] + 0.085 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.066 switch*wm_rt[t-1] + -0.008 switch*wm_rt[t-2] + -0.076 switch*wm_rt[t-3] + 0.156 switch*wm_rt[t-4] + -0.017 wm_rt[t-1]^2 + -0.009 wm_rt[t-1]*wm_rt[t-2] + -0.011 wm_rt[t-1]*wm_rt[t-3] + -0.02 wm_rt[t-1]*wm_rt[t-4] + -0.076 wm_rt[t-2]^2 + -0.073 wm_rt[t-2]*wm_rt[t-3] + -0.069 wm_rt[t-2]*wm_rt[t-4] + -0.01 wm_rt[t-3]^2 + 0.003 wm_rt[t-3]*wm_rt[t-4] + -0.046 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 74, 0, 90, 90, 0, 0, 0, 0, 0, 0, 90, 90, 89, 90, 0, 90, 90, 0, 90, 0, 0, 90, 0, 53, 0, 0, 47, 66, 65, 45, 0, 0, 0, 90, 90, 71\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 190/400 --- L(Train): 0.3912537 --- L(Val, RNN): 0.3635881 --- L(Val, SINDy): 0.6134088 --- Time: 4.21s; --- Convergence: 1.45e-03; LR: 1.00e-02; Metric: 0.3635881; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.557 value_reward_diff[t] + 1.574 reward_diff + -0.567 value_reward_diff^2 + 0.081 value_reward_diff*reward_diff + 1.565 reward_diff^2 \n",
      "value_persistance[t+1] = -0.386 1 + 0.772 value_persistance[t] + -0.085 repeat + 0.343 value_persistance^2 + -0.175 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.023 1 + 1.056 wm_rt[t] + 0.009 repeat + -0.001 switch + -0.059 wm_rt[t-1] + 0.169 wm_rt[t-2] + 0.06 wm_rt[t-3] + 0.078 wm_rt[t-4] + 0.072 wm_rt^2 + -0.054 wm_rt*repeat + 0.026 wm_rt*switch + 0.043 wm_rt*wm_rt[t-1] + -0.021 wm_rt*wm_rt[t-2] + -0.014 wm_rt*wm_rt[t-3] + -0.142 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + 0.0 repeat*switch + -0.261 repeat*wm_rt[t-1] + -0.007 repeat*wm_rt[t-2] + -0.068 repeat*wm_rt[t-3] + 0.085 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.065 switch*wm_rt[t-1] + -0.007 switch*wm_rt[t-2] + -0.075 switch*wm_rt[t-3] + 0.155 switch*wm_rt[t-4] + -0.016 wm_rt[t-1]^2 + -0.008 wm_rt[t-1]*wm_rt[t-2] + -0.01 wm_rt[t-1]*wm_rt[t-3] + -0.019 wm_rt[t-1]*wm_rt[t-4] + -0.076 wm_rt[t-2]^2 + -0.072 wm_rt[t-2]*wm_rt[t-3] + -0.069 wm_rt[t-2]*wm_rt[t-4] + -0.009 wm_rt[t-3]^2 + 0.003 wm_rt[t-3]*wm_rt[t-4] + -0.047 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 75, 0, 91, 91, 0, 0, 0, 0, 0, 0, 91, 91, 90, 91, 0, 91, 91, 0, 91, 0, 0, 91, 0, 54, 0, 0, 48, 67, 66, 46, 0, 0, 0, 91, 91, 72\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 191/400 --- L(Train): 0.3851278 --- L(Val, RNN): 0.3638941 --- L(Val, SINDy): 0.6284514 --- Time: 3.55s; --- Convergence: 8.80e-04; LR: 1.00e-02; Metric: 0.3635881; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.559 value_reward_diff[t] + 1.581 reward_diff + -0.565 value_reward_diff^2 + 0.082 value_reward_diff*reward_diff + 1.572 reward_diff^2 \n",
      "value_persistance[t+1] = -0.387 1 + 0.772 value_persistance[t] + -0.084 repeat + 0.344 value_persistance^2 + -0.176 value_persistance*repeat + -0.086 repeat^2 \n",
      "wm_rt[t+1] = -0.023 1 + 1.055 wm_rt[t] + 0.009 repeat + -0.001 switch + -0.057 wm_rt[t-1] + 0.17 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.078 wm_rt[t-4] + 0.071 wm_rt^2 + -0.054 wm_rt*repeat + 0.025 wm_rt*switch + 0.042 wm_rt*wm_rt[t-1] + -0.021 wm_rt*wm_rt[t-2] + -0.014 wm_rt*wm_rt[t-3] + -0.142 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + 0.0 repeat*switch + -0.261 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.067 repeat*wm_rt[t-3] + 0.085 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.063 switch*wm_rt[t-1] + -0.006 switch*wm_rt[t-2] + -0.074 switch*wm_rt[t-3] + 0.155 switch*wm_rt[t-4] + -0.014 wm_rt[t-1]^2 + -0.006 wm_rt[t-1]*wm_rt[t-2] + -0.008 wm_rt[t-1]*wm_rt[t-3] + -0.017 wm_rt[t-1]*wm_rt[t-4] + -0.074 wm_rt[t-2]^2 + -0.071 wm_rt[t-2]*wm_rt[t-3] + -0.068 wm_rt[t-2]*wm_rt[t-4] + -0.008 wm_rt[t-3]^2 + 0.004 wm_rt[t-3]*wm_rt[t-4] + -0.047 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 76, 0, 92, 92, 0, 0, 0, 0, 0, 0, 92, 92, 91, 92, 0, 92, 92, 0, 92, 0, 0, 92, 0, 55, 0, 0, 49, 68, 67, 47, 0, 0, 0, 92, 92, 73\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 192/400 --- L(Train): 0.3843620 --- L(Val, RNN): 0.3632169 --- L(Val, SINDy): 0.6280153 --- Time: 4.19s; --- Convergence: 7.78e-04; LR: 1.00e-02; Metric: 0.3632169; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.559 value_reward_diff[t] + 1.588 reward_diff + -0.564 value_reward_diff^2 + 0.082 value_reward_diff*reward_diff + 1.578 reward_diff^2 \n",
      "value_persistance[t+1] = -0.388 1 + 0.772 value_persistance[t] + -0.084 repeat + 0.344 value_persistance^2 + -0.176 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.024 1 + 1.054 wm_rt[t] + 0.008 repeat + -0.001 switch + -0.055 wm_rt[t-1] + 0.171 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.078 wm_rt[t-4] + 0.071 wm_rt^2 + -0.055 wm_rt*repeat + 0.025 wm_rt*switch + 0.042 wm_rt*wm_rt[t-1] + -0.021 wm_rt*wm_rt[t-2] + -0.014 wm_rt*wm_rt[t-3] + -0.143 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + 0.0 repeat*switch + -0.261 repeat*wm_rt[t-1] + -0.006 repeat*wm_rt[t-2] + -0.067 repeat*wm_rt[t-3] + 0.085 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.061 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.073 switch*wm_rt[t-3] + 0.155 switch*wm_rt[t-4] + -0.012 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + -0.006 wm_rt[t-1]*wm_rt[t-3] + -0.015 wm_rt[t-1]*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.07 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.007 wm_rt[t-3]^2 + 0.005 wm_rt[t-3]*wm_rt[t-4] + -0.047 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 77, 0, 93, 93, 0, 0, 0, 0, 0, 0, 93, 93, 92, 93, 0, 93, 93, 0, 93, 0, 0, 93, 0, 56, 0, 0, 50, 69, 68, 48, 0, 0, 0, 93, 93, 74\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 193/400 --- L(Train): 0.3903003 --- L(Val, RNN): 0.3613273 --- L(Val, SINDy): 0.6209998 --- Time: 3.74s; --- Convergence: 1.33e-03; LR: 1.00e-02; Metric: 0.3613273; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.559 value_reward_diff[t] + 1.595 reward_diff + -0.563 value_reward_diff^2 + 0.081 value_reward_diff*reward_diff + 1.585 reward_diff^2 \n",
      "value_persistance[t+1] = -0.388 1 + 0.772 value_persistance[t] + -0.084 repeat + 0.344 value_persistance^2 + -0.176 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.023 1 + 1.054 wm_rt[t] + 0.008 repeat + -0.0 switch + -0.054 wm_rt[t-1] + 0.172 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.077 wm_rt[t-4] + 0.071 wm_rt^2 + -0.056 wm_rt*repeat + 0.026 wm_rt*switch + 0.043 wm_rt*wm_rt[t-1] + -0.021 wm_rt*wm_rt[t-2] + -0.014 wm_rt*wm_rt[t-3] + -0.144 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + 0.0 repeat*switch + -0.262 repeat*wm_rt[t-1] + -0.005 repeat*wm_rt[t-2] + -0.067 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.06 switch*wm_rt[t-1] + -0.004 switch*wm_rt[t-2] + -0.073 switch*wm_rt[t-3] + 0.154 switch*wm_rt[t-4] + -0.011 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.006 wm_rt[t-1]*wm_rt[t-3] + -0.015 wm_rt[t-1]*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.069 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.007 wm_rt[t-3]^2 + 0.006 wm_rt[t-3]*wm_rt[t-4] + -0.048 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 78, 0, 94, 94, 0, 0, 0, 0, 0, 0, 94, 94, 93, 94, 0, 94, 94, 0, 94, 0, 0, 94, 0, 57, 0, 0, 51, 70, 69, 49, 0, 0, 0, 94, 94, 75\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 194/400 --- L(Train): 0.3883348 --- L(Val, RNN): 0.3628917 --- L(Val, SINDy): 0.6408830 --- Time: 4.18s; --- Convergence: 1.45e-03; LR: 1.00e-02; Metric: 0.3613273; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.56 value_reward_diff[t] + 1.602 reward_diff + -0.562 value_reward_diff^2 + 0.081 value_reward_diff*reward_diff + 1.592 reward_diff^2 \n",
      "value_persistance[t+1] = -0.389 1 + 0.771 value_persistance[t] + -0.083 repeat + 0.344 value_persistance^2 + -0.176 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.022 1 + 1.055 wm_rt[t] + 0.008 repeat + 0.0 switch + -0.054 wm_rt[t-1] + 0.172 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.076 wm_rt[t-4] + 0.071 wm_rt^2 + -0.056 wm_rt*repeat + 0.026 wm_rt*switch + 0.044 wm_rt*wm_rt[t-1] + -0.02 wm_rt*wm_rt[t-2] + -0.014 wm_rt*wm_rt[t-3] + -0.145 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + -0.0 repeat*switch + -0.262 repeat*wm_rt[t-1] + -0.005 repeat*wm_rt[t-2] + -0.067 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.06 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.074 switch*wm_rt[t-3] + 0.152 switch*wm_rt[t-4] + -0.011 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.006 wm_rt[t-1]*wm_rt[t-3] + -0.015 wm_rt[t-1]*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.07 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.007 wm_rt[t-3]^2 + 0.005 wm_rt[t-3]*wm_rt[t-4] + -0.05 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 79, 0, 95, 95, 0, 0, 0, 0, 0, 0, 95, 95, 94, 95, 0, 95, 95, 0, 95, 0, 0, 95, 0, 58, 0, 0, 52, 71, 70, 50, 0, 0, 0, 95, 95, 76\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 195/400 --- L(Train): 0.3892927 --- L(Val, RNN): 0.3622504 --- L(Val, SINDy): 0.6391966 --- Time: 3.63s; --- Convergence: 1.05e-03; LR: 1.00e-02; Metric: 0.3613273; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.561 value_reward_diff[t] + 1.609 reward_diff + -0.559 value_reward_diff^2 + 0.082 value_reward_diff*reward_diff + 1.598 reward_diff^2 \n",
      "value_persistance[t+1] = -0.39 1 + 0.771 value_persistance[t] + -0.083 repeat + 0.344 value_persistance^2 + -0.177 value_persistance*repeat + -0.084 repeat^2 \n",
      "wm_rt[t+1] = -0.022 1 + 1.055 wm_rt[t] + 0.008 repeat + 0.001 switch + -0.054 wm_rt[t-1] + 0.172 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.074 wm_rt[t-4] + 0.071 wm_rt^2 + -0.056 wm_rt*repeat + 0.027 wm_rt*switch + 0.045 wm_rt*wm_rt[t-1] + -0.02 wm_rt*wm_rt[t-2] + -0.014 wm_rt*wm_rt[t-3] + -0.145 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + -0.0 repeat*switch + -0.261 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.067 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.061 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.074 switch*wm_rt[t-3] + 0.151 switch*wm_rt[t-4] + -0.011 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.006 wm_rt[t-1]*wm_rt[t-3] + -0.015 wm_rt[t-1]*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.07 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.008 wm_rt[t-3]^2 + 0.005 wm_rt[t-3]*wm_rt[t-4] + -0.051 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 80, 0, 96, 96, 0, 0, 0, 0, 0, 0, 96, 96, 95, 96, 0, 96, 96, 0, 96, 0, 0, 96, 0, 59, 0, 0, 53, 72, 71, 51, 0, 0, 0, 96, 96, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 196/400 --- L(Train): 0.3846844 --- L(Val, RNN): 0.3602904 --- L(Val, SINDy): 0.6474510 --- Time: 4.04s; --- Convergence: 1.50e-03; LR: 1.00e-02; Metric: 0.3602904; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.562 value_reward_diff[t] + 1.615 reward_diff + -0.558 value_reward_diff^2 + 0.083 value_reward_diff*reward_diff + 1.605 reward_diff^2 \n",
      "value_persistance[t+1] = -0.39 1 + 0.771 value_persistance[t] + -0.082 repeat + 0.345 value_persistance^2 + -0.178 value_persistance*repeat + -0.084 repeat^2 \n",
      "wm_rt[t+1] = -0.022 1 + 1.055 wm_rt[t] + 0.008 repeat + 0.001 switch + -0.054 wm_rt[t-1] + 0.172 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.073 wm_rt[t-4] + 0.07 wm_rt^2 + -0.056 wm_rt*repeat + 0.027 wm_rt*switch + 0.045 wm_rt*wm_rt[t-1] + -0.02 wm_rt*wm_rt[t-2] + -0.014 wm_rt*wm_rt[t-3] + -0.146 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + -0.0 repeat*switch + -0.261 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.068 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.06 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.075 switch*wm_rt[t-3] + 0.149 switch*wm_rt[t-4] + -0.011 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.005 wm_rt[t-1]*wm_rt[t-3] + -0.014 wm_rt[t-1]*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.07 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.008 wm_rt[t-3]^2 + 0.004 wm_rt[t-3]*wm_rt[t-4] + -0.052 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 81, 0, 97, 97, 0, 0, 0, 0, 0, 0, 97, 97, 96, 97, 0, 97, 97, 0, 97, 0, 0, 97, 0, 60, 0, 0, 54, 73, 72, 52, 0, 0, 0, 97, 97, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 197/400 --- L(Train): 0.3868937 --- L(Val, RNN): 0.3601821 --- L(Val, SINDy): 0.6694884 --- Time: 4.51s; --- Convergence: 8.05e-04; LR: 1.00e-02; Metric: 0.3601821; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.564 value_reward_diff[t] + 1.622 reward_diff + -0.556 value_reward_diff^2 + 0.084 value_reward_diff*reward_diff + 1.612 reward_diff^2 \n",
      "value_persistance[t+1] = -0.389 1 + 0.77 value_persistance[t] + -0.081 repeat + 0.346 value_persistance^2 + -0.178 value_persistance*repeat + -0.083 repeat^2 \n",
      "wm_rt[t+1] = -0.023 1 + 1.054 wm_rt[t] + 0.008 repeat + 0.001 switch + -0.053 wm_rt[t-1] + 0.172 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.072 wm_rt[t-4] + 0.07 wm_rt^2 + -0.057 wm_rt*repeat + 0.027 wm_rt*switch + 0.045 wm_rt*wm_rt[t-1] + -0.02 wm_rt*wm_rt[t-2] + -0.014 wm_rt*wm_rt[t-3] + -0.146 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + 0.0 repeat*switch + -0.261 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.068 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.06 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.075 switch*wm_rt[t-3] + 0.148 switch*wm_rt[t-4] + -0.01 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.005 wm_rt[t-1]*wm_rt[t-3] + -0.014 wm_rt[t-1]*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.069 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.008 wm_rt[t-3]^2 + 0.004 wm_rt[t-3]*wm_rt[t-4] + -0.054 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 82, 0, 98, 98, 0, 0, 0, 0, 0, 0, 98, 98, 97, 98, 0, 98, 98, 0, 98, 0, 0, 98, 0, 61, 0, 0, 55, 74, 73, 53, 0, 0, 0, 98, 98, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 198/400 --- L(Train): 0.3816865 --- L(Val, RNN): 0.3601479 --- L(Val, SINDy): 0.6191171 --- Time: 4.22s; --- Convergence: 4.20e-04; LR: 1.00e-02; Metric: 0.3601479; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.566 value_reward_diff[t] + 1.629 reward_diff + -0.553 value_reward_diff^2 + 0.086 value_reward_diff*reward_diff + 1.618 reward_diff^2 \n",
      "value_persistance[t+1] = -0.389 1 + 0.769 value_persistance[t] + -0.08 repeat + 0.347 value_persistance^2 + -0.179 value_persistance*repeat + -0.082 repeat^2 \n",
      "wm_rt[t+1] = -0.023 1 + 1.054 wm_rt[t] + 0.008 repeat + 0.0 switch + -0.052 wm_rt[t-1] + 0.173 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.071 wm_rt[t-4] + 0.07 wm_rt^2 + -0.057 wm_rt*repeat + 0.027 wm_rt*switch + 0.045 wm_rt*wm_rt[t-1] + -0.02 wm_rt*wm_rt[t-2] + -0.015 wm_rt*wm_rt[t-3] + -0.147 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + 0.0 repeat*switch + -0.261 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.067 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.059 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.075 switch*wm_rt[t-3] + 0.147 switch*wm_rt[t-4] + -0.01 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + -0.013 wm_rt[t-1]*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.069 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.008 wm_rt[t-3]^2 + 0.004 wm_rt[t-3]*wm_rt[t-4] + -0.054 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 83, 0, 99, 99, 0, 0, 0, 0, 0, 0, 99, 99, 98, 99, 0, 99, 99, 0, 99, 0, 0, 99, 0, 62, 0, 0, 56, 75, 74, 54, 0, 0, 0, 99, 99, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 199/400 --- L(Train): 0.3826306 --- L(Val, RNN): 0.3581933 --- L(Val, SINDy): 0.5493963 --- Time: 3.65s; --- Convergence: 1.19e-03; LR: 1.00e-02; Metric: 0.3581933; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 47):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.567 value_reward_diff[t] + 1.636 reward_diff + -0.551 value_reward_diff^2 + 0.087 value_reward_diff*reward_diff + 1.625 reward_diff^2 \n",
      "value_persistance[t+1] = -0.389 1 + 0.768 value_persistance[t] + -0.08 repeat + 0.347 value_persistance^2 + -0.18 value_persistance*repeat + -0.081 repeat^2 \n",
      "wm_rt[t+1] = -0.022 1 + 1.054 wm_rt[t] + 0.009 repeat + 0.0 switch + -0.051 wm_rt[t-1] + 0.174 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.071 wm_rt[t-4] + 0.07 wm_rt^2 + -0.057 wm_rt*repeat + 0.026 wm_rt*switch + 0.046 wm_rt*wm_rt[t-1] + -0.019 wm_rt*wm_rt[t-2] + -0.015 wm_rt*wm_rt[t-3] + -0.148 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.26 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.067 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.059 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.075 switch*wm_rt[t-3] + 0.146 switch*wm_rt[t-4] + -0.009 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.012 wm_rt[t-1]*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.008 wm_rt[t-3]^2 + 0.004 wm_rt[t-3]*wm_rt[t-4] + -0.055 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 84, 0, 100, 100, 0, 0, 0, 0, 0, 0, 100, 100, 99, 100, 0, 100, -, 0, 100, 0, 0, 100, 0, 63, 0, 0, 57, 76, 75, 55, 0, 0, 0, 100, 100, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 200/400 --- L(Train): 0.3847971 --- L(Val, RNN): 0.3578106 --- L(Val, SINDy): 0.5236483 --- Time: 4.01s; --- Convergence: 7.85e-04; LR: 1.00e-02; Metric: 0.3578106; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 46):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.568 value_reward_diff[t] + 1.642 reward_diff + -0.55 value_reward_diff^2 + 0.087 value_reward_diff*reward_diff + 1.631 reward_diff^2 \n",
      "value_persistance[t+1] = -0.389 1 + 0.767 value_persistance[t] + -0.08 repeat + 0.348 value_persistance^2 + -0.179 value_persistance*repeat + -0.082 repeat^2 \n",
      "wm_rt[t+1] = -0.022 1 + 1.054 wm_rt[t] + 0.009 repeat + -0.05 wm_rt[t-1] + 0.175 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.07 wm_rt[t-4] + 0.069 wm_rt^2 + -0.057 wm_rt*repeat + 0.026 wm_rt*switch + 0.046 wm_rt*wm_rt[t-1] + -0.019 wm_rt*wm_rt[t-2] + -0.015 wm_rt*wm_rt[t-3] + -0.148 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.26 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.057 switch*wm_rt[t-1] + -0.004 switch*wm_rt[t-2] + -0.075 switch*wm_rt[t-3] + 0.145 switch*wm_rt[t-4] + -0.007 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.011 wm_rt[t-1]*wm_rt[t-4] + -0.071 wm_rt[t-2]^2 + -0.067 wm_rt[t-2]*wm_rt[t-3] + -0.064 wm_rt[t-2]*wm_rt[t-4] + -0.008 wm_rt[t-3]^2 + 0.005 wm_rt[t-3]*wm_rt[t-4] + -0.055 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 85, 0, 101, -, 1, 0, 0, 0, 0, 0, 101, 101, 100, 101, 0, 101, -, 0, 101, 0, 0, 101, 0, 64, 0, 0, 58, 77, 76, 56, 0, 0, 0, 101, 101, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 201/400 --- L(Train): 0.3861122 --- L(Val, RNN): 0.3572904 --- L(Val, SINDy): 0.5044495 --- Time: 4.16s; --- Convergence: 6.53e-04; LR: 1.00e-02; Metric: 0.3572904; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 45):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.569 value_reward_diff[t] + 1.649 reward_diff + -0.548 value_reward_diff^2 + 0.088 value_reward_diff*reward_diff + 1.638 reward_diff^2 \n",
      "value_persistance[t+1] = -0.39 1 + 0.767 value_persistance[t] + -0.081 repeat + 0.348 value_persistance^2 + -0.178 value_persistance*repeat + -0.083 repeat^2 \n",
      "wm_rt[t+1] = -0.022 1 + 1.053 wm_rt[t] + 0.009 repeat + -0.048 wm_rt[t-1] + 0.176 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.07 wm_rt[t-4] + 0.069 wm_rt^2 + -0.057 wm_rt*repeat + 0.026 wm_rt*switch + 0.046 wm_rt*wm_rt[t-1] + -0.019 wm_rt*wm_rt[t-2] + -0.015 wm_rt*wm_rt[t-3] + -0.149 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.26 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.066 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + -0.056 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + -0.074 switch*wm_rt[t-3] + 0.145 switch*wm_rt[t-4] + -0.006 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.01 wm_rt[t-1]*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.007 wm_rt[t-3]^2 + 0.005 wm_rt[t-3]*wm_rt[t-4] + -0.056 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 86, 0, 102, -, 2, 0, 0, 0, 0, 0, 102, 102, 101, 102, 0, 102, -, 0, 102, 0, 0, -, 0, 65, 0, 0, 59, 78, 77, 57, 0, 0, 0, 102, 102, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 202/400 --- L(Train): 0.3841091 --- L(Val, RNN): 0.3574191 --- L(Val, SINDy): 0.6015123 --- Time: 3.63s; --- Convergence: 3.91e-04; LR: 1.00e-02; Metric: 0.3572904; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 44):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.571 value_reward_diff[t] + 1.655 reward_diff + -0.546 value_reward_diff^2 + 0.089 value_reward_diff*reward_diff + 1.644 reward_diff^2 \n",
      "value_persistance[t+1] = -0.391 1 + 0.768 value_persistance[t] + -0.082 repeat + 0.347 value_persistance^2 + -0.177 value_persistance*repeat + -0.084 repeat^2 \n",
      "wm_rt[t+1] = -0.022 1 + 1.053 wm_rt[t] + 0.009 repeat + -0.047 wm_rt[t-1] + 0.177 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.07 wm_rt[t-4] + 0.069 wm_rt^2 + -0.057 wm_rt*repeat + 0.026 wm_rt*switch + 0.046 wm_rt*wm_rt[t-1] + -0.019 wm_rt*wm_rt[t-2] + -0.015 wm_rt*wm_rt[t-3] + -0.15 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.26 repeat*wm_rt[t-1] + -0.066 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + -0.055 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.074 switch*wm_rt[t-3] + 0.144 switch*wm_rt[t-4] + -0.005 wm_rt[t-1]^2 + 0.004 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.008 wm_rt[t-1]*wm_rt[t-4] + -0.069 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.007 wm_rt[t-3]^2 + 0.005 wm_rt[t-3]*wm_rt[t-4] + -0.056 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 87, 0, 103, -, 3, 0, 0, 0, 0, 0, 103, 103, 102, 103, 0, 103, -, 0, -, 0, 0, -, 0, 66, 0, 0, 60, 79, 78, 58, 0, 0, 0, 103, 103, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 203/400 --- L(Train): 0.3908754 --- L(Val, RNN): 0.3585845 --- L(Val, SINDy): 0.6204510 --- Time: 4.16s; --- Convergence: 7.78e-04; LR: 1.00e-02; Metric: 0.3572904; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 43):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.572 value_reward_diff[t] + 1.662 reward_diff + -0.544 value_reward_diff^2 + 0.09 value_reward_diff*reward_diff + 1.65 reward_diff^2 \n",
      "value_persistance[t+1] = -0.392 1 + 0.768 value_persistance[t] + -0.083 repeat + 0.347 value_persistance^2 + -0.176 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.021 1 + 1.053 wm_rt[t] + 0.009 repeat + -0.046 wm_rt[t-1] + 0.177 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.069 wm_rt[t-4] + 0.069 wm_rt^2 + -0.057 wm_rt*repeat + 0.026 wm_rt*switch + 0.046 wm_rt*wm_rt[t-1] + -0.019 wm_rt*wm_rt[t-2] + -0.016 wm_rt*wm_rt[t-3] + -0.15 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.26 repeat*wm_rt[t-1] + -0.066 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + -0.054 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.074 switch*wm_rt[t-3] + 0.143 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + 0.004 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.008 wm_rt[t-1]*wm_rt[t-4] + -0.069 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.007 wm_rt[t-3]^2 + -0.057 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 88, 0, 104, -, 4, 0, 0, 0, 0, 0, 104, 104, 103, 104, 0, 104, -, 0, -, 0, 0, -, 0, 67, 0, 0, 61, 80, 79, 59, 0, 0, 0, 104, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 204/400 --- L(Train): 0.3811143 --- L(Val, RNN): 0.3544853 --- L(Val, SINDy): 0.5742999 --- Time: 3.75s; --- Convergence: 2.44e-03; LR: 1.00e-02; Metric: 0.3544853; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 42):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.575 value_reward_diff[t] + 1.668 reward_diff + -0.541 value_reward_diff^2 + 0.092 value_reward_diff*reward_diff + 1.656 reward_diff^2 \n",
      "value_persistance[t+1] = -0.392 1 + 0.768 value_persistance[t] + -0.083 repeat + 0.347 value_persistance^2 + -0.176 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.021 1 + 1.053 wm_rt[t] + 0.008 repeat + -0.046 wm_rt[t-1] + 0.177 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.068 wm_rt[t-4] + 0.069 wm_rt^2 + -0.058 wm_rt*repeat + 0.027 wm_rt*switch + 0.046 wm_rt*wm_rt[t-1] + -0.019 wm_rt*wm_rt[t-2] + -0.016 wm_rt*wm_rt[t-3] + -0.151 wm_rt*wm_rt[t-4] + -0.261 repeat*wm_rt[t-1] + -0.067 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + -0.054 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.074 switch*wm_rt[t-3] + 0.142 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.005 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.007 wm_rt[t-1]*wm_rt[t-4] + -0.069 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.007 wm_rt[t-3]^2 + -0.058 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 89, 0, 105, -, 5, 0, 0, 0, 0, 0, 105, 105, 104, 105, 0, -, -, 0, -, 0, 0, -, 0, 68, 0, 0, 62, 81, 80, 60, 0, 0, 0, 105, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 205/400 --- L(Train): 0.3765336 --- L(Val, RNN): 0.3558202 --- L(Val, SINDy): 0.5726845 --- Time: 3.60s; --- Convergence: 1.89e-03; LR: 1.00e-02; Metric: 0.3544853; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 41):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.577 value_reward_diff[t] + 1.674 reward_diff + -0.538 value_reward_diff^2 + 0.094 value_reward_diff*reward_diff + 1.662 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.768 value_persistance[t] + -0.083 repeat + 0.348 value_persistance^2 + -0.176 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.02 1 + 1.054 wm_rt[t] + -0.045 wm_rt[t-1] + 0.178 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.067 wm_rt[t-4] + 0.069 wm_rt^2 + -0.059 wm_rt*repeat + 0.027 wm_rt*switch + 0.046 wm_rt*wm_rt[t-1] + -0.019 wm_rt*wm_rt[t-2] + -0.016 wm_rt*wm_rt[t-3] + -0.152 wm_rt*wm_rt[t-4] + -0.261 repeat*wm_rt[t-1] + -0.068 repeat*wm_rt[t-3] + 0.082 repeat*wm_rt[t-4] + -0.053 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.075 switch*wm_rt[t-3] + 0.141 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.005 wm_rt[t-1]*wm_rt[t-2] + 0.003 wm_rt[t-1]*wm_rt[t-3] + -0.007 wm_rt[t-1]*wm_rt[t-4] + -0.069 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.008 wm_rt[t-3]^2 + -0.06 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 90, 0, -, -, 6, 0, 0, 0, 0, 0, 106, 106, 105, 106, 0, -, -, 0, -, 0, 0, -, 0, 69, 0, 0, 63, 82, 81, 61, 0, 0, 0, 106, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 206/400 --- L(Train): 0.3901264 --- L(Val, RNN): 0.3539371 --- L(Val, SINDy): 0.6580911 --- Time: 3.80s; --- Convergence: 1.88e-03; LR: 1.00e-02; Metric: 0.3539371; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 40):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.578 value_reward_diff[t] + 1.681 reward_diff + -0.537 value_reward_diff^2 + 0.094 value_reward_diff*reward_diff + 1.668 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.767 value_persistance[t] + -0.083 repeat + 0.348 value_persistance^2 + -0.176 value_persistance*repeat + -0.085 repeat^2 \n",
      "wm_rt[t+1] = -0.019 1 + 1.054 wm_rt[t] + -0.045 wm_rt[t-1] + 0.178 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.066 wm_rt[t-4] + 0.069 wm_rt^2 + -0.058 wm_rt*repeat + 0.027 wm_rt*switch + 0.048 wm_rt*wm_rt[t-1] + -0.018 wm_rt*wm_rt[t-2] + -0.016 wm_rt*wm_rt[t-3] + -0.152 wm_rt*wm_rt[t-4] + -0.259 repeat*wm_rt[t-1] + -0.067 repeat*wm_rt[t-3] + 0.082 repeat*wm_rt[t-4] + -0.053 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.075 switch*wm_rt[t-3] + 0.14 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.006 wm_rt[t-1]*wm_rt[t-2] + 0.003 wm_rt[t-1]*wm_rt[t-3] + -0.006 wm_rt[t-1]*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.061 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 91, 0, -, -, 7, 0, 0, 0, 0, 0, 107, 107, 106, 107, 0, -, -, 0, -, 0, 0, -, 0, 70, 0, 0, 64, 83, 82, 62, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 207/400 --- L(Train): 0.3870858 --- L(Val, RNN): 0.3540839 --- L(Val, SINDy): 0.7905348 --- Time: 3.59s; --- Convergence: 1.02e-03; LR: 1.00e-02; Metric: 0.3539371; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 39):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.578 value_reward_diff[t] + 1.687 reward_diff + -0.536 value_reward_diff^2 + 0.095 value_reward_diff*reward_diff + 1.674 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.766 value_persistance[t] + -0.082 repeat + 0.349 value_persistance^2 + -0.177 value_persistance*repeat + -0.084 repeat^2 \n",
      "wm_rt[t+1] = -0.017 1 + 1.055 wm_rt[t] + -0.043 wm_rt[t-1] + 0.178 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.065 wm_rt[t-4] + 0.069 wm_rt^2 + -0.056 wm_rt*repeat + 0.027 wm_rt*switch + 0.051 wm_rt*wm_rt[t-1] + -0.018 wm_rt*wm_rt[t-2] + -0.152 wm_rt*wm_rt[t-4] + -0.256 repeat*wm_rt[t-1] + -0.066 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + -0.053 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.075 switch*wm_rt[t-3] + 0.138 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.007 wm_rt[t-1]*wm_rt[t-2] + 0.004 wm_rt[t-1]*wm_rt[t-3] + -0.006 wm_rt[t-1]*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.061 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 92, 0, -, -, 8, 0, 0, 0, 0, 0, 108, 0, 107, -, 0, -, -, 0, -, 0, 0, -, 0, 71, 0, 0, 65, 84, 83, 63, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 208/400 --- L(Train): 0.3874702 --- L(Val, RNN): 0.3563032 --- L(Val, SINDy): 0.6877763 --- Time: 4.26s; --- Convergence: 1.62e-03; LR: 1.00e-02; Metric: 0.3539371; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 38):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.579 value_reward_diff[t] + 1.693 reward_diff + -0.535 value_reward_diff^2 + 0.095 value_reward_diff*reward_diff + 1.68 reward_diff^2 \n",
      "value_persistance[t+1] = -0.392 1 + 0.765 value_persistance[t] + -0.081 repeat + 0.35 value_persistance^2 + -0.178 value_persistance*repeat + -0.083 repeat^2 \n",
      "wm_rt[t+1] = -0.016 1 + 1.055 wm_rt[t] + -0.042 wm_rt[t-1] + 0.179 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.065 wm_rt[t-4] + 0.069 wm_rt^2 + -0.054 wm_rt*repeat + 0.027 wm_rt*switch + 0.054 wm_rt*wm_rt[t-1] + -0.153 wm_rt*wm_rt[t-4] + -0.252 repeat*wm_rt[t-1] + -0.065 repeat*wm_rt[t-3] + 0.085 repeat*wm_rt[t-4] + -0.052 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.076 switch*wm_rt[t-3] + 0.137 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.007 wm_rt[t-1]*wm_rt[t-2] + 0.004 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.061 wm_rt[t-2]*wm_rt[t-4] + -0.062 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 93, 0, -, -, 9, 0, 0, 0, 0, 0, 109, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, 72, 0, 0, 66, 85, 84, 64, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 209/400 --- L(Train): 0.3788473 --- L(Val, RNN): 0.3516414 --- L(Val, SINDy): 0.7107434 --- Time: 3.72s; --- Convergence: 3.14e-03; LR: 1.00e-02; Metric: 0.3516414; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 37):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.579 value_reward_diff[t] + 1.699 reward_diff + -0.535 value_reward_diff^2 + 0.094 value_reward_diff*reward_diff + 1.686 reward_diff^2 \n",
      "value_persistance[t+1] = -0.392 1 + 0.764 value_persistance[t] + -0.08 repeat + 0.351 value_persistance^2 + -0.179 value_persistance*repeat + -0.081 repeat^2 \n",
      "wm_rt[t+1] = -0.015 1 + 1.055 wm_rt[t] + -0.041 wm_rt[t-1] + 0.179 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.064 wm_rt[t-4] + 0.069 wm_rt^2 + -0.051 wm_rt*repeat + 0.057 wm_rt*wm_rt[t-1] + -0.153 wm_rt*wm_rt[t-4] + -0.249 repeat*wm_rt[t-1] + -0.064 repeat*wm_rt[t-3] + 0.087 repeat*wm_rt[t-4] + -0.052 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.076 switch*wm_rt[t-3] + 0.135 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + 0.008 wm_rt[t-1]*wm_rt[t-2] + 0.005 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.061 wm_rt[t-2]*wm_rt[t-4] + -0.063 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 94, 0, -, -, 10, 0, 0, 0, 0, 0, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, 73, 0, 0, 67, 86, 85, 65, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 210/400 --- L(Train): 0.3755123 --- L(Val, RNN): 0.3517106 --- L(Val, SINDy): 0.7103005 --- Time: 4.26s; --- Convergence: 1.60e-03; LR: 1.00e-02; Metric: 0.3516414; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 37):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.582 value_reward_diff[t] + 1.706 reward_diff + -0.53 value_reward_diff^2 + 0.097 value_reward_diff*reward_diff + 1.692 reward_diff^2 \n",
      "value_persistance[t+1] = -0.392 1 + 0.763 value_persistance[t] + -0.078 repeat + 0.352 value_persistance^2 + -0.181 value_persistance*repeat + -0.08 repeat^2 \n",
      "wm_rt[t+1] = -0.016 1 + 1.055 wm_rt[t] + -0.04 wm_rt[t-1] + 0.179 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.063 wm_rt[t-4] + 0.069 wm_rt^2 + -0.049 wm_rt*repeat + 0.062 wm_rt*wm_rt[t-1] + -0.153 wm_rt*wm_rt[t-4] + -0.244 repeat*wm_rt[t-1] + -0.063 repeat*wm_rt[t-3] + 0.088 repeat*wm_rt[t-4] + -0.051 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.077 switch*wm_rt[t-3] + 0.133 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.008 wm_rt[t-1]*wm_rt[t-2] + 0.005 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.064 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 95, 0, -, -, 11, 0, 0, 0, 0, 1, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, 74, 0, 0, 68, 87, 86, 66, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 211/400 --- L(Train): 0.3801398 --- L(Val, RNN): 0.3521100 --- L(Val, SINDy): 0.6978589 --- Time: 3.99s; --- Convergence: 1.00e-03; LR: 1.00e-02; Metric: 0.3516414; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 36):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.583 value_reward_diff[t] + 1.712 reward_diff + -0.528 value_reward_diff^2 + 0.098 value_reward_diff*reward_diff + 1.699 reward_diff^2 \n",
      "value_persistance[t+1] = -0.392 1 + 0.763 value_persistance[t] + -0.077 repeat + 0.352 value_persistance^2 + -0.182 value_persistance*repeat + -0.079 repeat^2 \n",
      "wm_rt[t+1] = -0.016 1 + 1.054 wm_rt[t] + -0.039 wm_rt[t-1] + 0.18 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.063 wm_rt[t-4] + 0.068 wm_rt^2 + -0.046 wm_rt*repeat + 0.066 wm_rt*wm_rt[t-1] + -0.154 wm_rt*wm_rt[t-4] + -0.24 repeat*wm_rt[t-1] + -0.063 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + -0.05 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.077 switch*wm_rt[t-3] + 0.132 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.009 wm_rt[t-1]*wm_rt[t-2] + 0.006 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.065 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 96, 0, -, -, 12, 0, 0, 0, 0, 2, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, 75, 0, 0, 69, 88, 87, 67, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 212/400 --- L(Train): 0.3815905 --- L(Val, RNN): 0.3520148 --- L(Val, SINDy): 0.5415617 --- Time: 4.36s; --- Convergence: 5.49e-04; LR: 1.00e-02; Metric: 0.3516414; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 36):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.583 value_reward_diff[t] + 1.718 reward_diff + -0.528 value_reward_diff^2 + 0.097 value_reward_diff*reward_diff + 1.704 reward_diff^2 \n",
      "value_persistance[t+1] = -0.392 1 + 0.762 value_persistance[t] + -0.077 repeat + 0.353 value_persistance^2 + -0.182 value_persistance*repeat + -0.078 repeat^2 \n",
      "wm_rt[t+1] = -0.017 1 + 1.053 wm_rt[t] + -0.036 wm_rt[t-1] + 0.181 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.063 wm_rt[t-4] + 0.067 wm_rt^2 + -0.043 wm_rt*repeat + 0.071 wm_rt*wm_rt[t-1] + -0.154 wm_rt*wm_rt[t-4] + -0.236 repeat*wm_rt[t-1] + -0.062 repeat*wm_rt[t-3] + 0.09 repeat*wm_rt[t-4] + -0.049 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.077 switch*wm_rt[t-3] + 0.132 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + 0.011 wm_rt[t-1]*wm_rt[t-2] + 0.008 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.064 wm_rt[t-2]*wm_rt[t-3] + -0.061 wm_rt[t-2]*wm_rt[t-4] + -0.065 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 97, 0, -, -, 13, 0, 0, 0, 0, 3, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 1, 76, 0, 0, 70, 89, 88, 68, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 213/400 --- L(Train): 0.3829588 --- L(Val, RNN): 0.3502961 --- L(Val, SINDy): 0.5163708 --- Time: 4.78s; --- Convergence: 1.13e-03; LR: 1.00e-02; Metric: 0.3502961; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 35):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.582 value_reward_diff[t] + 1.724 reward_diff + -0.528 value_reward_diff^2 + 0.097 value_reward_diff*reward_diff + 1.71 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.762 value_persistance[t] + -0.076 repeat + 0.353 value_persistance^2 + -0.183 value_persistance*repeat + -0.078 repeat^2 \n",
      "wm_rt[t+1] = -0.018 1 + 1.052 wm_rt[t] + -0.033 wm_rt[t-1] + 0.182 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.063 wm_rt[t-4] + 0.067 wm_rt^2 + -0.04 wm_rt*repeat + 0.077 wm_rt*wm_rt[t-1] + -0.154 wm_rt*wm_rt[t-4] + -0.231 repeat*wm_rt[t-1] + -0.062 repeat*wm_rt[t-3] + 0.091 repeat*wm_rt[t-4] + -0.046 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.076 switch*wm_rt[t-3] + 0.131 switch*wm_rt[t-4] + 0.005 wm_rt[t-1]^2 + 0.014 wm_rt[t-1]*wm_rt[t-2] + 0.011 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.067 wm_rt[t-2]^2 + -0.063 wm_rt[t-2]*wm_rt[t-3] + -0.06 wm_rt[t-2]*wm_rt[t-4] + -0.066 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 98, 0, -, -, 14, 0, 0, 0, 0, 4, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 2, 77, 0, 0, 71, 90, 89, 69, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 214/400 --- L(Train): 0.3752081 --- L(Val, RNN): 0.3503493 --- L(Val, SINDy): 0.5516144 --- Time: 3.70s; --- Convergence: 5.93e-04; LR: 1.00e-02; Metric: 0.3502961; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 35):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.582 value_reward_diff[t] + 1.729 reward_diff + -0.527 value_reward_diff^2 + 0.097 value_reward_diff*reward_diff + 1.715 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.762 value_persistance[t] + -0.076 repeat + 0.353 value_persistance^2 + -0.183 value_persistance*repeat + -0.078 repeat^2 \n",
      "wm_rt[t+1] = -0.018 1 + 1.051 wm_rt[t] + -0.03 wm_rt[t-1] + 0.184 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.063 wm_rt[t-4] + 0.066 wm_rt^2 + -0.037 wm_rt*repeat + 0.083 wm_rt*wm_rt[t-1] + -0.154 wm_rt*wm_rt[t-4] + -0.226 repeat*wm_rt[t-1] + -0.062 repeat*wm_rt[t-3] + 0.092 repeat*wm_rt[t-4] + -0.044 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.076 switch*wm_rt[t-3] + 0.131 switch*wm_rt[t-4] + 0.008 wm_rt[t-1]^2 + 0.016 wm_rt[t-1]*wm_rt[t-2] + 0.013 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.062 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.066 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 99, 0, -, -, 15, 0, 0, 0, 0, 5, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 3, 78, 0, 0, 72, 91, 90, 70, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 215/400 --- L(Train): 0.3764844 --- L(Val, RNN): 0.3500100 --- L(Val, SINDy): 0.6224126 --- Time: 4.05s; --- Convergence: 4.66e-04; LR: 1.00e-02; Metric: 0.3500100; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 34):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.584 value_reward_diff[t] + 1.736 reward_diff + -0.525 value_reward_diff^2 + 0.098 value_reward_diff*reward_diff + 1.721 reward_diff^2 \n",
      "value_persistance[t+1] = -0.394 1 + 0.762 value_persistance[t] + -0.077 repeat + 0.353 value_persistance^2 + -0.182 value_persistance*repeat + -0.078 repeat^2 \n",
      "wm_rt[t+1] = 1.05 wm_rt[t] + -0.028 wm_rt[t-1] + 0.184 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.062 wm_rt[t-4] + 0.066 wm_rt^2 + -0.034 wm_rt*repeat + 0.089 wm_rt*wm_rt[t-1] + -0.154 wm_rt*wm_rt[t-4] + -0.221 repeat*wm_rt[t-1] + -0.063 repeat*wm_rt[t-3] + 0.092 repeat*wm_rt[t-4] + -0.043 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.076 switch*wm_rt[t-3] + 0.13 switch*wm_rt[t-4] + 0.009 wm_rt[t-1]^2 + 0.017 wm_rt[t-1]*wm_rt[t-2] + 0.014 wm_rt[t-1]*wm_rt[t-3] + 0.005 wm_rt[t-1]*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.062 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.067 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 0, -, -, 16, 0, 0, 0, 0, 6, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 4, 79, 0, 0, 73, 92, 91, 71, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 216/400 --- L(Train): 0.3831729 --- L(Val, RNN): 0.3478532 --- L(Val, SINDy): 0.7305207 --- Time: 4.74s; --- Convergence: 1.31e-03; LR: 1.00e-02; Metric: 0.3478532; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 34):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.587 value_reward_diff[t] + 1.742 reward_diff + -0.521 value_reward_diff^2 + 0.1 value_reward_diff*reward_diff + 1.728 reward_diff^2 \n",
      "value_persistance[t+1] = -0.395 1 + 0.762 value_persistance[t] + -0.077 repeat + 0.353 value_persistance^2 + -0.182 value_persistance*repeat + -0.078 repeat^2 \n",
      "wm_rt[t+1] = 1.045 wm_rt[t] + -0.028 wm_rt[t-1] + 0.184 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.06 wm_rt[t-4] + 0.062 wm_rt^2 + -0.033 wm_rt*repeat + 0.089 wm_rt*wm_rt[t-1] + -0.156 wm_rt*wm_rt[t-4] + -0.219 repeat*wm_rt[t-1] + -0.065 repeat*wm_rt[t-3] + 0.091 repeat*wm_rt[t-4] + -0.043 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.077 switch*wm_rt[t-3] + 0.128 switch*wm_rt[t-4] + 0.009 wm_rt[t-1]^2 + 0.017 wm_rt[t-1]*wm_rt[t-2] + 0.014 wm_rt[t-1]*wm_rt[t-3] + 0.005 wm_rt[t-1]*wm_rt[t-4] + -0.067 wm_rt[t-2]^2 + -0.063 wm_rt[t-2]*wm_rt[t-3] + -0.06 wm_rt[t-2]*wm_rt[t-4] + -0.069 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 1, -, -, 17, 0, 0, 0, 0, 7, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 5, 80, 0, 0, 74, 93, 92, 72, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 217/400 --- L(Train): 0.3774083 --- L(Val, RNN): 0.3509696 --- L(Val, SINDy): 0.6963665 --- Time: 3.85s; --- Convergence: 2.21e-03; LR: 1.00e-02; Metric: 0.3478532; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 34):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.589 value_reward_diff[t] + 1.749 reward_diff + -0.518 value_reward_diff^2 + 0.102 value_reward_diff*reward_diff + 1.734 reward_diff^2 \n",
      "value_persistance[t+1] = -0.394 1 + 0.761 value_persistance[t] + -0.076 repeat + 0.354 value_persistance^2 + -0.183 value_persistance*repeat + -0.077 repeat^2 \n",
      "wm_rt[t+1] = 1.036 wm_rt[t] + -0.03 wm_rt[t-1] + 0.181 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.057 wm_rt[t-4] + 0.057 wm_rt^2 + -0.033 wm_rt*repeat + 0.086 wm_rt*wm_rt[t-1] + -0.157 wm_rt*wm_rt[t-4] + -0.22 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.088 repeat*wm_rt[t-4] + -0.045 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.079 switch*wm_rt[t-3] + 0.125 switch*wm_rt[t-4] + 0.008 wm_rt[t-1]^2 + 0.016 wm_rt[t-1]*wm_rt[t-2] + 0.013 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.069 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.072 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 2, -, -, 18, 0, 0, 0, 0, 8, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 6, 81, 0, 0, 75, 94, 93, 73, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 218/400 --- L(Train): 0.3761300 --- L(Val, RNN): 0.3469640 --- L(Val, SINDy): 0.6778061 --- Time: 3.48s; --- Convergence: 3.11e-03; LR: 1.00e-02; Metric: 0.3469640; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.589 value_reward_diff[t] + 1.756 reward_diff + -0.517 value_reward_diff^2 + 0.101 value_reward_diff*reward_diff + 1.741 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.759 value_persistance[t] + -0.074 repeat + 0.356 value_persistance^2 + -0.184 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 1.027 wm_rt[t] + -0.033 wm_rt[t-1] + 0.178 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.053 wm_rt[t-4] + 0.052 wm_rt^2 + -0.034 wm_rt*repeat + 0.082 wm_rt*wm_rt[t-1] + -0.159 wm_rt*wm_rt[t-4] + -0.221 repeat*wm_rt[t-1] + -0.072 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + -0.048 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.082 switch*wm_rt[t-3] + 0.121 switch*wm_rt[t-4] + 0.005 wm_rt[t-1]^2 + 0.013 wm_rt[t-1]*wm_rt[t-2] + 0.01 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.076 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 3, -, -, 19, 0, 0, 0, 0, 9, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 7, 82, 0, 0, 76, 95, 94, 74, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 219/400 --- L(Train): 0.3783493 --- L(Val, RNN): 0.3483609 --- L(Val, SINDy): 0.5761052 --- Time: 3.70s; --- Convergence: 2.25e-03; LR: 1.00e-02; Metric: 0.3469640; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.052 1 + 0.59 value_reward_diff[t] + 1.763 reward_diff + -0.515 value_reward_diff^2 + 0.101 value_reward_diff*reward_diff + 1.748 reward_diff^2 \n",
      "value_persistance[t+1] = -0.392 1 + 0.757 value_persistance[t] + -0.073 repeat + 0.358 value_persistance^2 + -0.186 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 1.016 wm_rt[t] + -0.035 wm_rt[t-1] + 0.177 wm_rt[t-2] + 0.053 wm_rt[t-3] + 0.051 wm_rt[t-4] + 0.046 wm_rt^2 + -0.036 wm_rt*repeat + 0.076 wm_rt*wm_rt[t-1] + -0.16 wm_rt*wm_rt[t-4] + -0.224 repeat*wm_rt[t-1] + -0.075 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + -0.05 switch*wm_rt[t-1] + -0.006 switch*wm_rt[t-2] + -0.084 switch*wm_rt[t-3] + 0.119 switch*wm_rt[t-4] + 0.004 wm_rt[t-1]^2 + 0.012 wm_rt[t-1]*wm_rt[t-2] + 0.009 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.069 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.078 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 4, -, -, 20, 0, 0, 0, 1, 10, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 8, 83, 0, 0, 77, 96, 95, 75, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 220/400 --- L(Train): 0.3715366 --- L(Val, RNN): 0.3467727 --- L(Val, SINDy): 0.5060210 --- Time: 5.48s; --- Convergence: 1.92e-03; LR: 1.00e-02; Metric: 0.3467727; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.052 1 + 0.59 value_reward_diff[t] + 1.77 reward_diff + -0.514 value_reward_diff^2 + 0.102 value_reward_diff*reward_diff + 1.754 reward_diff^2 \n",
      "value_persistance[t+1] = -0.391 1 + 0.756 value_persistance[t] + -0.072 repeat + 0.358 value_persistance^2 + -0.186 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 1.007 wm_rt[t] + -0.036 wm_rt[t-1] + 0.177 wm_rt[t-2] + 0.052 wm_rt[t-3] + 0.05 wm_rt[t-4] + 0.04 wm_rt^2 + -0.038 wm_rt*repeat + 0.071 wm_rt*wm_rt[t-1] + -0.161 wm_rt*wm_rt[t-4] + -0.226 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + -0.05 switch*wm_rt[t-1] + -0.007 switch*wm_rt[t-2] + -0.084 switch*wm_rt[t-3] + 0.118 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + 0.012 wm_rt[t-1]*wm_rt[t-2] + 0.008 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.069 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.078 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 5, -, -, 21, 0, 0, 1, 2, 11, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, 84, 0, 0, 78, 97, 96, 76, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 221/400 --- L(Train): 0.3765546 --- L(Val, RNN): 0.3478671 --- L(Val, SINDy): 0.5266228 --- Time: 6.01s; --- Convergence: 1.51e-03; LR: 1.00e-02; Metric: 0.3467727; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.052 1 + 0.59 value_reward_diff[t] + 1.777 reward_diff + -0.513 value_reward_diff^2 + 0.102 value_reward_diff*reward_diff + 1.761 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.757 value_persistance[t] + -0.073 repeat + 0.357 value_persistance^2 + -0.185 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.997 wm_rt[t] + -0.035 wm_rt[t-1] + 0.179 wm_rt[t-2] + 0.054 wm_rt[t-3] + 0.051 wm_rt[t-4] + 0.035 wm_rt^2 + -0.039 wm_rt*repeat + 0.066 wm_rt*wm_rt[t-1] + -0.162 wm_rt*wm_rt[t-4] + -0.228 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.085 repeat*wm_rt[t-4] + -0.049 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.082 switch*wm_rt[t-3] + 0.12 switch*wm_rt[t-4] + 0.005 wm_rt[t-1]^2 + 0.013 wm_rt[t-1]*wm_rt[t-2] + 0.01 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.067 wm_rt[t-2]*wm_rt[t-3] + -0.064 wm_rt[t-2]*wm_rt[t-4] + -0.076 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 6, -, -, 22, 0, 0, 0, 3, 12, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 1, 85, 0, 0, 79, 98, 97, 77, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 222/400 --- L(Train): 0.3712677 --- L(Val, RNN): 0.3433875 --- L(Val, SINDy): 0.5904573 --- Time: 4.28s; --- Convergence: 2.99e-03; LR: 1.00e-02; Metric: 0.3433875; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.052 1 + 0.59 value_reward_diff[t] + 1.784 reward_diff + -0.512 value_reward_diff^2 + 0.102 value_reward_diff*reward_diff + 1.768 reward_diff^2 \n",
      "value_persistance[t+1] = -0.395 1 + 0.759 value_persistance[t] + -0.075 repeat + 0.356 value_persistance^2 + -0.184 value_persistance*repeat + -0.077 repeat^2 \n",
      "wm_rt[t+1] = 0.989 wm_rt[t] + -0.034 wm_rt[t-1] + 0.181 wm_rt[t-2] + 0.056 wm_rt[t-3] + 0.054 wm_rt[t-4] + 0.031 wm_rt^2 + -0.04 wm_rt*repeat + 0.062 wm_rt*wm_rt[t-1] + -0.163 wm_rt*wm_rt[t-4] + -0.23 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.086 repeat*wm_rt[t-4] + -0.048 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.079 switch*wm_rt[t-3] + 0.122 switch*wm_rt[t-4] + 0.006 wm_rt[t-1]^2 + 0.015 wm_rt[t-1]*wm_rt[t-2] + 0.011 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.067 wm_rt[t-2]^2 + -0.064 wm_rt[t-2]*wm_rt[t-3] + -0.061 wm_rt[t-2]*wm_rt[t-4] + -0.074 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 7, -, -, 23, 0, 0, 0, 4, 13, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 2, 86, 0, 0, 80, 99, 98, 78, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 223/400 --- L(Train): 0.3761177 --- L(Val, RNN): 0.3456516 --- L(Val, SINDy): 0.5943928 --- Time: 4.20s; --- Convergence: 2.63e-03; LR: 1.00e-02; Metric: 0.3433875; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.051 1 + 0.592 value_reward_diff[t] + 1.791 reward_diff + -0.509 value_reward_diff^2 + 0.103 value_reward_diff*reward_diff + 1.775 reward_diff^2 \n",
      "value_persistance[t+1] = -0.397 1 + 0.76 value_persistance[t] + -0.076 repeat + 0.355 value_persistance^2 + -0.182 value_persistance*repeat + -0.078 repeat^2 \n",
      "wm_rt[t+1] = 0.982 wm_rt[t] + -0.034 wm_rt[t-1] + 0.183 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.055 wm_rt[t-4] + 0.027 wm_rt^2 + -0.041 wm_rt*repeat + 0.059 wm_rt*wm_rt[t-1] + -0.164 wm_rt*wm_rt[t-4] + -0.231 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.088 repeat*wm_rt[t-4] + -0.048 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.077 switch*wm_rt[t-3] + 0.124 switch*wm_rt[t-4] + 0.007 wm_rt[t-1]^2 + 0.013 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.062 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.072 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 8, -, -, 24, 0, 0, 0, 5, 14, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 3, 87, 0, 0, 81, -, 99, 79, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 224/400 --- L(Train): 0.3719874 --- L(Val, RNN): 0.3453329 --- L(Val, SINDy): 0.6278545 --- Time: 4.54s; --- Convergence: 1.47e-03; LR: 1.00e-02; Metric: 0.3433875; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.051 1 + 0.593 value_reward_diff[t] + 1.797 reward_diff + -0.507 value_reward_diff^2 + 0.104 value_reward_diff*reward_diff + 1.781 reward_diff^2 \n",
      "value_persistance[t+1] = -0.398 1 + 0.76 value_persistance[t] + -0.076 repeat + 0.354 value_persistance^2 + -0.182 value_persistance*repeat + -0.078 repeat^2 \n",
      "wm_rt[t+1] = 0.975 wm_rt[t] + -0.034 wm_rt[t-1] + 0.184 wm_rt[t-2] + 0.059 wm_rt[t-3] + 0.056 wm_rt[t-4] + 0.023 wm_rt^2 + -0.042 wm_rt*repeat + 0.056 wm_rt*wm_rt[t-1] + -0.164 wm_rt*wm_rt[t-4] + -0.232 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + -0.048 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.076 switch*wm_rt[t-3] + 0.124 switch*wm_rt[t-4] + 0.008 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.071 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 9, -, -, 25, 0, 0, 0, 6, 15, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 4, 88, 0, 0, 82, -, -, 80, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 225/400 --- L(Train): 0.3712281 --- L(Val, RNN): 0.3430552 --- L(Val, SINDy): 0.6672872 --- Time: 4.60s; --- Convergence: 1.88e-03; LR: 1.00e-02; Metric: 0.3430552; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.051 1 + 0.594 value_reward_diff[t] + 1.804 reward_diff + -0.506 value_reward_diff^2 + 0.105 value_reward_diff*reward_diff + 1.787 reward_diff^2 \n",
      "value_persistance[t+1] = -0.398 1 + 0.76 value_persistance[t] + -0.075 repeat + 0.355 value_persistance^2 + -0.183 value_persistance*repeat + -0.077 repeat^2 \n",
      "wm_rt[t+1] = 0.97 wm_rt[t] + -0.035 wm_rt[t-1] + 0.184 wm_rt[t-2] + 0.059 wm_rt[t-3] + 0.056 wm_rt[t-4] + 0.02 wm_rt^2 + -0.042 wm_rt*repeat + 0.055 wm_rt*wm_rt[t-1] + -0.165 wm_rt*wm_rt[t-4] + -0.232 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.089 repeat*wm_rt[t-4] + -0.049 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.076 switch*wm_rt[t-3] + 0.124 switch*wm_rt[t-4] + 0.006 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.071 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 10, -, -, 26, 0, 0, 0, 7, 16, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 5, 89, 0, 0, 83, -, -, 81, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 226/400 --- L(Train): 0.3650155 --- L(Val, RNN): 0.3460008 --- L(Val, SINDy): 0.6679980 --- Time: 4.13s; --- Convergence: 2.41e-03; LR: 1.00e-02; Metric: 0.3430552; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.051 1 + 0.596 value_reward_diff[t] + 1.81 reward_diff + -0.504 value_reward_diff^2 + 0.106 value_reward_diff*reward_diff + 1.794 reward_diff^2 \n",
      "value_persistance[t+1] = -0.396 1 + 0.757 value_persistance[t] + -0.072 repeat + 0.357 value_persistance^2 + -0.186 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 0.967 wm_rt[t] + -0.038 wm_rt[t-1] + 0.182 wm_rt[t-2] + 0.056 wm_rt[t-3] + 0.053 wm_rt[t-4] + 0.017 wm_rt^2 + -0.042 wm_rt*repeat + 0.054 wm_rt*wm_rt[t-1] + -0.165 wm_rt*wm_rt[t-4] + -0.23 repeat*wm_rt[t-1] + -0.08 repeat*wm_rt[t-3] + 0.087 repeat*wm_rt[t-4] + -0.052 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.079 switch*wm_rt[t-3] + 0.121 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.063 wm_rt[t-2]*wm_rt[t-3] + -0.06 wm_rt[t-2]*wm_rt[t-4] + -0.074 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 11, -, -, 27, 0, 0, 0, 8, 17, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, 90, 0, 0, 84, -, -, 82, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 227/400 --- L(Train): 0.3799177 --- L(Val, RNN): 0.3416687 --- L(Val, SINDy): 0.6299783 --- Time: 4.06s; --- Convergence: 3.37e-03; LR: 1.00e-02; Metric: 0.3416687; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.052 1 + 0.597 value_reward_diff[t] + 1.817 reward_diff + -0.502 value_reward_diff^2 + 0.107 value_reward_diff*reward_diff + 1.8 reward_diff^2 \n",
      "value_persistance[t+1] = -0.395 1 + 0.755 value_persistance[t] + -0.07 repeat + 0.359 value_persistance^2 + -0.188 value_persistance*repeat + -0.072 repeat^2 \n",
      "wm_rt[t+1] = 0.964 wm_rt[t] + -0.041 wm_rt[t-1] + 0.18 wm_rt[t-2] + 0.053 wm_rt[t-3] + 0.05 wm_rt[t-4] + 0.015 wm_rt^2 + -0.041 wm_rt*repeat + 0.055 wm_rt*wm_rt[t-1] + -0.166 wm_rt*wm_rt[t-4] + -0.228 repeat*wm_rt[t-1] + -0.082 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + -0.055 switch*wm_rt[t-1] + -0.004 switch*wm_rt[t-2] + -0.081 switch*wm_rt[t-3] + 0.118 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-4] + -0.069 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.077 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 12, -, -, 28, 0, 0, 0, 9, 18, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, 91, 0, 0, 85, -, -, 83, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 228/400 --- L(Train): 0.3788608 --- L(Val, RNN): 0.3439500 --- L(Val, SINDy): 0.5473695 --- Time: 4.32s; --- Convergence: 2.83e-03; LR: 1.00e-02; Metric: 0.3416687; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.597 value_reward_diff[t] + 1.823 reward_diff + -0.502 value_reward_diff^2 + 0.106 value_reward_diff*reward_diff + 1.806 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.754 value_persistance[t] + -0.069 repeat + 0.361 value_persistance^2 + -0.189 value_persistance*repeat + -0.07 repeat^2 \n",
      "wm_rt[t+1] = 0.962 wm_rt[t] + -0.041 wm_rt[t-1] + 0.179 wm_rt[t-2] + 0.052 wm_rt[t-3] + 0.048 wm_rt[t-4] + 0.014 wm_rt^2 + -0.04 wm_rt*repeat + 0.056 wm_rt*wm_rt[t-1] + -0.166 wm_rt*wm_rt[t-4] + -0.225 repeat*wm_rt[t-1] + -0.084 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + -0.056 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.082 switch*wm_rt[t-3] + 0.117 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.006 wm_rt[t-1]*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.079 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 13, -, -, 29, 0, 0, 1, 10, 19, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, 92, 0, 0, 86, -, -, 84, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 229/400 --- L(Train): 0.3693214 --- L(Val, RNN): 0.3436151 --- L(Val, SINDy): 0.4964711 --- Time: 3.59s; --- Convergence: 1.58e-03; LR: 1.00e-02; Metric: 0.3416687; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.594 value_reward_diff[t] + 1.829 reward_diff + -0.504 value_reward_diff^2 + 0.103 value_reward_diff*reward_diff + 1.812 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.753 value_persistance[t] + -0.069 repeat + 0.361 value_persistance^2 + -0.189 value_persistance*repeat + -0.071 repeat^2 \n",
      "wm_rt[t+1] = 0.961 wm_rt[t] + -0.04 wm_rt[t-1] + 0.18 wm_rt[t-2] + 0.052 wm_rt[t-3] + 0.048 wm_rt[t-4] + 0.012 wm_rt^2 + -0.039 wm_rt*repeat + 0.058 wm_rt*wm_rt[t-1] + -0.167 wm_rt*wm_rt[t-4] + -0.221 repeat*wm_rt[t-1] + -0.085 repeat*wm_rt[t-3] + 0.082 repeat*wm_rt[t-4] + -0.056 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.082 switch*wm_rt[t-3] + 0.116 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.08 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 14, -, -, 30, 0, 0, 2, 11, 20, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, 93, 0, 0, 87, -, -, 85, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 230/400 --- L(Train): 0.3672607 --- L(Val, RNN): 0.3406153 --- L(Val, SINDy): 0.4708936 --- Time: 3.78s; --- Convergence: 2.29e-03; LR: 1.00e-02; Metric: 0.3406153; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.595 value_reward_diff[t] + 1.835 reward_diff + -0.502 value_reward_diff^2 + 0.103 value_reward_diff*reward_diff + 1.818 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.753 value_persistance[t] + -0.071 repeat + 0.362 value_persistance^2 + -0.187 value_persistance*repeat + -0.072 repeat^2 \n",
      "wm_rt[t+1] = 0.96 wm_rt[t] + -0.038 wm_rt[t-1] + 0.181 wm_rt[t-2] + 0.052 wm_rt[t-3] + 0.048 wm_rt[t-4] + 0.011 wm_rt^2 + -0.038 wm_rt*repeat + 0.061 wm_rt*wm_rt[t-1] + -0.167 wm_rt*wm_rt[t-4] + -0.217 repeat*wm_rt[t-1] + -0.085 repeat*wm_rt[t-3] + 0.082 repeat*wm_rt[t-4] + -0.054 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + -0.081 switch*wm_rt[t-3] + 0.117 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.069 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.08 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 15, -, -, 31, 0, 0, 3, 12, 21, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, 94, 0, 0, 88, -, -, 86, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 231/400 --- L(Train): 0.3688669 --- L(Val, RNN): 0.3421704 --- L(Val, SINDy): 0.4781499 --- Time: 3.47s; --- Convergence: 1.92e-03; LR: 1.00e-02; Metric: 0.3406153; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.598 value_reward_diff[t] + 1.842 reward_diff + -0.498 value_reward_diff^2 + 0.105 value_reward_diff*reward_diff + 1.824 reward_diff^2 \n",
      "value_persistance[t+1] = -0.394 1 + 0.752 value_persistance[t] + -0.072 repeat + 0.362 value_persistance^2 + -0.186 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 0.96 wm_rt[t] + -0.035 wm_rt[t-1] + 0.183 wm_rt[t-2] + 0.053 wm_rt[t-3] + 0.049 wm_rt[t-4] + 0.01 wm_rt^2 + -0.037 wm_rt*repeat + 0.063 wm_rt*wm_rt[t-1] + -0.167 wm_rt*wm_rt[t-4] + -0.212 repeat*wm_rt[t-1] + -0.085 repeat*wm_rt[t-3] + 0.082 repeat*wm_rt[t-4] + -0.052 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.08 switch*wm_rt[t-3] + 0.117 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.067 wm_rt[t-2]^2 + -0.064 wm_rt[t-2]*wm_rt[t-3] + -0.061 wm_rt[t-2]*wm_rt[t-4] + -0.08 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 16, -, -, 32, 0, 0, 4, 13, 22, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, 95, 0, 0, 89, -, -, 87, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 232/400 --- L(Train): 0.3684065 --- L(Val, RNN): 0.3398055 --- L(Val, SINDy): 0.4729296 --- Time: 3.91s; --- Convergence: 2.14e-03; LR: 1.00e-02; Metric: 0.3398055; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.599 value_reward_diff[t] + 1.848 reward_diff + -0.497 value_reward_diff^2 + 0.104 value_reward_diff*reward_diff + 1.83 reward_diff^2 \n",
      "value_persistance[t+1] = -0.394 1 + 0.752 value_persistance[t] + -0.073 repeat + 0.363 value_persistance^2 + -0.184 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.96 wm_rt[t] + -0.032 wm_rt[t-1] + 0.185 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.049 wm_rt[t-4] + 0.009 wm_rt^2 + -0.035 wm_rt*repeat + 0.066 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.208 repeat*wm_rt[t-1] + -0.084 repeat*wm_rt[t-3] + 0.082 repeat*wm_rt[t-4] + -0.05 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.079 switch*wm_rt[t-3] + 0.118 switch*wm_rt[t-4] + 0.005 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.062 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.08 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 17, -, -, 33, 0, 0, 5, 14, 23, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 1, 96, 0, 0, 90, -, -, 88, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 233/400 --- L(Train): 0.3752493 --- L(Val, RNN): 0.3402133 --- L(Val, SINDy): 0.5316688 --- Time: 4.65s; --- Convergence: 1.28e-03; LR: 1.00e-02; Metric: 0.3398055; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.598 value_reward_diff[t] + 1.854 reward_diff + -0.497 value_reward_diff^2 + 0.102 value_reward_diff*reward_diff + 1.836 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.75 value_persistance[t] + -0.074 repeat + 0.364 value_persistance^2 + -0.184 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.961 wm_rt[t] + -0.031 wm_rt[t-1] + 0.186 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.048 wm_rt[t-4] + 0.009 wm_rt^2 + -0.034 wm_rt*repeat + 0.068 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.204 repeat*wm_rt[t-1] + -0.084 repeat*wm_rt[t-3] + 0.081 repeat*wm_rt[t-4] + -0.048 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.079 switch*wm_rt[t-3] + 0.117 switch*wm_rt[t-4] + 0.005 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.062 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.081 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 18, -, -, 34, 0, 0, 6, 15, 24, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 2, 97, 0, 0, 91, -, -, 89, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 234/400 --- L(Train): 0.3712935 --- L(Val, RNN): 0.3394431 --- L(Val, SINDy): 0.5686166 --- Time: 5.28s; --- Convergence: 1.02e-03; LR: 1.00e-02; Metric: 0.3394431; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.6 value_reward_diff[t] + 1.861 reward_diff + -0.494 value_reward_diff^2 + 0.103 value_reward_diff*reward_diff + 1.842 reward_diff^2 \n",
      "value_persistance[t+1] = -0.392 1 + 0.749 value_persistance[t] + -0.074 repeat + 0.365 value_persistance^2 + -0.184 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.962 wm_rt[t] + -0.03 wm_rt[t-1] + 0.186 wm_rt[t-2] + 0.054 wm_rt[t-3] + 0.047 wm_rt[t-4] + 0.008 wm_rt^2 + -0.033 wm_rt*repeat + 0.071 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.201 repeat*wm_rt[t-1] + -0.084 repeat*wm_rt[t-3] + 0.08 repeat*wm_rt[t-4] + -0.048 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.079 switch*wm_rt[t-3] + 0.116 switch*wm_rt[t-4] + 0.005 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.067 wm_rt[t-2]^2 + -0.063 wm_rt[t-2]*wm_rt[t-3] + -0.06 wm_rt[t-2]*wm_rt[t-4] + -0.083 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 19, -, -, 35, 0, 0, 7, 16, 25, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 3, 98, 0, 0, 92, -, -, 90, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 235/400 --- L(Train): 0.3643494 --- L(Val, RNN): 0.3386018 --- L(Val, SINDy): 0.5539800 --- Time: 3.65s; --- Convergence: 9.32e-04; LR: 1.00e-02; Metric: 0.3386018; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.601 value_reward_diff[t] + 1.868 reward_diff + -0.491 value_reward_diff^2 + 0.104 value_reward_diff*reward_diff + 1.849 reward_diff^2 \n",
      "value_persistance[t+1] = -0.39 1 + 0.746 value_persistance[t] + -0.074 repeat + 0.367 value_persistance^2 + -0.184 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.962 wm_rt[t] + -0.031 wm_rt[t-1] + 0.185 wm_rt[t-2] + 0.053 wm_rt[t-3] + 0.045 wm_rt[t-4] + 0.008 wm_rt^2 + -0.032 wm_rt*repeat + 0.073 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.198 repeat*wm_rt[t-1] + -0.084 repeat*wm_rt[t-3] + 0.079 repeat*wm_rt[t-4] + -0.049 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.081 switch*wm_rt[t-3] + 0.114 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.086 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 20, -, -, 36, 0, 0, 8, 17, 26, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 4, 99, 0, 0, 93, -, -, 91, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 236/400 --- L(Train): 0.3704686 --- L(Val, RNN): 0.3414581 --- L(Val, SINDy): 0.5292729 --- Time: 4.03s; --- Convergence: 1.89e-03; LR: 1.00e-02; Metric: 0.3386018; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.602 value_reward_diff[t] + 1.874 reward_diff + -0.49 value_reward_diff^2 + 0.104 value_reward_diff*reward_diff + 1.855 reward_diff^2 \n",
      "value_persistance[t+1] = -0.388 1 + 0.745 value_persistance[t] + -0.074 repeat + 0.369 value_persistance^2 + -0.184 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.963 wm_rt[t] + -0.031 wm_rt[t-1] + 0.184 wm_rt[t-2] + 0.052 wm_rt[t-3] + 0.043 wm_rt[t-4] + 0.007 wm_rt^2 + -0.032 wm_rt*repeat + 0.076 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.196 repeat*wm_rt[t-1] + -0.084 repeat*wm_rt[t-3] + 0.078 repeat*wm_rt[t-4] + -0.05 switch*wm_rt[t-1] + -0.082 switch*wm_rt[t-3] + 0.112 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.069 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.088 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 21, -, -, 37, 0, 0, 9, 18, 27, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, 94, -, -, 92, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 237/400 --- L(Train): 0.3662750 --- L(Val, RNN): 0.3372492 --- L(Val, SINDy): 0.4464039 --- Time: 4.92s; --- Convergence: 3.05e-03; LR: 1.00e-02; Metric: 0.3372492; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.605 value_reward_diff[t] + 1.881 reward_diff + -0.486 value_reward_diff^2 + 0.106 value_reward_diff*reward_diff + 1.861 reward_diff^2 \n",
      "value_persistance[t+1] = -0.387 1 + 0.743 value_persistance[t] + -0.074 repeat + 0.371 value_persistance^2 + -0.184 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.965 wm_rt[t] + -0.03 wm_rt[t-1] + 0.185 wm_rt[t-2] + 0.052 wm_rt[t-3] + 0.043 wm_rt[t-4] + 0.007 wm_rt^2 + -0.031 wm_rt*repeat + 0.078 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.193 repeat*wm_rt[t-1] + -0.082 repeat*wm_rt[t-3] + 0.078 repeat*wm_rt[t-4] + -0.049 switch*wm_rt[t-1] + -0.082 switch*wm_rt[t-3] + 0.112 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.069 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.088 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 22, -, -, 38, 0, 0, 10, 19, 28, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 1, -, 0, 0, 95, -, -, 93, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 238/400 --- L(Train): 0.3657957 --- L(Val, RNN): 0.3360905 --- L(Val, SINDy): 0.4071403 --- Time: 4.56s; --- Convergence: 2.11e-03; LR: 1.00e-02; Metric: 0.3360905; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.606 value_reward_diff[t] + 1.888 reward_diff + -0.483 value_reward_diff^2 + 0.107 value_reward_diff*reward_diff + 1.868 reward_diff^2 \n",
      "value_persistance[t+1] = -0.387 1 + 0.742 value_persistance[t] + -0.074 repeat + 0.371 value_persistance^2 + -0.183 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.966 wm_rt[t] + -0.027 wm_rt[t-1] + 0.187 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.045 wm_rt[t-4] + 0.007 wm_rt^2 + -0.03 wm_rt*repeat + 0.081 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.19 repeat*wm_rt[t-1] + -0.08 repeat*wm_rt[t-3] + 0.081 repeat*wm_rt[t-4] + -0.047 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-3] + 0.113 switch*wm_rt[t-4] + 0.004 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.064 wm_rt[t-2]*wm_rt[t-3] + -0.061 wm_rt[t-2]*wm_rt[t-4] + -0.087 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 23, -, -, 39, 0, 0, 11, 20, 29, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 2, -, 0, 0, 96, -, -, 94, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 239/400 --- L(Train): 0.3602305 --- L(Val, RNN): 0.3373370 --- L(Val, SINDy): 0.4373196 --- Time: 4.49s; --- Convergence: 1.68e-03; LR: 1.00e-02; Metric: 0.3360905; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.604 value_reward_diff[t] + 1.893 reward_diff + -0.484 value_reward_diff^2 + 0.105 value_reward_diff*reward_diff + 1.873 reward_diff^2 \n",
      "value_persistance[t+1] = -0.387 1 + 0.742 value_persistance[t] + -0.075 repeat + 0.371 value_persistance^2 + -0.182 value_persistance*repeat + -0.077 repeat^2 \n",
      "wm_rt[t+1] = 0.968 wm_rt[t] + -0.024 wm_rt[t-1] + 0.19 wm_rt[t-2] + 0.057 wm_rt[t-3] + 0.047 wm_rt[t-4] + 0.007 wm_rt^2 + -0.03 wm_rt*repeat + 0.084 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.188 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + -0.044 switch*wm_rt[t-1] + -0.079 switch*wm_rt[t-3] + 0.114 switch*wm_rt[t-4] + 0.006 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.062 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.086 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 24, -, -, 40, 0, 0, 12, 21, 30, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 3, -, 0, 0, 97, -, -, 95, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 240/400 --- L(Train): 0.3684425 --- L(Val, RNN): 0.3349297 --- L(Val, SINDy): 0.4529174 --- Time: 4.36s; --- Convergence: 2.04e-03; LR: 1.00e-02; Metric: 0.3349297; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.604 value_reward_diff[t] + 1.9 reward_diff + -0.482 value_reward_diff^2 + 0.105 value_reward_diff*reward_diff + 1.879 reward_diff^2 \n",
      "value_persistance[t+1] = -0.388 1 + 0.742 value_persistance[t] + -0.076 repeat + 0.371 value_persistance^2 + -0.182 value_persistance*repeat + -0.078 repeat^2 \n",
      "wm_rt[t+1] = 0.97 wm_rt[t] + -0.021 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.048 wm_rt[t-4] + 0.007 wm_rt^2 + -0.029 wm_rt*repeat + 0.086 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.186 repeat*wm_rt[t-1] + -0.075 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + -0.042 switch*wm_rt[t-1] + -0.077 switch*wm_rt[t-3] + 0.115 switch*wm_rt[t-4] + 0.007 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.085 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 25, -, -, 41, 0, 0, 13, 22, 31, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 4, -, 0, 0, 98, -, -, 96, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 241/400 --- L(Train): 0.3602788 --- L(Val, RNN): 0.3378578 --- L(Val, SINDy): 0.4560818 --- Time: 3.61s; --- Convergence: 2.48e-03; LR: 1.00e-02; Metric: 0.3349297; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.606 value_reward_diff[t] + 1.907 reward_diff + -0.479 value_reward_diff^2 + 0.107 value_reward_diff*reward_diff + 1.885 reward_diff^2 \n",
      "value_persistance[t+1] = -0.389 1 + 0.743 value_persistance[t] + -0.077 repeat + 0.371 value_persistance^2 + -0.181 value_persistance*repeat + -0.078 repeat^2 \n",
      "wm_rt[t+1] = 0.972 wm_rt[t] + -0.02 wm_rt[t-1] + 0.192 wm_rt[t-2] + 0.059 wm_rt[t-3] + 0.048 wm_rt[t-4] + 0.007 wm_rt^2 + -0.029 wm_rt*repeat + 0.089 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.185 repeat*wm_rt[t-1] + -0.074 repeat*wm_rt[t-3] + 0.084 repeat*wm_rt[t-4] + -0.041 switch*wm_rt[t-1] + -0.077 switch*wm_rt[t-3] + 0.115 switch*wm_rt[t-4] + 0.008 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.086 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 26, -, -, 42, 0, 0, 14, 23, 32, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 5, -, 0, 0, 99, -, -, 97, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 242/400 --- L(Train): 0.3659065 --- L(Val, RNN): 0.3343919 --- L(Val, SINDy): 0.5128184 --- Time: 3.12s; --- Convergence: 2.98e-03; LR: 1.00e-02; Metric: 0.3343919; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.606 value_reward_diff[t] + 1.913 reward_diff + -0.478 value_reward_diff^2 + 0.107 value_reward_diff*reward_diff + 1.891 reward_diff^2 \n",
      "value_persistance[t+1] = -0.391 1 + 0.744 value_persistance[t] + -0.077 repeat + 0.37 value_persistance^2 + -0.18 value_persistance*repeat + -0.079 repeat^2 \n",
      "wm_rt[t+1] = 0.974 wm_rt[t] + -0.019 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.047 wm_rt[t-4] + 0.007 wm_rt^2 + -0.029 wm_rt*repeat + 0.091 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.184 repeat*wm_rt[t-1] + -0.075 repeat*wm_rt[t-3] + 0.083 repeat*wm_rt[t-4] + -0.04 switch*wm_rt[t-1] + -0.078 switch*wm_rt[t-3] + 0.114 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.063 wm_rt[t-2]*wm_rt[t-3] + -0.06 wm_rt[t-2]*wm_rt[t-4] + -0.088 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 27, -, -, 43, 0, 0, 15, 24, 33, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 6, -, 0, 0, -, -, -, 98, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 243/400 --- L(Train): 0.3716421 --- L(Val, RNN): 0.3346872 --- L(Val, SINDy): 0.5221938 --- Time: 4.90s; --- Convergence: 1.64e-03; LR: 1.00e-02; Metric: 0.3343919; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.606 value_reward_diff[t] + 1.918 reward_diff + -0.476 value_reward_diff^2 + 0.107 value_reward_diff*reward_diff + 1.896 reward_diff^2 \n",
      "value_persistance[t+1] = -0.392 1 + 0.745 value_persistance[t] + -0.078 repeat + 0.369 value_persistance^2 + -0.18 value_persistance*repeat + -0.079 repeat^2 \n",
      "wm_rt[t+1] = 0.975 wm_rt[t] + -0.019 wm_rt[t-1] + 0.19 wm_rt[t-2] + 0.057 wm_rt[t-3] + 0.045 wm_rt[t-4] + 0.007 wm_rt^2 + -0.028 wm_rt*repeat + 0.094 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.183 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.081 repeat*wm_rt[t-4] + -0.04 switch*wm_rt[t-1] + -0.079 switch*wm_rt[t-3] + 0.112 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.064 wm_rt[t-2]*wm_rt[t-3] + -0.061 wm_rt[t-2]*wm_rt[t-4] + -0.09 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 28, -, -, 44, 0, 0, 16, 25, 34, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 7, -, 0, 0, -, -, -, 99, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 244/400 --- L(Train): 0.3663174 --- L(Val, RNN): 0.3341941 --- L(Val, SINDy): 0.5123182 --- Time: 3.88s; --- Convergence: 1.06e-03; LR: 1.00e-02; Metric: 0.3341941; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.605 value_reward_diff[t] + 1.924 reward_diff + -0.476 value_reward_diff^2 + 0.107 value_reward_diff*reward_diff + 1.901 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.745 value_persistance[t] + -0.077 repeat + 0.369 value_persistance^2 + -0.18 value_persistance*repeat + -0.079 repeat^2 \n",
      "wm_rt[t+1] = 0.977 wm_rt[t] + -0.02 wm_rt[t-1] + 0.189 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.043 wm_rt[t-4] + 0.007 wm_rt^2 + -0.028 wm_rt*repeat + 0.096 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.183 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.08 repeat*wm_rt[t-4] + -0.041 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-3] + 0.111 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.092 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 29, -, -, 45, 0, 0, 17, 26, 35, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 8, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 245/400 --- L(Train): 0.3582117 --- L(Val, RNN): 0.3327114 --- L(Val, SINDy): 0.4950001 --- Time: 3.68s; --- Convergence: 1.27e-03; LR: 1.00e-02; Metric: 0.3327114; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.605 value_reward_diff[t] + 1.93 reward_diff + -0.475 value_reward_diff^2 + 0.107 value_reward_diff*reward_diff + 1.907 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.744 value_persistance[t] + -0.075 repeat + 0.37 value_persistance^2 + -0.182 value_persistance*repeat + -0.077 repeat^2 \n",
      "wm_rt[t+1] = 0.979 wm_rt[t] + -0.02 wm_rt[t-1] + 0.188 wm_rt[t-2] + 0.054 wm_rt[t-3] + 0.041 wm_rt[t-4] + 0.007 wm_rt^2 + -0.028 wm_rt*repeat + 0.098 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.183 repeat*wm_rt[t-1] + -0.078 repeat*wm_rt[t-3] + 0.078 repeat*wm_rt[t-4] + -0.041 switch*wm_rt[t-1] + -0.082 switch*wm_rt[t-3] + 0.109 switch*wm_rt[t-4] + -0.071 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.094 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 30, -, -, 46, 0, 0, 18, 27, 36, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 9, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 246/400 --- L(Train): 0.3617372 --- L(Val, RNN): 0.3336352 --- L(Val, SINDy): 0.4435624 --- Time: 4.14s; --- Convergence: 1.10e-03; LR: 1.00e-02; Metric: 0.3327114; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.607 value_reward_diff[t] + 1.936 reward_diff + -0.472 value_reward_diff^2 + 0.109 value_reward_diff*reward_diff + 1.913 reward_diff^2 \n",
      "value_persistance[t+1] = -0.392 1 + 0.743 value_persistance[t] + -0.073 repeat + 0.371 value_persistance^2 + -0.184 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 0.98 wm_rt[t] + -0.02 wm_rt[t-1] + 0.188 wm_rt[t-2] + 0.054 wm_rt[t-3] + 0.04 wm_rt[t-4] + 0.007 wm_rt^2 + -0.028 wm_rt*repeat + 0.099 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.184 repeat*wm_rt[t-1] + -0.079 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.041 switch*wm_rt[t-1] + -0.082 switch*wm_rt[t-3] + 0.108 switch*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.096 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 31, -, -, 47, 0, 0, 19, 28, 37, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 10, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 247/400 --- L(Train): 0.3569545 --- L(Val, RNN): 0.3315488 --- L(Val, SINDy): 0.4507711 --- Time: 3.76s; --- Convergence: 1.59e-03; LR: 1.00e-02; Metric: 0.3315488; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.607 value_reward_diff[t] + 1.942 reward_diff + -0.471 value_reward_diff^2 + 0.11 value_reward_diff*reward_diff + 1.918 reward_diff^2 \n",
      "value_persistance[t+1] = -0.391 1 + 0.741 value_persistance[t] + -0.071 repeat + 0.372 value_persistance^2 + -0.186 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 0.982 wm_rt[t] + -0.018 wm_rt[t-1] + 0.189 wm_rt[t-2] + 0.054 wm_rt[t-3] + 0.04 wm_rt[t-4] + 0.008 wm_rt^2 + -0.028 wm_rt*repeat + 0.101 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.185 repeat*wm_rt[t-1] + -0.079 repeat*wm_rt[t-3] + 0.075 repeat*wm_rt[t-4] + -0.039 switch*wm_rt[t-1] + -0.081 switch*wm_rt[t-3] + 0.109 switch*wm_rt[t-4] + -0.071 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.096 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 32, -, -, 48, 0, 0, 20, 29, 38, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 11, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 248/400 --- L(Train): 0.3610939 --- L(Val, RNN): 0.3309983 --- L(Val, SINDy): 0.4464530 --- Time: 3.49s; --- Convergence: 1.07e-03; LR: 1.00e-02; Metric: 0.3309983; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.607 value_reward_diff[t] + 1.948 reward_diff + -0.47 value_reward_diff^2 + 0.11 value_reward_diff*reward_diff + 1.924 reward_diff^2 \n",
      "value_persistance[t+1] = -0.392 1 + 0.741 value_persistance[t] + -0.071 repeat + 0.372 value_persistance^2 + -0.186 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 0.983 wm_rt[t] + -0.016 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.056 wm_rt[t-3] + 0.041 wm_rt[t-4] + 0.008 wm_rt^2 + -0.028 wm_rt*repeat + 0.102 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.186 repeat*wm_rt[t-1] + -0.078 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.036 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-3] + 0.11 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.095 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 33, -, -, 49, 0, 0, 21, 30, 39, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 12, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 249/400 --- L(Train): 0.3574665 --- L(Val, RNN): 0.3340014 --- L(Val, SINDy): 0.4410345 --- Time: 3.51s; --- Convergence: 2.04e-03; LR: 1.00e-02; Metric: 0.3309983; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.607 value_reward_diff[t] + 1.953 reward_diff + -0.468 value_reward_diff^2 + 0.11 value_reward_diff*reward_diff + 1.929 reward_diff^2 \n",
      "value_persistance[t+1] = -0.393 1 + 0.742 value_persistance[t] + -0.072 repeat + 0.372 value_persistance^2 + -0.184 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 0.985 wm_rt[t] + -0.013 wm_rt[t-1] + 0.193 wm_rt[t-2] + 0.057 wm_rt[t-3] + 0.042 wm_rt[t-4] + 0.008 wm_rt^2 + -0.029 wm_rt*repeat + 0.103 wm_rt*wm_rt[t-1] + -0.168 wm_rt*wm_rt[t-4] + -0.187 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.034 switch*wm_rt[t-1] + -0.078 switch*wm_rt[t-3] + 0.111 switch*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.094 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 34, -, -, 50, 0, 0, 22, 31, 40, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 13, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 250/400 --- L(Train): 0.3587687 --- L(Val, RNN): 0.3307016 --- L(Val, SINDy): 0.4291784 --- Time: 3.62s; --- Convergence: 2.67e-03; LR: 1.00e-02; Metric: 0.3307016; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.605 value_reward_diff[t] + 1.959 reward_diff + -0.47 value_reward_diff^2 + 0.108 value_reward_diff*reward_diff + 1.934 reward_diff^2 \n",
      "value_persistance[t+1] = -0.394 1 + 0.743 value_persistance[t] + -0.074 repeat + 0.371 value_persistance^2 + -0.182 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.986 wm_rt[t] + -0.012 wm_rt[t-1] + 0.193 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.042 wm_rt[t-4] + 0.008 wm_rt^2 + -0.029 wm_rt*repeat + 0.104 wm_rt*wm_rt[t-1] + -0.169 wm_rt*wm_rt[t-4] + -0.189 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.032 switch*wm_rt[t-1] + -0.078 switch*wm_rt[t-3] + 0.111 switch*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.064 wm_rt[t-2]*wm_rt[t-3] + -0.061 wm_rt[t-2]*wm_rt[t-4] + -0.095 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 35, -, -, 51, 0, 0, 23, 32, 41, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 14, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 251/400 --- L(Train): 0.3571668 --- L(Val, RNN): 0.3311384 --- L(Val, SINDy): 0.4870162 --- Time: 3.66s; --- Convergence: 1.55e-03; LR: 1.00e-02; Metric: 0.3307016; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.606 value_reward_diff[t] + 1.965 reward_diff + -0.467 value_reward_diff^2 + 0.109 value_reward_diff*reward_diff + 1.939 reward_diff^2 \n",
      "value_persistance[t+1] = -0.396 1 + 0.744 value_persistance[t] + -0.076 repeat + 0.37 value_persistance^2 + -0.181 value_persistance*repeat + -0.077 repeat^2 \n",
      "wm_rt[t+1] = 0.987 wm_rt[t] + -0.011 wm_rt[t-1] + 0.193 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.041 wm_rt[t-4] + 0.008 wm_rt^2 + -0.03 wm_rt*repeat + 0.105 wm_rt*wm_rt[t-1] + -0.169 wm_rt*wm_rt[t-4] + -0.192 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.075 repeat*wm_rt[t-4] + -0.031 switch*wm_rt[t-1] + -0.078 switch*wm_rt[t-3] + 0.11 switch*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.064 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.096 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 36, -, -, 52, 0, 0, 24, 33, 42, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 15, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 252/400 --- L(Train): 0.3624892 --- L(Val, RNN): 0.3277607 --- L(Val, SINDy): 0.4888749 --- Time: 3.61s; --- Convergence: 2.47e-03; LR: 1.00e-02; Metric: 0.3277607; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.609 value_reward_diff[t] + 1.972 reward_diff + -0.463 value_reward_diff^2 + 0.111 value_reward_diff*reward_diff + 1.946 reward_diff^2 \n",
      "value_persistance[t+1] = -0.396 1 + 0.743 value_persistance[t] + -0.076 repeat + 0.37 value_persistance^2 + -0.18 value_persistance*repeat + -0.078 repeat^2 \n",
      "wm_rt[t+1] = 0.988 wm_rt[t] + -0.01 wm_rt[t-1] + 0.193 wm_rt[t-2] + 0.057 wm_rt[t-3] + 0.04 wm_rt[t-4] + 0.008 wm_rt^2 + -0.03 wm_rt*repeat + 0.105 wm_rt*wm_rt[t-1] + -0.169 wm_rt*wm_rt[t-4] + -0.196 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.075 repeat*wm_rt[t-4] + -0.03 switch*wm_rt[t-1] + -0.078 switch*wm_rt[t-3] + 0.109 switch*wm_rt[t-4] + -0.069 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.097 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 37, -, -, 53, 0, 0, 25, 34, 43, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 16, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 253/400 --- L(Train): 0.3612432 --- L(Val, RNN): 0.3296055 --- L(Val, SINDy): 0.4847864 --- Time: 3.57s; --- Convergence: 2.16e-03; LR: 1.00e-02; Metric: 0.3277607; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.609 value_reward_diff[t] + 1.978 reward_diff + -0.461 value_reward_diff^2 + 0.111 value_reward_diff*reward_diff + 1.951 reward_diff^2 \n",
      "value_persistance[t+1] = -0.395 1 + 0.742 value_persistance[t] + -0.076 repeat + 0.371 value_persistance^2 + -0.181 value_persistance*repeat + -0.077 repeat^2 \n",
      "wm_rt[t+1] = 0.988 wm_rt[t] + -0.01 wm_rt[t-1] + 0.192 wm_rt[t-2] + 0.057 wm_rt[t-3] + 0.039 wm_rt[t-4] + 0.008 wm_rt^2 + -0.03 wm_rt*repeat + 0.106 wm_rt*wm_rt[t-1] + -0.169 wm_rt*wm_rt[t-4] + -0.199 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.074 repeat*wm_rt[t-4] + -0.03 switch*wm_rt[t-1] + -0.079 switch*wm_rt[t-3] + 0.107 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.099 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 38, -, -, 54, 0, 0, 26, 35, 44, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 17, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 254/400 --- L(Train): 0.3631796 --- L(Val, RNN): 0.3284261 --- L(Val, SINDy): 0.4826920 --- Time: 4.42s; --- Convergence: 1.67e-03; LR: 1.00e-02; Metric: 0.3277607; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.61 value_reward_diff[t] + 1.985 reward_diff + -0.459 value_reward_diff^2 + 0.111 value_reward_diff*reward_diff + 1.958 reward_diff^2 \n",
      "value_persistance[t+1] = -0.394 1 + 0.741 value_persistance[t] + -0.074 repeat + 0.372 value_persistance^2 + -0.182 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.989 wm_rt[t] + -0.011 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.056 wm_rt[t-3] + 0.037 wm_rt[t-4] + 0.008 wm_rt^2 + -0.031 wm_rt*repeat + 0.106 wm_rt*wm_rt[t-1] + -0.169 wm_rt*wm_rt[t-4] + -0.203 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.074 repeat*wm_rt[t-4] + -0.03 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-3] + 0.106 switch*wm_rt[t-4] + -0.071 wm_rt[t-2]^2 + -0.067 wm_rt[t-2]*wm_rt[t-3] + -0.064 wm_rt[t-2]*wm_rt[t-4] + -0.1 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 39, -, -, 55, 0, 0, 27, 36, 45, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 18, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 255/400 --- L(Train): 0.3517542 --- L(Val, RNN): 0.3274153 --- L(Val, SINDy): 0.4257673 --- Time: 3.67s; --- Convergence: 1.34e-03; LR: 1.00e-02; Metric: 0.3274153; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.611 value_reward_diff[t] + 1.992 reward_diff + -0.456 value_reward_diff^2 + 0.113 value_reward_diff*reward_diff + 1.964 reward_diff^2 \n",
      "value_persistance[t+1] = -0.394 1 + 0.74 value_persistance[t] + -0.074 repeat + 0.373 value_persistance^2 + -0.182 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.99 wm_rt[t] + -0.012 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.036 wm_rt[t-4] + 0.008 wm_rt^2 + -0.032 wm_rt*repeat + 0.106 wm_rt*wm_rt[t-1] + -0.169 wm_rt*wm_rt[t-4] + -0.207 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + -0.03 switch*wm_rt[t-1] + -0.081 switch*wm_rt[t-3] + 0.104 switch*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.101 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 40, -, -, 56, 0, 0, 28, 37, 46, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 19, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 256/400 --- L(Train): 0.3522370 --- L(Val, RNN): 0.3295946 --- L(Val, SINDy): 0.4284626 --- Time: 4.27s; --- Convergence: 1.76e-03; LR: 1.00e-02; Metric: 0.3274153; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.611 value_reward_diff[t] + 1.998 reward_diff + -0.455 value_reward_diff^2 + 0.113 value_reward_diff*reward_diff + 1.97 reward_diff^2 \n",
      "value_persistance[t+1] = -0.394 1 + 0.74 value_persistance[t] + -0.073 repeat + 0.373 value_persistance^2 + -0.183 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.99 wm_rt[t] + -0.012 wm_rt[t-1] + 0.19 wm_rt[t-2] + 0.055 wm_rt[t-3] + 0.036 wm_rt[t-4] + 0.008 wm_rt^2 + -0.032 wm_rt*repeat + 0.107 wm_rt*wm_rt[t-1] + -0.169 wm_rt*wm_rt[t-4] + -0.212 repeat*wm_rt[t-1] + -0.075 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + -0.03 switch*wm_rt[t-1] + -0.082 switch*wm_rt[t-3] + 0.103 switch*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.102 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 41, -, -, 57, 0, 0, 29, 38, 47, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 20, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 257/400 --- L(Train): 0.3511505 --- L(Val, RNN): 0.3274062 --- L(Val, SINDy): 0.4267913 --- Time: 3.91s; --- Convergence: 1.97e-03; LR: 1.00e-02; Metric: 0.3274062; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.61 value_reward_diff[t] + 2.005 reward_diff + -0.455 value_reward_diff^2 + 0.112 value_reward_diff*reward_diff + 1.976 reward_diff^2 \n",
      "value_persistance[t+1] = -0.395 1 + 0.74 value_persistance[t] + -0.074 repeat + 0.373 value_persistance^2 + -0.182 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.991 wm_rt[t] + -0.013 wm_rt[t-1] + 0.19 wm_rt[t-2] + 0.056 wm_rt[t-3] + 0.035 wm_rt[t-4] + 0.008 wm_rt^2 + -0.033 wm_rt*repeat + 0.107 wm_rt*wm_rt[t-1] + -0.169 wm_rt*wm_rt[t-4] + -0.216 repeat*wm_rt[t-1] + -0.074 repeat*wm_rt[t-3] + 0.074 repeat*wm_rt[t-4] + -0.03 switch*wm_rt[t-1] + -0.082 switch*wm_rt[t-3] + 0.103 switch*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.069 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.102 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 42, -, -, 58, 0, 0, 30, 39, 48, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 21, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 258/400 --- L(Train): 0.3602524 --- L(Val, RNN): 0.3283031 --- L(Val, SINDy): 0.4217606 --- Time: 3.54s; --- Convergence: 1.44e-03; LR: 1.00e-02; Metric: 0.3274062; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.052 1 + 0.611 value_reward_diff[t] + 2.011 reward_diff + -0.453 value_reward_diff^2 + 0.114 value_reward_diff*reward_diff + 1.983 reward_diff^2 \n",
      "value_persistance[t+1] = -0.397 1 + 0.741 value_persistance[t] + -0.075 repeat + 0.372 value_persistance^2 + -0.181 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.991 wm_rt[t] + -0.012 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.057 wm_rt[t-3] + 0.036 wm_rt[t-4] + 0.008 wm_rt^2 + -0.033 wm_rt*repeat + 0.107 wm_rt*wm_rt[t-1] + -0.17 wm_rt*wm_rt[t-4] + -0.221 repeat*wm_rt[t-1] + -0.072 repeat*wm_rt[t-3] + 0.075 repeat*wm_rt[t-4] + -0.029 switch*wm_rt[t-1] + -0.081 switch*wm_rt[t-3] + 0.103 switch*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.102 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 43, -, -, 59, 0, 0, 31, 40, 49, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 22, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 259/400 --- L(Train): 0.3496601 --- L(Val, RNN): 0.3241635 --- L(Val, SINDy): 0.4197165 --- Time: 3.75s; --- Convergence: 2.79e-03; LR: 1.00e-02; Metric: 0.3241635; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.61 value_reward_diff[t] + 2.018 reward_diff + -0.452 value_reward_diff^2 + 0.113 value_reward_diff*reward_diff + 1.988 reward_diff^2 \n",
      "value_persistance[t+1] = -0.398 1 + 0.742 value_persistance[t] + -0.076 repeat + 0.371 value_persistance^2 + -0.18 value_persistance*repeat + -0.077 repeat^2 \n",
      "wm_rt[t+1] = 0.992 wm_rt[t] + -0.011 wm_rt[t-1] + 0.192 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.037 wm_rt[t-4] + 0.008 wm_rt^2 + -0.034 wm_rt*repeat + 0.106 wm_rt*wm_rt[t-1] + -0.17 wm_rt*wm_rt[t-4] + -0.226 repeat*wm_rt[t-1] + -0.071 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.028 switch*wm_rt[t-1] + -0.079 switch*wm_rt[t-3] + 0.104 switch*wm_rt[t-4] + -0.071 wm_rt[t-2]^2 + -0.067 wm_rt[t-2]*wm_rt[t-3] + -0.064 wm_rt[t-2]*wm_rt[t-4] + -0.101 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 44, -, -, 60, 0, 0, 32, 41, 50, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 23, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 260/400 --- L(Train): 0.3613530 --- L(Val, RNN): 0.3257336 --- L(Val, SINDy): 0.4741668 --- Time: 4.17s; --- Convergence: 2.18e-03; LR: 1.00e-02; Metric: 0.3241635; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.609 value_reward_diff[t] + 2.023 reward_diff + -0.452 value_reward_diff^2 + 0.112 value_reward_diff*reward_diff + 1.993 reward_diff^2 \n",
      "value_persistance[t+1] = -0.399 1 + 0.743 value_persistance[t] + -0.076 repeat + 0.37 value_persistance^2 + -0.18 value_persistance*repeat + -0.077 repeat^2 \n",
      "wm_rt[t+1] = 0.992 wm_rt[t] + -0.011 wm_rt[t-1] + 0.193 wm_rt[t-2] + 0.06 wm_rt[t-3] + 0.038 wm_rt[t-4] + 0.008 wm_rt^2 + -0.034 wm_rt*repeat + 0.106 wm_rt*wm_rt[t-1] + -0.17 wm_rt*wm_rt[t-4] + -0.231 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.027 switch*wm_rt[t-1] + -0.078 switch*wm_rt[t-3] + 0.105 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.1 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 45, -, -, 61, 0, 0, 33, 42, 51, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 24, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 261/400 --- L(Train): 0.3532757 --- L(Val, RNN): 0.3271685 --- L(Val, SINDy): 0.4749278 --- Time: 4.38s; --- Convergence: 1.81e-03; LR: 1.00e-02; Metric: 0.3241635; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.612 value_reward_diff[t] + 2.03 reward_diff + -0.449 value_reward_diff^2 + 0.115 value_reward_diff*reward_diff + 2.0 reward_diff^2 \n",
      "value_persistance[t+1] = -0.4 1 + 0.743 value_persistance[t] + -0.075 repeat + 0.371 value_persistance^2 + -0.18 value_persistance*repeat + -0.077 repeat^2 \n",
      "wm_rt[t+1] = 0.992 wm_rt[t] + -0.011 wm_rt[t-1] + 0.193 wm_rt[t-2] + 0.06 wm_rt[t-3] + 0.037 wm_rt[t-4] + 0.008 wm_rt^2 + -0.035 wm_rt*repeat + 0.106 wm_rt*wm_rt[t-1] + -0.171 wm_rt*wm_rt[t-4] + -0.237 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.026 switch*wm_rt[t-1] + -0.078 switch*wm_rt[t-3] + 0.105 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.1 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 46, -, -, 62, 0, 0, 34, 43, 52, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 25, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 262/400 --- L(Train): 0.3617637 --- L(Val, RNN): 0.3233854 --- L(Val, SINDy): 0.4924845 --- Time: 3.67s; --- Convergence: 2.79e-03; LR: 1.00e-02; Metric: 0.3233854; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.053 1 + 0.614 value_reward_diff[t] + 2.036 reward_diff + -0.446 value_reward_diff^2 + 0.117 value_reward_diff*reward_diff + 2.005 reward_diff^2 \n",
      "value_persistance[t+1] = -0.4 1 + 0.742 value_persistance[t] + -0.074 repeat + 0.371 value_persistance^2 + -0.181 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.993 wm_rt[t] + -0.012 wm_rt[t-1] + 0.192 wm_rt[t-2] + 0.06 wm_rt[t-3] + 0.037 wm_rt[t-4] + 0.008 wm_rt^2 + -0.035 wm_rt*repeat + 0.106 wm_rt*wm_rt[t-1] + -0.171 wm_rt*wm_rt[t-4] + -0.243 repeat*wm_rt[t-1] + -0.07 repeat*wm_rt[t-3] + 0.075 repeat*wm_rt[t-4] + -0.027 switch*wm_rt[t-1] + -0.078 switch*wm_rt[t-3] + 0.104 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.101 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 47, -, -, 63, 0, 0, 35, 44, 53, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 26, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 263/400 --- L(Train): 0.3533898 --- L(Val, RNN): 0.3253407 --- L(Val, SINDy): 0.4848839 --- Time: 3.72s; --- Convergence: 2.38e-03; LR: 1.00e-02; Metric: 0.3233854; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.614 value_reward_diff[t] + 2.041 reward_diff + -0.444 value_reward_diff^2 + 0.117 value_reward_diff*reward_diff + 2.01 reward_diff^2 \n",
      "value_persistance[t+1] = -0.4 1 + 0.742 value_persistance[t] + -0.073 repeat + 0.371 value_persistance^2 + -0.182 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.993 wm_rt[t] + -0.014 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.059 wm_rt[t-3] + 0.036 wm_rt[t-4] + 0.008 wm_rt^2 + -0.036 wm_rt*repeat + 0.106 wm_rt*wm_rt[t-1] + -0.171 wm_rt*wm_rt[t-4] + -0.249 repeat*wm_rt[t-1] + -0.071 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + -0.027 switch*wm_rt[t-1] + -0.078 switch*wm_rt[t-3] + 0.103 switch*wm_rt[t-4] + -0.071 wm_rt[t-2]^2 + -0.067 wm_rt[t-2]*wm_rt[t-3] + -0.064 wm_rt[t-2]*wm_rt[t-4] + -0.102 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 48, -, -, 64, 0, 0, 36, 45, 54, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 27, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 264/400 --- L(Train): 0.3454739 --- L(Val, RNN): 0.3250694 --- L(Val, SINDy): 0.4290259 --- Time: 4.56s; --- Convergence: 1.32e-03; LR: 1.00e-02; Metric: 0.3233854; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.614 value_reward_diff[t] + 2.046 reward_diff + -0.444 value_reward_diff^2 + 0.117 value_reward_diff*reward_diff + 2.015 reward_diff^2 \n",
      "value_persistance[t+1] = -0.4 1 + 0.741 value_persistance[t] + -0.073 repeat + 0.372 value_persistance^2 + -0.183 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 0.993 wm_rt[t] + -0.016 wm_rt[t-1] + 0.19 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.034 wm_rt[t-4] + 0.008 wm_rt^2 + -0.036 wm_rt*repeat + 0.106 wm_rt*wm_rt[t-1] + -0.171 wm_rt*wm_rt[t-4] + -0.255 repeat*wm_rt[t-1] + -0.072 repeat*wm_rt[t-3] + 0.072 repeat*wm_rt[t-4] + -0.029 switch*wm_rt[t-1] + -0.079 switch*wm_rt[t-3] + 0.102 switch*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.104 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 49, -, -, 65, 0, 0, 37, 46, 55, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 28, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 265/400 --- L(Train): 0.3470435 --- L(Val, RNN): 0.3220328 --- L(Val, SINDy): 0.4189433 --- Time: 4.36s; --- Convergence: 2.18e-03; LR: 1.00e-02; Metric: 0.3220328; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.613 value_reward_diff[t] + 2.051 reward_diff + -0.444 value_reward_diff^2 + 0.116 value_reward_diff*reward_diff + 2.019 reward_diff^2 \n",
      "value_persistance[t+1] = -0.4 1 + 0.741 value_persistance[t] + -0.072 repeat + 0.372 value_persistance^2 + -0.183 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 0.993 wm_rt[t] + -0.018 wm_rt[t-1] + 0.189 wm_rt[t-2] + 0.057 wm_rt[t-3] + 0.032 wm_rt[t-4] + 0.008 wm_rt^2 + -0.037 wm_rt*repeat + 0.105 wm_rt*wm_rt[t-1] + -0.171 wm_rt*wm_rt[t-4] + -0.261 repeat*wm_rt[t-1] + -0.074 repeat*wm_rt[t-3] + 0.07 repeat*wm_rt[t-4] + -0.03 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-3] + 0.1 switch*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.069 wm_rt[t-2]*wm_rt[t-3] + -0.067 wm_rt[t-2]*wm_rt[t-4] + -0.105 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 50, -, -, 66, 0, 0, 38, 47, 56, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 29, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 266/400 --- L(Train): 0.3505208 --- L(Val, RNN): 0.3245875 --- L(Val, SINDy): 0.4242559 --- Time: 5.03s; --- Convergence: 2.37e-03; LR: 1.00e-02; Metric: 0.3220328; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.613 value_reward_diff[t] + 2.055 reward_diff + -0.443 value_reward_diff^2 + 0.116 value_reward_diff*reward_diff + 2.023 reward_diff^2 \n",
      "value_persistance[t+1] = -0.401 1 + 0.742 value_persistance[t] + -0.073 repeat + 0.372 value_persistance^2 + -0.183 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 0.993 wm_rt[t] + -0.02 wm_rt[t-1] + 0.188 wm_rt[t-2] + 0.056 wm_rt[t-3] + 0.031 wm_rt[t-4] + 0.008 wm_rt^2 + -0.037 wm_rt*repeat + 0.105 wm_rt*wm_rt[t-1] + -0.171 wm_rt*wm_rt[t-4] + -0.268 repeat*wm_rt[t-1] + -0.075 repeat*wm_rt[t-3] + 0.069 repeat*wm_rt[t-4] + -0.032 switch*wm_rt[t-1] + -0.081 switch*wm_rt[t-3] + 0.099 switch*wm_rt[t-4] + -0.074 wm_rt[t-2]^2 + -0.07 wm_rt[t-2]*wm_rt[t-3] + -0.067 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 51, -, -, 67, 0, 0, 39, 48, 57, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 30, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 267/400 --- L(Train): 0.3531392 --- L(Val, RNN): 0.3207727 --- L(Val, SINDy): 0.4180111 --- Time: 4.22s; --- Convergence: 3.09e-03; LR: 1.00e-02; Metric: 0.3207727; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.614 value_reward_diff[t] + 2.061 reward_diff + -0.441 value_reward_diff^2 + 0.116 value_reward_diff*reward_diff + 2.028 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.742 value_persistance[t] + -0.073 repeat + 0.371 value_persistance^2 + -0.182 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.993 wm_rt[t] + -0.023 wm_rt[t-1] + 0.188 wm_rt[t-2] + 0.056 wm_rt[t-3] + 0.03 wm_rt[t-4] + 0.008 wm_rt^2 + -0.037 wm_rt*repeat + 0.105 wm_rt*wm_rt[t-1] + -0.172 wm_rt*wm_rt[t-4] + -0.274 repeat*wm_rt[t-1] + -0.075 repeat*wm_rt[t-3] + 0.068 repeat*wm_rt[t-4] + -0.033 switch*wm_rt[t-1] + -0.081 switch*wm_rt[t-3] + 0.099 switch*wm_rt[t-4] + -0.074 wm_rt[t-2]^2 + -0.07 wm_rt[t-2]*wm_rt[t-3] + -0.067 wm_rt[t-2]*wm_rt[t-4] + -0.107 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 52, -, -, 68, 0, 0, 40, 49, 58, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 31, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 268/400 --- L(Train): 0.3448787 --- L(Val, RNN): 0.3215200 --- L(Val, SINDy): 0.4661544 --- Time: 4.25s; --- Convergence: 1.92e-03; LR: 1.00e-02; Metric: 0.3207727; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.616 value_reward_diff[t] + 2.066 reward_diff + -0.439 value_reward_diff^2 + 0.117 value_reward_diff*reward_diff + 2.033 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.742 value_persistance[t] + -0.073 repeat + 0.371 value_persistance^2 + -0.182 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.994 wm_rt[t] + -0.025 wm_rt[t-1] + 0.188 wm_rt[t-2] + 0.057 wm_rt[t-3] + 0.031 wm_rt[t-4] + 0.008 wm_rt^2 + -0.037 wm_rt*repeat + 0.105 wm_rt*wm_rt[t-1] + -0.172 wm_rt*wm_rt[t-4] + -0.281 repeat*wm_rt[t-1] + -0.073 repeat*wm_rt[t-3] + 0.07 repeat*wm_rt[t-4] + -0.034 switch*wm_rt[t-1] + -0.081 switch*wm_rt[t-3] + 0.099 switch*wm_rt[t-4] + -0.074 wm_rt[t-2]^2 + -0.069 wm_rt[t-2]*wm_rt[t-3] + -0.067 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 53, -, -, 69, 0, 0, 41, 50, 59, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 32, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 269/400 --- L(Train): 0.3431897 --- L(Val, RNN): 0.3226514 --- L(Val, SINDy): 0.4601779 --- Time: 4.27s; --- Convergence: 1.53e-03; LR: 1.00e-02; Metric: 0.3207727; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.615 value_reward_diff[t] + 2.072 reward_diff + -0.438 value_reward_diff^2 + 0.116 value_reward_diff*reward_diff + 2.038 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.741 value_persistance[t] + -0.074 repeat + 0.372 value_persistance^2 + -0.181 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.994 wm_rt[t] + -0.027 wm_rt[t-1] + 0.189 wm_rt[t-2] + 0.057 wm_rt[t-3] + 0.031 wm_rt[t-4] + 0.008 wm_rt^2 + -0.037 wm_rt*repeat + 0.105 wm_rt*wm_rt[t-1] + -0.173 wm_rt*wm_rt[t-4] + -0.287 repeat*wm_rt[t-1] + -0.072 repeat*wm_rt[t-3] + 0.071 repeat*wm_rt[t-4] + -0.036 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-3] + 0.099 switch*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.069 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 54, -, -, 70, 0, 0, 42, 51, 60, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 33, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 270/400 --- L(Train): 0.3448460 --- L(Val, RNN): 0.3199458 --- L(Val, SINDy): 0.4062680 --- Time: 3.89s; --- Convergence: 2.12e-03; LR: 1.00e-02; Metric: 0.3199458; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.615 value_reward_diff[t] + 2.077 reward_diff + -0.438 value_reward_diff^2 + 0.115 value_reward_diff*reward_diff + 2.043 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.741 value_persistance[t] + -0.074 repeat + 0.372 value_persistance^2 + -0.181 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.994 wm_rt[t] + -0.029 wm_rt[t-1] + 0.189 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.031 wm_rt[t-4] + 0.008 wm_rt^2 + -0.037 wm_rt*repeat + 0.105 wm_rt*wm_rt[t-1] + -0.174 wm_rt*wm_rt[t-4] + -0.293 repeat*wm_rt[t-1] + -0.07 repeat*wm_rt[t-3] + 0.071 repeat*wm_rt[t-4] + -0.037 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-3] + 0.099 switch*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.105 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 55, -, -, 71, 0, 0, 43, 52, 61, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 34, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 271/400 --- L(Train): 0.3459137 --- L(Val, RNN): 0.3238459 --- L(Val, SINDy): 0.4048460 --- Time: 4.00s; --- Convergence: 3.01e-03; LR: 1.00e-02; Metric: 0.3199458; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.614 value_reward_diff[t] + 2.083 reward_diff + -0.437 value_reward_diff^2 + 0.115 value_reward_diff*reward_diff + 2.048 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.741 value_persistance[t] + -0.074 repeat + 0.372 value_persistance^2 + -0.181 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.994 wm_rt[t] + -0.032 wm_rt[t-1] + 0.189 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.031 wm_rt[t-4] + 0.008 wm_rt^2 + -0.037 wm_rt*repeat + 0.105 wm_rt*wm_rt[t-1] + -0.175 wm_rt*wm_rt[t-4] + -0.3 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.072 repeat*wm_rt[t-4] + -0.039 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-3] + 0.099 switch*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.105 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 56, -, -, 72, 0, 0, 44, 53, 62, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 35, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 272/400 --- L(Train): 0.3505564 --- L(Val, RNN): 0.3180888 --- L(Val, SINDy): 0.4180909 --- Time: 3.54s; --- Convergence: 4.38e-03; LR: 1.00e-02; Metric: 0.3180888; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.614 value_reward_diff[t] + 2.088 reward_diff + -0.437 value_reward_diff^2 + 0.114 value_reward_diff*reward_diff + 2.053 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.74 value_persistance[t] + -0.074 repeat + 0.372 value_persistance^2 + -0.181 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.994 wm_rt[t] + -0.034 wm_rt[t-1] + 0.189 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.031 wm_rt[t-4] + 0.008 wm_rt^2 + -0.037 wm_rt*repeat + 0.105 wm_rt*wm_rt[t-1] + -0.175 wm_rt*wm_rt[t-4] + -0.306 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.072 repeat*wm_rt[t-4] + -0.041 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-3] + 0.098 switch*wm_rt[t-4] + -0.071 wm_rt[t-2]^2 + -0.067 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.105 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 57, -, -, 73, 0, 0, 45, 54, 63, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 36, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 273/400 --- L(Train): 0.3488207 --- L(Val, RNN): 0.3192853 --- L(Val, SINDy): 0.4174870 --- Time: 3.95s; --- Convergence: 2.79e-03; LR: 1.00e-02; Metric: 0.3180888; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.615 value_reward_diff[t] + 2.094 reward_diff + -0.435 value_reward_diff^2 + 0.114 value_reward_diff*reward_diff + 2.058 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.74 value_persistance[t] + -0.074 repeat + 0.373 value_persistance^2 + -0.181 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.995 wm_rt[t] + -0.037 wm_rt[t-1] + 0.189 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.03 wm_rt[t-4] + 0.008 wm_rt^2 + -0.036 wm_rt*repeat + 0.106 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.312 repeat*wm_rt[t-1] + -0.068 repeat*wm_rt[t-3] + 0.071 repeat*wm_rt[t-4] + -0.043 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-3] + 0.097 switch*wm_rt[t-4] + -0.071 wm_rt[t-2]^2 + -0.067 wm_rt[t-2]*wm_rt[t-3] + -0.064 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 58, -, -, 74, 0, 0, 46, 55, 64, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 37, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 274/400 --- L(Train): 0.3462032 --- L(Val, RNN): 0.3218985 --- L(Val, SINDy): 0.4132949 --- Time: 4.19s; --- Convergence: 2.70e-03; LR: 1.00e-02; Metric: 0.3180888; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.617 value_reward_diff[t] + 2.1 reward_diff + -0.432 value_reward_diff^2 + 0.116 value_reward_diff*reward_diff + 2.064 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.739 value_persistance[t] + -0.074 repeat + 0.374 value_persistance^2 + -0.181 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.995 wm_rt[t] + -0.04 wm_rt[t-1] + 0.189 wm_rt[t-2] + 0.058 wm_rt[t-3] + 0.029 wm_rt[t-4] + 0.008 wm_rt^2 + -0.035 wm_rt*repeat + 0.106 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.319 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.07 repeat*wm_rt[t-4] + -0.045 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-3] + 0.097 switch*wm_rt[t-4] + -0.071 wm_rt[t-2]^2 + -0.067 wm_rt[t-2]*wm_rt[t-3] + -0.064 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 59, -, -, 75, 0, 0, 47, 56, 65, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 38, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 275/400 --- L(Train): 0.3464102 --- L(Val, RNN): 0.3155973 --- L(Val, SINDy): 0.3929034 --- Time: 3.57s; --- Convergence: 4.50e-03; LR: 1.00e-02; Metric: 0.3155973; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.617 value_reward_diff[t] + 2.105 reward_diff + -0.431 value_reward_diff^2 + 0.116 value_reward_diff*reward_diff + 2.069 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.739 value_persistance[t] + -0.074 repeat + 0.374 value_persistance^2 + -0.181 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.995 wm_rt[t] + -0.041 wm_rt[t-1] + 0.19 wm_rt[t-2] + 0.059 wm_rt[t-3] + 0.029 wm_rt[t-4] + 0.008 wm_rt^2 + -0.035 wm_rt*repeat + 0.107 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.326 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.069 repeat*wm_rt[t-4] + -0.046 switch*wm_rt[t-1] + -0.08 switch*wm_rt[t-3] + 0.097 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.064 wm_rt[t-2]*wm_rt[t-4] + -0.107 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 60, -, -, 76, 0, 0, 48, 57, 66, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 39, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 276/400 --- L(Train): 0.3505235 --- L(Val, RNN): 0.3181428 --- L(Val, SINDy): 0.3847487 --- Time: 3.77s; --- Convergence: 3.52e-03; LR: 1.00e-02; Metric: 0.3155973; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.617 value_reward_diff[t] + 2.11 reward_diff + -0.43 value_reward_diff^2 + 0.116 value_reward_diff*reward_diff + 2.074 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.74 value_persistance[t] + -0.075 repeat + 0.373 value_persistance^2 + -0.18 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.996 wm_rt[t] + -0.041 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.06 wm_rt[t-3] + 0.029 wm_rt[t-4] + 0.008 wm_rt^2 + -0.034 wm_rt*repeat + 0.107 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.334 repeat*wm_rt[t-1] + -0.07 repeat*wm_rt[t-3] + 0.068 repeat*wm_rt[t-4] + -0.045 switch*wm_rt[t-1] + -0.078 switch*wm_rt[t-3] + 0.098 switch*wm_rt[t-4] + -0.069 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 61, -, -, 77, 0, 0, 49, 58, 67, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 40, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 277/400 --- L(Train): 0.3470559 --- L(Val, RNN): 0.3197383 --- L(Val, SINDy): 0.4183443 --- Time: 3.57s; --- Convergence: 2.56e-03; LR: 1.00e-02; Metric: 0.3155973; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.616 value_reward_diff[t] + 2.116 reward_diff + -0.43 value_reward_diff^2 + 0.115 value_reward_diff*reward_diff + 2.079 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.74 value_persistance[t] + -0.075 repeat + 0.373 value_persistance^2 + -0.179 value_persistance*repeat + -0.077 repeat^2 \n",
      "wm_rt[t+1] = 0.996 wm_rt[t] + -0.041 wm_rt[t-1] + 0.192 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.03 wm_rt[t-4] + 0.008 wm_rt^2 + -0.033 wm_rt*repeat + 0.108 wm_rt*wm_rt[t-1] + -0.177 wm_rt*wm_rt[t-4] + -0.341 repeat*wm_rt[t-1] + -0.07 repeat*wm_rt[t-3] + 0.066 repeat*wm_rt[t-4] + -0.043 switch*wm_rt[t-1] + -0.076 switch*wm_rt[t-3] + 0.099 switch*wm_rt[t-4] + -0.067 wm_rt[t-2]^2 + -0.063 wm_rt[t-2]*wm_rt[t-3] + -0.06 wm_rt[t-2]*wm_rt[t-4] + -0.105 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 62, -, -, 78, 0, 0, 50, 59, 68, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 41, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 278/400 --- L(Train): 0.3384455 --- L(Val, RNN): 0.3159746 --- L(Val, SINDy): 0.4195998 --- Time: 4.38s; --- Convergence: 3.16e-03; LR: 1.00e-02; Metric: 0.3155973; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.615 value_reward_diff[t] + 2.121 reward_diff + -0.431 value_reward_diff^2 + 0.114 value_reward_diff*reward_diff + 2.084 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.74 value_persistance[t] + -0.075 repeat + 0.372 value_persistance^2 + -0.179 value_persistance*repeat + -0.077 repeat^2 \n",
      "wm_rt[t+1] = 0.996 wm_rt[t] + -0.041 wm_rt[t-1] + 0.193 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.03 wm_rt[t-4] + 0.008 wm_rt^2 + -0.032 wm_rt*repeat + 0.108 wm_rt*wm_rt[t-1] + -0.177 wm_rt*wm_rt[t-4] + -0.349 repeat*wm_rt[t-1] + -0.071 repeat*wm_rt[t-3] + 0.064 repeat*wm_rt[t-4] + -0.042 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.1 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.062 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.105 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 63, -, -, 79, 0, 0, 51, 60, 69, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 42, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 279/400 --- L(Train): 0.3416603 --- L(Val, RNN): 0.3170880 --- L(Val, SINDy): 0.4239002 --- Time: 3.83s; --- Convergence: 2.14e-03; LR: 1.00e-02; Metric: 0.3155973; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.617 value_reward_diff[t] + 2.128 reward_diff + -0.427 value_reward_diff^2 + 0.116 value_reward_diff*reward_diff + 2.09 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.739 value_persistance[t] + -0.074 repeat + 0.373 value_persistance^2 + -0.18 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.997 wm_rt[t] + -0.041 wm_rt[t-1] + 0.193 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.029 wm_rt[t-4] + 0.008 wm_rt^2 + -0.031 wm_rt*repeat + 0.109 wm_rt*wm_rt[t-1] + -0.177 wm_rt*wm_rt[t-4] + -0.356 repeat*wm_rt[t-1] + -0.072 repeat*wm_rt[t-3] + 0.063 repeat*wm_rt[t-4] + -0.042 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.1 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 64, -, -, 80, 0, 0, 52, 61, 70, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 43, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 280/400 --- L(Train): 0.3420409 --- L(Val, RNN): 0.3167079 --- L(Val, SINDy): 0.4209885 --- Time: 4.08s; --- Convergence: 1.26e-03; LR: 1.00e-02; Metric: 0.3155973; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.619 value_reward_diff[t] + 2.134 reward_diff + -0.424 value_reward_diff^2 + 0.118 value_reward_diff*reward_diff + 2.096 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.738 value_persistance[t] + -0.073 repeat + 0.375 value_persistance^2 + -0.181 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 0.997 wm_rt[t] + -0.042 wm_rt[t-1] + 0.193 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.028 wm_rt[t-4] + 0.008 wm_rt^2 + -0.03 wm_rt*repeat + 0.11 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.363 repeat*wm_rt[t-1] + -0.073 repeat*wm_rt[t-3] + 0.062 repeat*wm_rt[t-4] + -0.042 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.099 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 65, -, -, 81, 0, 0, 53, 62, 71, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 44, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 281/400 --- L(Train): 0.3444774 --- L(Val, RNN): 0.3152113 --- L(Val, SINDy): 0.3820634 --- Time: 3.80s; --- Convergence: 1.38e-03; LR: 1.00e-02; Metric: 0.3152113; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.62 value_reward_diff[t] + 2.14 reward_diff + -0.422 value_reward_diff^2 + 0.119 value_reward_diff*reward_diff + 2.102 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.737 value_persistance[t] + -0.072 repeat + 0.376 value_persistance^2 + -0.182 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 0.997 wm_rt[t] + -0.043 wm_rt[t-1] + 0.192 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.027 wm_rt[t-4] + 0.008 wm_rt^2 + -0.029 wm_rt*repeat + 0.11 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.37 repeat*wm_rt[t-1] + -0.073 repeat*wm_rt[t-3] + 0.062 repeat*wm_rt[t-4] + -0.042 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.098 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.062 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.107 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 66, -, -, 82, 0, 0, 54, 63, 72, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 45, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 282/400 --- L(Train): 0.3434885 --- L(Val, RNN): 0.3159383 --- L(Val, SINDy): 0.3771321 --- Time: 3.79s; --- Convergence: 1.05e-03; LR: 1.00e-02; Metric: 0.3152113; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.621 value_reward_diff[t] + 2.147 reward_diff + -0.42 value_reward_diff^2 + 0.12 value_reward_diff*reward_diff + 2.108 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.736 value_persistance[t] + -0.072 repeat + 0.376 value_persistance^2 + -0.182 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 0.998 wm_rt[t] + -0.044 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.026 wm_rt[t-4] + 0.008 wm_rt^2 + -0.028 wm_rt*repeat + 0.111 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.377 repeat*wm_rt[t-1] + -0.072 repeat*wm_rt[t-3] + 0.062 repeat*wm_rt[t-4] + -0.042 switch*wm_rt[t-1] + -0.076 switch*wm_rt[t-3] + 0.096 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.062 wm_rt[t-2]*wm_rt[t-3] + -0.06 wm_rt[t-2]*wm_rt[t-4] + -0.108 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 67, -, -, 83, 0, 0, 55, 64, 73, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 46, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 283/400 --- L(Train): 0.3420712 --- L(Val, RNN): 0.3149461 --- L(Val, SINDy): 0.3746878 --- Time: 4.16s; --- Convergence: 1.02e-03; LR: 1.00e-02; Metric: 0.3149461; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.622 value_reward_diff[t] + 2.154 reward_diff + -0.417 value_reward_diff^2 + 0.122 value_reward_diff*reward_diff + 2.115 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.737 value_persistance[t] + -0.073 repeat + 0.376 value_persistance^2 + -0.181 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 0.998 wm_rt[t] + -0.045 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.06 wm_rt[t-3] + 0.025 wm_rt[t-4] + 0.008 wm_rt^2 + -0.027 wm_rt*repeat + 0.111 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.384 repeat*wm_rt[t-1] + -0.072 repeat*wm_rt[t-3] + 0.063 repeat*wm_rt[t-4] + -0.043 switch*wm_rt[t-1] + -0.076 switch*wm_rt[t-3] + 0.095 switch*wm_rt[t-4] + -0.067 wm_rt[t-2]^2 + -0.062 wm_rt[t-2]*wm_rt[t-3] + -0.06 wm_rt[t-2]*wm_rt[t-4] + -0.108 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 68, -, -, 84, 0, 0, 56, 65, 74, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 47, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 284/400 --- L(Train): 0.3343581 --- L(Val, RNN): 0.3148888 --- L(Val, SINDy): 0.3778482 --- Time: 3.89s; --- Convergence: 5.40e-04; LR: 1.00e-02; Metric: 0.3148888; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.622 value_reward_diff[t] + 2.16 reward_diff + -0.417 value_reward_diff^2 + 0.122 value_reward_diff*reward_diff + 2.12 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.737 value_persistance[t] + -0.073 repeat + 0.376 value_persistance^2 + -0.18 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.998 wm_rt[t] + -0.045 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.026 wm_rt[t-4] + 0.008 wm_rt^2 + -0.026 wm_rt*repeat + 0.112 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.39 repeat*wm_rt[t-1] + -0.07 repeat*wm_rt[t-3] + 0.065 repeat*wm_rt[t-4] + -0.042 switch*wm_rt[t-1] + -0.076 switch*wm_rt[t-3] + 0.095 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.062 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.108 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 69, -, -, 85, 0, 0, 57, 66, 75, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 48, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 285/400 --- L(Train): 0.3438667 --- L(Val, RNN): 0.3145563 --- L(Val, SINDy): 0.3788446 --- Time: 3.38s; --- Convergence: 4.36e-04; LR: 1.00e-02; Metric: 0.3145563; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.622 value_reward_diff[t] + 2.166 reward_diff + -0.416 value_reward_diff^2 + 0.122 value_reward_diff*reward_diff + 2.126 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.737 value_persistance[t] + -0.074 repeat + 0.376 value_persistance^2 + -0.18 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.998 wm_rt[t] + -0.044 wm_rt[t-1] + 0.192 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.026 wm_rt[t-4] + 0.008 wm_rt^2 + -0.025 wm_rt*repeat + 0.112 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.396 repeat*wm_rt[t-1] + -0.068 repeat*wm_rt[t-3] + 0.067 repeat*wm_rt[t-4] + -0.04 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.095 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.107 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 70, -, -, 86, 0, 0, 58, 67, 76, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 49, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 286/400 --- L(Train): 0.3382867 --- L(Val, RNN): 0.3120793 --- L(Val, SINDy): 0.4140396 --- Time: 3.20s; --- Convergence: 1.46e-03; LR: 1.00e-02; Metric: 0.3120793; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.622 value_reward_diff[t] + 2.172 reward_diff + -0.415 value_reward_diff^2 + 0.122 value_reward_diff*reward_diff + 2.131 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.736 value_persistance[t] + -0.074 repeat + 0.376 value_persistance^2 + -0.18 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.999 wm_rt[t] + -0.043 wm_rt[t-1] + 0.193 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.027 wm_rt[t-4] + 0.008 wm_rt^2 + -0.025 wm_rt*repeat + 0.112 wm_rt*wm_rt[t-1] + -0.177 wm_rt*wm_rt[t-4] + -0.402 repeat*wm_rt[t-1] + -0.067 repeat*wm_rt[t-3] + 0.068 repeat*wm_rt[t-4] + -0.039 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.095 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 71, -, -, 87, 0, 0, 59, 68, 77, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 50, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 287/400 --- L(Train): 0.3398775 --- L(Val, RNN): 0.3136728 --- L(Val, SINDy): 0.4140128 --- Time: 4.44s; --- Convergence: 1.53e-03; LR: 1.00e-02; Metric: 0.3120793; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.622 value_reward_diff[t] + 2.178 reward_diff + -0.414 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.137 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.736 value_persistance[t] + -0.074 repeat + 0.376 value_persistance^2 + -0.18 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 0.999 wm_rt[t] + -0.043 wm_rt[t-1] + 0.193 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.027 wm_rt[t-4] + 0.008 wm_rt^2 + -0.024 wm_rt*repeat + 0.113 wm_rt*wm_rt[t-1] + -0.177 wm_rt*wm_rt[t-4] + -0.408 repeat*wm_rt[t-1] + -0.067 repeat*wm_rt[t-3] + 0.069 repeat*wm_rt[t-4] + -0.037 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.095 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.059 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.105 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 72, -, -, 88, 0, 0, 60, 69, 78, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 51, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 288/400 --- L(Train): 0.3315264 --- L(Val, RNN): 0.3137526 --- L(Val, SINDy): 0.3966953 --- Time: 4.40s; --- Convergence: 8.02e-04; LR: 1.00e-02; Metric: 0.3120793; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.621 value_reward_diff[t] + 2.183 reward_diff + -0.414 value_reward_diff^2 + 0.122 value_reward_diff*reward_diff + 2.142 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.735 value_persistance[t] + -0.074 repeat + 0.377 value_persistance^2 + -0.18 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.999 wm_rt[t] + -0.042 wm_rt[t-1] + 0.192 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.027 wm_rt[t-4] + 0.008 wm_rt^2 + -0.023 wm_rt*repeat + 0.113 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.414 repeat*wm_rt[t-1] + -0.067 repeat*wm_rt[t-3] + 0.069 repeat*wm_rt[t-4] + -0.036 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.095 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.105 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 73, -, -, 89, 0, 0, 61, 70, 79, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 52, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 289/400 --- L(Train): 0.3360131 --- L(Val, RNN): 0.3120782 --- L(Val, SINDy): 0.3609864 --- Time: 4.03s; --- Convergence: 1.24e-03; LR: 1.00e-02; Metric: 0.3120782; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.621 value_reward_diff[t] + 2.189 reward_diff + -0.413 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.146 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.735 value_persistance[t] + -0.073 repeat + 0.377 value_persistance^2 + -0.18 value_persistance*repeat + -0.075 repeat^2 \n",
      "wm_rt[t+1] = 0.999 wm_rt[t] + -0.042 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.026 wm_rt[t-4] + 0.008 wm_rt^2 + -0.022 wm_rt*repeat + 0.113 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.419 repeat*wm_rt[t-1] + -0.068 repeat*wm_rt[t-3] + 0.068 repeat*wm_rt[t-4] + -0.036 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.094 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 74, -, -, 90, 0, 0, 62, 71, 80, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 53, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 290/400 --- L(Train): 0.3381738 --- L(Val, RNN): 0.3131448 --- L(Val, SINDy): 0.3373377 --- Time: 4.09s; --- Convergence: 1.15e-03; LR: 1.00e-02; Metric: 0.3120782; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.622 value_reward_diff[t] + 2.194 reward_diff + -0.411 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.735 value_persistance[t] + -0.073 repeat + 0.378 value_persistance^2 + -0.181 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 0.999 wm_rt[t] + -0.042 wm_rt[t-1] + 0.191 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.026 wm_rt[t-4] + 0.008 wm_rt^2 + -0.021 wm_rt*repeat + 0.113 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.424 repeat*wm_rt[t-1] + -0.07 repeat*wm_rt[t-3] + 0.067 repeat*wm_rt[t-4] + -0.035 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.093 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.107 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 75, -, -, 91, 0, 0, 63, 72, 81, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 54, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 291/400 --- L(Train): 0.3352332 --- L(Val, RNN): 0.3111452 --- L(Val, SINDy): 0.3356256 --- Time: 4.55s; --- Convergence: 1.58e-03; LR: 1.00e-02; Metric: 0.3111452; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.62 value_reward_diff[t] + 2.199 reward_diff + -0.413 value_reward_diff^2 + 0.121 value_reward_diff*reward_diff + 2.155 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.735 value_persistance[t] + -0.073 repeat + 0.378 value_persistance^2 + -0.181 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 0.999 wm_rt[t] + -0.042 wm_rt[t-1] + 0.19 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.025 wm_rt[t-4] + 0.008 wm_rt^2 + -0.02 wm_rt*repeat + 0.114 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.43 repeat*wm_rt[t-1] + -0.071 repeat*wm_rt[t-3] + 0.066 repeat*wm_rt[t-4] + -0.034 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.093 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.107 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 76, -, -, 92, 0, 0, 64, 73, 82, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 55, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 292/400 --- L(Train): 0.3478944 --- L(Val, RNN): 0.3094935 --- L(Val, SINDy): 0.3341536 --- Time: 3.98s; --- Convergence: 1.61e-03; LR: 1.00e-02; Metric: 0.3094935; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.06 1 + 0.619 value_reward_diff[t] + 2.204 reward_diff + -0.412 value_reward_diff^2 + 0.121 value_reward_diff*reward_diff + 2.16 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.735 value_persistance[t] + -0.073 repeat + 0.377 value_persistance^2 + -0.18 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + -0.041 wm_rt[t-1] + 0.189 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.025 wm_rt[t-4] + 0.008 wm_rt^2 + -0.019 wm_rt*repeat + 0.114 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.435 repeat*wm_rt[t-1] + -0.072 repeat*wm_rt[t-3] + 0.066 repeat*wm_rt[t-4] + -0.032 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-3] + 0.093 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.107 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 77, -, -, 93, 0, 0, 65, 74, 83, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 56, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 293/400 --- L(Train): 0.3367698 --- L(Val, RNN): 0.3101163 --- L(Val, SINDy): 0.3344829 --- Time: 4.11s; --- Convergence: 1.12e-03; LR: 1.00e-02; Metric: 0.3094935; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.06 1 + 0.62 value_reward_diff[t] + 2.209 reward_diff + -0.411 value_reward_diff^2 + 0.121 value_reward_diff*reward_diff + 2.165 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.735 value_persistance[t] + -0.073 repeat + 0.377 value_persistance^2 + -0.181 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + -0.039 wm_rt[t-1] + 0.19 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.025 wm_rt[t-4] + 0.008 wm_rt^2 + -0.019 wm_rt*repeat + 0.114 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.439 repeat*wm_rt[t-1] + -0.071 repeat*wm_rt[t-3] + 0.067 repeat*wm_rt[t-4] + -0.03 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-3] + 0.093 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 78, -, -, 94, 0, 0, 66, 75, 84, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 57, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 294/400 --- L(Train): 0.3308600 --- L(Val, RNN): 0.3101164 --- L(Val, SINDy): 0.3404899 --- Time: 4.18s; --- Convergence: 5.59e-04; LR: 1.00e-02; Metric: 0.3094935; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.622 value_reward_diff[t] + 2.215 reward_diff + -0.407 value_reward_diff^2 + 0.122 value_reward_diff*reward_diff + 2.171 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.735 value_persistance[t] + -0.072 repeat + 0.378 value_persistance^2 + -0.181 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + -0.037 wm_rt[t-1] + 0.19 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.026 wm_rt[t-4] + 0.008 wm_rt^2 + -0.018 wm_rt*repeat + 0.114 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.442 repeat*wm_rt[t-1] + -0.07 repeat*wm_rt[t-3] + 0.069 repeat*wm_rt[t-4] + -0.028 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.094 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.105 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 79, -, -, 95, 0, 0, 67, 76, 85, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 58, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 295/400 --- L(Train): 0.3301915 --- L(Val, RNN): 0.3104427 --- L(Val, SINDy): 0.3702578 --- Time: 3.91s; --- Convergence: 4.43e-04; LR: 1.00e-02; Metric: 0.3094935; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.625 value_reward_diff[t] + 2.221 reward_diff + -0.403 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 2.176 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.734 value_persistance[t] + -0.071 repeat + 0.378 value_persistance^2 + -0.182 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + -0.035 wm_rt[t-1] + 0.19 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.027 wm_rt[t-4] + 0.008 wm_rt^2 + -0.017 wm_rt*repeat + 0.114 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.446 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.07 repeat*wm_rt[t-4] + -0.025 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.094 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.104 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 80, -, -, 96, 0, 0, 68, 77, 86, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 59, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 296/400 --- L(Train): 0.3394299 --- L(Val, RNN): 0.3078098 --- L(Val, SINDy): 0.3661317 --- Time: 4.31s; --- Convergence: 1.54e-03; LR: 1.00e-02; Metric: 0.3078098; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.625 value_reward_diff[t] + 2.226 reward_diff + -0.402 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.181 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.733 value_persistance[t] + -0.07 repeat + 0.379 value_persistance^2 + -0.182 value_persistance*repeat + -0.072 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + -0.034 wm_rt[t-1] + 0.19 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.027 wm_rt[t-4] + 0.008 wm_rt^2 + -0.017 wm_rt*repeat + 0.114 wm_rt*wm_rt[t-1] + -0.175 wm_rt*wm_rt[t-4] + -0.449 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.071 repeat*wm_rt[t-4] + -0.024 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.093 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.104 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 81, -, -, 97, 0, 0, 69, 78, 87, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 60, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 297/400 --- L(Train): 0.3403367 --- L(Val, RNN): 0.3098288 --- L(Val, SINDy): 0.3510817 --- Time: 3.59s; --- Convergence: 1.78e-03; LR: 1.00e-02; Metric: 0.3078098; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.625 value_reward_diff[t] + 2.232 reward_diff + -0.401 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.185 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.733 value_persistance[t] + -0.07 repeat + 0.379 value_persistance^2 + -0.183 value_persistance*repeat + -0.072 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + -0.034 wm_rt[t-1] + 0.189 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.026 wm_rt[t-4] + 0.008 wm_rt^2 + -0.016 wm_rt*repeat + 0.114 wm_rt*wm_rt[t-1] + -0.175 wm_rt*wm_rt[t-4] + -0.452 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.072 repeat*wm_rt[t-4] + -0.023 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.092 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.105 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 82, -, -, 98, 0, 0, 70, 79, 88, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 61, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 298/400 --- L(Train): 0.3390909 --- L(Val, RNN): 0.3077896 --- L(Val, SINDy): 0.3908022 --- Time: 4.09s; --- Convergence: 1.91e-03; LR: 1.00e-02; Metric: 0.3077896; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.624 value_reward_diff[t] + 2.237 reward_diff + -0.4 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.19 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.732 value_persistance[t] + -0.07 repeat + 0.38 value_persistance^2 + -0.182 value_persistance*repeat + -0.072 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + -0.034 wm_rt[t-1] + 0.188 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.025 wm_rt[t-4] + 0.008 wm_rt^2 + -0.016 wm_rt*repeat + 0.113 wm_rt*wm_rt[t-1] + -0.175 wm_rt*wm_rt[t-4] + -0.455 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + -0.023 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-3] + 0.091 switch*wm_rt[t-4] + -0.067 wm_rt[t-2]^2 + -0.062 wm_rt[t-2]*wm_rt[t-3] + -0.06 wm_rt[t-2]*wm_rt[t-4] + -0.106 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 83, -, -, 99, 0, 0, 71, 80, 89, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 62, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 299/400 --- L(Train): 0.3365946 --- L(Val, RNN): 0.3078986 --- L(Val, SINDy): 0.4237907 --- Time: 3.59s; --- Convergence: 1.01e-03; LR: 1.00e-02; Metric: 0.3077896; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.623 value_reward_diff[t] + 2.242 reward_diff + -0.401 value_reward_diff^2 + 0.121 value_reward_diff*reward_diff + 2.195 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.733 value_persistance[t] + -0.071 repeat + 0.379 value_persistance^2 + -0.182 value_persistance*repeat + -0.072 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.186 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.024 wm_rt[t-4] + 0.008 wm_rt^2 + -0.015 wm_rt*repeat + 0.113 wm_rt*wm_rt[t-1] + -0.175 wm_rt*wm_rt[t-4] + -0.457 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + -0.023 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.089 switch*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.064 wm_rt[t-2]*wm_rt[t-3] + -0.061 wm_rt[t-2]*wm_rt[t-4] + -0.107 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 84, -, -, -, 0, 0, 72, 81, 90, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 63, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 300/400 --- L(Train): 0.3400289 --- L(Val, RNN): 0.3067748 --- L(Val, SINDy): 0.4368207 --- Time: 4.39s; --- Convergence: 1.07e-03; LR: 1.00e-02; Metric: 0.3067748; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.622 value_reward_diff[t] + 2.247 reward_diff + -0.401 value_reward_diff^2 + 0.121 value_reward_diff*reward_diff + 2.199 reward_diff^2 \n",
      "value_persistance[t+1] = -0.406 1 + 0.732 value_persistance[t] + -0.071 repeat + 0.38 value_persistance^2 + -0.181 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.185 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.023 wm_rt[t-4] + 0.008 wm_rt^2 + -0.015 wm_rt*repeat + 0.113 wm_rt*wm_rt[t-1] + -0.175 wm_rt*wm_rt[t-4] + -0.46 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + -0.024 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.088 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.108 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 85, -, -, -, 0, 0, 73, 82, 91, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 64, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 301/400 --- L(Train): 0.3319547 --- L(Val, RNN): 0.3072083 --- L(Val, SINDy): 0.4415395 --- Time: 3.75s; --- Convergence: 7.50e-04; LR: 1.00e-02; Metric: 0.3067748; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.624 value_reward_diff[t] + 2.253 reward_diff + -0.398 value_reward_diff^2 + 0.122 value_reward_diff*reward_diff + 2.205 reward_diff^2 \n",
      "value_persistance[t+1] = -0.406 1 + 0.732 value_persistance[t] + -0.071 repeat + 0.38 value_persistance^2 + -0.181 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.183 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.022 wm_rt[t-4] + 0.008 wm_rt^2 + -0.014 wm_rt*repeat + 0.113 wm_rt*wm_rt[t-1] + -0.175 wm_rt*wm_rt[t-4] + -0.463 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + -0.026 switch*wm_rt[t-1] + -0.077 switch*wm_rt[t-3] + 0.086 switch*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.067 wm_rt[t-2]*wm_rt[t-3] + -0.064 wm_rt[t-2]*wm_rt[t-4] + -0.109 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 86, -, -, -, 0, 0, 74, 83, 92, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 65, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 302/400 --- L(Train): 0.3326214 --- L(Val, RNN): 0.3079399 --- L(Val, SINDy): 0.4279540 --- Time: 3.94s; --- Convergence: 7.41e-04; LR: 1.00e-02; Metric: 0.3067748; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.624 value_reward_diff[t] + 2.258 reward_diff + -0.397 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.209 reward_diff^2 \n",
      "value_persistance[t+1] = -0.406 1 + 0.732 value_persistance[t] + -0.072 repeat + 0.38 value_persistance^2 + -0.181 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.182 wm_rt[t-2] + 0.06 wm_rt[t-3] + 0.021 wm_rt[t-4] + 0.008 wm_rt^2 + -0.013 wm_rt*repeat + 0.113 wm_rt*wm_rt[t-1] + -0.175 wm_rt*wm_rt[t-4] + -0.465 repeat*wm_rt[t-1] + -0.067 repeat*wm_rt[t-3] + 0.074 repeat*wm_rt[t-4] + -0.029 switch*wm_rt[t-1] + -0.078 switch*wm_rt[t-3] + 0.084 switch*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.11 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 87, -, -, -, 0, 0, 75, 84, 93, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 66, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 303/400 --- L(Train): 0.3278669 --- L(Val, RNN): 0.3057618 --- L(Val, SINDy): 0.3975671 --- Time: 3.50s; --- Convergence: 1.46e-03; LR: 1.00e-02; Metric: 0.3057618; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.622 value_reward_diff[t] + 2.262 reward_diff + -0.397 value_reward_diff^2 + 0.121 value_reward_diff*reward_diff + 2.213 reward_diff^2 \n",
      "value_persistance[t+1] = -0.406 1 + 0.731 value_persistance[t] + -0.072 repeat + 0.38 value_persistance^2 + -0.18 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.181 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.021 wm_rt[t-4] + 0.008 wm_rt^2 + -0.013 wm_rt*repeat + 0.113 wm_rt*wm_rt[t-1] + -0.176 wm_rt*wm_rt[t-4] + -0.466 repeat*wm_rt[t-1] + -0.064 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.032 switch*wm_rt[t-1] + -0.079 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.11 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 88, -, -, -, 0, 0, 76, 85, 94, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 67, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 304/400 --- L(Train): 0.3300849 --- L(Val, RNN): 0.3056772 --- L(Val, SINDy): 0.4043163 --- Time: 3.71s; --- Convergence: 7.72e-04; LR: 1.00e-02; Metric: 0.3056772; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.624 value_reward_diff[t] + 2.267 reward_diff + -0.394 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.218 reward_diff^2 \n",
      "value_persistance[t+1] = -0.406 1 + 0.731 value_persistance[t] + -0.072 repeat + 0.38 value_persistance^2 + -0.18 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.181 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.021 wm_rt[t-4] + 0.008 wm_rt^2 + -0.012 wm_rt*repeat + 0.112 wm_rt*wm_rt[t-1] + -0.177 wm_rt*wm_rt[t-4] + -0.468 repeat*wm_rt[t-1] + -0.062 repeat*wm_rt[t-3] + 0.078 repeat*wm_rt[t-4] + -0.035 switch*wm_rt[t-1] + -0.079 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.073 wm_rt[t-2]^2 + -0.068 wm_rt[t-2]*wm_rt[t-3] + -0.066 wm_rt[t-2]*wm_rt[t-4] + -0.109 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 89, -, -, -, 0, 0, 77, 86, 95, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 68, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 305/400 --- L(Train): 0.3343783 --- L(Val, RNN): 0.3051659 --- L(Val, SINDy): 0.4102312 --- Time: 3.92s; --- Convergence: 6.42e-04; LR: 1.00e-02; Metric: 0.3051659; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.626 value_reward_diff[t] + 2.273 reward_diff + -0.391 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 2.223 reward_diff^2 \n",
      "value_persistance[t+1] = -0.407 1 + 0.731 value_persistance[t] + -0.072 repeat + 0.38 value_persistance^2 + -0.18 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.182 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.022 wm_rt[t-4] + 0.008 wm_rt^2 + -0.012 wm_rt*repeat + 0.112 wm_rt*wm_rt[t-1] + -0.178 wm_rt*wm_rt[t-4] + -0.47 repeat*wm_rt[t-1] + -0.059 repeat*wm_rt[t-3] + 0.079 repeat*wm_rt[t-4] + -0.038 switch*wm_rt[t-1] + -0.078 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.072 wm_rt[t-2]^2 + -0.067 wm_rt[t-2]*wm_rt[t-3] + -0.065 wm_rt[t-2]*wm_rt[t-4] + -0.108 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 90, -, -, -, 0, 0, 78, 87, 96, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 69, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 306/400 --- L(Train): 0.3407353 --- L(Val, RNN): 0.3037344 --- L(Val, SINDy): 0.4021707 --- Time: 4.26s; --- Convergence: 1.04e-03; LR: 1.00e-02; Metric: 0.3037344; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.623 value_reward_diff[t] + 2.277 reward_diff + -0.393 value_reward_diff^2 + 0.121 value_reward_diff*reward_diff + 2.226 reward_diff^2 \n",
      "value_persistance[t+1] = -0.407 1 + 0.731 value_persistance[t] + -0.072 repeat + 0.381 value_persistance^2 + -0.18 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.182 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.022 wm_rt[t-4] + 0.008 wm_rt^2 + -0.011 wm_rt*repeat + 0.111 wm_rt*wm_rt[t-1] + -0.18 wm_rt*wm_rt[t-4] + -0.472 repeat*wm_rt[t-1] + -0.057 repeat*wm_rt[t-3] + 0.079 repeat*wm_rt[t-4] + -0.042 switch*wm_rt[t-1] + -0.077 switch*wm_rt[t-3] + 0.084 switch*wm_rt[t-4] + -0.071 wm_rt[t-2]^2 + -0.067 wm_rt[t-2]*wm_rt[t-3] + -0.064 wm_rt[t-2]*wm_rt[t-4] + -0.108 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 91, -, -, -, 0, 0, 79, 88, 97, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 70, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 307/400 --- L(Train): 0.3257001 --- L(Val, RNN): 0.3049996 --- L(Val, SINDy): 0.4188658 --- Time: 3.88s; --- Convergence: 1.15e-03; LR: 1.00e-02; Metric: 0.3037344; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.621 value_reward_diff[t] + 2.281 reward_diff + -0.394 value_reward_diff^2 + 0.12 value_reward_diff*reward_diff + 2.23 reward_diff^2 \n",
      "value_persistance[t+1] = -0.407 1 + 0.731 value_persistance[t] + -0.072 repeat + 0.381 value_persistance^2 + -0.18 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.183 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.022 wm_rt[t-4] + 0.008 wm_rt^2 + -0.01 wm_rt*repeat + 0.111 wm_rt*wm_rt[t-1] + -0.181 wm_rt*wm_rt[t-4] + -0.473 repeat*wm_rt[t-1] + -0.057 repeat*wm_rt[t-3] + 0.079 repeat*wm_rt[t-4] + -0.045 switch*wm_rt[t-1] + -0.076 switch*wm_rt[t-3] + 0.084 switch*wm_rt[t-4] + -0.071 wm_rt[t-2]^2 + -0.066 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.108 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 92, -, -, -, 0, 0, 80, 89, 98, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 71, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 308/400 --- L(Train): 0.3314631 --- L(Val, RNN): 0.3044204 --- L(Val, SINDy): 0.4868366 --- Time: 3.68s; --- Convergence: 8.65e-04; LR: 1.00e-02; Metric: 0.3037344; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.624 value_reward_diff[t] + 2.287 reward_diff + -0.39 value_reward_diff^2 + 0.122 value_reward_diff*reward_diff + 2.236 reward_diff^2 \n",
      "value_persistance[t+1] = -0.407 1 + 0.73 value_persistance[t] + -0.071 repeat + 0.382 value_persistance^2 + -0.181 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.184 wm_rt[t-2] + 0.066 wm_rt[t-3] + 0.022 wm_rt[t-4] + 0.008 wm_rt^2 + -0.01 wm_rt*repeat + 0.111 wm_rt*wm_rt[t-1] + -0.182 wm_rt*wm_rt[t-4] + -0.476 repeat*wm_rt[t-1] + -0.057 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.049 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.085 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.108 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 93, -, -, -, 0, 0, 81, 90, 99, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 72, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 309/400 --- L(Train): 0.3309585 --- L(Val, RNN): 0.3033262 --- L(Val, SINDy): 0.4699264 --- Time: 3.92s; --- Convergence: 9.80e-04; LR: 1.00e-02; Metric: 0.3033262; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.625 value_reward_diff[t] + 2.293 reward_diff + -0.387 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 2.241 reward_diff^2 \n",
      "value_persistance[t+1] = -0.407 1 + 0.73 value_persistance[t] + -0.071 repeat + 0.382 value_persistance^2 + -0.181 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.184 wm_rt[t-2] + 0.066 wm_rt[t-3] + 0.021 wm_rt[t-4] + 0.008 wm_rt^2 + 0.11 wm_rt*wm_rt[t-1] + -0.183 wm_rt*wm_rt[t-4] + -0.478 repeat*wm_rt[t-1] + -0.058 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + -0.054 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.086 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.108 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 94, -, -, -, 0, 0, 82, 91, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 310/400 --- L(Train): 0.3293089 --- L(Val, RNN): 0.3046889 --- L(Val, SINDy): 0.4501051 --- Time: 4.17s; --- Convergence: 1.17e-03; LR: 1.00e-02; Metric: 0.3033262; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.625 value_reward_diff[t] + 2.298 reward_diff + -0.386 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 2.246 reward_diff^2 \n",
      "value_persistance[t+1] = -0.407 1 + 0.729 value_persistance[t] + -0.071 repeat + 0.382 value_persistance^2 + -0.18 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.183 wm_rt[t-2] + 0.066 wm_rt[t-3] + 0.02 wm_rt[t-4] + 0.008 wm_rt^2 + 0.11 wm_rt*wm_rt[t-1] + -0.184 wm_rt*wm_rt[t-4] + -0.479 repeat*wm_rt[t-1] + -0.06 repeat*wm_rt[t-3] + 0.07 repeat*wm_rt[t-4] + -0.059 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.086 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.109 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 95, -, -, -, 0, 0, 83, 92, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 311/400 --- L(Train): 0.3305541 --- L(Val, RNN): 0.3015375 --- L(Val, SINDy): 0.3883989 --- Time: 4.13s; --- Convergence: 2.16e-03; LR: 1.00e-02; Metric: 0.3015375; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.625 value_reward_diff[t] + 2.304 reward_diff + -0.385 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 2.251 reward_diff^2 \n",
      "value_persistance[t+1] = -0.407 1 + 0.729 value_persistance[t] + -0.072 repeat + 0.383 value_persistance^2 + -0.18 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.183 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.019 wm_rt[t-4] + 0.008 wm_rt^2 + 0.109 wm_rt*wm_rt[t-1] + -0.185 wm_rt*wm_rt[t-4] + -0.481 repeat*wm_rt[t-1] + -0.062 repeat*wm_rt[t-3] + 0.067 repeat*wm_rt[t-4] + -0.064 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.085 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 96, -, -, -, 0, 0, 84, 93, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 312/400 --- L(Train): 0.3325356 --- L(Val, RNN): 0.3028208 --- L(Val, SINDy): 0.3777921 --- Time: 4.07s; --- Convergence: 1.72e-03; LR: 1.00e-02; Metric: 0.3015375; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.625 value_reward_diff[t] + 2.31 reward_diff + -0.384 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 2.256 reward_diff^2 \n",
      "value_persistance[t+1] = -0.407 1 + 0.729 value_persistance[t] + -0.072 repeat + 0.383 value_persistance^2 + -0.18 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.183 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.018 wm_rt[t-4] + 0.008 wm_rt^2 + 0.108 wm_rt*wm_rt[t-1] + -0.186 wm_rt*wm_rt[t-4] + -0.484 repeat*wm_rt[t-1] + -0.063 repeat*wm_rt[t-3] + 0.063 repeat*wm_rt[t-4] + -0.069 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-3] + 0.085 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.112 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 97, -, -, -, 0, 0, 85, 94, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 313/400 --- L(Train): 0.3335408 --- L(Val, RNN): 0.3005131 --- L(Val, SINDy): 0.3740478 --- Time: 3.77s; --- Convergence: 2.01e-03; LR: 1.00e-02; Metric: 0.3005131; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.623 value_reward_diff[t] + 2.315 reward_diff + -0.386 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.261 reward_diff^2 \n",
      "value_persistance[t+1] = -0.407 1 + 0.728 value_persistance[t] + -0.072 repeat + 0.383 value_persistance^2 + -0.18 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.183 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.017 wm_rt[t-4] + 0.008 wm_rt^2 + 0.108 wm_rt*wm_rt[t-1] + -0.188 wm_rt*wm_rt[t-4] + -0.484 repeat*wm_rt[t-1] + -0.063 repeat*wm_rt[t-3] + 0.062 repeat*wm_rt[t-4] + -0.076 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.085 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.112 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 98, -, -, -, 0, 0, 86, 95, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 314/400 --- L(Train): 0.3323367 --- L(Val, RNN): 0.3015162 --- L(Val, SINDy): 0.3631458 --- Time: 3.71s; --- Convergence: 1.51e-03; LR: 1.00e-02; Metric: 0.3005131; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.624 value_reward_diff[t] + 2.322 reward_diff + -0.383 value_reward_diff^2 + 0.125 value_reward_diff*reward_diff + 2.268 reward_diff^2 \n",
      "value_persistance[t+1] = -0.406 1 + 0.728 value_persistance[t] + -0.072 repeat + 0.384 value_persistance^2 + -0.18 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.183 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.016 wm_rt[t-4] + 0.008 wm_rt^2 + 0.106 wm_rt*wm_rt[t-1] + -0.189 wm_rt*wm_rt[t-4] + -0.484 repeat*wm_rt[t-1] + -0.062 repeat*wm_rt[t-3] + 0.062 repeat*wm_rt[t-4] + -0.083 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.084 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, 99, -, -, -, 0, 0, 87, 96, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 315/400 --- L(Train): 0.3277132 --- L(Val, RNN): 0.3007627 --- L(Val, SINDy): 0.3552819 --- Time: 4.16s; --- Convergence: 1.13e-03; LR: 1.00e-02; Metric: 0.3005131; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.625 value_reward_diff[t] + 2.328 reward_diff + -0.381 value_reward_diff^2 + 0.126 value_reward_diff*reward_diff + 2.274 reward_diff^2 \n",
      "value_persistance[t+1] = -0.406 1 + 0.727 value_persistance[t] + -0.072 repeat + 0.384 value_persistance^2 + -0.179 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.183 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.016 wm_rt[t-4] + 0.008 wm_rt^2 + 0.105 wm_rt*wm_rt[t-1] + -0.192 wm_rt*wm_rt[t-4] + -0.483 repeat*wm_rt[t-1] + -0.06 repeat*wm_rt[t-3] + 0.063 repeat*wm_rt[t-4] + -0.09 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.07 wm_rt[t-2]^2 + -0.065 wm_rt[t-2]*wm_rt[t-3] + -0.063 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, 88, 97, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 316/400 --- L(Train): 0.3187996 --- L(Val, RNN): 0.3001676 --- L(Val, SINDy): 0.3974967 --- Time: 3.70s; --- Convergence: 8.63e-04; LR: 1.00e-02; Metric: 0.3001676; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.625 value_reward_diff[t] + 2.334 reward_diff + -0.38 value_reward_diff^2 + 0.127 value_reward_diff*reward_diff + 2.279 reward_diff^2 \n",
      "value_persistance[t+1] = -0.406 1 + 0.726 value_persistance[t] + -0.072 repeat + 0.385 value_persistance^2 + -0.18 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.183 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.016 wm_rt[t-4] + 0.008 wm_rt^2 + 0.104 wm_rt*wm_rt[t-1] + -0.194 wm_rt*wm_rt[t-4] + -0.483 repeat*wm_rt[t-1] + -0.058 repeat*wm_rt[t-3] + 0.064 repeat*wm_rt[t-4] + -0.096 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.082 switch*wm_rt[t-4] + -0.069 wm_rt[t-2]^2 + -0.064 wm_rt[t-2]*wm_rt[t-3] + -0.062 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, 89, 98, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 317/400 --- L(Train): 0.3221453 --- L(Val, RNN): 0.3025476 --- L(Val, SINDy): 0.4081520 --- Time: 3.48s; --- Convergence: 1.62e-03; LR: 1.00e-02; Metric: 0.3001676; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.626 value_reward_diff[t] + 2.34 reward_diff + -0.378 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 2.285 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.725 value_persistance[t] + -0.071 repeat + 0.386 value_persistance^2 + -0.18 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.185 wm_rt[t-2] + 0.066 wm_rt[t-3] + 0.016 wm_rt[t-4] + 0.009 wm_rt^2 + 0.102 wm_rt*wm_rt[t-1] + -0.197 wm_rt*wm_rt[t-4] + -0.482 repeat*wm_rt[t-1] + -0.056 repeat*wm_rt[t-3] + 0.066 repeat*wm_rt[t-4] + -0.101 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.068 wm_rt[t-2]^2 + -0.063 wm_rt[t-2]*wm_rt[t-3] + -0.061 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, 90, 99, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 318/400 --- L(Train): 0.3321941 --- L(Val, RNN): 0.2973688 --- L(Val, SINDy): 0.4121403 --- Time: 3.57s; --- Convergence: 3.40e-03; LR: 1.00e-02; Metric: 0.2973688; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.626 value_reward_diff[t] + 2.345 reward_diff + -0.377 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 2.29 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.724 value_persistance[t] + -0.071 repeat + 0.387 value_persistance^2 + -0.18 value_persistance*repeat + -0.072 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.187 wm_rt[t-2] + 0.068 wm_rt[t-3] + 0.018 wm_rt[t-4] + 0.1 wm_rt*wm_rt[t-1] + -0.199 wm_rt*wm_rt[t-4] + -0.48 repeat*wm_rt[t-1] + -0.053 repeat*wm_rt[t-3] + 0.067 repeat*wm_rt[t-4] + -0.105 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, 91, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 319/400 --- L(Train): 0.3271460 --- L(Val, RNN): 0.2987848 --- L(Val, SINDy): 0.4122117 --- Time: 3.56s; --- Convergence: 2.41e-03; LR: 1.00e-02; Metric: 0.2973688; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.626 value_reward_diff[t] + 2.351 reward_diff + -0.376 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 2.294 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.723 value_persistance[t] + -0.07 repeat + 0.388 value_persistance^2 + -0.181 value_persistance*repeat + -0.072 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.188 wm_rt[t-2] + 0.069 wm_rt[t-3] + 0.019 wm_rt[t-4] + 0.099 wm_rt*wm_rt[t-1] + -0.201 wm_rt*wm_rt[t-4] + -0.479 repeat*wm_rt[t-1] + -0.052 repeat*wm_rt[t-3] + 0.069 repeat*wm_rt[t-4] + -0.109 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.084 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.059 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.11 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, 92, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 320/400 --- L(Train): 0.3280776 --- L(Val, RNN): 0.2991120 --- L(Val, SINDy): 0.3633400 --- Time: 3.69s; --- Convergence: 1.37e-03; LR: 1.00e-02; Metric: 0.2973688; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.627 value_reward_diff[t] + 2.356 reward_diff + -0.374 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 2.299 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.723 value_persistance[t] + -0.07 repeat + 0.388 value_persistance^2 + -0.18 value_persistance*repeat + -0.072 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.189 wm_rt[t-2] + 0.07 wm_rt[t-3] + 0.019 wm_rt[t-4] + 0.097 wm_rt*wm_rt[t-1] + -0.203 wm_rt*wm_rt[t-4] + -0.478 repeat*wm_rt[t-1] + -0.052 repeat*wm_rt[t-3] + 0.068 repeat*wm_rt[t-4] + -0.113 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.084 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.059 wm_rt[t-2]*wm_rt[t-3] + -0.056 wm_rt[t-2]*wm_rt[t-4] + -0.11 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, 93, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 321/400 --- L(Train): 0.3240631 --- L(Val, RNN): 0.2985488 --- L(Val, SINDy): 0.3786218 --- Time: 3.59s; --- Convergence: 9.65e-04; LR: 1.00e-02; Metric: 0.2973688; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.627 value_reward_diff[t] + 2.361 reward_diff + -0.373 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 2.303 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.722 value_persistance[t] + -0.071 repeat + 0.389 value_persistance^2 + -0.18 value_persistance*repeat + -0.072 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.188 wm_rt[t-2] + 0.069 wm_rt[t-3] + 0.018 wm_rt[t-4] + 0.096 wm_rt*wm_rt[t-1] + -0.204 wm_rt*wm_rt[t-4] + -0.477 repeat*wm_rt[t-1] + -0.054 repeat*wm_rt[t-3] + 0.066 repeat*wm_rt[t-4] + -0.117 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.084 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.059 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, 94, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 322/400 --- L(Train): 0.3178616 --- L(Val, RNN): 0.2992027 --- L(Val, SINDy): 0.3887990 --- Time: 4.14s; --- Convergence: 8.10e-04; LR: 1.00e-02; Metric: 0.2973688; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.626 value_reward_diff[t] + 2.365 reward_diff + -0.373 value_reward_diff^2 + 0.127 value_reward_diff*reward_diff + 2.307 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.722 value_persistance[t] + -0.071 repeat + 0.389 value_persistance^2 + -0.179 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.188 wm_rt[t-2] + 0.068 wm_rt[t-3] + 0.017 wm_rt[t-4] + 0.095 wm_rt*wm_rt[t-1] + -0.204 wm_rt*wm_rt[t-4] + -0.476 repeat*wm_rt[t-1] + -0.056 repeat*wm_rt[t-3] + 0.064 repeat*wm_rt[t-4] + -0.121 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.059 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, 95, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 323/400 --- L(Train): 0.3202820 --- L(Val, RNN): 0.2970037 --- L(Val, SINDy): 0.3957784 --- Time: 4.15s; --- Convergence: 1.50e-03; LR: 1.00e-02; Metric: 0.2970037; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.625 value_reward_diff[t] + 2.369 reward_diff + -0.374 value_reward_diff^2 + 0.126 value_reward_diff*reward_diff + 2.311 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.722 value_persistance[t] + -0.072 repeat + 0.389 value_persistance^2 + -0.179 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.187 wm_rt[t-2] + 0.067 wm_rt[t-3] + 0.015 wm_rt[t-4] + 0.093 wm_rt*wm_rt[t-1] + -0.205 wm_rt*wm_rt[t-4] + -0.475 repeat*wm_rt[t-1] + -0.058 repeat*wm_rt[t-3] + 0.062 repeat*wm_rt[t-4] + -0.125 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, 96, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 324/400 --- L(Train): 0.3256041 --- L(Val, RNN): 0.2974610 --- L(Val, SINDy): 0.3946098 --- Time: 3.99s; --- Convergence: 9.81e-04; LR: 1.00e-02; Metric: 0.2970037; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.627 value_reward_diff[t] + 2.374 reward_diff + -0.371 value_reward_diff^2 + 0.126 value_reward_diff*reward_diff + 2.315 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.721 value_persistance[t] + -0.071 repeat + 0.39 value_persistance^2 + -0.179 value_persistance*repeat + -0.073 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.187 wm_rt[t-2] + 0.066 wm_rt[t-3] + 0.014 wm_rt[t-4] + 0.092 wm_rt*wm_rt[t-1] + -0.205 wm_rt*wm_rt[t-4] + -0.474 repeat*wm_rt[t-1] + -0.06 repeat*wm_rt[t-3] + 0.06 repeat*wm_rt[t-4] + -0.129 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-3] + 0.082 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.115 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, 97, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 325/400 --- L(Train): 0.3215222 --- L(Val, RNN): 0.2967951 --- L(Val, SINDy): 0.4473838 --- Time: 3.99s; --- Convergence: 8.23e-04; LR: 1.00e-02; Metric: 0.2967951; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.06 1 + 0.627 value_reward_diff[t] + 2.378 reward_diff + -0.371 value_reward_diff^2 + 0.125 value_reward_diff*reward_diff + 2.319 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.722 value_persistance[t] + -0.072 repeat + 0.389 value_persistance^2 + -0.178 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.187 wm_rt[t-2] + 0.066 wm_rt[t-3] + 0.014 wm_rt[t-4] + 0.09 wm_rt*wm_rt[t-1] + -0.206 wm_rt*wm_rt[t-4] + -0.473 repeat*wm_rt[t-1] + -0.061 repeat*wm_rt[t-3] + 0.059 repeat*wm_rt[t-4] + -0.132 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.082 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.115 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, 98, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 326/400 --- L(Train): 0.3270840 --- L(Val, RNN): 0.2959006 --- L(Val, SINDy): 0.4388635 --- Time: 4.22s; --- Convergence: 8.59e-04; LR: 1.00e-02; Metric: 0.2959006; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.061 1 + 0.625 value_reward_diff[t] + 2.382 reward_diff + -0.372 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.323 reward_diff^2 \n",
      "value_persistance[t+1] = -0.406 1 + 0.724 value_persistance[t] + -0.075 repeat + 0.387 value_persistance^2 + -0.176 value_persistance*repeat + -0.076 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.188 wm_rt[t-2] + 0.067 wm_rt[t-3] + 0.014 wm_rt[t-4] + 0.088 wm_rt*wm_rt[t-1] + -0.208 wm_rt*wm_rt[t-4] + -0.47 repeat*wm_rt[t-1] + -0.061 repeat*wm_rt[t-3] + 0.06 repeat*wm_rt[t-4] + -0.134 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.082 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.059 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.115 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, 99, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 327/400 --- L(Train): 0.3207085 --- L(Val, RNN): 0.2962732 --- L(Val, SINDy): 0.4493873 --- Time: 3.58s; --- Convergence: 6.16e-04; LR: 1.00e-02; Metric: 0.2959006; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.061 1 + 0.626 value_reward_diff[t] + 2.387 reward_diff + -0.37 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 2.326 reward_diff^2 \n",
      "value_persistance[t+1] = -0.409 1 + 0.727 value_persistance[t] + -0.076 repeat + 0.384 value_persistance^2 + -0.174 value_persistance*repeat + -0.078 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.189 wm_rt[t-2] + 0.067 wm_rt[t-3] + 0.086 wm_rt*wm_rt[t-1] + -0.209 wm_rt*wm_rt[t-4] + -0.468 repeat*wm_rt[t-1] + -0.06 repeat*wm_rt[t-3] + 0.061 repeat*wm_rt[t-4] + -0.136 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.063 wm_rt[t-2]^2 + -0.058 wm_rt[t-2]*wm_rt[t-3] + -0.056 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 328/400 --- L(Train): 0.3223554 --- L(Val, RNN): 0.2945677 --- L(Val, SINDy): 0.4359658 --- Time: 4.11s; --- Convergence: 1.16e-03; LR: 1.00e-02; Metric: 0.2945677; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.061 1 + 0.627 value_reward_diff[t] + 2.391 reward_diff + -0.368 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 2.331 reward_diff^2 \n",
      "value_persistance[t+1] = -0.411 1 + 0.729 value_persistance[t] + -0.076 repeat + 0.383 value_persistance^2 + -0.174 value_persistance*repeat + -0.078 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.191 wm_rt[t-2] + 0.069 wm_rt[t-3] + 0.084 wm_rt*wm_rt[t-1] + -0.211 wm_rt*wm_rt[t-4] + -0.466 repeat*wm_rt[t-1] + -0.059 repeat*wm_rt[t-3] + 0.063 repeat*wm_rt[t-4] + -0.138 switch*wm_rt[t-1] + -0.07 switch*wm_rt[t-3] + 0.084 switch*wm_rt[t-4] + -0.061 wm_rt[t-2]^2 + -0.057 wm_rt[t-2]*wm_rt[t-3] + -0.054 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 329/400 --- L(Train): 0.3259081 --- L(Val, RNN): 0.2969436 --- L(Val, SINDy): 0.3723361 --- Time: 3.86s; --- Convergence: 1.77e-03; LR: 1.00e-02; Metric: 0.2945677; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.062 1 + 0.626 value_reward_diff[t] + 2.396 reward_diff + -0.368 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.334 reward_diff^2 \n",
      "value_persistance[t+1] = -0.41 1 + 0.728 value_persistance[t] + -0.073 repeat + 0.383 value_persistance^2 + -0.177 value_persistance*repeat + -0.074 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.192 wm_rt[t-2] + 0.07 wm_rt[t-3] + 0.081 wm_rt*wm_rt[t-1] + -0.213 wm_rt*wm_rt[t-4] + -0.464 repeat*wm_rt[t-1] + -0.058 repeat*wm_rt[t-3] + 0.065 repeat*wm_rt[t-4] + -0.14 switch*wm_rt[t-1] + -0.069 switch*wm_rt[t-3] + 0.084 switch*wm_rt[t-4] + -0.06 wm_rt[t-2]^2 + -0.055 wm_rt[t-2]*wm_rt[t-3] + -0.053 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 330/400 --- L(Train): 0.3234551 --- L(Val, RNN): 0.2943005 --- L(Val, SINDy): 0.3541069 --- Time: 4.01s; --- Convergence: 2.21e-03; LR: 1.00e-02; Metric: 0.2943005; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.063 1 + 0.626 value_reward_diff[t] + 2.4 reward_diff + -0.368 value_reward_diff^2 + 0.122 value_reward_diff*reward_diff + 2.337 reward_diff^2 \n",
      "value_persistance[t+1] = -0.407 1 + 0.725 value_persistance[t] + -0.068 repeat + 0.385 value_persistance^2 + -0.182 value_persistance*repeat + -0.069 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.192 wm_rt[t-2] + 0.07 wm_rt[t-3] + 0.079 wm_rt*wm_rt[t-1] + -0.215 wm_rt*wm_rt[t-4] + -0.461 repeat*wm_rt[t-1] + -0.057 repeat*wm_rt[t-3] + 0.067 repeat*wm_rt[t-4] + -0.143 switch*wm_rt[t-1] + -0.069 switch*wm_rt[t-3] + 0.084 switch*wm_rt[t-4] + -0.06 wm_rt[t-2]^2 + -0.055 wm_rt[t-2]*wm_rt[t-3] + -0.053 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 331/400 --- L(Train): 0.3210994 --- L(Val, RNN): 0.2936573 --- L(Val, SINDy): 0.3319583 --- Time: 3.65s; --- Convergence: 1.42e-03; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.063 1 + 0.625 value_reward_diff[t] + 2.404 reward_diff + -0.367 value_reward_diff^2 + 0.121 value_reward_diff*reward_diff + 2.341 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.723 value_persistance[t] + -0.064 repeat + 0.387 value_persistance^2 + -0.185 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.192 wm_rt[t-2] + 0.07 wm_rt[t-3] + 0.077 wm_rt*wm_rt[t-1] + -0.217 wm_rt*wm_rt[t-4] + -0.459 repeat*wm_rt[t-1] + -0.056 repeat*wm_rt[t-3] + 0.069 repeat*wm_rt[t-4] + -0.147 switch*wm_rt[t-1] + -0.07 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.06 wm_rt[t-2]^2 + -0.055 wm_rt[t-2]*wm_rt[t-3] + -0.053 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 332/400 --- L(Train): 0.3251639 --- L(Val, RNN): 0.2941051 --- L(Val, SINDy): 0.3594560 --- Time: 4.36s; --- Convergence: 9.36e-04; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.063 1 + 0.625 value_reward_diff[t] + 2.408 reward_diff + -0.366 value_reward_diff^2 + 0.121 value_reward_diff*reward_diff + 2.344 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.722 value_persistance[t] + -0.061 repeat + 0.388 value_persistance^2 + -0.187 value_persistance*repeat + -0.063 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.191 wm_rt[t-2] + 0.069 wm_rt[t-3] + 0.074 wm_rt*wm_rt[t-1] + -0.218 wm_rt*wm_rt[t-4] + -0.457 repeat*wm_rt[t-1] + -0.057 repeat*wm_rt[t-3] + 0.07 repeat*wm_rt[t-4] + -0.15 switch*wm_rt[t-1] + -0.07 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.061 wm_rt[t-2]^2 + -0.056 wm_rt[t-2]*wm_rt[t-3] + -0.053 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 333/400 --- L(Train): 0.3319841 --- L(Val, RNN): 0.3222405 --- L(Val, SINDy): 0.4037085 --- Time: 3.52s; --- Convergence: 1.45e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.062 1 + 0.627 value_reward_diff[t] + 2.412 reward_diff + -0.364 value_reward_diff^2 + 0.122 value_reward_diff*reward_diff + 2.348 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.722 value_persistance[t] + -0.061 repeat + 0.389 value_persistance^2 + -0.188 value_persistance*repeat + -0.062 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.191 wm_rt[t-2] + 0.068 wm_rt[t-3] + 0.072 wm_rt*wm_rt[t-1] + -0.219 wm_rt*wm_rt[t-4] + -0.456 repeat*wm_rt[t-1] + -0.057 repeat*wm_rt[t-3] + 0.071 repeat*wm_rt[t-4] + -0.154 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.082 switch*wm_rt[t-4] + -0.061 wm_rt[t-2]^2 + -0.056 wm_rt[t-2]*wm_rt[t-3] + -0.054 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 334/400 --- L(Train): 10.2070990 --- L(Val, RNN): 0.3400777 --- L(Val, SINDy): 0.3476900 --- Time: 3.61s; --- Convergence: 1.62e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.061 1 + 0.628 value_reward_diff[t] + 2.416 reward_diff + -0.362 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.352 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.721 value_persistance[t] + -0.06 repeat + 0.389 value_persistance^2 + -0.188 value_persistance*repeat + -0.061 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.19 wm_rt[t-2] + 0.068 wm_rt[t-3] + 0.07 wm_rt*wm_rt[t-1] + -0.22 wm_rt*wm_rt[t-4] + -0.454 repeat*wm_rt[t-1] + -0.057 repeat*wm_rt[t-3] + 0.072 repeat*wm_rt[t-4] + -0.157 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.081 switch*wm_rt[t-4] + -0.062 wm_rt[t-2]^2 + -0.057 wm_rt[t-2]*wm_rt[t-3] + -0.055 wm_rt[t-2]*wm_rt[t-4] + -0.112 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 335/400 --- L(Train): 6.8366408 --- L(Val, RNN): 0.3580654 --- L(Val, SINDy): 0.3442738 --- Time: 4.25s; --- Convergence: 1.71e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.061 1 + 0.628 value_reward_diff[t] + 2.42 reward_diff + -0.36 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 2.355 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.721 value_persistance[t] + -0.059 repeat + 0.389 value_persistance^2 + -0.189 value_persistance*repeat + -0.061 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.189 wm_rt[t-2] + 0.067 wm_rt[t-3] + 0.069 wm_rt*wm_rt[t-1] + -0.221 wm_rt*wm_rt[t-4] + -0.453 repeat*wm_rt[t-1] + -0.058 repeat*wm_rt[t-3] + 0.072 repeat*wm_rt[t-4] + -0.16 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.063 wm_rt[t-2]^2 + -0.058 wm_rt[t-2]*wm_rt[t-3] + -0.055 wm_rt[t-2]*wm_rt[t-4] + -0.112 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 336/400 --- L(Train): 7.9578753 --- L(Val, RNN): 0.3705816 --- L(Val, SINDy): 0.3503918 --- Time: 3.81s; --- Convergence: 1.48e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.06 1 + 0.629 value_reward_diff[t] + 2.423 reward_diff + -0.358 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 2.358 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.721 value_persistance[t] + -0.059 repeat + 0.389 value_persistance^2 + -0.189 value_persistance*repeat + -0.06 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.189 wm_rt[t-2] + 0.066 wm_rt[t-3] + 0.067 wm_rt*wm_rt[t-1] + -0.222 wm_rt*wm_rt[t-4] + -0.452 repeat*wm_rt[t-1] + -0.058 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + -0.163 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.063 wm_rt[t-2]^2 + -0.058 wm_rt[t-2]*wm_rt[t-3] + -0.056 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 337/400 --- L(Train): 5.1120715 --- L(Val, RNN): 0.3764606 --- L(Val, SINDy): 0.3607942 --- Time: 3.65s; --- Convergence: 1.03e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 6/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.63 value_reward_diff[t] + 2.426 reward_diff + -0.356 value_reward_diff^2 + 0.125 value_reward_diff*reward_diff + 2.36 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.721 value_persistance[t] + -0.058 repeat + 0.389 value_persistance^2 + -0.189 value_persistance*repeat + -0.06 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.188 wm_rt[t-2] + 0.066 wm_rt[t-3] + 0.066 wm_rt*wm_rt[t-1] + -0.223 wm_rt*wm_rt[t-4] + -0.45 repeat*wm_rt[t-1] + -0.059 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + -0.165 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.079 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.059 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 338/400 --- L(Train): 3.3121078 --- L(Val, RNN): 0.3782036 --- L(Val, SINDy): 0.3685272 --- Time: 4.07s; --- Convergence: 6.04e-03; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 7/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.631 value_reward_diff[t] + 2.428 reward_diff + -0.355 value_reward_diff^2 + 0.126 value_reward_diff*reward_diff + 2.362 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.72 value_persistance[t] + -0.058 repeat + 0.39 value_persistance^2 + -0.19 value_persistance*repeat + -0.06 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.188 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.065 wm_rt*wm_rt[t-1] + -0.224 wm_rt*wm_rt[t-4] + -0.449 repeat*wm_rt[t-1] + -0.059 repeat*wm_rt[t-3] + 0.074 repeat*wm_rt[t-4] + -0.167 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.079 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.059 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 339/400 --- L(Train): 3.5699401 --- L(Val, RNN): 0.3843220 --- L(Val, SINDy): 0.3742573 --- Time: 3.79s; --- Convergence: 6.08e-03; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 8/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.058 1 + 0.632 value_reward_diff[t] + 2.431 reward_diff + -0.353 value_reward_diff^2 + 0.126 value_reward_diff*reward_diff + 2.365 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.72 value_persistance[t] + -0.058 repeat + 0.39 value_persistance^2 + -0.19 value_persistance*repeat + -0.059 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.187 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.063 wm_rt*wm_rt[t-1] + -0.224 wm_rt*wm_rt[t-4] + -0.448 repeat*wm_rt[t-1] + -0.059 repeat*wm_rt[t-3] + 0.074 repeat*wm_rt[t-4] + -0.169 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.078 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.057 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 340/400 --- L(Train): 3.6587813 --- L(Val, RNN): 0.4460352 --- L(Val, SINDy): 0.3804890 --- Time: 3.87s; --- Convergence: 3.39e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 9/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.633 value_reward_diff[t] + 2.433 reward_diff + -0.352 value_reward_diff^2 + 0.127 value_reward_diff*reward_diff + 2.367 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.72 value_persistance[t] + -0.058 repeat + 0.39 value_persistance^2 + -0.19 value_persistance*repeat + -0.059 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.187 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.062 wm_rt*wm_rt[t-1] + -0.225 wm_rt*wm_rt[t-4] + -0.447 repeat*wm_rt[t-1] + -0.059 repeat*wm_rt[t-3] + 0.074 repeat*wm_rt[t-4] + -0.171 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.078 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 341/400 --- L(Train): 8.6485453 --- L(Val, RNN): 0.4330454 --- L(Val, SINDy): 0.3856001 --- Time: 3.94s; --- Convergence: 2.34e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 10/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.634 value_reward_diff[t] + 2.435 reward_diff + -0.35 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 2.368 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.72 value_persistance[t] + -0.057 repeat + 0.39 value_persistance^2 + -0.19 value_persistance*repeat + -0.059 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.187 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.061 wm_rt*wm_rt[t-1] + -0.226 wm_rt*wm_rt[t-4] + -0.447 repeat*wm_rt[t-1] + -0.06 repeat*wm_rt[t-3] + 0.075 repeat*wm_rt[t-4] + -0.172 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.077 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 342/400 --- L(Train): 8.7584801 --- L(Val, RNN): 0.4172839 --- L(Val, SINDy): 0.3898446 --- Time: 4.06s; --- Convergence: 1.96e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 11/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.634 value_reward_diff[t] + 2.437 reward_diff + -0.349 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 2.37 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.72 value_persistance[t] + -0.057 repeat + 0.39 value_persistance^2 + -0.19 value_persistance*repeat + -0.059 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.186 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.061 wm_rt*wm_rt[t-1] + -0.226 wm_rt*wm_rt[t-4] + -0.446 repeat*wm_rt[t-1] + -0.06 repeat*wm_rt[t-3] + 0.075 repeat*wm_rt[t-4] + -0.174 switch*wm_rt[t-1] + -0.076 switch*wm_rt[t-3] + 0.077 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 343/400 --- L(Train): 4.0566211 --- L(Val, RNN): 0.4055131 --- L(Val, SINDy): 0.3935468 --- Time: 3.52s; --- Convergence: 1.57e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 12/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.635 value_reward_diff[t] + 2.439 reward_diff + -0.348 value_reward_diff^2 + 0.129 value_reward_diff*reward_diff + 2.371 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.72 value_persistance[t] + -0.057 repeat + 0.39 value_persistance^2 + -0.19 value_persistance*repeat + -0.059 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.186 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.06 wm_rt*wm_rt[t-1] + -0.226 wm_rt*wm_rt[t-4] + -0.445 repeat*wm_rt[t-1] + -0.06 repeat*wm_rt[t-3] + 0.075 repeat*wm_rt[t-4] + -0.175 switch*wm_rt[t-1] + -0.076 switch*wm_rt[t-3] + 0.077 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 344/400 --- L(Train): 2.2072220 --- L(Val, RNN): 0.3908136 --- L(Val, SINDy): 0.3956687 --- Time: 3.73s; --- Convergence: 1.52e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 13/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.635 value_reward_diff[t] + 2.44 reward_diff + -0.347 value_reward_diff^2 + 0.129 value_reward_diff*reward_diff + 2.373 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.72 value_persistance[t] + -0.057 repeat + 0.39 value_persistance^2 + -0.19 value_persistance*repeat + -0.059 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.186 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.059 wm_rt*wm_rt[t-1] + -0.227 wm_rt*wm_rt[t-4] + -0.445 repeat*wm_rt[t-1] + -0.06 repeat*wm_rt[t-3] + 0.075 repeat*wm_rt[t-4] + -0.176 switch*wm_rt[t-1] + -0.076 switch*wm_rt[t-3] + 0.076 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 345/400 --- L(Train): 1.5487916 --- L(Val, RNN): 0.3344834 --- L(Val, SINDy): 0.3964165 --- Time: 3.83s; --- Convergence: 3.58e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 14/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.636 value_reward_diff[t] + 2.442 reward_diff + -0.346 value_reward_diff^2 + 0.13 value_reward_diff*reward_diff + 2.374 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.72 value_persistance[t] + -0.057 repeat + 0.39 value_persistance^2 + -0.19 value_persistance*repeat + -0.059 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.186 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.058 wm_rt*wm_rt[t-1] + -0.227 wm_rt*wm_rt[t-4] + -0.444 repeat*wm_rt[t-1] + -0.06 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.177 switch*wm_rt[t-1] + -0.076 switch*wm_rt[t-3] + 0.076 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 346/400 --- L(Train): 1.2669982 --- L(Val, RNN): 0.3291667 --- L(Val, SINDy): 0.3935139 --- Time: 3.80s; --- Convergence: 2.05e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 15/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.635 value_reward_diff[t] + 2.443 reward_diff + -0.347 value_reward_diff^2 + 0.129 value_reward_diff*reward_diff + 2.375 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.72 value_persistance[t] + -0.057 repeat + 0.39 value_persistance^2 + -0.19 value_persistance*repeat + -0.059 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.186 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.058 wm_rt*wm_rt[t-1] + -0.228 wm_rt*wm_rt[t-4] + -0.443 repeat*wm_rt[t-1] + -0.06 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.178 switch*wm_rt[t-1] + -0.076 switch*wm_rt[t-3] + 0.076 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.059 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 347/400 --- L(Train): 1.1449565 --- L(Val, RNN): 0.3250363 --- L(Val, SINDy): 0.3859073 --- Time: 3.66s; --- Convergence: 1.23e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 16/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.056 1 + 0.633 value_reward_diff[t] + 2.444 reward_diff + -0.349 value_reward_diff^2 + 0.127 value_reward_diff*reward_diff + 2.376 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.72 value_persistance[t] + -0.058 repeat + 0.39 value_persistance^2 + -0.189 value_persistance*repeat + -0.059 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.186 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.057 wm_rt*wm_rt[t-1] + -0.229 wm_rt*wm_rt[t-4] + -0.443 repeat*wm_rt[t-1] + -0.06 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.179 switch*wm_rt[t-1] + -0.076 switch*wm_rt[t-3] + 0.077 switch*wm_rt[t-4] + -0.066 wm_rt[t-2]^2 + -0.061 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 348/400 --- L(Train): 0.8095630 --- L(Val, RNN): 0.3170498 --- L(Val, SINDy): 0.3698272 --- Time: 4.11s; --- Convergence: 1.02e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 17/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.627 value_reward_diff[t] + 2.444 reward_diff + -0.354 value_reward_diff^2 + 0.121 value_reward_diff*reward_diff + 2.376 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.72 value_persistance[t] + -0.059 repeat + 0.39 value_persistance^2 + -0.188 value_persistance*repeat + -0.061 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.187 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.056 wm_rt*wm_rt[t-1] + -0.23 wm_rt*wm_rt[t-4] + -0.442 repeat*wm_rt[t-1] + -0.061 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.179 switch*wm_rt[t-1] + -0.075 switch*wm_rt[t-3] + 0.077 switch*wm_rt[t-4] + -0.065 wm_rt[t-2]^2 + -0.06 wm_rt[t-2]*wm_rt[t-3] + -0.058 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 349/400 --- L(Train): 0.6844192 --- L(Val, RNN): 0.3031698 --- L(Val, SINDy): 0.3619367 --- Time: 3.99s; --- Convergence: 1.20e-02; LR: 1.00e-02; Metric: 0.2936573; Bad epochs: 18/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.064 1 + 0.619 value_reward_diff[t] + 2.443 reward_diff + -0.363 value_reward_diff^2 + 0.113 value_reward_diff*reward_diff + 2.375 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.72 value_persistance[t] + -0.061 repeat + 0.39 value_persistance^2 + -0.187 value_persistance*repeat + -0.063 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.188 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.055 wm_rt*wm_rt[t-1] + -0.231 wm_rt*wm_rt[t-4] + -0.442 repeat*wm_rt[t-1] + -0.061 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.179 switch*wm_rt[t-1] + -0.074 switch*wm_rt[t-3] + 0.079 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.059 wm_rt[t-2]*wm_rt[t-3] + -0.056 wm_rt[t-2]*wm_rt[t-4] + -0.112 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 350/400 --- L(Train): 0.5987103 --- L(Val, RNN): 0.2911579 --- L(Val, SINDy): 0.3576818 --- Time: 3.86s; --- Convergence: 1.20e-02; LR: 1.00e-02; Metric: 0.2911579; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.068 1 + 0.612 value_reward_diff[t] + 2.444 reward_diff + -0.369 value_reward_diff^2 + 0.105 value_reward_diff*reward_diff + 2.375 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.721 value_persistance[t] + -0.063 repeat + 0.389 value_persistance^2 + -0.184 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.19 wm_rt[t-2] + 0.066 wm_rt[t-3] + 0.055 wm_rt*wm_rt[t-1] + -0.232 wm_rt*wm_rt[t-4] + -0.443 repeat*wm_rt[t-1] + -0.062 repeat*wm_rt[t-3] + 0.075 repeat*wm_rt[t-4] + -0.179 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.062 wm_rt[t-2]^2 + -0.057 wm_rt[t-2]*wm_rt[t-3] + -0.055 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 351/400 --- L(Train): 0.5687265 --- L(Val, RNN): 0.2881581 --- L(Val, SINDy): 0.4093290 --- Time: 3.49s; --- Convergence: 7.51e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 0/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.067 1 + 0.61 value_reward_diff[t] + 2.446 reward_diff + -0.369 value_reward_diff^2 + 0.103 value_reward_diff*reward_diff + 2.377 reward_diff^2 \n",
      "value_persistance[t+1] = -0.406 1 + 0.721 value_persistance[t] + -0.066 repeat + 0.389 value_persistance^2 + -0.182 value_persistance*repeat + -0.067 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.191 wm_rt[t-2] + 0.067 wm_rt[t-3] + 0.054 wm_rt*wm_rt[t-1] + -0.233 wm_rt*wm_rt[t-4] + -0.443 repeat*wm_rt[t-1] + -0.063 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + -0.178 switch*wm_rt[t-1] + -0.07 switch*wm_rt[t-3] + 0.082 switch*wm_rt[t-4] + -0.061 wm_rt[t-2]^2 + -0.056 wm_rt[t-2]*wm_rt[t-3] + -0.054 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 352/400 --- L(Train): 0.5609778 --- L(Val, RNN): 0.2923753 --- L(Val, SINDy): 0.4177465 --- Time: 3.92s; --- Convergence: 5.86e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 1/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.065 1 + 0.611 value_reward_diff[t] + 2.449 reward_diff + -0.366 value_reward_diff^2 + 0.102 value_reward_diff*reward_diff + 2.38 reward_diff^2 \n",
      "value_persistance[t+1] = -0.406 1 + 0.721 value_persistance[t] + -0.068 repeat + 0.389 value_persistance^2 + -0.181 value_persistance*repeat + -0.069 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.192 wm_rt[t-2] + 0.067 wm_rt[t-3] + 0.053 wm_rt*wm_rt[t-1] + -0.233 wm_rt*wm_rt[t-4] + -0.444 repeat*wm_rt[t-1] + -0.065 repeat*wm_rt[t-3] + 0.072 repeat*wm_rt[t-4] + -0.179 switch*wm_rt[t-1] + -0.069 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.06 wm_rt[t-2]^2 + -0.055 wm_rt[t-2]*wm_rt[t-3] + -0.053 wm_rt[t-2]*wm_rt[t-4] + -0.11 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 353/400 --- L(Train): 0.5449799 --- L(Val, RNN): 0.3000332 --- L(Val, SINDy): 0.4326588 --- Time: 3.63s; --- Convergence: 6.76e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 2/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.063 1 + 0.612 value_reward_diff[t] + 2.452 reward_diff + -0.364 value_reward_diff^2 + 0.102 value_reward_diff*reward_diff + 2.384 reward_diff^2 \n",
      "value_persistance[t+1] = -0.406 1 + 0.72 value_persistance[t] + -0.069 repeat + 0.389 value_persistance^2 + -0.18 value_persistance*repeat + -0.07 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.192 wm_rt[t-2] + 0.067 wm_rt[t-3] + 0.052 wm_rt*wm_rt[t-1] + -0.233 wm_rt*wm_rt[t-4] + -0.444 repeat*wm_rt[t-1] + -0.066 repeat*wm_rt[t-3] + 0.07 repeat*wm_rt[t-4] + -0.18 switch*wm_rt[t-1] + -0.069 switch*wm_rt[t-3] + 0.083 switch*wm_rt[t-4] + -0.06 wm_rt[t-2]^2 + -0.055 wm_rt[t-2]*wm_rt[t-3] + -0.052 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 354/400 --- L(Train): 0.5520222 --- L(Val, RNN): 0.2987082 --- L(Val, SINDy): 0.4588440 --- Time: 3.53s; --- Convergence: 4.04e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 3/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.062 1 + 0.612 value_reward_diff[t] + 2.456 reward_diff + -0.362 value_reward_diff^2 + 0.101 value_reward_diff*reward_diff + 2.387 reward_diff^2 \n",
      "value_persistance[t+1] = -0.405 1 + 0.719 value_persistance[t] + -0.069 repeat + 0.391 value_persistance^2 + -0.18 value_persistance*repeat + -0.07 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.192 wm_rt[t-2] + 0.066 wm_rt[t-3] + 0.052 wm_rt*wm_rt[t-1] + -0.233 wm_rt*wm_rt[t-4] + -0.445 repeat*wm_rt[t-1] + -0.068 repeat*wm_rt[t-3] + 0.067 repeat*wm_rt[t-4] + -0.182 switch*wm_rt[t-1] + -0.07 switch*wm_rt[t-3] + 0.082 switch*wm_rt[t-4] + -0.06 wm_rt[t-2]^2 + -0.055 wm_rt[t-2]*wm_rt[t-3] + -0.053 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 355/400 --- L(Train): 0.5071290 --- L(Val, RNN): 0.2908295 --- L(Val, SINDy): 0.4796678 --- Time: 3.89s; --- Convergence: 5.96e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 4/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.061 1 + 0.612 value_reward_diff[t] + 2.459 reward_diff + -0.361 value_reward_diff^2 + 0.099 value_reward_diff*reward_diff + 2.39 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.717 value_persistance[t] + -0.068 repeat + 0.392 value_persistance^2 + -0.18 value_persistance*repeat + -0.069 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.192 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.051 wm_rt*wm_rt[t-1] + -0.232 wm_rt*wm_rt[t-4] + -0.446 repeat*wm_rt[t-1] + -0.071 repeat*wm_rt[t-3] + 0.064 repeat*wm_rt[t-4] + -0.184 switch*wm_rt[t-1] + -0.07 switch*wm_rt[t-3] + 0.082 switch*wm_rt[t-4] + -0.061 wm_rt[t-2]^2 + -0.056 wm_rt[t-2]*wm_rt[t-3] + -0.053 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 356/400 --- L(Train): 0.4701505 --- L(Val, RNN): 0.2890383 --- L(Val, SINDy): 0.4371158 --- Time: 3.90s; --- Convergence: 3.88e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 5/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.057 1 + 0.615 value_reward_diff[t] + 2.464 reward_diff + -0.356 value_reward_diff^2 + 0.099 value_reward_diff*reward_diff + 2.395 reward_diff^2 \n",
      "value_persistance[t+1] = -0.4 1 + 0.715 value_persistance[t] + -0.066 repeat + 0.395 value_persistance^2 + -0.182 value_persistance*repeat + -0.067 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.191 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.05 wm_rt*wm_rt[t-1] + -0.232 wm_rt*wm_rt[t-4] + -0.448 repeat*wm_rt[t-1] + -0.073 repeat*wm_rt[t-3] + 0.061 repeat*wm_rt[t-4] + -0.186 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.081 switch*wm_rt[t-4] + -0.061 wm_rt[t-2]^2 + -0.056 wm_rt[t-2]*wm_rt[t-3] + -0.054 wm_rt[t-2]*wm_rt[t-4] + -0.116 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 1, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 357/400 --- L(Train): 0.4792953 --- L(Val, RNN): 0.2943577 --- L(Val, SINDy): 0.4229468 --- Time: 3.74s; --- Convergence: 4.60e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 6/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.052 1 + 0.621 value_reward_diff[t] + 2.468 reward_diff + -0.349 value_reward_diff^2 + 0.101 value_reward_diff*reward_diff + 2.399 reward_diff^2 \n",
      "value_persistance[t+1] = -0.398 1 + 0.712 value_persistance[t] + -0.064 repeat + 0.397 value_persistance^2 + -0.184 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.191 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.049 wm_rt*wm_rt[t-1] + -0.232 wm_rt*wm_rt[t-4] + -0.45 repeat*wm_rt[t-1] + -0.075 repeat*wm_rt[t-3] + 0.059 repeat*wm_rt[t-4] + -0.188 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.081 switch*wm_rt[t-4] + -0.061 wm_rt[t-2]^2 + -0.056 wm_rt[t-2]*wm_rt[t-3] + -0.054 wm_rt[t-2]*wm_rt[t-4] + -0.117 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 2, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 358/400 --- L(Train): 0.4582182 --- L(Val, RNN): 0.3032284 --- L(Val, SINDy): 0.4114348 --- Time: 4.38s; --- Convergence: 6.73e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 7/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.047 1 + 0.626 value_reward_diff[t] + 2.473 reward_diff + -0.341 value_reward_diff^2 + 0.103 value_reward_diff*reward_diff + 2.404 reward_diff^2 \n",
      "value_persistance[t+1] = -0.396 1 + 0.71 value_persistance[t] + -0.062 repeat + 0.398 value_persistance^2 + -0.185 value_persistance*repeat + -0.063 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.191 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.048 wm_rt*wm_rt[t-1] + -0.232 wm_rt*wm_rt[t-4] + -0.451 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.057 repeat*wm_rt[t-4] + -0.19 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.061 wm_rt[t-2]^2 + -0.056 wm_rt[t-2]*wm_rt[t-3] + -0.054 wm_rt[t-2]*wm_rt[t-4] + -0.118 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 1, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 3, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 359/400 --- L(Train): 0.4614193 --- L(Val, RNN): 0.3034038 --- L(Val, SINDy): 0.3806470 --- Time: 3.48s; --- Convergence: 3.45e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 8/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.042 1 + 0.631 value_reward_diff[t] + 2.476 reward_diff + -0.334 value_reward_diff^2 + 0.104 value_reward_diff*reward_diff + 2.407 reward_diff^2 \n",
      "value_persistance[t+1] = -0.396 1 + 0.709 value_persistance[t] + -0.061 repeat + 0.399 value_persistance^2 + -0.186 value_persistance*repeat + -0.063 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.191 wm_rt[t-2] + 0.061 wm_rt[t-3] + 0.047 wm_rt*wm_rt[t-1] + -0.232 wm_rt*wm_rt[t-4] + -0.452 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.056 repeat*wm_rt[t-4] + -0.191 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.061 wm_rt[t-2]^2 + -0.056 wm_rt[t-2]*wm_rt[t-3] + -0.054 wm_rt[t-2]*wm_rt[t-4] + -0.119 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 2, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 4, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 360/400 --- L(Train): 0.4471986 --- L(Val, RNN): 0.2964518 --- L(Val, SINDy): 0.3606977 --- Time: 3.47s; --- Convergence: 5.20e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 9/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.039 1 + 0.635 value_reward_diff[t] + 2.478 reward_diff + -0.329 value_reward_diff^2 + 0.104 value_reward_diff*reward_diff + 2.409 reward_diff^2 \n",
      "value_persistance[t+1] = -0.396 1 + 0.709 value_persistance[t] + -0.061 repeat + 0.399 value_persistance^2 + -0.186 value_persistance*repeat + -0.063 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.192 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.045 wm_rt*wm_rt[t-1] + -0.233 wm_rt*wm_rt[t-4] + -0.453 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.056 repeat*wm_rt[t-4] + -0.193 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.06 wm_rt[t-2]^2 + -0.055 wm_rt[t-2]*wm_rt[t-3] + -0.053 wm_rt[t-2]*wm_rt[t-4] + -0.119 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 3, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 5, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 361/400 --- L(Train): 0.4066335 --- L(Val, RNN): 0.2952452 --- L(Val, SINDy): 0.3606545 --- Time: 4.35s; --- Convergence: 3.20e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 10/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.037 1 + 0.637 value_reward_diff[t] + 2.479 reward_diff + -0.326 value_reward_diff^2 + 0.101 value_reward_diff*reward_diff + 2.409 reward_diff^2 \n",
      "value_persistance[t+1] = -0.397 1 + 0.71 value_persistance[t] + -0.063 repeat + 0.399 value_persistance^2 + -0.185 value_persistance*repeat + -0.064 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.193 wm_rt[t-2] + 0.062 wm_rt[t-3] + 0.043 wm_rt*wm_rt[t-1] + -0.234 wm_rt*wm_rt[t-4] + -0.455 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.056 repeat*wm_rt[t-4] + -0.194 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.059 wm_rt[t-2]^2 + -0.054 wm_rt[t-2]*wm_rt[t-3] + -0.052 wm_rt[t-2]*wm_rt[t-4] + -0.118 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 4, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 6, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 362/400 --- L(Train): 0.3992549 --- L(Val, RNN): 0.2998573 --- L(Val, SINDy): 0.3618727 --- Time: 3.93s; --- Convergence: 3.91e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 11/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.039 1 + 0.636 value_reward_diff[t] + 2.478 reward_diff + -0.327 value_reward_diff^2 + 0.094 value_reward_diff*reward_diff + 2.407 reward_diff^2 \n",
      "value_persistance[t+1] = -0.399 1 + 0.711 value_persistance[t] + -0.064 repeat + 0.398 value_persistance^2 + -0.183 value_persistance*repeat + -0.066 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.194 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.041 wm_rt*wm_rt[t-1] + -0.236 wm_rt*wm_rt[t-4] + -0.455 repeat*wm_rt[t-1] + -0.075 repeat*wm_rt[t-3] + 0.057 repeat*wm_rt[t-4] + -0.195 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.058 wm_rt[t-2]^2 + -0.053 wm_rt[t-2]*wm_rt[t-3] + -0.051 wm_rt[t-2]*wm_rt[t-4] + -0.118 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 5, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 7, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 363/400 --- L(Train): 0.4092784 --- L(Val, RNN): 0.2997425 --- L(Val, SINDy): 0.3739857 --- Time: 4.08s; --- Convergence: 2.01e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 12/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.044 1 + 0.63 value_reward_diff[t] + 2.474 reward_diff + -0.332 value_reward_diff^2 + 0.084 value_reward_diff*reward_diff + 2.403 reward_diff^2 \n",
      "value_persistance[t+1] = -0.401 1 + 0.713 value_persistance[t] + -0.066 repeat + 0.396 value_persistance^2 + -0.182 value_persistance*repeat + -0.068 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.195 wm_rt[t-2] + 0.063 wm_rt[t-3] + 0.039 wm_rt*wm_rt[t-1] + -0.238 wm_rt*wm_rt[t-4] + -0.455 repeat*wm_rt[t-1] + -0.073 repeat*wm_rt[t-3] + 0.059 repeat*wm_rt[t-4] + -0.197 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.057 wm_rt[t-2]^2 + -0.053 wm_rt[t-2]*wm_rt[t-3] + -0.05 wm_rt[t-2]*wm_rt[t-4] + -0.118 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 6, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 8, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 364/400 --- L(Train): 0.3761092 --- L(Val, RNN): 0.2941927 --- L(Val, SINDy): 0.4201332 --- Time: 4.22s; --- Convergence: 3.78e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 13/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.05 1 + 0.623 value_reward_diff[t] + 2.469 reward_diff + -0.339 value_reward_diff^2 + 0.072 value_reward_diff*reward_diff + 2.398 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.714 value_persistance[t] + -0.067 repeat + 0.395 value_persistance^2 + -0.18 value_persistance*repeat + -0.069 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.196 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.036 wm_rt*wm_rt[t-1] + -0.24 wm_rt*wm_rt[t-4] + -0.455 repeat*wm_rt[t-1] + -0.07 repeat*wm_rt[t-3] + 0.061 repeat*wm_rt[t-4] + -0.199 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.056 wm_rt[t-2]^2 + -0.052 wm_rt[t-2]*wm_rt[t-3] + -0.049 wm_rt[t-2]*wm_rt[t-4] + -0.117 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 9, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 1, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 365/400 --- L(Train): 0.3848359 --- L(Val, RNN): 0.2915271 --- L(Val, SINDy): 0.4219346 --- Time: 3.75s; --- Convergence: 3.22e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 14/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.62 value_reward_diff[t] + 2.465 reward_diff + -0.342 value_reward_diff^2 + 0.063 value_reward_diff*reward_diff + 2.393 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.715 value_persistance[t] + -0.068 repeat + 0.394 value_persistance^2 + -0.18 value_persistance*repeat + -0.069 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.197 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.034 wm_rt*wm_rt[t-1] + -0.242 wm_rt*wm_rt[t-4] + -0.455 repeat*wm_rt[t-1] + -0.068 repeat*wm_rt[t-3] + 0.063 repeat*wm_rt[t-4] + -0.2 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.055 wm_rt[t-2]^2 + -0.051 wm_rt[t-2]*wm_rt[t-3] + -0.048 wm_rt[t-2]*wm_rt[t-4] + -0.116 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 10, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 2, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 366/400 --- L(Train): 0.3750812 --- L(Val, RNN): 0.2940610 --- L(Val, SINDy): 0.4178765 --- Time: 4.10s; --- Convergence: 2.88e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 15/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.054 1 + 0.62 value_reward_diff[t] + 2.462 reward_diff + -0.34 value_reward_diff^2 + 0.056 value_reward_diff*reward_diff + 2.389 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.714 value_persistance[t] + -0.066 repeat + 0.395 value_persistance^2 + -0.181 value_persistance*repeat + -0.068 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.197 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.031 wm_rt*wm_rt[t-1] + -0.244 wm_rt*wm_rt[t-4] + -0.456 repeat*wm_rt[t-1] + -0.066 repeat*wm_rt[t-3] + 0.064 repeat*wm_rt[t-4] + -0.202 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.079 switch*wm_rt[t-4] + -0.055 wm_rt[t-2]^2 + -0.05 wm_rt[t-2]*wm_rt[t-3] + -0.048 wm_rt[t-2]*wm_rt[t-4] + -0.116 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 11, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 3, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 367/400 --- L(Train): 0.3729753 --- L(Val, RNN): 0.2941380 --- L(Val, SINDy): 0.3991528 --- Time: 3.49s; --- Convergence: 1.48e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 16/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.055 1 + 0.619 value_reward_diff[t] + 2.458 reward_diff + -0.34 value_reward_diff^2 + 0.049 value_reward_diff*reward_diff + 2.384 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.713 value_persistance[t] + -0.064 repeat + 0.396 value_persistance^2 + -0.183 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.197 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.028 wm_rt*wm_rt[t-1] + -0.245 wm_rt*wm_rt[t-4] + -0.455 repeat*wm_rt[t-1] + -0.065 repeat*wm_rt[t-3] + 0.065 repeat*wm_rt[t-4] + -0.205 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.078 switch*wm_rt[t-4] + -0.055 wm_rt[t-2]^2 + -0.05 wm_rt[t-2]*wm_rt[t-3] + -0.048 wm_rt[t-2]*wm_rt[t-4] + -0.117 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 1, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 12, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 4, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 368/400 --- L(Train): 0.3648540 --- L(Val, RNN): 0.2911413 --- L(Val, SINDy): 0.3208027 --- Time: 3.63s; --- Convergence: 2.24e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 17/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.059 1 + 0.615 value_reward_diff[t] + 2.453 reward_diff + -0.344 value_reward_diff^2 + 0.039 value_reward_diff*reward_diff + 2.379 reward_diff^2 \n",
      "value_persistance[t+1] = -0.401 1 + 0.71 value_persistance[t] + -0.061 repeat + 0.398 value_persistance^2 + -0.186 value_persistance*repeat + -0.062 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.197 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.026 wm_rt*wm_rt[t-1] + -0.246 wm_rt*wm_rt[t-4] + -0.455 repeat*wm_rt[t-1] + -0.064 repeat*wm_rt[t-3] + 0.065 repeat*wm_rt[t-4] + -0.208 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-3] + 0.077 switch*wm_rt[t-4] + -0.056 wm_rt[t-2]^2 + -0.051 wm_rt[t-2]*wm_rt[t-3] + -0.048 wm_rt[t-2]*wm_rt[t-4] + -0.118 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 2, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 13, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 5, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 369/400 --- L(Train): 0.3695311 --- L(Val, RNN): 0.2931706 --- L(Val, SINDy): 0.3232698 --- Time: 3.38s; --- Convergence: 2.13e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 18/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.064 1 + 0.611 value_reward_diff[t] + 2.449 reward_diff + -0.348 value_reward_diff^2 + 0.029 value_reward_diff*reward_diff + 2.374 reward_diff^2 \n",
      "value_persistance[t+1] = -0.399 1 + 0.708 value_persistance[t] + -0.058 repeat + 0.4 value_persistance^2 + -0.188 value_persistance*repeat + -0.06 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.197 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.023 wm_rt*wm_rt[t-1] + -0.247 wm_rt*wm_rt[t-4] + -0.456 repeat*wm_rt[t-1] + -0.064 repeat*wm_rt[t-3] + 0.065 repeat*wm_rt[t-4] + -0.21 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-3] + 0.077 switch*wm_rt[t-4] + -0.055 wm_rt[t-2]^2 + -0.051 wm_rt[t-2]*wm_rt[t-3] + -0.048 wm_rt[t-2]*wm_rt[t-4] + -0.119 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 3, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 14, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 6, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 370/400 --- L(Train): 0.3529593 --- L(Val, RNN): 0.2979772 --- L(Val, SINDy): 0.3489371 --- Time: 4.51s; --- Convergence: 3.47e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 19/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.067 1 + 0.608 value_reward_diff[t] + 2.446 reward_diff + -0.35 value_reward_diff^2 + 0.021 value_reward_diff*reward_diff + 2.369 reward_diff^2 \n",
      "value_persistance[t+1] = -0.398 1 + 0.707 value_persistance[t] + -0.057 repeat + 0.401 value_persistance^2 + -0.189 value_persistance*repeat + -0.059 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.197 wm_rt[t-2] + 0.064 wm_rt[t-3] + 0.021 wm_rt*wm_rt[t-1] + -0.248 wm_rt*wm_rt[t-4] + -0.457 repeat*wm_rt[t-1] + -0.065 repeat*wm_rt[t-3] + 0.064 repeat*wm_rt[t-4] + -0.21 switch*wm_rt[t-1] + -0.073 switch*wm_rt[t-3] + 0.077 switch*wm_rt[t-4] + -0.055 wm_rt[t-2]^2 + -0.05 wm_rt[t-2]*wm_rt[t-3] + -0.048 wm_rt[t-2]*wm_rt[t-4] + -0.119 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 4, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 15, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 7, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 371/400 --- L(Train): 0.3588549 --- L(Val, RNN): 0.2961972 --- L(Val, SINDy): 0.3585460 --- Time: 3.45s; --- Convergence: 2.62e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 20/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.068 1 + 0.608 value_reward_diff[t] + 2.443 reward_diff + -0.35 value_reward_diff^2 + 0.016 value_reward_diff*reward_diff + 2.366 reward_diff^2 \n",
      "value_persistance[t+1] = -0.398 1 + 0.706 value_persistance[t] + -0.057 repeat + 0.402 value_persistance^2 + -0.189 value_persistance*repeat + -0.059 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.198 wm_rt[t-2] + 0.065 wm_rt[t-3] + 0.018 wm_rt*wm_rt[t-1] + -0.248 wm_rt*wm_rt[t-4] + -0.457 repeat*wm_rt[t-1] + -0.065 repeat*wm_rt[t-3] + 0.064 repeat*wm_rt[t-4] + -0.21 switch*wm_rt[t-1] + -0.072 switch*wm_rt[t-3] + 0.077 switch*wm_rt[t-4] + -0.054 wm_rt[t-2]^2 + -0.049 wm_rt[t-2]*wm_rt[t-3] + -0.047 wm_rt[t-2]*wm_rt[t-4] + -0.119 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 5, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 16, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 1, 8, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 372/400 --- L(Train): 0.3564335 --- L(Val, RNN): 0.2907134 --- L(Val, SINDy): 0.3715449 --- Time: 3.64s; --- Convergence: 4.05e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 21/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.067 1 + 0.61 value_reward_diff[t] + 2.442 reward_diff + -0.347 value_reward_diff^2 + 0.013 value_reward_diff*reward_diff + 2.364 reward_diff^2 \n",
      "value_persistance[t+1] = -0.399 1 + 0.706 value_persistance[t] + -0.06 repeat + 0.402 value_persistance^2 + -0.187 value_persistance*repeat + -0.061 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.2 wm_rt[t-2] + 0.066 wm_rt[t-3] + 0.015 wm_rt*wm_rt[t-1] + -0.249 wm_rt*wm_rt[t-4] + -0.456 repeat*wm_rt[t-1] + -0.065 repeat*wm_rt[t-3] + 0.063 repeat*wm_rt[t-4] + -0.209 switch*wm_rt[t-1] + -0.071 switch*wm_rt[t-3] + 0.078 switch*wm_rt[t-4] + -0.053 wm_rt[t-2]^2 + -0.048 wm_rt[t-2]*wm_rt[t-3] + -0.045 wm_rt[t-2]*wm_rt[t-4] + -0.118 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 6, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 17, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 2, 9, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 373/400 --- L(Train): 0.3661337 --- L(Val, RNN): 0.2916912 --- L(Val, SINDy): 0.3939929 --- Time: 3.44s; --- Convergence: 2.52e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 22/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.066 1 + 0.612 value_reward_diff[t] + 2.441 reward_diff + -0.344 value_reward_diff^2 + 0.012 value_reward_diff*reward_diff + 2.363 reward_diff^2 \n",
      "value_persistance[t+1] = -0.4 1 + 0.707 value_persistance[t] + -0.063 repeat + 0.401 value_persistance^2 + -0.184 value_persistance*repeat + -0.064 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.201 wm_rt[t-2] + 0.067 wm_rt[t-3] + 0.012 wm_rt*wm_rt[t-1] + -0.249 wm_rt*wm_rt[t-4] + -0.455 repeat*wm_rt[t-1] + -0.066 repeat*wm_rt[t-3] + 0.062 repeat*wm_rt[t-4] + -0.207 switch*wm_rt[t-1] + -0.069 switch*wm_rt[t-3] + 0.079 switch*wm_rt[t-4] + -0.051 wm_rt[t-2]^2 + -0.046 wm_rt[t-2]*wm_rt[t-3] + -0.044 wm_rt[t-2]*wm_rt[t-4] + -0.117 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 7, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 18, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 3, 10, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 374/400 --- L(Train): 0.3692876 --- L(Val, RNN): 0.2975858 --- L(Val, SINDy): 0.4060548 --- Time: 3.68s; --- Convergence: 4.21e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 23/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = -0.065 1 + 0.613 value_reward_diff[t] + 2.44 reward_diff + -0.342 value_reward_diff^2 + 0.01 value_reward_diff*reward_diff + 2.361 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.708 value_persistance[t] + -0.066 repeat + 0.4 value_persistance^2 + -0.181 value_persistance*repeat + -0.068 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.202 wm_rt[t-2] + 0.068 wm_rt[t-3] + 0.009 wm_rt*wm_rt[t-1] + -0.249 wm_rt*wm_rt[t-4] + -0.455 repeat*wm_rt[t-1] + -0.067 repeat*wm_rt[t-3] + 0.061 repeat*wm_rt[t-4] + -0.205 switch*wm_rt[t-1] + -0.068 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.05 wm_rt[t-2]^2 + -0.045 wm_rt[t-2]*wm_rt[t-3] + -0.043 wm_rt[t-2]*wm_rt[t-4] + -0.117 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 8, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 19, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 4, 11, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 375/400 --- L(Train): 0.3524695 --- L(Val, RNN): 0.3028924 --- L(Val, SINDy): 0.3988884 --- Time: 4.05s; --- Convergence: 4.76e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 24/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.066 1 + 0.613 value_reward_diff[t] + 2.439 reward_diff + -0.343 value_reward_diff^2 + 0.007 value_reward_diff*reward_diff + 2.359 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.709 value_persistance[t] + -0.068 repeat + 0.4 value_persistance^2 + -0.179 value_persistance*repeat + -0.07 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.202 wm_rt[t-2] + 0.067 wm_rt[t-3] + 0.006 wm_rt*wm_rt[t-1] + -0.248 wm_rt*wm_rt[t-4] + -0.454 repeat*wm_rt[t-1] + -0.069 repeat*wm_rt[t-3] + 0.061 repeat*wm_rt[t-4] + -0.204 switch*wm_rt[t-1] + -0.068 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.05 wm_rt[t-2]^2 + -0.046 wm_rt[t-2]*wm_rt[t-3] + -0.043 wm_rt[t-2]*wm_rt[t-4] + -0.118 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 9, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 20, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 5, 12, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 376/400 --- L(Train): 0.3543617 --- L(Val, RNN): 0.3007138 --- L(Val, SINDy): 0.3801033 --- Time: 3.65s; --- Convergence: 3.47e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 25/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.068 1 + 0.611 value_reward_diff[t] + 2.437 reward_diff + -0.344 value_reward_diff^2 + 0.003 value_reward_diff*reward_diff + 2.356 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.708 value_persistance[t] + -0.069 repeat + 0.4 value_persistance^2 + -0.178 value_persistance*repeat + -0.071 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.202 wm_rt[t-2] + 0.066 wm_rt[t-3] + 0.003 wm_rt*wm_rt[t-1] + -0.247 wm_rt*wm_rt[t-4] + -0.453 repeat*wm_rt[t-1] + -0.07 repeat*wm_rt[t-3] + 0.06 repeat*wm_rt[t-4] + -0.204 switch*wm_rt[t-1] + -0.068 switch*wm_rt[t-3] + 0.079 switch*wm_rt[t-4] + -0.051 wm_rt[t-2]^2 + -0.046 wm_rt[t-2]*wm_rt[t-3] + -0.044 wm_rt[t-2]*wm_rt[t-4] + -0.118 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 10, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 21, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 6, 13, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 377/400 --- L(Train): 0.3532551 --- L(Val, RNN): 0.2935531 --- L(Val, SINDy): 0.3762131 --- Time: 3.76s; --- Convergence: 5.31e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 26/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.069 1 + 0.61 value_reward_diff[t] + 2.435 reward_diff + -0.344 value_reward_diff^2 + 0.0 value_reward_diff*reward_diff + 2.354 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.708 value_persistance[t] + -0.069 repeat + 0.401 value_persistance^2 + -0.179 value_persistance*repeat + -0.071 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.201 wm_rt[t-2] + 0.066 wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-1] + -0.246 wm_rt*wm_rt[t-4] + -0.451 repeat*wm_rt[t-1] + -0.071 repeat*wm_rt[t-3] + 0.06 repeat*wm_rt[t-4] + -0.203 switch*wm_rt[t-1] + -0.069 switch*wm_rt[t-3] + 0.078 switch*wm_rt[t-4] + -0.052 wm_rt[t-2]^2 + -0.047 wm_rt[t-2]*wm_rt[t-3] + -0.045 wm_rt[t-2]*wm_rt[t-4] + -0.119 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 11, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 22, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 7, 14, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 378/400 --- L(Train): 0.3606052 --- L(Val, RNN): 0.2923979 --- L(Val, SINDy): 0.3737542 --- Time: 3.69s; --- Convergence: 3.23e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 27/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.069 1 + 0.611 value_reward_diff[t] + 2.434 reward_diff + -0.343 value_reward_diff^2 + -0.001 value_reward_diff*reward_diff + 2.351 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.706 value_persistance[t] + -0.067 repeat + 0.402 value_persistance^2 + -0.18 value_persistance*repeat + -0.069 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.2 wm_rt[t-2] + 0.065 wm_rt[t-3] + -0.003 wm_rt*wm_rt[t-1] + -0.246 wm_rt*wm_rt[t-4] + -0.449 repeat*wm_rt[t-1] + -0.072 repeat*wm_rt[t-3] + 0.061 repeat*wm_rt[t-4] + -0.202 switch*wm_rt[t-1] + -0.069 switch*wm_rt[t-3] + 0.078 switch*wm_rt[t-4] + -0.052 wm_rt[t-2]^2 + -0.048 wm_rt[t-2]*wm_rt[t-3] + -0.045 wm_rt[t-2]*wm_rt[t-4] + -0.119 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 12, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 23, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 8, 15, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 379/400 --- L(Train): 0.3582234 --- L(Val, RNN): 0.2960790 --- L(Val, SINDy): 0.3586715 --- Time: 4.10s; --- Convergence: 3.46e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 28/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.07 1 + 0.61 value_reward_diff[t] + 2.433 reward_diff + -0.343 value_reward_diff^2 + -0.003 value_reward_diff*reward_diff + 2.349 reward_diff^2 \n",
      "value_persistance[t+1] = -0.401 1 + 0.704 value_persistance[t] + -0.065 repeat + 0.404 value_persistance^2 + -0.182 value_persistance*repeat + -0.066 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.2 wm_rt[t-2] + 0.065 wm_rt[t-3] + -0.007 wm_rt*wm_rt[t-1] + -0.246 wm_rt*wm_rt[t-4] + -0.448 repeat*wm_rt[t-1] + -0.072 repeat*wm_rt[t-3] + 0.062 repeat*wm_rt[t-4] + -0.201 switch*wm_rt[t-1] + -0.07 switch*wm_rt[t-3] + 0.077 switch*wm_rt[t-4] + -0.053 wm_rt[t-2]^2 + -0.048 wm_rt[t-2]*wm_rt[t-3] + -0.046 wm_rt[t-2]*wm_rt[t-4] + -0.119 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 13, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 24, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 9, 16, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 380/400 --- L(Train): 0.3431002 --- L(Val, RNN): 0.2974947 --- L(Val, SINDy): 0.3447491 --- Time: 3.79s; --- Convergence: 2.44e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 29/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.072 1 + 0.608 value_reward_diff[t] + 2.431 reward_diff + -0.344 value_reward_diff^2 + -0.006 value_reward_diff*reward_diff + 2.347 reward_diff^2 \n",
      "value_persistance[t+1] = -0.399 1 + 0.702 value_persistance[t] + -0.062 repeat + 0.405 value_persistance^2 + -0.184 value_persistance*repeat + -0.064 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.2 wm_rt[t-2] + 0.065 wm_rt[t-3] + -0.01 wm_rt*wm_rt[t-1] + -0.245 wm_rt*wm_rt[t-4] + -0.445 repeat*wm_rt[t-1] + -0.072 repeat*wm_rt[t-3] + 0.064 repeat*wm_rt[t-4] + -0.2 switch*wm_rt[t-1] + -0.07 switch*wm_rt[t-3] + 0.077 switch*wm_rt[t-4] + -0.053 wm_rt[t-2]^2 + -0.048 wm_rt[t-2]*wm_rt[t-3] + -0.046 wm_rt[t-2]*wm_rt[t-4] + -0.119 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 14, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 25, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 10, 17, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 381/400 --- L(Train): 0.3452118 --- L(Val, RNN): 0.2940641 --- L(Val, SINDy): 0.3568200 --- Time: 3.72s; --- Convergence: 2.93e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 30/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.073 1 + 0.606 value_reward_diff[t] + 2.431 reward_diff + -0.346 value_reward_diff^2 + -0.008 value_reward_diff*reward_diff + 2.346 reward_diff^2 \n",
      "value_persistance[t+1] = -0.398 1 + 0.7 value_persistance[t] + -0.06 repeat + 0.407 value_persistance^2 + -0.186 value_persistance*repeat + -0.062 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.201 wm_rt[t-2] + 0.066 wm_rt[t-3] + -0.014 wm_rt*wm_rt[t-1] + -0.245 wm_rt*wm_rt[t-4] + -0.443 repeat*wm_rt[t-1] + -0.071 repeat*wm_rt[t-3] + 0.066 repeat*wm_rt[t-4] + -0.197 switch*wm_rt[t-1] + -0.069 switch*wm_rt[t-3] + 0.078 switch*wm_rt[t-4] + -0.052 wm_rt[t-2]^2 + -0.047 wm_rt[t-2]*wm_rt[t-3] + -0.045 wm_rt[t-2]*wm_rt[t-4] + -0.118 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 15, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 26, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 11, 18, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 382/400 --- L(Train): 0.3410305 --- L(Val, RNN): 0.2931768 --- L(Val, SINDy): 0.3538828 --- Time: 3.83s; --- Convergence: 1.91e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 31/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.072 1 + 0.608 value_reward_diff[t] + 2.432 reward_diff + -0.343 value_reward_diff^2 + -0.007 value_reward_diff*reward_diff + 2.346 reward_diff^2 \n",
      "value_persistance[t+1] = -0.398 1 + 0.7 value_persistance[t] + -0.06 repeat + 0.408 value_persistance^2 + -0.186 value_persistance*repeat + -0.062 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.202 wm_rt[t-2] + 0.067 wm_rt[t-3] + -0.017 wm_rt*wm_rt[t-1] + -0.245 wm_rt*wm_rt[t-4] + -0.441 repeat*wm_rt[t-1] + -0.071 repeat*wm_rt[t-3] + 0.068 repeat*wm_rt[t-4] + -0.193 switch*wm_rt[t-1] + -0.067 switch*wm_rt[t-3] + 0.079 switch*wm_rt[t-4] + -0.051 wm_rt[t-2]^2 + -0.046 wm_rt[t-2]*wm_rt[t-3] + -0.044 wm_rt[t-2]*wm_rt[t-4] + -0.116 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 16, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 27, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 12, 19, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 383/400 --- L(Train): 0.3453758 --- L(Val, RNN): 0.2967222 --- L(Val, SINDy): 0.3282687 --- Time: 4.23s; --- Convergence: 2.73e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 32/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.069 1 + 0.61 value_reward_diff[t] + 2.435 reward_diff + -0.341 value_reward_diff^2 + -0.004 value_reward_diff*reward_diff + 2.349 reward_diff^2 \n",
      "value_persistance[t+1] = -0.399 1 + 0.7 value_persistance[t] + -0.061 repeat + 0.408 value_persistance^2 + -0.186 value_persistance*repeat + -0.062 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.202 wm_rt[t-2] + 0.067 wm_rt[t-3] + -0.021 wm_rt*wm_rt[t-1] + -0.245 wm_rt*wm_rt[t-4] + -0.438 repeat*wm_rt[t-1] + -0.071 repeat*wm_rt[t-3] + 0.069 repeat*wm_rt[t-4] + -0.191 switch*wm_rt[t-1] + -0.067 switch*wm_rt[t-3] + 0.079 switch*wm_rt[t-4] + -0.051 wm_rt[t-2]^2 + -0.046 wm_rt[t-2]*wm_rt[t-3] + -0.044 wm_rt[t-2]*wm_rt[t-4] + -0.116 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 17, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 28, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 13, 20, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 384/400 --- L(Train): 0.3477151 --- L(Val, RNN): 0.2963360 --- L(Val, SINDy): 0.3610722 --- Time: 3.39s; --- Convergence: 1.56e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 33/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.07 1 + 0.609 value_reward_diff[t] + 2.436 reward_diff + -0.341 value_reward_diff^2 + -0.005 value_reward_diff*reward_diff + 2.35 reward_diff^2 \n",
      "value_persistance[t+1] = -0.4 1 + 0.7 value_persistance[t] + -0.062 repeat + 0.407 value_persistance^2 + -0.184 value_persistance*repeat + -0.064 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.201 wm_rt[t-2] + 0.067 wm_rt[t-3] + -0.024 wm_rt*wm_rt[t-1] + -0.244 wm_rt*wm_rt[t-4] + -0.436 repeat*wm_rt[t-1] + -0.071 repeat*wm_rt[t-3] + 0.07 repeat*wm_rt[t-4] + -0.189 switch*wm_rt[t-1] + -0.067 switch*wm_rt[t-3] + 0.078 switch*wm_rt[t-4] + -0.052 wm_rt[t-2]^2 + -0.047 wm_rt[t-2]*wm_rt[t-3] + -0.045 wm_rt[t-2]*wm_rt[t-4] + -0.115 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 18, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 29, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 14, 21, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 385/400 --- L(Train): 0.3422132 --- L(Val, RNN): 0.2938316 --- L(Val, SINDy): 0.3652672 --- Time: 3.34s; --- Convergence: 2.03e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 34/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.072 1 + 0.605 value_reward_diff[t] + 2.437 reward_diff + -0.344 value_reward_diff^2 + -0.007 value_reward_diff*reward_diff + 2.35 reward_diff^2 \n",
      "value_persistance[t+1] = -0.401 1 + 0.7 value_persistance[t] + -0.064 repeat + 0.407 value_persistance^2 + -0.183 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.2 wm_rt[t-2] + 0.067 wm_rt[t-3] + -0.027 wm_rt*wm_rt[t-1] + -0.243 wm_rt*wm_rt[t-4] + -0.434 repeat*wm_rt[t-1] + -0.072 repeat*wm_rt[t-3] + 0.071 repeat*wm_rt[t-4] + -0.186 switch*wm_rt[t-1] + -0.068 switch*wm_rt[t-3] + 0.078 switch*wm_rt[t-4] + -0.053 wm_rt[t-2]^2 + -0.048 wm_rt[t-2]*wm_rt[t-3] + -0.046 wm_rt[t-2]*wm_rt[t-4] + -0.116 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 19, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 30, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 15, 22, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 386/400 --- L(Train): 0.3367218 --- L(Val, RNN): 0.2963387 --- L(Val, SINDy): 0.3636920 --- Time: 3.26s; --- Convergence: 2.27e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 35/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.074 1 + 0.602 value_reward_diff[t] + 2.439 reward_diff + -0.346 value_reward_diff^2 + -0.009 value_reward_diff*reward_diff + 2.351 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.7 value_persistance[t] + -0.065 repeat + 0.407 value_persistance^2 + -0.182 value_persistance*repeat + -0.067 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.2 wm_rt[t-2] + 0.066 wm_rt[t-3] + -0.03 wm_rt*wm_rt[t-1] + -0.241 wm_rt*wm_rt[t-4] + -0.432 repeat*wm_rt[t-1] + -0.073 repeat*wm_rt[t-3] + 0.071 repeat*wm_rt[t-4] + -0.183 switch*wm_rt[t-1] + -0.067 switch*wm_rt[t-3] + 0.078 switch*wm_rt[t-4] + -0.054 wm_rt[t-2]^2 + -0.049 wm_rt[t-2]*wm_rt[t-3] + -0.047 wm_rt[t-2]*wm_rt[t-4] + -0.116 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 20, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 31, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 16, 23, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 387/400 --- L(Train): 0.3315535 --- L(Val, RNN): 0.3010991 --- L(Val, SINDy): 0.3599519 --- Time: 3.24s; --- Convergence: 3.51e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 36/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.075 1 + 0.601 value_reward_diff[t] + 2.442 reward_diff + -0.347 value_reward_diff^2 + -0.008 value_reward_diff*reward_diff + 2.353 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.7 value_persistance[t] + -0.066 repeat + 0.407 value_persistance^2 + -0.181 value_persistance*repeat + -0.067 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.199 wm_rt[t-2] + 0.066 wm_rt[t-3] + -0.034 wm_rt*wm_rt[t-1] + -0.24 wm_rt*wm_rt[t-4] + -0.43 repeat*wm_rt[t-1] + -0.075 repeat*wm_rt[t-3] + 0.071 repeat*wm_rt[t-4] + -0.179 switch*wm_rt[t-1] + -0.067 switch*wm_rt[t-3] + 0.078 switch*wm_rt[t-4] + -0.054 wm_rt[t-2]^2 + -0.049 wm_rt[t-2]*wm_rt[t-3] + -0.047 wm_rt[t-2]*wm_rt[t-4] + -0.115 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 21, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 32, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 17, 24, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 388/400 --- L(Train): 0.3319102 --- L(Val, RNN): 0.2995806 --- L(Val, SINDy): 0.3152009 --- Time: 3.76s; --- Convergence: 2.52e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 37/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.076 1 + 0.599 value_reward_diff[t] + 2.445 reward_diff + -0.348 value_reward_diff^2 + -0.008 value_reward_diff*reward_diff + 2.355 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.699 value_persistance[t] + -0.066 repeat + 0.408 value_persistance^2 + -0.181 value_persistance*repeat + -0.067 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.199 wm_rt[t-2] + 0.067 wm_rt[t-3] + -0.037 wm_rt*wm_rt[t-1] + -0.239 wm_rt*wm_rt[t-4] + -0.428 repeat*wm_rt[t-1] + -0.075 repeat*wm_rt[t-3] + 0.072 repeat*wm_rt[t-4] + -0.174 switch*wm_rt[t-1] + -0.066 switch*wm_rt[t-3] + 0.079 switch*wm_rt[t-4] + -0.054 wm_rt[t-2]^2 + -0.049 wm_rt[t-2]*wm_rt[t-3] + -0.047 wm_rt[t-2]*wm_rt[t-4] + -0.114 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 22, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 33, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 18, 25, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 389/400 --- L(Train): 0.3392321 --- L(Val, RNN): 0.2938426 --- L(Val, SINDy): 0.3176097 --- Time: 3.57s; --- Convergence: 4.13e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 38/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.079 1 + 0.595 value_reward_diff[t] + 2.446 reward_diff + -0.352 value_reward_diff^2 + -0.01 value_reward_diff*reward_diff + 2.356 reward_diff^2 \n",
      "value_persistance[t+1] = -0.401 1 + 0.698 value_persistance[t] + -0.064 repeat + 0.409 value_persistance^2 + -0.182 value_persistance*repeat + -0.066 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.2 wm_rt[t-2] + 0.068 wm_rt[t-3] + -0.04 wm_rt*wm_rt[t-1] + -0.238 wm_rt*wm_rt[t-4] + -0.426 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.073 repeat*wm_rt[t-4] + -0.168 switch*wm_rt[t-1] + -0.064 switch*wm_rt[t-3] + 0.081 switch*wm_rt[t-4] + -0.054 wm_rt[t-2]^2 + -0.049 wm_rt[t-2]*wm_rt[t-3] + -0.047 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 23, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 34, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 19, 26, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 390/400 --- L(Train): 0.3365639 --- L(Val, RNN): 0.2934859 --- L(Val, SINDy): 0.3203229 --- Time: 3.72s; --- Convergence: 2.24e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 39/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.082 1 + 0.591 value_reward_diff[t] + 2.448 reward_diff + -0.355 value_reward_diff^2 + -0.01 value_reward_diff*reward_diff + 2.358 reward_diff^2 \n",
      "value_persistance[t+1] = -0.4 1 + 0.696 value_persistance[t] + -0.063 repeat + 0.41 value_persistance^2 + -0.184 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.199 wm_rt[t-2] + 0.069 wm_rt[t-3] + -0.043 wm_rt*wm_rt[t-1] + -0.236 wm_rt*wm_rt[t-4] + -0.424 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.074 repeat*wm_rt[t-4] + -0.162 switch*wm_rt[t-1] + -0.063 switch*wm_rt[t-3] + 0.081 switch*wm_rt[t-4] + -0.054 wm_rt[t-2]^2 + -0.049 wm_rt[t-2]*wm_rt[t-3] + -0.047 wm_rt[t-2]*wm_rt[t-4] + -0.112 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 24, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 35, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 20, 27, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 391/400 --- L(Train): 0.3312308 --- L(Val, RNN): 0.2973779 --- L(Val, SINDy): 0.3214544 --- Time: 3.40s; --- Convergence: 3.07e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 40/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.082 1 + 0.59 value_reward_diff[t] + 2.453 reward_diff + -0.355 value_reward_diff^2 + -0.008 value_reward_diff*reward_diff + 2.361 reward_diff^2 \n",
      "value_persistance[t+1] = -0.4 1 + 0.696 value_persistance[t] + -0.062 repeat + 0.411 value_persistance^2 + -0.184 value_persistance*repeat + -0.064 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.198 wm_rt[t-2] + 0.069 wm_rt[t-3] + -0.046 wm_rt*wm_rt[t-1] + -0.234 wm_rt*wm_rt[t-4] + -0.421 repeat*wm_rt[t-1] + -0.076 repeat*wm_rt[t-3] + 0.075 repeat*wm_rt[t-4] + -0.158 switch*wm_rt[t-1] + -0.063 switch*wm_rt[t-3] + 0.081 switch*wm_rt[t-4] + -0.055 wm_rt[t-2]^2 + -0.05 wm_rt[t-2]*wm_rt[t-3] + -0.048 wm_rt[t-2]*wm_rt[t-4] + -0.112 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 25, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 36, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 28, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 392/400 --- L(Train): 0.3317843 --- L(Val, RNN): 0.2966814 --- L(Val, SINDy): 0.3213012 --- Time: 3.41s; --- Convergence: 1.88e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 41/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.081 1 + 0.591 value_reward_diff[t] + 2.458 reward_diff + -0.354 value_reward_diff^2 + -0.004 value_reward_diff*reward_diff + 2.366 reward_diff^2 \n",
      "value_persistance[t+1] = -0.401 1 + 0.696 value_persistance[t] + -0.063 repeat + 0.411 value_persistance^2 + -0.184 value_persistance*repeat + -0.064 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.198 wm_rt[t-2] + 0.069 wm_rt[t-3] + -0.049 wm_rt*wm_rt[t-1] + -0.232 wm_rt*wm_rt[t-4] + -0.418 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.076 repeat*wm_rt[t-4] + -0.152 switch*wm_rt[t-1] + -0.063 switch*wm_rt[t-3] + 0.081 switch*wm_rt[t-4] + -0.056 wm_rt[t-2]^2 + -0.051 wm_rt[t-2]*wm_rt[t-3] + -0.049 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 26, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 37, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 29, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 393/400 --- L(Train): 0.3244576 --- L(Val, RNN): 0.2931349 --- L(Val, SINDy): 0.3569534 --- Time: 3.40s; --- Convergence: 2.71e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 42/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.08 1 + 0.592 value_reward_diff[t] + 2.464 reward_diff + -0.352 value_reward_diff^2 + 0.001 value_reward_diff*reward_diff + 2.371 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.696 value_persistance[t] + -0.063 repeat + 0.41 value_persistance^2 + -0.183 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.197 wm_rt[t-2] + 0.07 wm_rt[t-3] + -0.052 wm_rt*wm_rt[t-1] + -0.231 wm_rt*wm_rt[t-4] + -0.415 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.077 repeat*wm_rt[t-4] + -0.145 switch*wm_rt[t-1] + -0.061 switch*wm_rt[t-3] + 0.082 switch*wm_rt[t-4] + -0.057 wm_rt[t-2]^2 + -0.051 wm_rt[t-2]*wm_rt[t-3] + -0.049 wm_rt[t-2]*wm_rt[t-4] + -0.11 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 27, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 30, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 394/400 --- L(Train): 0.3288216 --- L(Val, RNN): 0.2940400 --- L(Val, SINDy): 0.3687012 --- Time: 3.61s; --- Convergence: 1.81e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 43/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.079 1 + 0.593 value_reward_diff[t] + 2.47 reward_diff + -0.35 value_reward_diff^2 + 0.006 value_reward_diff*reward_diff + 2.376 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.696 value_persistance[t] + -0.064 repeat + 0.41 value_persistance^2 + -0.183 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.196 wm_rt[t-2] + 0.07 wm_rt[t-3] + -0.055 wm_rt*wm_rt[t-1] + -0.229 wm_rt*wm_rt[t-4] + -0.413 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.078 repeat*wm_rt[t-4] + -0.139 switch*wm_rt[t-1] + -0.061 switch*wm_rt[t-3] + 0.082 switch*wm_rt[t-4] + -0.058 wm_rt[t-2]^2 + -0.052 wm_rt[t-2]*wm_rt[t-3] + -0.05 wm_rt[t-2]*wm_rt[t-4] + -0.11 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 28, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 395/400 --- L(Train): 0.3308240 --- L(Val, RNN): 0.2963427 --- L(Val, SINDy): 0.3709573 --- Time: 3.17s; --- Convergence: 2.06e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 44/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.078 1 + 0.594 value_reward_diff[t] + 2.475 reward_diff + -0.349 value_reward_diff^2 + 0.011 value_reward_diff*reward_diff + 2.381 reward_diff^2 \n",
      "value_persistance[t+1] = -0.404 1 + 0.696 value_persistance[t] + -0.063 repeat + 0.411 value_persistance^2 + -0.183 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.194 wm_rt[t-2] + 0.069 wm_rt[t-3] + -0.057 wm_rt*wm_rt[t-1] + -0.226 wm_rt*wm_rt[t-4] + -0.409 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.078 repeat*wm_rt[t-4] + -0.135 switch*wm_rt[t-1] + -0.063 switch*wm_rt[t-3] + 0.081 switch*wm_rt[t-4] + -0.06 wm_rt[t-2]^2 + -0.055 wm_rt[t-2]*wm_rt[t-3] + -0.052 wm_rt[t-2]*wm_rt[t-4] + -0.111 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 29, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 396/400 --- L(Train): 0.3233334 --- L(Val, RNN): 0.2959606 --- L(Val, SINDy): 0.3729308 --- Time: 3.68s; --- Convergence: 1.22e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 45/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.078 1 + 0.594 value_reward_diff[t] + 2.48 reward_diff + -0.348 value_reward_diff^2 + 0.014 value_reward_diff*reward_diff + 2.386 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.695 value_persistance[t] + -0.062 repeat + 0.412 value_persistance^2 + -0.184 value_persistance*repeat + -0.064 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.192 wm_rt[t-2] + 0.069 wm_rt[t-3] + -0.06 wm_rt*wm_rt[t-1] + -0.224 wm_rt*wm_rt[t-4] + -0.406 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.077 repeat*wm_rt[t-4] + -0.131 switch*wm_rt[t-1] + -0.063 switch*wm_rt[t-3] + 0.08 switch*wm_rt[t-4] + -0.062 wm_rt[t-2]^2 + -0.056 wm_rt[t-2]*wm_rt[t-3] + -0.054 wm_rt[t-2]*wm_rt[t-4] + -0.112 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 30, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 397/400 --- L(Train): 0.3228332 --- L(Val, RNN): 0.2944060 --- L(Val, SINDy): 0.3818824 --- Time: 3.67s; --- Convergence: 1.39e-03; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 46/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.079 1 + 0.594 value_reward_diff[t] + 2.485 reward_diff + -0.347 value_reward_diff^2 + 0.018 value_reward_diff*reward_diff + 2.39 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.693 value_persistance[t] + -0.062 repeat + 0.413 value_persistance^2 + -0.185 value_persistance*repeat + -0.063 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.191 wm_rt[t-2] + 0.069 wm_rt[t-3] + -0.062 wm_rt*wm_rt[t-1] + -0.221 wm_rt*wm_rt[t-4] + -0.404 repeat*wm_rt[t-1] + -0.078 repeat*wm_rt[t-3] + 0.077 repeat*wm_rt[t-4] + -0.125 switch*wm_rt[t-1] + -0.063 switch*wm_rt[t-3] + 0.079 switch*wm_rt[t-4] + -0.063 wm_rt[t-2]^2 + -0.057 wm_rt[t-2]*wm_rt[t-3] + -0.055 wm_rt[t-2]*wm_rt[t-4] + -0.112 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 31, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 398/400 --- L(Train): 0.3243285 --- L(Val, RNN): 0.2947918 --- L(Val, SINDy): 0.3532613 --- Time: 3.41s; --- Convergence: 8.86e-04; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 47/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.079 1 + 0.594 value_reward_diff[t] + 2.489 reward_diff + -0.347 value_reward_diff^2 + 0.021 value_reward_diff*reward_diff + 2.394 reward_diff^2 \n",
      "value_persistance[t+1] = -0.402 1 + 0.692 value_persistance[t] + -0.061 repeat + 0.414 value_persistance^2 + -0.185 value_persistance*repeat + -0.063 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.191 wm_rt[t-2] + 0.069 wm_rt[t-3] + -0.065 wm_rt*wm_rt[t-1] + -0.219 wm_rt*wm_rt[t-4] + -0.401 repeat*wm_rt[t-1] + -0.077 repeat*wm_rt[t-3] + 0.078 repeat*wm_rt[t-4] + -0.12 switch*wm_rt[t-1] + -0.063 switch*wm_rt[t-3] + 0.079 switch*wm_rt[t-4] + -0.063 wm_rt[t-2]^2 + -0.058 wm_rt[t-2]*wm_rt[t-3] + -0.055 wm_rt[t-2]*wm_rt[t-4] + -0.112 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 32, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 399/400 --- L(Train): 0.3186980 --- L(Val, RNN): 0.2955758 --- L(Val, SINDy): 0.3544793 --- Time: 4.17s; --- Convergence: 8.35e-04; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 48/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.081 1 + 0.593 value_reward_diff[t] + 2.493 reward_diff + -0.348 value_reward_diff^2 + 0.022 value_reward_diff*reward_diff + 2.396 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.692 value_persistance[t] + -0.062 repeat + 0.414 value_persistance^2 + -0.184 value_persistance*repeat + -0.064 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.19 wm_rt[t-2] + 0.069 wm_rt[t-3] + -0.068 wm_rt*wm_rt[t-1] + -0.218 wm_rt*wm_rt[t-4] + -0.396 repeat*wm_rt[t-1] + -0.075 repeat*wm_rt[t-3] + 0.079 repeat*wm_rt[t-4] + -0.116 switch*wm_rt[t-1] + -0.064 switch*wm_rt[t-3] + 0.078 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.058 wm_rt[t-2]*wm_rt[t-3] + -0.056 wm_rt[t-2]*wm_rt[t-4] + -0.112 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 33, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 400/400 --- L(Train): 0.3282743 --- L(Val, RNN): 0.2947629 --- L(Val, SINDy): 0.3518316 --- Time: 3.33s; --- Convergence: 8.24e-04; LR: 1.00e-02; Metric: 0.2881581; Bad epochs: 49/100\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = -0.085 1 + 0.591 value_reward_diff[t] + 2.495 reward_diff + -0.35 value_reward_diff^2 + 0.022 value_reward_diff*reward_diff + 2.397 reward_diff^2 \n",
      "value_persistance[t+1] = -0.403 1 + 0.692 value_persistance[t] + -0.063 repeat + 0.414 value_persistance^2 + -0.183 value_persistance*repeat + -0.065 repeat^2 \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.19 wm_rt[t-2] + 0.069 wm_rt[t-3] + -0.07 wm_rt*wm_rt[t-1] + -0.216 wm_rt*wm_rt[t-4] + -0.394 repeat*wm_rt[t-1] + -0.073 repeat*wm_rt[t-3] + 0.079 repeat*wm_rt[t-4] + -0.113 switch*wm_rt[t-1] + -0.065 switch*wm_rt[t-3] + 0.077 switch*wm_rt[t-4] + -0.064 wm_rt[t-2]^2 + -0.059 wm_rt[t-2]*wm_rt[t-3] + -0.056 wm_rt[t-2]*wm_rt[t-4] + -0.113 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 34, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: -, -, -, -, -, 0, 0, -, -, -, -, 0, -, -, 0, -, -, 0, -, 0, 0, -, 0, -, 0, 0, -, -, -, -, 0, 0, 0, -, -, 0\n",
      "================================================================================\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "================================================================================\n",
      "Starting second stage SINDy fitting (threshold=0.05, single model)\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 1/1000 --- L(Train): 0.8526841 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.008 1 + 0.988 value_reward_diff[t] + 0.008 reward_diff + -0.01 value_reward_diff^2 + -0.009 value_reward_diff*reward_diff + 0.009 reward_diff^2 \n",
      "value_persistance[t+1] = -0.01 1 + 0.991 value_persistance[t] + 0.009 repeat + -0.009 value_persistance^2 + -0.01 value_persistance*repeat + 0.01 repeat^2 \n",
      "wm_rt[t+1] = 0.009 1 + 0.99 wm_rt[t] + 0.01 repeat + -0.009 switch + 0.009 wm_rt[t-1] + -0.009 wm_rt[t-2] + 0.01 wm_rt[t-3] + -0.01 wm_rt[t-4] + -0.01 wm_rt^2 + -0.009 wm_rt*repeat + -0.009 wm_rt*switch + 0.007 wm_rt*wm_rt[t-1] + -0.009 wm_rt*wm_rt[t-2] + -0.009 wm_rt*wm_rt[t-3] + -0.01 wm_rt*wm_rt[t-4] + -0.01 repeat^2 + -0.01 repeat*switch + 0.009 repeat*wm_rt[t-1] + 0.01 repeat*wm_rt[t-2] + 0.009 repeat*wm_rt[t-3] + 0.009 repeat*wm_rt[t-4] + -0.009 switch^2 + -0.009 switch*wm_rt[t-1] + -0.01 switch*wm_rt[t-2] + 0.01 switch*wm_rt[t-3] + 0.009 switch*wm_rt[t-4] + 0.009 wm_rt[t-1]^2 + 0.009 wm_rt[t-1]*wm_rt[t-2] + -0.008 wm_rt[t-1]*wm_rt[t-3] + -0.009 wm_rt[t-1]*wm_rt[t-4] + -0.009 wm_rt[t-2]^2 + 0.009 wm_rt[t-2]*wm_rt[t-3] + -0.008 wm_rt[t-2]*wm_rt[t-4] + 0.009 wm_rt[t-3]^2 + 0.009 wm_rt[t-3]*wm_rt[t-4] + 0.01 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 2/1000 --- L(Train): 0.8287538 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = -0.002 1 + 0.978 value_reward_diff[t] + 0.018 reward_diff + -0.02 value_reward_diff^2 + -0.018 value_reward_diff*reward_diff + 0.019 reward_diff^2 \n",
      "value_persistance[t+1] = -0.02 1 + 0.997 value_persistance[t] + 0.002 repeat + -0.013 value_persistance^2 + -0.005 value_persistance*repeat + 0.004 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.992 wm_rt[t] + 0.004 repeat + -0.008 switch + 0.004 wm_rt[t-1] + -0.014 wm_rt[t-2] + 0.004 wm_rt[t-3] + -0.017 wm_rt[t-4] + -0.009 wm_rt^2 + -0.008 wm_rt*repeat + -0.009 wm_rt*switch + 0.006 wm_rt*wm_rt[t-1] + -0.008 wm_rt*wm_rt[t-2] + -0.009 wm_rt*wm_rt[t-3] + -0.009 wm_rt*wm_rt[t-4] + -0.019 repeat^2 + -0.009 repeat*switch + 0.008 repeat*wm_rt[t-1] + 0.009 repeat*wm_rt[t-2] + 0.008 repeat*wm_rt[t-3] + 0.008 repeat*wm_rt[t-4] + -0.009 switch^2 + -0.012 switch*wm_rt[t-1] + -0.012 switch*wm_rt[t-2] + 0.006 switch*wm_rt[t-3] + 0.005 switch*wm_rt[t-4] + 0.006 wm_rt[t-1]^2 + 0.006 wm_rt[t-1]*wm_rt[t-2] + -0.011 wm_rt[t-1]*wm_rt[t-3] + -0.012 wm_rt[t-1]*wm_rt[t-4] + -0.012 wm_rt[t-2]^2 + 0.005 wm_rt[t-2]*wm_rt[t-3] + -0.011 wm_rt[t-2]*wm_rt[t-4] + 0.006 wm_rt[t-3]^2 + 0.006 wm_rt[t-3]*wm_rt[t-4] + 0.006 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 3/1000 --- L(Train): 0.8034011 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.007 1 + 0.968 value_reward_diff[t] + 0.028 reward_diff + -0.03 value_reward_diff^2 + -0.028 value_reward_diff*reward_diff + 0.029 reward_diff^2 \n",
      "value_persistance[t+1] = -0.021 1 + 0.995 value_persistance[t] + -0.002 repeat + -0.008 value_persistance^2 + -0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.004 1 + 0.994 wm_rt[t] + 0.002 repeat + -0.003 switch + -0.004 wm_rt[t-1] + -0.015 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.019 wm_rt[t-4] + -0.005 wm_rt^2 + -0.004 wm_rt*repeat + -0.006 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.004 wm_rt*wm_rt[t-2] + -0.004 wm_rt*wm_rt[t-3] + -0.005 wm_rt*wm_rt[t-4] + -0.016 repeat^2 + -0.005 repeat*switch + 0.004 repeat*wm_rt[t-1] + 0.005 repeat*wm_rt[t-2] + 0.004 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.003 switch^2 + -0.01 switch*wm_rt[t-1] + -0.01 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.009 wm_rt[t-1]*wm_rt[t-3] + -0.01 wm_rt[t-1]*wm_rt[t-4] + -0.01 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.009 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 4/1000 --- L(Train): 0.7795184 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.015 1 + 0.958 value_reward_diff[t] + 0.038 reward_diff + -0.04 value_reward_diff^2 + -0.038 value_reward_diff*reward_diff + 0.039 reward_diff^2 \n",
      "value_persistance[t+1] = -0.019 1 + 0.989 value_persistance[t] + -0.0 repeat + -0.002 value_persistance^2 + 0.003 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.007 1 + 0.998 wm_rt[t] + 0.002 repeat + 0.004 switch + -0.007 wm_rt[t-1] + -0.012 wm_rt[t-2] + -0.005 wm_rt[t-3] + -0.017 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.0 wm_rt*switch + -0.004 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.011 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + 0.003 switch^2 + -0.006 switch*wm_rt[t-1] + -0.006 switch*wm_rt[t-2] + -0.007 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.005 wm_rt[t-1]*wm_rt[t-3] + -0.006 wm_rt[t-1]*wm_rt[t-4] + -0.006 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.004 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 5/1000 --- L(Train): 0.7564582 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.025 1 + 0.948 value_reward_diff[t] + 0.048 reward_diff + -0.05 value_reward_diff^2 + -0.048 value_reward_diff*reward_diff + 0.049 reward_diff^2 \n",
      "value_persistance[t+1] = -0.015 1 + 0.984 value_persistance[t] + 0.003 repeat + 0.006 value_persistance^2 + 0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.008 1 + 1.004 wm_rt[t] + 0.001 repeat + 0.004 switch + -0.008 wm_rt[t-1] + -0.007 wm_rt[t-2] + -0.005 wm_rt[t-3] + -0.013 wm_rt[t-4] + 0.003 wm_rt^2 + 0.003 wm_rt*repeat + 0.006 wm_rt*switch + -0.005 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + 0.003 wm_rt*wm_rt[t-3] + 0.003 wm_rt*wm_rt[t-4] + -0.004 repeat^2 + 0.002 repeat*switch + -0.003 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.004 repeat*wm_rt[t-3] + -0.004 repeat*wm_rt[t-4] + 0.004 switch^2 + -0.001 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.01 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 6/1000 --- L(Train): 0.7337774 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.034 1 + 0.938 value_reward_diff[t] + 0.058 reward_diff + -0.06 value_reward_diff^2 + -0.057 value_reward_diff*reward_diff + 0.059 reward_diff^2 \n",
      "value_persistance[t+1] = -0.013 1 + 0.98 value_persistance[t] + 0.004 repeat + 0.011 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.005 1 + 1.007 wm_rt[t] + -0.002 repeat + 0.002 switch + -0.007 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.008 wm_rt[t-4] + 0.002 wm_rt^2 + 0.003 wm_rt*repeat + 0.01 wm_rt*switch + -0.005 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.002 repeat*switch + -0.003 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.003 repeat*wm_rt[t-3] + -0.004 repeat*wm_rt[t-4] + 0.002 switch^2 + 0.005 switch*wm_rt[t-1] + 0.006 switch*wm_rt[t-2] + -0.01 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + 0.005 wm_rt[t-1]*wm_rt[t-4] + 0.006 wm_rt[t-2]^2 + 0.003 wm_rt[t-2]*wm_rt[t-3] + 0.003 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 7/1000 --- L(Train): 0.7116039 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.044 1 + 0.928 value_reward_diff[t] + 0.068 reward_diff + -0.07 value_reward_diff^2 + -0.066 value_reward_diff*reward_diff + 0.069 reward_diff^2 \n",
      "value_persistance[t+1] = -0.014 1 + 0.979 value_persistance[t] + 0.003 repeat + 0.013 value_persistance^2 + 0.001 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.009 wm_rt[t] + -0.004 repeat + -0.002 switch + -0.006 wm_rt[t-1] + 0.003 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.0 wm_rt^2 + 0.0 wm_rt*repeat + 0.011 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + -0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.002 switch^2 + 0.007 switch*wm_rt[t-1] + 0.008 switch*wm_rt[t-2] + -0.009 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + 0.007 wm_rt[t-1]*wm_rt[t-4] + 0.009 wm_rt[t-2]^2 + 0.003 wm_rt[t-2]*wm_rt[t-3] + 0.003 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 8/1000 --- L(Train): 0.6901050 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.054 1 + 0.918 value_reward_diff[t] + 0.078 reward_diff + -0.079 value_reward_diff^2 + -0.075 value_reward_diff*reward_diff + 0.079 reward_diff^2 \n",
      "value_persistance[t+1] = -0.017 1 + 0.98 value_persistance[t] + 0.0 repeat + 0.014 value_persistance^2 + 0.002 value_persistance*repeat + -0.005 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 1.008 wm_rt[t] + -0.006 repeat + -0.004 switch + -0.005 wm_rt[t-1] + 0.004 wm_rt[t-2] + 0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.004 wm_rt*repeat + 0.01 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.004 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + -0.001 repeat*switch + 0.004 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.003 switch^2 + 0.007 switch*wm_rt[t-1] + 0.008 switch*wm_rt[t-2] + -0.008 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + 0.007 wm_rt[t-1]*wm_rt[t-4] + 0.009 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 9/1000 --- L(Train): 0.6693019 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.064 1 + 0.909 value_reward_diff[t] + 0.088 reward_diff + -0.089 value_reward_diff^2 + -0.084 value_reward_diff*reward_diff + 0.089 reward_diff^2 \n",
      "value_persistance[t+1] = -0.02 1 + 0.982 value_persistance[t] + -0.002 repeat + 0.013 value_persistance^2 + 0.001 value_persistance*repeat + -0.006 repeat^2 \n",
      "wm_rt[t+1] = -0.004 1 + 1.005 wm_rt[t] + -0.005 repeat + -0.005 switch + -0.003 wm_rt[t-1] + 0.003 wm_rt[t-2] + 0.0 wm_rt[t-3] + 0.002 wm_rt[t-4] + 0.001 wm_rt^2 + -0.005 wm_rt*repeat + 0.007 wm_rt*switch + 0.003 wm_rt*wm_rt[t-1] + -0.005 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.006 repeat^2 + 0.0 repeat*switch + 0.005 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.005 repeat*wm_rt[t-3] + 0.004 repeat*wm_rt[t-4] + -0.003 switch^2 + 0.004 switch*wm_rt[t-1] + 0.006 switch*wm_rt[t-2] + -0.005 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + -0.005 wm_rt[t-1]*wm_rt[t-3] + 0.004 wm_rt[t-1]*wm_rt[t-4] + 0.007 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 10/1000 --- L(Train): 0.6489792 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.074 1 + 0.899 value_reward_diff[t] + 0.098 reward_diff + -0.099 value_reward_diff^2 + -0.093 value_reward_diff*reward_diff + 0.099 reward_diff^2 \n",
      "value_persistance[t+1] = -0.023 1 + 0.983 value_persistance[t] + -0.004 repeat + 0.012 value_persistance^2 + -0.001 value_persistance*repeat + -0.006 repeat^2 \n",
      "wm_rt[t+1] = -0.004 1 + 1.0 wm_rt[t] + -0.004 repeat + -0.003 switch + 0.0 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + -0.005 wm_rt*repeat + 0.003 wm_rt*switch + 0.003 wm_rt*wm_rt[t-1] + -0.005 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.005 repeat^2 + 0.0 repeat*switch + 0.005 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.005 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.0 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + -0.005 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.003 wm_rt[t-2]^2 + -0.005 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.006 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 11/1000 --- L(Train): 0.6291169 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.084 1 + 0.889 value_reward_diff[t] + 0.108 reward_diff + -0.108 value_reward_diff^2 + -0.101 value_reward_diff*reward_diff + 0.108 reward_diff^2 \n",
      "value_persistance[t+1] = -0.025 1 + 0.984 value_persistance[t] + -0.003 repeat + 0.011 value_persistance^2 + -0.003 value_persistance*repeat + -0.004 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 0.997 wm_rt[t] + -0.001 repeat + -0.0 switch + 0.0 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.001 wm_rt^2 + -0.003 wm_rt*repeat + -0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.003 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.003 repeat^2 + -0.002 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.004 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.004 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.006 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.006 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 12/1000 --- L(Train): 0.6097959 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.095 1 + 0.88 value_reward_diff[t] + 0.118 reward_diff + -0.118 value_reward_diff^2 + -0.108 value_reward_diff*reward_diff + 0.118 reward_diff^2 \n",
      "value_persistance[t+1] = -0.026 1 + 0.983 value_persistance[t] + -0.001 repeat + 0.01 value_persistance^2 + -0.004 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.996 wm_rt[t] + 0.002 repeat + 0.003 switch + -0.002 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.005 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.002 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.004 switch^2 + -0.008 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + 0.006 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.008 wm_rt[t-1]*wm_rt[t-4] + -0.004 wm_rt[t-2]^2 + -0.006 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.006 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 13/1000 --- L(Train): 0.5910940 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.105 1 + 0.87 value_reward_diff[t] + 0.128 reward_diff + -0.127 value_reward_diff^2 + -0.116 value_reward_diff*reward_diff + 0.128 reward_diff^2 \n",
      "value_persistance[t+1] = -0.026 1 + 0.982 value_persistance[t] + 0.001 repeat + 0.009 value_persistance^2 + -0.005 value_persistance*repeat + 0.003 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.996 wm_rt[t] + 0.003 repeat + 0.004 switch + -0.003 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.0 wm_rt^2 + 0.003 wm_rt*repeat + -0.007 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.004 repeat^2 + -0.0 repeat*switch + -0.003 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.004 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + 0.004 switch^2 + -0.01 switch*wm_rt[t-1] + -0.006 switch*wm_rt[t-2] + 0.007 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.01 wm_rt[t-1]*wm_rt[t-4] + -0.005 wm_rt[t-2]^2 + -0.004 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 14/1000 --- L(Train): 0.5730280 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.116 1 + 0.861 value_reward_diff[t] + 0.138 reward_diff + -0.136 value_reward_diff^2 + -0.122 value_reward_diff*reward_diff + 0.138 reward_diff^2 \n",
      "value_persistance[t+1] = -0.026 1 + 0.981 value_persistance[t] + 0.003 repeat + 0.008 value_persistance^2 + -0.004 value_persistance*repeat + 0.005 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + 0.004 repeat + 0.003 switch + -0.003 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.0 wm_rt^2 + 0.005 wm_rt*repeat + -0.006 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.005 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.006 repeat^2 + 0.002 repeat*switch + -0.005 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + -0.005 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.003 switch^2 + -0.01 switch*wm_rt[t-1] + -0.007 switch*wm_rt[t-2] + 0.005 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.004 wm_rt[t-1]^2 + 0.004 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.01 wm_rt[t-1]*wm_rt[t-4] + -0.006 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 15/1000 --- L(Train): 0.5555236 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.127 1 + 0.852 value_reward_diff[t] + 0.148 reward_diff + -0.146 value_reward_diff^2 + -0.128 value_reward_diff*reward_diff + 0.148 reward_diff^2 \n",
      "value_persistance[t+1] = -0.027 1 + 0.98 value_persistance[t] + 0.003 repeat + 0.006 value_persistance^2 + -0.002 value_persistance*repeat + 0.006 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.004 repeat + 0.002 switch + -0.003 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.001 wm_rt^2 + 0.006 wm_rt*repeat + -0.005 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.005 wm_rt*wm_rt[t-2] + 0.003 wm_rt*wm_rt[t-3] + 0.003 wm_rt*wm_rt[t-4] + -0.006 repeat^2 + 0.003 repeat*switch + -0.006 repeat*wm_rt[t-1] + 0.004 repeat*wm_rt[t-2] + -0.006 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.01 switch*wm_rt[t-1] + -0.006 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.01 wm_rt[t-1]*wm_rt[t-4] + -0.005 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 16/1000 --- L(Train): 0.5385271 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.138 1 + 0.842 value_reward_diff[t] + 0.158 reward_diff + -0.155 value_reward_diff^2 + -0.134 value_reward_diff*reward_diff + 0.158 reward_diff^2 \n",
      "value_persistance[t+1] = -0.027 1 + 0.979 value_persistance[t] + 0.003 repeat + 0.003 value_persistance^2 + 0.001 value_persistance*repeat + 0.007 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.002 wm_rt[t] + 0.004 repeat + -0.0 switch + -0.001 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.001 wm_rt^2 + 0.005 wm_rt*repeat + -0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.005 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.005 repeat^2 + 0.002 repeat*switch + -0.005 repeat*wm_rt[t-1] + 0.004 repeat*wm_rt[t-2] + -0.005 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.009 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.009 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + 0.003 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 17/1000 --- L(Train): 0.5220515 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.149 1 + 0.833 value_reward_diff[t] + 0.168 reward_diff + -0.164 value_reward_diff^2 + -0.138 value_reward_diff*reward_diff + 0.168 reward_diff^2 \n",
      "value_persistance[t+1] = -0.028 1 + 0.979 value_persistance[t] + 0.002 repeat + 0.001 value_persistance^2 + 0.003 value_persistance*repeat + 0.006 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.002 wm_rt[t] + 0.004 repeat + -0.001 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + 0.003 wm_rt*repeat + 0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + 0.001 repeat*switch + -0.003 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + -0.003 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.004 switch^2 + -0.007 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.004 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.007 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 18/1000 --- L(Train): 0.5061215 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.16 1 + 0.824 value_reward_diff[t] + 0.178 reward_diff + -0.173 value_reward_diff^2 + -0.142 value_reward_diff*reward_diff + 0.178 reward_diff^2 \n",
      "value_persistance[t+1] = -0.028 1 + 0.977 value_persistance[t] + 0.002 repeat + -0.002 value_persistance^2 + 0.003 value_persistance*repeat + 0.005 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.003 repeat + -0.002 switch + 0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.002 wm_rt[t-4] + 0.002 wm_rt^2 + -0.0 wm_rt*repeat + 0.003 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.005 switch^2 + -0.004 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.005 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 19/1000 --- L(Train): 0.4907633 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.171 1 + 0.816 value_reward_diff[t] + 0.187 reward_diff + -0.181 value_reward_diff^2 + -0.144 value_reward_diff*reward_diff + 0.188 reward_diff^2 \n",
      "value_persistance[t+1] = -0.027 1 + 0.975 value_persistance[t] + 0.001 repeat + -0.002 value_persistance^2 + 0.002 value_persistance*repeat + 0.004 repeat^2 \n",
      "wm_rt[t+1] = 0.002 1 + 0.998 wm_rt[t] + -0.0 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.003 wm_rt[t-4] + 0.003 wm_rt^2 + -0.002 wm_rt*repeat + 0.003 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.003 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.003 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.004 switch^2 + -0.0 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.005 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.005 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.004 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 20/1000 --- L(Train): 0.4759436 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.182 1 + 0.807 value_reward_diff[t] + 0.197 reward_diff + -0.19 value_reward_diff^2 + -0.146 value_reward_diff*reward_diff + 0.198 reward_diff^2 \n",
      "value_persistance[t+1] = -0.026 1 + 0.973 value_persistance[t] + 0.0 repeat + -0.001 value_persistance^2 + 0.0 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.002 1 + 0.997 wm_rt[t] + -0.003 repeat + 0.0 switch + -0.002 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.003 wm_rt^2 + -0.002 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.003 wm_rt*wm_rt[t-4] + 0.004 repeat^2 + -0.003 repeat*switch + 0.006 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.003 switch^2 + 0.004 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.004 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.005 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + 0.004 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 21/1000 --- L(Train): 0.4616155 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.194 1 + 0.798 value_reward_diff[t] + 0.207 reward_diff + -0.198 value_reward_diff^2 + -0.147 value_reward_diff*reward_diff + 0.207 reward_diff^2 \n",
      "value_persistance[t+1] = -0.025 1 + 0.97 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + -0.003 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.002 1 + 0.997 wm_rt[t] + -0.005 repeat + 0.0 switch + -0.003 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.0 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + 0.003 repeat^2 + -0.002 repeat*switch + 0.006 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.002 switch^2 + 0.006 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.005 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + -0.004 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 22/1000 --- L(Train): 0.4477701 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.205 1 + 0.79 value_reward_diff[t] + 0.217 reward_diff + -0.207 value_reward_diff^2 + -0.147 value_reward_diff*reward_diff + 0.217 reward_diff^2 \n",
      "value_persistance[t+1] = -0.024 1 + 0.968 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + -0.005 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.998 wm_rt[t] + -0.007 repeat + 0.0 switch + -0.003 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.0 repeat*switch + 0.006 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.006 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + 0.006 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 23/1000 --- L(Train): 0.4344237 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.217 1 + 0.782 value_reward_diff[t] + 0.227 reward_diff + -0.215 value_reward_diff^2 + -0.146 value_reward_diff*reward_diff + 0.227 reward_diff^2 \n",
      "value_persistance[t+1] = -0.024 1 + 0.967 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + -0.005 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + -0.006 repeat + -0.001 switch + -0.002 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.005 wm_rt[t-4] + -0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.004 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.003 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.001 repeat*switch + 0.004 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.005 switch*wm_rt[t-1] + -0.004 switch*wm_rt[t-2] + 0.003 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.004 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 24/1000 --- L(Train): 0.4215893 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.229 1 + 0.773 value_reward_diff[t] + 0.236 reward_diff + -0.223 value_reward_diff^2 + -0.144 value_reward_diff*reward_diff + 0.237 reward_diff^2 \n",
      "value_persistance[t+1] = -0.025 1 + 0.967 value_persistance[t] + -0.002 repeat + 0.002 value_persistance^2 + -0.004 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + -0.004 repeat + -0.002 switch + -0.0 wm_rt[t-1] + -0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + -0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.004 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.005 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.001 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.003 switch*wm_rt[t-1] + -0.004 switch*wm_rt[t-2] + 0.004 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 25/1000 --- L(Train): 0.4092362 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.24 1 + 0.765 value_reward_diff[t] + 0.246 reward_diff + -0.23 value_reward_diff^2 + -0.142 value_reward_diff*reward_diff + 0.246 reward_diff^2 \n",
      "value_persistance[t+1] = -0.027 1 + 0.967 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + -0.003 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + -0.0 repeat + -0.002 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + -0.001 wm_rt^2 + 0.0 wm_rt*repeat + -0.003 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.005 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.004 switch*wm_rt[t-2] + 0.004 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.004 wm_rt[t-1]^2 + 0.004 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 26/1000 --- L(Train): 0.3973536 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.252 1 + 0.758 value_reward_diff[t] + 0.256 reward_diff + -0.238 value_reward_diff^2 + -0.138 value_reward_diff*reward_diff + 0.256 reward_diff^2 \n",
      "value_persistance[t+1] = -0.028 1 + 0.967 value_persistance[t] + -0.001 repeat + -0.001 value_persistance^2 + -0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.004 repeat + -0.002 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.001 wm_rt^2 + -0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.004 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.001 repeat*switch + -0.003 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.003 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + 0.003 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.003 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 27/1000 --- L(Train): 0.3859240 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.264 1 + 0.75 value_reward_diff[t] + 0.266 reward_diff + -0.245 value_reward_diff^2 + -0.133 value_reward_diff*reward_diff + 0.266 reward_diff^2 \n",
      "value_persistance[t+1] = -0.03 1 + 0.967 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + 0.002 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.999 wm_rt[t] + 0.006 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.002 wm_rt^2 + -0.004 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + -0.004 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.004 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.003 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 28/1000 --- L(Train): 0.3749238 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.276 1 + 0.742 value_reward_diff[t] + 0.275 reward_diff + -0.253 value_reward_diff^2 + -0.128 value_reward_diff*reward_diff + 0.275 reward_diff^2 \n",
      "value_persistance[t+1] = -0.031 1 + 0.967 value_persistance[t] + 0.002 repeat + -0.001 value_persistance^2 + 0.004 value_persistance*repeat + 0.003 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.006 repeat + 0.0 switch + -0.003 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.002 wm_rt^2 + -0.003 wm_rt*repeat + 0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.0 repeat*switch + -0.003 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.005 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 29/1000 --- L(Train): 0.3643615 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.288 1 + 0.735 value_reward_diff[t] + 0.285 reward_diff + -0.26 value_reward_diff^2 + -0.122 value_reward_diff*reward_diff + 0.285 reward_diff^2 \n",
      "value_persistance[t+1] = -0.031 1 + 0.966 value_persistance[t] + 0.003 repeat + 0.001 value_persistance^2 + 0.004 value_persistance*repeat + 0.004 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.001 wm_rt[t] + 0.005 repeat + 0.001 switch + -0.003 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.001 wm_rt^2 + -0.002 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.004 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 30/1000 --- L(Train): 0.3542332 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.3 1 + 0.728 value_reward_diff[t] + 0.295 reward_diff + -0.267 value_reward_diff^2 + -0.115 value_reward_diff*reward_diff + 0.295 reward_diff^2 \n",
      "value_persistance[t+1] = -0.032 1 + 0.966 value_persistance[t] + 0.003 repeat + 0.003 value_persistance^2 + 0.003 value_persistance*repeat + 0.004 repeat^2 \n",
      "wm_rt[t+1] = -0.004 1 + 1.002 wm_rt[t] + 0.002 repeat + 0.001 switch + -0.003 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.004 wm_rt[t-3] + 0.002 wm_rt[t-4] + -0.002 wm_rt^2 + 0.0 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.003 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 31/1000 --- L(Train): 0.3445098 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.312 1 + 0.721 value_reward_diff[t] + 0.304 reward_diff + -0.273 value_reward_diff^2 + -0.108 value_reward_diff*reward_diff + 0.304 reward_diff^2 \n",
      "value_persistance[t+1] = -0.033 1 + 0.965 value_persistance[t] + 0.002 repeat + 0.004 value_persistance^2 + 0.002 value_persistance*repeat + 0.003 repeat^2 \n",
      "wm_rt[t+1] = -0.004 1 + 1.001 wm_rt[t] + 0.0 repeat + 0.001 switch + -0.003 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.002 wm_rt[t-4] + -0.003 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.003 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.004 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 32/1000 --- L(Train): 0.3351811 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.324 1 + 0.714 value_reward_diff[t] + 0.314 reward_diff + -0.28 value_reward_diff^2 + -0.1 value_reward_diff*reward_diff + 0.314 reward_diff^2 \n",
      "value_persistance[t+1] = -0.034 1 + 0.965 value_persistance[t] + 0.001 repeat + 0.004 value_persistance^2 + -0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 0.999 wm_rt[t] + -0.002 repeat + 0.0 switch + -0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.002 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.004 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.002 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.004 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 33/1000 --- L(Train): 0.3262380 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.336 1 + 0.707 value_reward_diff[t] + 0.323 reward_diff + -0.286 value_reward_diff^2 + -0.092 value_reward_diff*reward_diff + 0.323 reward_diff^2 \n",
      "value_persistance[t+1] = -0.035 1 + 0.964 value_persistance[t] + 0.0 repeat + 0.004 value_persistance^2 + -0.002 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.998 wm_rt[t] + -0.002 repeat + -0.001 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.003 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.004 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 34/1000 --- L(Train): 0.3176649 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.348 1 + 0.701 value_reward_diff[t] + 0.333 reward_diff + -0.293 value_reward_diff^2 + -0.083 value_reward_diff*reward_diff + 0.333 reward_diff^2 \n",
      "value_persistance[t+1] = -0.035 1 + 0.963 value_persistance[t] + -0.001 repeat + 0.003 value_persistance^2 + -0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.998 wm_rt[t] + -0.002 repeat + -0.002 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.002 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.0 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.003 wm_rt*wm_rt[t-4] + 0.004 repeat^2 + 0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.004 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.002 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 35/1000 --- L(Train): 0.3094536 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.36 1 + 0.694 value_reward_diff[t] + 0.342 reward_diff + -0.299 value_reward_diff^2 + -0.074 value_reward_diff*reward_diff + 0.342 reward_diff^2 \n",
      "value_persistance[t+1] = -0.035 1 + 0.961 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + -0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.002 switch + -0.001 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.002 wm_rt^2 + -0.0 wm_rt*repeat + 0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + 0.004 repeat^2 + 0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 36/1000 --- L(Train): 0.3015884 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.372 1 + 0.688 value_reward_diff[t] + 0.351 reward_diff + -0.304 value_reward_diff^2 + -0.064 value_reward_diff*reward_diff + 0.351 reward_diff^2 \n",
      "value_persistance[t+1] = -0.034 1 + 0.96 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.001 wm_rt[t] + -0.0 repeat + -0.002 switch + -0.003 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.003 repeat^2 + -0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.004 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.003 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 37/1000 --- L(Train): 0.2940570 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.384 1 + 0.682 value_reward_diff[t] + 0.361 reward_diff + -0.31 value_reward_diff^2 + -0.054 value_reward_diff*reward_diff + 0.361 reward_diff^2 \n",
      "value_persistance[t+1] = -0.034 1 + 0.958 value_persistance[t] + 0.001 repeat + -0.0 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.002 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.003 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.004 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 38/1000 --- L(Train): 0.2868444 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.396 1 + 0.676 value_reward_diff[t] + 0.37 reward_diff + -0.316 value_reward_diff^2 + -0.044 value_reward_diff*reward_diff + 0.37 reward_diff^2 \n",
      "value_persistance[t+1] = -0.034 1 + 0.956 value_persistance[t] + 0.002 repeat + -0.0 value_persistance^2 + 0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + -0.0 repeat + 0.0 switch + -0.003 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.002 wm_rt^2 + 0.002 wm_rt*repeat + 0.0 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.003 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.005 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 39/1000 --- L(Train): 0.2799484 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.408 1 + 0.67 value_reward_diff[t] + 0.379 reward_diff + -0.321 value_reward_diff^2 + -0.034 value_reward_diff*reward_diff + 0.379 reward_diff^2 \n",
      "value_persistance[t+1] = -0.034 1 + 0.955 value_persistance[t] + 0.002 repeat + 0.001 value_persistance^2 + 0.003 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.001 switch + -0.003 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.002 wm_rt^2 + 0.001 wm_rt*repeat + 0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + -0.002 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.005 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 40/1000 --- L(Train): 0.2733435 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.42 1 + 0.665 value_reward_diff[t] + 0.388 reward_diff + -0.326 value_reward_diff^2 + -0.023 value_reward_diff*reward_diff + 0.388 reward_diff^2 \n",
      "value_persistance[t+1] = -0.035 1 + 0.955 value_persistance[t] + 0.002 repeat + 0.001 value_persistance^2 + 0.003 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + 0.001 repeat + 0.0 switch + -0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + 0.0 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.004 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 41/1000 --- L(Train): 0.2670195 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.432 1 + 0.659 value_reward_diff[t] + 0.398 reward_diff + -0.331 value_reward_diff^2 + -0.012 value_reward_diff*reward_diff + 0.397 reward_diff^2 \n",
      "value_persistance[t+1] = -0.036 1 + 0.954 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + 0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.997 wm_rt[t] + 0.001 repeat + -0.001 switch + 0.0 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.003 wm_rt[t-3] + 0.002 wm_rt[t-4] + -0.001 wm_rt^2 + -0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.001 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.003 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.003 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 42/1000 --- L(Train): 0.2609653 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.443 1 + 0.654 value_reward_diff[t] + 0.407 reward_diff + -0.336 value_reward_diff^2 + -0.001 value_reward_diff*reward_diff + 0.406 reward_diff^2 \n",
      "value_persistance[t+1] = -0.037 1 + 0.954 value_persistance[t] + -0.0 repeat + -0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.997 wm_rt[t] + 0.0 repeat + -0.001 switch + 0.0 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.002 wm_rt[t-4] + 0.001 wm_rt^2 + -0.002 wm_rt*repeat + 0.0 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 43/1000 --- L(Train): 0.2551684 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.455 1 + 0.649 value_reward_diff[t] + 0.416 reward_diff + -0.34 value_reward_diff^2 + 0.011 value_reward_diff*reward_diff + 0.415 reward_diff^2 \n",
      "value_persistance[t+1] = -0.038 1 + 0.953 value_persistance[t] + -0.0 repeat + -0.0 value_persistance^2 + -0.002 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.998 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.002 wm_rt[t-1] + -0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.002 wm_rt^2 + -0.001 wm_rt*repeat + -0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 44/1000 --- L(Train): 0.2496175 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.467 1 + 0.644 value_reward_diff[t] + 0.425 reward_diff + -0.345 value_reward_diff^2 + 0.022 value_reward_diff*reward_diff + 0.424 reward_diff^2 \n",
      "value_persistance[t+1] = -0.039 1 + 0.953 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + -0.004 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.999 wm_rt[t] + -0.002 repeat + -0.0 switch + -0.003 wm_rt[t-1] + -0.0 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.002 wm_rt^2 + 0.001 wm_rt*repeat + 0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.001 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.003 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 45/1000 --- L(Train): 0.2443112 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.478 1 + 0.639 value_reward_diff[t] + 0.434 reward_diff + -0.349 value_reward_diff^2 + 0.033 value_reward_diff*reward_diff + 0.433 reward_diff^2 \n",
      "value_persistance[t+1] = -0.04 1 + 0.952 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + -0.004 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.002 wm_rt[t] + -0.002 repeat + 0.0 switch + -0.004 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.001 wm_rt^2 + 0.001 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.003 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.005 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 46/1000 --- L(Train): 0.2392272 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.49 1 + 0.634 value_reward_diff[t] + 0.442 reward_diff + -0.354 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 0.442 reward_diff^2 \n",
      "value_persistance[t+1] = -0.041 1 + 0.952 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + -0.004 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.003 wm_rt[t] + -0.002 repeat + 0.0 switch + -0.004 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.007 wm_rt[t-4] + -0.001 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.006 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 47/1000 --- L(Train): 0.2343537 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.501 1 + 0.63 value_reward_diff[t] + 0.451 reward_diff + -0.358 value_reward_diff^2 + 0.056 value_reward_diff*reward_diff + 0.451 reward_diff^2 \n",
      "value_persistance[t+1] = -0.042 1 + 0.952 value_persistance[t] + -0.0 repeat + 0.002 value_persistance^2 + -0.003 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.003 wm_rt[t] + -0.0 repeat + -0.0 switch + -0.003 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.007 wm_rt[t-4] + -0.002 wm_rt^2 + -0.0 wm_rt*repeat + -0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.003 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.006 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 48/1000 --- L(Train): 0.2296822 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.512 1 + 0.625 value_reward_diff[t] + 0.46 reward_diff + -0.362 value_reward_diff^2 + 0.067 value_reward_diff*reward_diff + 0.459 reward_diff^2 \n",
      "value_persistance[t+1] = -0.043 1 + 0.951 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.002 wm_rt[t] + 0.002 repeat + -0.0 switch + -0.002 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.007 wm_rt[t-4] + -0.002 wm_rt^2 + -0.0 wm_rt*repeat + -0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.003 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.001 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.003 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.005 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 49/1000 --- L(Train): 0.2252025 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.523 1 + 0.621 value_reward_diff[t] + 0.469 reward_diff + -0.365 value_reward_diff^2 + 0.079 value_reward_diff*reward_diff + 0.468 reward_diff^2 \n",
      "value_persistance[t+1] = -0.043 1 + 0.95 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.003 repeat + 0.001 switch + -0.0 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.006 wm_rt[t-4] + -0.001 wm_rt^2 + 0.0 wm_rt*repeat + 0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + -0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.004 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 50/1000 --- L(Train): 0.2209081 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.534 1 + 0.617 value_reward_diff[t] + 0.477 reward_diff + -0.369 value_reward_diff^2 + 0.09 value_reward_diff*reward_diff + 0.476 reward_diff^2 \n",
      "value_persistance[t+1] = -0.043 1 + 0.948 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + 0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.997 wm_rt[t] + 0.004 repeat + 0.0 switch + 0.002 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.001 wm_rt^2 + 0.0 wm_rt*repeat + 0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + 0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.004 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 51/1000 --- L(Train): 0.2167893 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.545 1 + 0.613 value_reward_diff[t] + 0.486 reward_diff + -0.373 value_reward_diff^2 + 0.101 value_reward_diff*reward_diff + 0.485 reward_diff^2 \n",
      "value_persistance[t+1] = -0.042 1 + 0.946 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + 0.002 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.996 wm_rt[t] + 0.003 repeat + -0.001 switch + 0.002 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.002 wm_rt^2 + -0.001 wm_rt*repeat + 0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.003 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.003 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 52/1000 --- L(Train): 0.2128296 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.555 1 + 0.609 value_reward_diff[t] + 0.494 reward_diff + -0.376 value_reward_diff^2 + 0.113 value_reward_diff*reward_diff + 0.493 reward_diff^2 \n",
      "value_persistance[t+1] = -0.042 1 + 0.944 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + 0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.996 wm_rt[t] + 0.001 repeat + -0.001 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.002 wm_rt^2 + -0.001 wm_rt*repeat + 0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.004 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.003 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 53/1000 --- L(Train): 0.2090278 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.566 1 + 0.605 value_reward_diff[t] + 0.503 reward_diff + -0.38 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 0.502 reward_diff^2 \n",
      "value_persistance[t+1] = -0.042 1 + 0.943 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.996 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.002 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.002 wm_rt[t-4] + 0.001 wm_rt^2 + -0.0 wm_rt*repeat + -0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.004 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.003 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 54/1000 --- L(Train): 0.2053727 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.576 1 + 0.601 value_reward_diff[t] + 0.511 reward_diff + -0.383 value_reward_diff^2 + 0.135 value_reward_diff*reward_diff + 0.51 reward_diff^2 \n",
      "value_persistance[t+1] = -0.043 1 + 0.943 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + -0.003 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + -0.002 repeat + -0.0 switch + -0.005 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.003 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.003 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.004 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.002 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.002 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 55/1000 --- L(Train): 0.2018576 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.586 1 + 0.597 value_reward_diff[t] + 0.519 reward_diff + -0.386 value_reward_diff^2 + 0.146 value_reward_diff*reward_diff + 0.518 reward_diff^2 \n",
      "value_persistance[t+1] = -0.044 1 + 0.943 value_persistance[t] + -0.002 repeat + 0.0 value_persistance^2 + -0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + -0.003 repeat + 0.001 switch + -0.006 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.003 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.003 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 56/1000 --- L(Train): 0.1984711 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.596 1 + 0.593 value_reward_diff[t] + 0.527 reward_diff + -0.389 value_reward_diff^2 + 0.158 value_reward_diff*reward_diff + 0.526 reward_diff^2 \n",
      "value_persistance[t+1] = -0.046 1 + 0.943 value_persistance[t] + -0.002 repeat + -0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + -0.002 repeat + 0.001 switch + -0.007 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 57/1000 --- L(Train): 0.1952076 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.606 1 + 0.59 value_reward_diff[t] + 0.536 reward_diff + -0.392 value_reward_diff^2 + 0.169 value_reward_diff*reward_diff + 0.534 reward_diff^2 \n",
      "value_persistance[t+1] = -0.046 1 + 0.942 value_persistance[t] + -0.001 repeat + -0.001 value_persistance^2 + -0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + 0.0 repeat + -0.0 switch + -0.007 wm_rt[t-1] + -0.003 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.001 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 58/1000 --- L(Train): 0.1920600 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.616 1 + 0.586 value_reward_diff[t] + 0.544 reward_diff + -0.395 value_reward_diff^2 + 0.18 value_reward_diff*reward_diff + 0.542 reward_diff^2 \n",
      "value_persistance[t+1] = -0.047 1 + 0.941 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + 0.002 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.007 wm_rt[t-1] + -0.003 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.002 wm_rt^2 + -0.002 wm_rt*repeat + 0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 59/1000 --- L(Train): 0.1890305 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.625 1 + 0.583 value_reward_diff[t] + 0.552 reward_diff + -0.398 value_reward_diff^2 + 0.191 value_reward_diff*reward_diff + 0.55 reward_diff^2 \n",
      "value_persistance[t+1] = -0.047 1 + 0.94 value_persistance[t] + 0.003 repeat + 0.002 value_persistance^2 + 0.003 value_persistance*repeat + 0.003 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + 0.001 repeat + 0.001 switch + -0.006 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.002 wm_rt^2 + -0.003 wm_rt*repeat + 0.003 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.001 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 60/1000 --- L(Train): 0.1860995 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.635 1 + 0.58 value_reward_diff[t] + 0.559 reward_diff + -0.401 value_reward_diff^2 + 0.202 value_reward_diff*reward_diff + 0.558 reward_diff^2 \n",
      "value_persistance[t+1] = -0.048 1 + 0.939 value_persistance[t] + 0.003 repeat + 0.002 value_persistance^2 + 0.002 value_persistance*repeat + 0.003 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.001 repeat + 0.0 switch + -0.004 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.002 wm_rt^2 + -0.003 wm_rt*repeat + 0.003 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 61/1000 --- L(Train): 0.1832628 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.644 1 + 0.577 value_reward_diff[t] + 0.567 reward_diff + -0.404 value_reward_diff^2 + 0.213 value_reward_diff*reward_diff + 0.566 reward_diff^2 \n",
      "value_persistance[t+1] = -0.049 1 + 0.939 value_persistance[t] + 0.002 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.003 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 62/1000 --- L(Train): 0.1805197 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.652 1 + 0.573 value_reward_diff[t] + 0.575 reward_diff + -0.406 value_reward_diff^2 + 0.224 value_reward_diff*reward_diff + 0.573 reward_diff^2 \n",
      "value_persistance[t+1] = -0.051 1 + 0.939 value_persistance[t] + 0.001 repeat + -0.0 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.0 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.001 wm_rt^2 + 0.0 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.003 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 63/1000 --- L(Train): 0.1778670 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.661 1 + 0.57 value_reward_diff[t] + 0.583 reward_diff + -0.409 value_reward_diff^2 + 0.234 value_reward_diff*reward_diff + 0.581 reward_diff^2 \n",
      "value_persistance[t+1] = -0.051 1 + 0.939 value_persistance[t] + 0.0 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + 0.0 repeat + -0.001 switch + 0.002 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + 0.001 wm_rt*repeat + -0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 64/1000 --- L(Train): 0.1752960 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.67 1 + 0.567 value_reward_diff[t] + 0.59 reward_diff + -0.412 value_reward_diff^2 + 0.245 value_reward_diff*reward_diff + 0.589 reward_diff^2 \n",
      "value_persistance[t+1] = -0.051 1 + 0.937 value_persistance[t] + -0.001 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + 0.001 repeat + 0.0 switch + 0.003 wm_rt[t-1] + -0.003 wm_rt[t-2] + 0.0 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.001 wm_rt^2 + 0.001 wm_rt*repeat + -0.003 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 65/1000 --- L(Train): 0.1728009 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.678 1 + 0.564 value_reward_diff[t] + 0.598 reward_diff + -0.414 value_reward_diff^2 + 0.256 value_reward_diff*reward_diff + 0.596 reward_diff^2 \n",
      "value_persistance[t+1] = -0.05 1 + 0.935 value_persistance[t] + -0.0 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + 0.001 repeat + 0.0 switch + 0.002 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.0 wm_rt^2 + -0.0 wm_rt*repeat + -0.004 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 66/1000 --- L(Train): 0.1703841 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.686 1 + 0.562 value_reward_diff[t] + 0.605 reward_diff + -0.417 value_reward_diff^2 + 0.266 value_reward_diff*reward_diff + 0.603 reward_diff^2 \n",
      "value_persistance[t+1] = -0.05 1 + 0.933 value_persistance[t] + 0.001 repeat + -0.0 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.006 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + -0.001 wm_rt*repeat + -0.003 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 67/1000 --- L(Train): 0.1680274 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.693 1 + 0.559 value_reward_diff[t] + 0.613 reward_diff + -0.419 value_reward_diff^2 + 0.277 value_reward_diff*reward_diff + 0.611 reward_diff^2 \n",
      "value_persistance[t+1] = -0.05 1 + 0.931 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + 0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.002 repeat + -0.001 switch + -0.003 wm_rt[t-1] + -0.006 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.003 wm_rt^2 + 0.0 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 68/1000 --- L(Train): 0.1657384 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.701 1 + 0.556 value_reward_diff[t] + 0.62 reward_diff + -0.421 value_reward_diff^2 + 0.287 value_reward_diff*reward_diff + 0.618 reward_diff^2 \n",
      "value_persistance[t+1] = -0.05 1 + 0.931 value_persistance[t] + 0.001 repeat + -0.0 value_persistance^2 + 0.003 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + -0.002 repeat + -0.001 switch + -0.004 wm_rt[t-1] + -0.006 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.003 wm_rt^2 + 0.0 wm_rt*repeat + 0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 69/1000 --- L(Train): 0.1635134 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.708 1 + 0.553 value_reward_diff[t] + 0.627 reward_diff + -0.424 value_reward_diff^2 + 0.297 value_reward_diff*reward_diff + 0.625 reward_diff^2 \n",
      "value_persistance[t+1] = -0.052 1 + 0.931 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + 0.003 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.998 wm_rt[t] + -0.002 repeat + -0.0 switch + -0.005 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 70/1000 --- L(Train): 0.1613490 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.715 1 + 0.551 value_reward_diff[t] + 0.634 reward_diff + -0.426 value_reward_diff^2 + 0.308 value_reward_diff*reward_diff + 0.632 reward_diff^2 \n",
      "value_persistance[t+1] = -0.053 1 + 0.931 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + -0.0 repeat + 0.001 switch + -0.005 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 71/1000 --- L(Train): 0.1592329 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.722 1 + 0.548 value_reward_diff[t] + 0.641 reward_diff + -0.428 value_reward_diff^2 + 0.318 value_reward_diff*reward_diff + 0.639 reward_diff^2 \n",
      "value_persistance[t+1] = -0.054 1 + 0.93 value_persistance[t] + 0.0 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.002 repeat + 0.001 switch + -0.004 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.0 wm_rt*repeat + 0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 72/1000 --- L(Train): 0.1571706 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.729 1 + 0.545 value_reward_diff[t] + 0.649 reward_diff + -0.431 value_reward_diff^2 + 0.328 value_reward_diff*reward_diff + 0.646 reward_diff^2 \n",
      "value_persistance[t+1] = -0.055 1 + 0.93 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.002 wm_rt[t] + 0.002 repeat + 0.0 switch + -0.003 wm_rt[t-1] + 0.001 wm_rt[t-2] + 0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.001 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 73/1000 --- L(Train): 0.1551574 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.735 1 + 0.543 value_reward_diff[t] + 0.655 reward_diff + -0.433 value_reward_diff^2 + 0.338 value_reward_diff*reward_diff + 0.653 reward_diff^2 \n",
      "value_persistance[t+1] = -0.056 1 + 0.93 value_persistance[t] + 0.001 repeat + -0.0 value_persistance^2 + -0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.003 wm_rt[t] + 0.002 repeat + -0.002 switch + -0.001 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.0 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 74/1000 --- L(Train): 0.1531926 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.741 1 + 0.54 value_reward_diff[t] + 0.662 reward_diff + -0.435 value_reward_diff^2 + 0.347 value_reward_diff*reward_diff + 0.66 reward_diff^2 \n",
      "value_persistance[t+1] = -0.056 1 + 0.929 value_persistance[t] + -0.0 repeat + 0.002 value_persistance^2 + -0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 1.003 wm_rt[t] + 0.0 repeat + -0.002 switch + 0.001 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.0 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 75/1000 --- L(Train): 0.1512766 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.747 1 + 0.538 value_reward_diff[t] + 0.669 reward_diff + -0.437 value_reward_diff^2 + 0.357 value_reward_diff*reward_diff + 0.667 reward_diff^2 \n",
      "value_persistance[t+1] = -0.057 1 + 0.928 value_persistance[t] + 0.0 repeat + 0.003 value_persistance^2 + -0.0 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.002 wm_rt[t] + -0.002 repeat + -0.002 switch + 0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.001 wm_rt^2 + 0.0 wm_rt*repeat + 0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.002 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 76/1000 --- L(Train): 0.1494009 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.753 1 + 0.535 value_reward_diff[t] + 0.676 reward_diff + -0.439 value_reward_diff^2 + 0.367 value_reward_diff*reward_diff + 0.673 reward_diff^2 \n",
      "value_persistance[t+1] = -0.057 1 + 0.927 value_persistance[t] + 0.001 repeat + 0.003 value_persistance^2 + 0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + -0.002 repeat + -0.0 switch + -0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.001 wm_rt^2 + -0.002 wm_rt*repeat + 0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + 0.002 switch^2 + 0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 77/1000 --- L(Train): 0.1475642 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.759 1 + 0.533 value_reward_diff[t] + 0.683 reward_diff + -0.441 value_reward_diff^2 + 0.376 value_reward_diff*reward_diff + 0.68 reward_diff^2 \n",
      "value_persistance[t+1] = -0.057 1 + 0.926 value_persistance[t] + 0.001 repeat + 0.003 value_persistance^2 + 0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.998 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.002 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.001 wm_rt^2 + -0.003 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.001 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 78/1000 --- L(Train): 0.1457664 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.764 1 + 0.531 value_reward_diff[t] + 0.689 reward_diff + -0.443 value_reward_diff^2 + 0.386 value_reward_diff*reward_diff + 0.686 reward_diff^2 \n",
      "value_persistance[t+1] = -0.057 1 + 0.925 value_persistance[t] + -0.0 repeat + 0.003 value_persistance^2 + 0.002 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.998 wm_rt[t] + 0.001 repeat + 0.001 switch + -0.002 wm_rt[t-1] + -0.005 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.001 wm_rt^2 + -0.003 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.003 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 79/1000 --- L(Train): 0.1440069 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.769 1 + 0.529 value_reward_diff[t] + 0.696 reward_diff + -0.445 value_reward_diff^2 + 0.395 value_reward_diff*reward_diff + 0.693 reward_diff^2 \n",
      "value_persistance[t+1] = -0.058 1 + 0.924 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + 0.0 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.002 wm_rt[t-1] + -0.005 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + -0.002 wm_rt*repeat + 0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.003 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 80/1000 --- L(Train): 0.1422833 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.774 1 + 0.526 value_reward_diff[t] + 0.702 reward_diff + -0.447 value_reward_diff^2 + 0.404 value_reward_diff*reward_diff + 0.699 reward_diff^2 \n",
      "value_persistance[t+1] = -0.058 1 + 0.923 value_persistance[t] + 0.0 repeat + -0.0 value_persistance^2 + -0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + -0.0 repeat + -0.0 switch + -0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.0 wm_rt^2 + 0.0 wm_rt*repeat + 0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.003 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 81/1000 --- L(Train): 0.1405989 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.779 1 + 0.524 value_reward_diff[t] + 0.709 reward_diff + -0.449 value_reward_diff^2 + 0.414 value_reward_diff*reward_diff + 0.705 reward_diff^2 \n",
      "value_persistance[t+1] = -0.059 1 + 0.923 value_persistance[t] + -0.001 repeat + -0.0 value_persistance^2 + -0.004 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.002 wm_rt[t] + 0.0 repeat + 0.0 switch + -0.0 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.001 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 82/1000 --- L(Train): 0.1389390 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.783 1 + 0.522 value_reward_diff[t] + 0.715 reward_diff + -0.451 value_reward_diff^2 + 0.423 value_reward_diff*reward_diff + 0.712 reward_diff^2 \n",
      "value_persistance[t+1] = -0.06 1 + 0.922 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + -0.004 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.0 repeat + 0.0 switch + 0.002 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.001 wm_rt^2 + 0.0 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 83/1000 --- L(Train): 0.1373134 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.788 1 + 0.52 value_reward_diff[t] + 0.721 reward_diff + -0.453 value_reward_diff^2 + 0.432 value_reward_diff*reward_diff + 0.718 reward_diff^2 \n",
      "value_persistance[t+1] = -0.06 1 + 0.921 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + -0.003 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.001 switch + 0.002 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.0 wm_rt^2 + -0.001 wm_rt*repeat + 0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.0 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 84/1000 --- L(Train): 0.1357187 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.792 1 + 0.518 value_reward_diff[t] + 0.728 reward_diff + -0.455 value_reward_diff^2 + 0.441 value_reward_diff*reward_diff + 0.724 reward_diff^2 \n",
      "value_persistance[t+1] = -0.061 1 + 0.921 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.0 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.001 wm_rt*repeat + 0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 85/1000 --- L(Train): 0.1341527 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.796 1 + 0.516 value_reward_diff[t] + 0.734 reward_diff + -0.457 value_reward_diff^2 + 0.449 value_reward_diff*reward_diff + 0.73 reward_diff^2 \n",
      "value_persistance[t+1] = -0.062 1 + 0.92 value_persistance[t] + -0.0 repeat + 0.0 value_persistance^2 + 0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.001 wm_rt[t-1] + 0.001 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.0 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 86/1000 --- L(Train): 0.1326189 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.799 1 + 0.514 value_reward_diff[t] + 0.74 reward_diff + -0.458 value_reward_diff^2 + 0.458 value_reward_diff*reward_diff + 0.736 reward_diff^2 \n",
      "value_persistance[t+1] = -0.062 1 + 0.919 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + 0.0 repeat + -0.0 switch + -0.002 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 87/1000 --- L(Train): 0.1311106 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.803 1 + 0.512 value_reward_diff[t] + 0.746 reward_diff + -0.46 value_reward_diff^2 + 0.467 value_reward_diff*reward_diff + 0.742 reward_diff^2 \n",
      "value_persistance[t+1] = -0.062 1 + 0.918 value_persistance[t] + 0.001 repeat + -0.0 value_persistance^2 + 0.0 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.001 wm_rt[t] + 0.001 repeat + 0.001 switch + -0.002 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + 0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 88/1000 --- L(Train): 0.1296230 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.806 1 + 0.51 value_reward_diff[t] + 0.752 reward_diff + -0.462 value_reward_diff^2 + 0.475 value_reward_diff*reward_diff + 0.748 reward_diff^2 \n",
      "value_persistance[t+1] = -0.062 1 + 0.917 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.001 switch + -0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.002 wm_rt^2 + 0.002 wm_rt*repeat + 0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 89/1000 --- L(Train): 0.1281688 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.81 1 + 0.509 value_reward_diff[t] + 0.758 reward_diff + -0.463 value_reward_diff^2 + 0.484 value_reward_diff*reward_diff + 0.754 reward_diff^2 \n",
      "value_persistance[t+1] = -0.062 1 + 0.916 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + -0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + -0.0 repeat + -0.0 switch + -0.0 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.003 wm_rt^2 + 0.0 wm_rt*repeat + 0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.0 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 90/1000 --- L(Train): 0.1267401 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.813 1 + 0.507 value_reward_diff[t] + 0.764 reward_diff + -0.465 value_reward_diff^2 + 0.492 value_reward_diff*reward_diff + 0.76 reward_diff^2 \n",
      "value_persistance[t+1] = -0.063 1 + 0.916 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + -0.0 repeat + -0.001 switch + 0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.003 wm_rt^2 + -0.002 wm_rt*repeat + 0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 91/1000 --- L(Train): 0.1253350 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.816 1 + 0.505 value_reward_diff[t] + 0.77 reward_diff + -0.467 value_reward_diff^2 + 0.5 value_reward_diff*reward_diff + 0.765 reward_diff^2 \n",
      "value_persistance[t+1] = -0.064 1 + 0.916 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + 0.001 repeat + -0.0 switch + 0.001 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.001 wm_rt^2 + -0.003 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.003 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 92/1000 --- L(Train): 0.1239466 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.818 1 + 0.503 value_reward_diff[t] + 0.775 reward_diff + -0.468 value_reward_diff^2 + 0.509 value_reward_diff*reward_diff + 0.771 reward_diff^2 \n",
      "value_persistance[t+1] = -0.065 1 + 0.915 value_persistance[t] + -0.001 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.0 switch + -0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.0 wm_rt^2 + -0.003 wm_rt*repeat + -0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.0 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 93/1000 --- L(Train): 0.1225865 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.821 1 + 0.502 value_reward_diff[t] + 0.781 reward_diff + -0.47 value_reward_diff^2 + 0.517 value_reward_diff*reward_diff + 0.777 reward_diff^2 \n",
      "value_persistance[t+1] = -0.065 1 + 0.914 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.002 wm_rt[t] + -0.001 repeat + 0.0 switch + -0.003 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.002 wm_rt*repeat + -0.003 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.0 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 94/1000 --- L(Train): 0.1212462 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.824 1 + 0.5 value_reward_diff[t] + 0.787 reward_diff + -0.471 value_reward_diff^2 + 0.525 value_reward_diff*reward_diff + 0.782 reward_diff^2 \n",
      "value_persistance[t+1] = -0.065 1 + 0.913 value_persistance[t] + 0.002 repeat + -0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.003 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.003 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.0 wm_rt*repeat + -0.003 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 95/1000 --- L(Train): 0.1199298 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.826 1 + 0.499 value_reward_diff[t] + 0.792 reward_diff + -0.473 value_reward_diff^2 + 0.533 value_reward_diff*reward_diff + 0.788 reward_diff^2 \n",
      "value_persistance[t+1] = -0.065 1 + 0.912 value_persistance[t] + 0.003 repeat + 0.0 value_persistance^2 + -0.0 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.002 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.003 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.0 wm_rt^2 + 0.002 wm_rt*repeat + -0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.002 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 96/1000 --- L(Train): 0.1186340 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.828 1 + 0.497 value_reward_diff[t] + 0.798 reward_diff + -0.474 value_reward_diff^2 + 0.54 value_reward_diff*reward_diff + 0.793 reward_diff^2 \n",
      "value_persistance[t+1] = -0.066 1 + 0.912 value_persistance[t] + 0.002 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.002 1 + 1.001 wm_rt[t] + 0.001 repeat + 0.0 switch + -0.003 wm_rt[t-1] + 0.001 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + 0.004 wm_rt*repeat + -0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 97/1000 --- L(Train): 0.1173581 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.83 1 + 0.496 value_reward_diff[t] + 0.804 reward_diff + -0.475 value_reward_diff^2 + 0.548 value_reward_diff*reward_diff + 0.799 reward_diff^2 \n",
      "value_persistance[t+1] = -0.067 1 + 0.911 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + 0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.002 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.002 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + 0.004 wm_rt*repeat + 0.003 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.0 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 98/1000 --- L(Train): 0.1161043 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.832 1 + 0.494 value_reward_diff[t] + 0.809 reward_diff + -0.477 value_reward_diff^2 + 0.556 value_reward_diff*reward_diff + 0.804 reward_diff^2 \n",
      "value_persistance[t+1] = -0.068 1 + 0.911 value_persistance[t] + 0.0 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.998 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.0 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + 0.003 wm_rt*repeat + 0.004 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.0 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 99/1000 --- L(Train): 0.1148688 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.834 1 + 0.493 value_reward_diff[t] + 0.814 reward_diff + -0.478 value_reward_diff^2 + 0.563 value_reward_diff*reward_diff + 0.809 reward_diff^2 \n",
      "value_persistance[t+1] = -0.068 1 + 0.91 value_persistance[t] + -0.001 repeat + -0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + -0.002 repeat + -0.001 switch + 0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + 0.004 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + 0.003 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 100/1000 --- L(Train): 0.1136521 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.836 1 + 0.492 value_reward_diff[t] + 0.82 reward_diff + -0.479 value_reward_diff^2 + 0.571 value_reward_diff*reward_diff + 0.815 reward_diff^2 \n",
      "value_persistance[t+1] = -0.068 1 + 0.909 value_persistance[t] + -0.001 repeat + -0.0 value_persistance^2 + -0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.0 switch + 0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + 0.0 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.003 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 101/1000 --- L(Train): 0.1124579 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.837 1 + 0.491 value_reward_diff[t] + 0.825 reward_diff + -0.48 value_reward_diff^2 + 0.578 value_reward_diff*reward_diff + 0.82 reward_diff^2 \n",
      "value_persistance[t+1] = -0.068 1 + 0.908 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.001 switch + -0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.0 wm_rt^2 + -0.002 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 102/1000 --- L(Train): 0.1112826 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.839 1 + 0.489 value_reward_diff[t] + 0.831 reward_diff + -0.481 value_reward_diff^2 + 0.586 value_reward_diff*reward_diff + 0.825 reward_diff^2 \n",
      "value_persistance[t+1] = -0.068 1 + 0.907 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + -0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.001 repeat + 0.002 switch + -0.002 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.002 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 103/1000 --- L(Train): 0.1101241 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.84 1 + 0.488 value_reward_diff[t] + 0.836 reward_diff + -0.482 value_reward_diff^2 + 0.593 value_reward_diff*reward_diff + 0.83 reward_diff^2 \n",
      "value_persistance[t+1] = -0.069 1 + 0.907 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + 0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.001 switch + -0.002 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.001 wm_rt*repeat + -0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.0 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.0 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 104/1000 --- L(Train): 0.1089825 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.841 1 + 0.487 value_reward_diff[t] + 0.841 reward_diff + -0.484 value_reward_diff^2 + 0.6 value_reward_diff*reward_diff + 0.835 reward_diff^2 \n",
      "value_persistance[t+1] = -0.07 1 + 0.907 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + 0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.0 switch + -0.002 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.002 wm_rt^2 + 0.001 wm_rt*repeat + -0.003 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.0 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 105/1000 --- L(Train): 0.1078646 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.843 1 + 0.486 value_reward_diff[t] + 0.846 reward_diff + -0.485 value_reward_diff^2 + 0.607 value_reward_diff*reward_diff + 0.84 reward_diff^2 \n",
      "value_persistance[t+1] = -0.071 1 + 0.907 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.001 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.0 wm_rt^2 + 0.001 wm_rt*repeat + -0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.002 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 106/1000 --- L(Train): 0.1067563 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.844 1 + 0.485 value_reward_diff[t] + 0.851 reward_diff + -0.486 value_reward_diff^2 + 0.614 value_reward_diff*reward_diff + 0.845 reward_diff^2 \n",
      "value_persistance[t+1] = -0.071 1 + 0.906 value_persistance[t] + 0.0 repeat + -0.001 value_persistance^2 + -0.0 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.0 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.002 wm_rt^2 + 0.001 wm_rt*repeat + -0.0 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.002 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 107/1000 --- L(Train): 0.1056686 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.845 1 + 0.484 value_reward_diff[t] + 0.856 reward_diff + -0.487 value_reward_diff^2 + 0.621 value_reward_diff*reward_diff + 0.85 reward_diff^2 \n",
      "value_persistance[t+1] = -0.071 1 + 0.905 value_persistance[t] + -0.001 repeat + -0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.0 repeat + -0.001 switch + 0.001 wm_rt[t-1] + -0.0 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.003 wm_rt^2 + -0.001 wm_rt*repeat + 0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 108/1000 --- L(Train): 0.1045970 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.845 1 + 0.483 value_reward_diff[t] + 0.862 reward_diff + -0.487 value_reward_diff^2 + 0.627 value_reward_diff*reward_diff + 0.855 reward_diff^2 \n",
      "value_persistance[t+1] = -0.07 1 + 0.903 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + -0.001 repeat + -0.0 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.003 wm_rt^2 + -0.001 wm_rt*repeat + 0.003 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 109/1000 --- L(Train): 0.1035440 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.846 1 + 0.482 value_reward_diff[t] + 0.867 reward_diff + -0.488 value_reward_diff^2 + 0.634 value_reward_diff*reward_diff + 0.86 reward_diff^2 \n",
      "value_persistance[t+1] = -0.07 1 + 0.902 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.002 wm_rt^2 + -0.0 wm_rt*repeat + 0.004 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.003 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.004 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 110/1000 --- L(Train): 0.1025056 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.847 1 + 0.481 value_reward_diff[t] + 0.872 reward_diff + -0.489 value_reward_diff^2 + 0.641 value_reward_diff*reward_diff + 0.865 reward_diff^2 \n",
      "value_persistance[t+1] = -0.071 1 + 0.902 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.998 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.003 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.005 wm_rt[t-4] + -0.0 wm_rt^2 + 0.001 wm_rt*repeat + 0.003 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.003 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 111/1000 --- L(Train): 0.1014863 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.848 1 + 0.48 value_reward_diff[t] + 0.877 reward_diff + -0.49 value_reward_diff^2 + 0.647 value_reward_diff*reward_diff + 0.87 reward_diff^2 \n",
      "value_persistance[t+1] = -0.073 1 + 0.903 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + 0.002 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.998 wm_rt[t] + 0.001 repeat + 0.0 switch + -0.003 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + 0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.0 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.006 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 112/1000 --- L(Train): 0.1004794 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.848 1 + 0.479 value_reward_diff[t] + 0.881 reward_diff + -0.491 value_reward_diff^2 + 0.653 value_reward_diff*reward_diff + 0.875 reward_diff^2 \n",
      "value_persistance[t+1] = -0.074 1 + 0.903 value_persistance[t] + -0.001 repeat + -0.001 value_persistance^2 + 0.002 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + 0.003 repeat + -0.001 switch + -0.003 wm_rt[t-1] + 0.001 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.0 wm_rt^2 + 0.001 wm_rt*repeat + -0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.006 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 113/1000 --- L(Train): 0.0994869 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.848 1 + 0.479 value_reward_diff[t] + 0.886 reward_diff + -0.492 value_reward_diff^2 + 0.66 value_reward_diff*reward_diff + 0.879 reward_diff^2 \n",
      "value_persistance[t+1] = -0.074 1 + 0.902 value_persistance[t] + -0.0 repeat + -0.0 value_persistance^2 + -0.0 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + 0.003 repeat + -0.002 switch + -0.003 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + -0.0 wm_rt*repeat + -0.003 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.001 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.0 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 114/1000 --- L(Train): 0.0985102 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.849 1 + 0.478 value_reward_diff[t] + 0.891 reward_diff + -0.492 value_reward_diff^2 + 0.666 value_reward_diff*reward_diff + 0.884 reward_diff^2 \n",
      "value_persistance[t+1] = -0.074 1 + 0.901 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.001 wm_rt[t] + 0.002 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.004 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 115/1000 --- L(Train): 0.0975491 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.849 1 + 0.477 value_reward_diff[t] + 0.896 reward_diff + -0.493 value_reward_diff^2 + 0.672 value_reward_diff*reward_diff + 0.889 reward_diff^2 \n",
      "value_persistance[t+1] = -0.074 1 + 0.9 value_persistance[t] + 0.002 repeat + 0.002 value_persistance^2 + -0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.002 wm_rt[t] + -0.001 repeat + -0.001 switch + 0.0 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.0 wm_rt^2 + 0.0 wm_rt*repeat + -0.003 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 116/1000 --- L(Train): 0.0965992 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.849 1 + 0.477 value_reward_diff[t] + 0.901 reward_diff + -0.494 value_reward_diff^2 + 0.678 value_reward_diff*reward_diff + 0.893 reward_diff^2 \n",
      "value_persistance[t+1] = -0.076 1 + 0.901 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.002 wm_rt[t] + -0.002 repeat + 0.001 switch + -0.0 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.0 wm_rt*repeat + -0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 117/1000 --- L(Train): 0.0956669 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.849 1 + 0.476 value_reward_diff[t] + 0.906 reward_diff + -0.494 value_reward_diff^2 + 0.683 value_reward_diff*reward_diff + 0.898 reward_diff^2 \n",
      "value_persistance[t+1] = -0.076 1 + 0.901 value_persistance[t] + -0.0 repeat + 0.002 value_persistance^2 + 0.001 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + -0.002 repeat + 0.001 switch + 0.0 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + 0.0 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.0 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 118/1000 --- L(Train): 0.0947437 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.849 1 + 0.475 value_reward_diff[t] + 0.91 reward_diff + -0.495 value_reward_diff^2 + 0.689 value_reward_diff*reward_diff + 0.903 reward_diff^2 \n",
      "value_persistance[t+1] = -0.076 1 + 0.899 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + -0.0 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.0 switch + -0.001 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.002 wm_rt^2 + -0.0 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 119/1000 --- L(Train): 0.0938403 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.849 1 + 0.475 value_reward_diff[t] + 0.915 reward_diff + -0.496 value_reward_diff^2 + 0.695 value_reward_diff*reward_diff + 0.907 reward_diff^2 \n",
      "value_persistance[t+1] = -0.075 1 + 0.898 value_persistance[t] + 0.001 repeat + -0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.998 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.002 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + 0.001 wm_rt*repeat + 0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 120/1000 --- L(Train): 0.0929500 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.849 1 + 0.474 value_reward_diff[t] + 0.92 reward_diff + -0.496 value_reward_diff^2 + 0.7 value_reward_diff*reward_diff + 0.912 reward_diff^2 \n",
      "value_persistance[t+1] = -0.075 1 + 0.896 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + 0.002 repeat + -0.001 switch + -0.002 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + 0.0 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.0 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.004 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 121/1000 --- L(Train): 0.0920694 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.849 1 + 0.474 value_reward_diff[t] + 0.924 reward_diff + -0.497 value_reward_diff^2 + 0.706 value_reward_diff*reward_diff + 0.916 reward_diff^2 \n",
      "value_persistance[t+1] = -0.075 1 + 0.895 value_persistance[t] + 0.0 repeat + -0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.002 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.003 wm_rt^2 + -0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 122/1000 --- L(Train): 0.0912043 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.849 1 + 0.473 value_reward_diff[t] + 0.929 reward_diff + -0.497 value_reward_diff^2 + 0.711 value_reward_diff*reward_diff + 0.92 reward_diff^2 \n",
      "value_persistance[t+1] = -0.076 1 + 0.895 value_persistance[t] + -0.002 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.001 wm_rt[t] + -0.0 repeat + 0.001 switch + -0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.003 wm_rt^2 + -0.001 wm_rt*repeat + -0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 123/1000 --- L(Train): 0.0903542 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.848 1 + 0.473 value_reward_diff[t] + 0.933 reward_diff + -0.498 value_reward_diff^2 + 0.716 value_reward_diff*reward_diff + 0.925 reward_diff^2 \n",
      "value_persistance[t+1] = -0.077 1 + 0.895 value_persistance[t] + -0.002 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.002 wm_rt[t] + -0.001 repeat + 0.001 switch + 0.0 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.002 wm_rt^2 + -0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.002 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 124/1000 --- L(Train): 0.0895131 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.848 1 + 0.472 value_reward_diff[t] + 0.938 reward_diff + -0.498 value_reward_diff^2 + 0.721 value_reward_diff*reward_diff + 0.929 reward_diff^2 \n",
      "value_persistance[t+1] = -0.077 1 + 0.895 value_persistance[t] + -0.001 repeat + -0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.0 repeat + 0.0 switch + -0.0 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.0 wm_rt^2 + 0.001 wm_rt*repeat + -0.0 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 125/1000 --- L(Train): 0.0886845 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.847 1 + 0.472 value_reward_diff[t] + 0.942 reward_diff + -0.499 value_reward_diff^2 + 0.726 value_reward_diff*reward_diff + 0.934 reward_diff^2 \n",
      "value_persistance[t+1] = -0.077 1 + 0.894 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + -0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.002 switch + -0.0 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.003 wm_rt^2 + 0.001 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 126/1000 --- L(Train): 0.0878683 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.847 1 + 0.471 value_reward_diff[t] + 0.947 reward_diff + -0.499 value_reward_diff^2 + 0.731 value_reward_diff*reward_diff + 0.938 reward_diff^2 \n",
      "value_persistance[t+1] = -0.078 1 + 0.894 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + -0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.999 wm_rt[t] + -0.0 repeat + -0.002 switch + 0.0 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.004 wm_rt^2 + 0.001 wm_rt*repeat + 0.003 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 127/1000 --- L(Train): 0.0870659 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.846 1 + 0.471 value_reward_diff[t] + 0.951 reward_diff + -0.499 value_reward_diff^2 + 0.736 value_reward_diff*reward_diff + 0.942 reward_diff^2 \n",
      "value_persistance[t+1] = -0.079 1 + 0.894 value_persistance[t] + 0.002 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.002 switch + -0.001 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.004 wm_rt^2 + -0.0 wm_rt*repeat + 0.003 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.003 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 128/1000 --- L(Train): 0.0862773 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.846 1 + 0.47 value_reward_diff[t] + 0.956 reward_diff + -0.5 value_reward_diff^2 + 0.741 value_reward_diff*reward_diff + 0.947 reward_diff^2 \n",
      "value_persistance[t+1] = -0.08 1 + 0.894 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + 0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.001 wm_rt[t] + -0.0 repeat + -0.002 switch + -0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.004 wm_rt^2 + -0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.003 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.003 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 129/1000 --- L(Train): 0.0855017 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.845 1 + 0.47 value_reward_diff[t] + 0.96 reward_diff + -0.5 value_reward_diff^2 + 0.745 value_reward_diff*reward_diff + 0.951 reward_diff^2 \n",
      "value_persistance[t+1] = -0.08 1 + 0.894 value_persistance[t] + -0.0 repeat + -0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.002 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.0 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.0 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 130/1000 --- L(Train): 0.0847300 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.844 1 + 0.47 value_reward_diff[t] + 0.965 reward_diff + -0.501 value_reward_diff^2 + 0.75 value_reward_diff*reward_diff + 0.955 reward_diff^2 \n",
      "value_persistance[t+1] = -0.08 1 + 0.893 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + 0.0 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.001 wm_rt[t] + 0.002 repeat + 0.001 switch + -0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 131/1000 --- L(Train): 0.0839707 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.843 1 + 0.47 value_reward_diff[t] + 0.969 reward_diff + -0.501 value_reward_diff^2 + 0.754 value_reward_diff*reward_diff + 0.959 reward_diff^2 \n",
      "value_persistance[t+1] = -0.08 1 + 0.892 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + -0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + 0.001 repeat + 0.002 switch + 0.0 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.002 switch^2 + 0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 132/1000 --- L(Train): 0.0832262 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.842 1 + 0.469 value_reward_diff[t] + 0.973 reward_diff + -0.501 value_reward_diff^2 + 0.758 value_reward_diff*reward_diff + 0.963 reward_diff^2 \n",
      "value_persistance[t+1] = -0.08 1 + 0.891 value_persistance[t] + -0.0 repeat + 0.003 value_persistance^2 + -0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + -0.0 repeat + 0.001 switch + -0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.002 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.001 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.005 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 133/1000 --- L(Train): 0.0824922 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.841 1 + 0.469 value_reward_diff[t] + 0.978 reward_diff + -0.502 value_reward_diff^2 + 0.762 value_reward_diff*reward_diff + 0.968 reward_diff^2 \n",
      "value_persistance[t+1] = -0.08 1 + 0.891 value_persistance[t] + -0.0 repeat + 0.003 value_persistance^2 + -0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + 0.0 switch + -0.001 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + 0.0 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.002 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.006 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 134/1000 --- L(Train): 0.0817657 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.84 1 + 0.469 value_reward_diff[t] + 0.982 reward_diff + -0.502 value_reward_diff^2 + 0.766 value_reward_diff*reward_diff + 0.972 reward_diff^2 \n",
      "value_persistance[t+1] = -0.081 1 + 0.891 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + -0.0 repeat + -0.002 switch + -0.0 wm_rt[t-1] + -0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.002 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.006 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 135/1000 --- L(Train): 0.0810527 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.839 1 + 0.468 value_reward_diff[t] + 0.986 reward_diff + -0.502 value_reward_diff^2 + 0.77 value_reward_diff*reward_diff + 0.976 reward_diff^2 \n",
      "value_persistance[t+1] = -0.082 1 + 0.891 value_persistance[t] + 0.001 repeat + -0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.002 switch + 0.001 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.003 wm_rt*repeat + 0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.005 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 136/1000 --- L(Train): 0.0803474 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.838 1 + 0.468 value_reward_diff[t] + 0.991 reward_diff + -0.502 value_reward_diff^2 + 0.774 value_reward_diff*reward_diff + 0.98 reward_diff^2 \n",
      "value_persistance[t+1] = -0.082 1 + 0.891 value_persistance[t] + 0.0 repeat + -0.0 value_persistance^2 + 0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.002 switch + -0.0 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.003 wm_rt*repeat + 0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.003 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.004 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 137/1000 --- L(Train): 0.0796544 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.837 1 + 0.468 value_reward_diff[t] + 0.995 reward_diff + -0.503 value_reward_diff^2 + 0.778 value_reward_diff*reward_diff + 0.984 reward_diff^2 \n",
      "value_persistance[t+1] = -0.082 1 + 0.89 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.0 repeat + -0.001 switch + -0.0 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.001 wm_rt^2 + -0.002 wm_rt*repeat + -0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.003 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.001 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + -0.005 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 138/1000 --- L(Train): 0.0789715 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.836 1 + 0.468 value_reward_diff[t] + 0.999 reward_diff + -0.503 value_reward_diff^2 + 0.781 value_reward_diff*reward_diff + 0.988 reward_diff^2 \n",
      "value_persistance[t+1] = -0.082 1 + 0.889 value_persistance[t] + -0.0 repeat + 0.002 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.001 wm_rt[t] + -0.0 repeat + 0.0 switch + -0.0 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.004 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.001 wm_rt^2 + -0.0 wm_rt*repeat + -0.003 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.006 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 139/1000 --- L(Train): 0.0782976 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.835 1 + 0.468 value_reward_diff[t] + 1.003 reward_diff + -0.503 value_reward_diff^2 + 0.785 value_reward_diff*reward_diff + 0.992 reward_diff^2 \n",
      "value_persistance[t+1] = -0.081 1 + 0.888 value_persistance[t] + -0.0 repeat + 0.002 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.004 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.0 wm_rt^2 + 0.002 wm_rt*repeat + -0.003 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.006 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 140/1000 --- L(Train): 0.0776327 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.833 1 + 0.468 value_reward_diff[t] + 1.007 reward_diff + -0.503 value_reward_diff^2 + 0.788 value_reward_diff*reward_diff + 0.996 reward_diff^2 \n",
      "value_persistance[t+1] = -0.082 1 + 0.887 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.999 wm_rt[t] + -0.0 repeat + -0.0 switch + -0.0 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.002 wm_rt^2 + 0.004 wm_rt*repeat + -0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.005 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 141/1000 --- L(Train): 0.0769826 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.832 1 + 0.467 value_reward_diff[t] + 1.011 reward_diff + -0.503 value_reward_diff^2 + 0.791 value_reward_diff*reward_diff + 1.0 reward_diff^2 \n",
      "value_persistance[t+1] = -0.083 1 + 0.888 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.0 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.002 wm_rt^2 + 0.004 wm_rt*repeat + -0.0 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.004 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 142/1000 --- L(Train): 0.0763388 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.831 1 + 0.467 value_reward_diff[t] + 1.016 reward_diff + -0.504 value_reward_diff^2 + 0.794 value_reward_diff*reward_diff + 1.004 reward_diff^2 \n",
      "value_persistance[t+1] = -0.084 1 + 0.888 value_persistance[t] + 0.0 repeat + -0.001 value_persistance^2 + 0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + 0.0 repeat + -0.0 switch + -0.0 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.001 wm_rt^2 + 0.003 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.005 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.002 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 143/1000 --- L(Train): 0.0757035 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.829 1 + 0.467 value_reward_diff[t] + 1.02 reward_diff + -0.504 value_reward_diff^2 + 0.798 value_reward_diff*reward_diff + 1.008 reward_diff^2 \n",
      "value_persistance[t+1] = -0.084 1 + 0.888 value_persistance[t] + -0.0 repeat + -0.002 value_persistance^2 + 0.003 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.001 repeat + 0.001 switch + 0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.002 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.0 wm_rt^2 + 0.002 wm_rt*repeat + 0.004 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.004 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.0 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.005 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 144/1000 --- L(Train): 0.0750740 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.828 1 + 0.467 value_reward_diff[t] + 1.024 reward_diff + -0.504 value_reward_diff^2 + 0.801 value_reward_diff*reward_diff + 1.012 reward_diff^2 \n",
      "value_persistance[t+1] = -0.083 1 + 0.886 value_persistance[t] + 0.0 repeat + -0.0 value_persistance^2 + 0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.999 wm_rt[t] + -0.001 repeat + 0.0 switch + -0.0 wm_rt[t-1] + -0.003 wm_rt[t-2] + 0.002 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.0 wm_rt^2 + -0.001 wm_rt*repeat + 0.004 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.002 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.004 switch*wm_rt[t-4] + -0.005 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 145/1000 --- L(Train): 0.0744572 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.827 1 + 0.467 value_reward_diff[t] + 1.028 reward_diff + -0.504 value_reward_diff^2 + 0.803 value_reward_diff*reward_diff + 1.016 reward_diff^2 \n",
      "value_persistance[t+1] = -0.083 1 + 0.885 value_persistance[t] + 0.001 repeat + 0.003 value_persistance^2 + 0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.003 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.0 wm_rt^2 + -0.002 wm_rt*repeat + 0.003 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.003 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.004 switch*wm_rt[t-4] + -0.005 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 146/1000 --- L(Train): 0.0738504 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.825 1 + 0.467 value_reward_diff[t] + 1.032 reward_diff + -0.504 value_reward_diff^2 + 0.806 value_reward_diff*reward_diff + 1.02 reward_diff^2 \n",
      "value_persistance[t+1] = -0.083 1 + 0.884 value_persistance[t] + 0.0 repeat + 0.004 value_persistance^2 + -0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.0 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.0 wm_rt^2 + -0.002 wm_rt*repeat + 0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.003 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 147/1000 --- L(Train): 0.0732526 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.824 1 + 0.467 value_reward_diff[t] + 1.036 reward_diff + -0.504 value_reward_diff^2 + 0.809 value_reward_diff*reward_diff + 1.024 reward_diff^2 \n",
      "value_persistance[t+1] = -0.084 1 + 0.885 value_persistance[t] + -0.001 repeat + 0.005 value_persistance^2 + -0.004 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + -0.001 repeat + -0.001 switch + 0.001 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.004 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.0 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 148/1000 --- L(Train): 0.0726594 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.822 1 + 0.467 value_reward_diff[t] + 1.04 reward_diff + -0.504 value_reward_diff^2 + 0.811 value_reward_diff*reward_diff + 1.027 reward_diff^2 \n",
      "value_persistance[t+1] = -0.086 1 + 0.886 value_persistance[t] + -0.002 repeat + 0.004 value_persistance^2 + -0.004 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.002 wm_rt[t] + -0.001 repeat + 0.0 switch + -0.0 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.005 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.0 wm_rt^2 + 0.001 wm_rt*repeat + -0.003 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.003 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 149/1000 --- L(Train): 0.0720733 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.821 1 + 0.467 value_reward_diff[t] + 1.044 reward_diff + -0.504 value_reward_diff^2 + 0.814 value_reward_diff*reward_diff + 1.031 reward_diff^2 \n",
      "value_persistance[t+1] = -0.088 1 + 0.887 value_persistance[t] + -0.002 repeat + 0.003 value_persistance^2 + -0.003 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + -0.0 repeat + -0.0 switch + -0.001 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.005 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + 0.001 wm_rt*repeat + -0.003 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.005 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 150/1000 --- L(Train): 0.0714967 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.819 1 + 0.467 value_reward_diff[t] + 1.048 reward_diff + -0.504 value_reward_diff^2 + 0.816 value_reward_diff*reward_diff + 1.035 reward_diff^2 \n",
      "value_persistance[t+1] = -0.088 1 + 0.887 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + -0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.002 repeat + -0.0 switch + -0.0 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.002 wm_rt^2 + 0.001 wm_rt*repeat + -0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.004 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.005 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 151/1000 --- L(Train): 0.0709322 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.817 1 + 0.467 value_reward_diff[t] + 1.052 reward_diff + -0.504 value_reward_diff^2 + 0.818 value_reward_diff*reward_diff + 1.039 reward_diff^2 \n",
      "value_persistance[t+1] = -0.087 1 + 0.886 value_persistance[t] + 0.002 repeat + 0.001 value_persistance^2 + 0.0 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + 0.002 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.004 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 152/1000 --- L(Train): 0.0703737 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.816 1 + 0.467 value_reward_diff[t] + 1.056 reward_diff + -0.504 value_reward_diff^2 + 0.821 value_reward_diff*reward_diff + 1.043 reward_diff^2 \n",
      "value_persistance[t+1] = -0.086 1 + 0.884 value_persistance[t] + 0.004 repeat + -0.0 value_persistance^2 + 0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + 0.0 switch + -0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.004 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.002 switch^2 + 0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 153/1000 --- L(Train): 0.0698221 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.814 1 + 0.467 value_reward_diff[t] + 1.06 reward_diff + -0.504 value_reward_diff^2 + 0.823 value_reward_diff*reward_diff + 1.046 reward_diff^2 \n",
      "value_persistance[t+1] = -0.085 1 + 0.882 value_persistance[t] + 0.005 repeat + -0.0 value_persistance^2 + 0.0 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.001 wm_rt[t] + 0.0 repeat + -0.0 switch + -0.001 wm_rt[t-1] + -0.006 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.0 wm_rt*repeat + 0.003 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.004 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 154/1000 --- L(Train): 0.0692745 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.813 1 + 0.467 value_reward_diff[t] + 1.064 reward_diff + -0.504 value_reward_diff^2 + 0.825 value_reward_diff*reward_diff + 1.05 reward_diff^2 \n",
      "value_persistance[t+1] = -0.084 1 + 0.881 value_persistance[t] + 0.005 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.001 wm_rt[t] + -0.002 repeat + 0.0 switch + -0.001 wm_rt[t-1] + -0.006 wm_rt[t-2] + 0.002 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + 0.003 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.003 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.005 switch*wm_rt[t-4] + -0.005 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 155/1000 --- L(Train): 0.0687381 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.811 1 + 0.467 value_reward_diff[t] + 1.068 reward_diff + -0.504 value_reward_diff^2 + 0.827 value_reward_diff*reward_diff + 1.054 reward_diff^2 \n",
      "value_persistance[t+1] = -0.086 1 + 0.882 value_persistance[t] + 0.004 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.001 wm_rt[t] + -0.003 repeat + -0.001 switch + 0.0 wm_rt[t-1] + -0.006 wm_rt[t-2] + 0.003 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + 0.002 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.003 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.005 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.004 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 156/1000 --- L(Train): 0.0682131 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.809 1 + 0.467 value_reward_diff[t] + 1.072 reward_diff + -0.504 value_reward_diff^2 + 0.828 value_reward_diff*reward_diff + 1.058 reward_diff^2 \n",
      "value_persistance[t+1] = -0.088 1 + 0.884 value_persistance[t] + 0.001 repeat + -0.0 value_persistance^2 + 0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + -0.003 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + 0.002 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.004 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.003 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 157/1000 --- L(Train): 0.0676928 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.807 1 + 0.467 value_reward_diff[t] + 1.076 reward_diff + -0.504 value_reward_diff^2 + 0.83 value_reward_diff*reward_diff + 1.061 reward_diff^2 \n",
      "value_persistance[t+1] = -0.089 1 + 0.884 value_persistance[t] + -0.001 repeat + 0.0 value_persistance^2 + 0.0 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.998 wm_rt[t] + -0.002 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.0 wm_rt^2 + 0.0 wm_rt*repeat + -0.002 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.0 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.003 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 158/1000 --- L(Train): 0.0671786 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.806 1 + 0.467 value_reward_diff[t] + 1.079 reward_diff + -0.504 value_reward_diff^2 + 0.832 value_reward_diff*reward_diff + 1.065 reward_diff^2 \n",
      "value_persistance[t+1] = -0.089 1 + 0.884 value_persistance[t] + -0.002 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.0 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.001 wm_rt^2 + -0.002 wm_rt*repeat + -0.004 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.002 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.003 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 159/1000 --- L(Train): 0.0666710 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.804 1 + 0.467 value_reward_diff[t] + 1.083 reward_diff + -0.504 value_reward_diff^2 + 0.833 value_reward_diff*reward_diff + 1.069 reward_diff^2 \n",
      "value_persistance[t+1] = -0.087 1 + 0.882 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + 0.001 switch + 0.0 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.002 wm_rt^2 + -0.003 wm_rt*repeat + -0.004 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.002 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + 0.003 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 160/1000 --- L(Train): 0.0661721 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.802 1 + 0.467 value_reward_diff[t] + 1.087 reward_diff + -0.504 value_reward_diff^2 + 0.835 value_reward_diff*reward_diff + 1.072 reward_diff^2 \n",
      "value_persistance[t+1] = -0.086 1 + 0.88 value_persistance[t] + 0.0 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.002 wm_rt[t] + 0.001 repeat + 0.001 switch + -0.001 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.001 wm_rt^2 + -0.003 wm_rt*repeat + -0.003 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.002 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.004 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 161/1000 --- L(Train): 0.0656824 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.8 1 + 0.467 value_reward_diff[t] + 1.091 reward_diff + -0.504 value_reward_diff^2 + 0.836 value_reward_diff*reward_diff + 1.076 reward_diff^2 \n",
      "value_persistance[t+1] = -0.086 1 + 0.879 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + 0.0 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.003 wm_rt[t] + -0.0 repeat + -0.0 switch + -0.001 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.0 wm_rt^2 + -0.002 wm_rt*repeat + -0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.004 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.004 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 162/1000 --- L(Train): 0.0651932 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.799 1 + 0.467 value_reward_diff[t] + 1.095 reward_diff + -0.504 value_reward_diff^2 + 0.837 value_reward_diff*reward_diff + 1.08 reward_diff^2 \n",
      "value_persistance[t+1] = -0.086 1 + 0.878 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + 0.0 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.003 wm_rt[t] + -0.001 repeat + -0.0 switch + -0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + -0.0 wm_rt*repeat + 0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.002 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.003 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.005 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 163/1000 --- L(Train): 0.0647155 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.797 1 + 0.468 value_reward_diff[t] + 1.099 reward_diff + -0.504 value_reward_diff^2 + 0.838 value_reward_diff*reward_diff + 1.083 reward_diff^2 \n",
      "value_persistance[t+1] = -0.086 1 + 0.878 value_persistance[t] + -0.0 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + 0.0 repeat + 0.0 switch + 0.0 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.0 wm_rt^2 + 0.002 wm_rt*repeat + 0.002 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.0 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.005 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 164/1000 --- L(Train): 0.0642430 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.795 1 + 0.468 value_reward_diff[t] + 1.102 reward_diff + -0.504 value_reward_diff^2 + 0.84 value_reward_diff*reward_diff + 1.087 reward_diff^2 \n",
      "value_persistance[t+1] = -0.087 1 + 0.879 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + 0.0 repeat + 0.0 switch + -0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + 0.002 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.0 wm_rt^2 + 0.004 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.004 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 165/1000 --- L(Train): 0.0637783 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.793 1 + 0.468 value_reward_diff[t] + 1.106 reward_diff + -0.504 value_reward_diff^2 + 0.841 value_reward_diff*reward_diff + 1.091 reward_diff^2 \n",
      "value_persistance[t+1] = -0.088 1 + 0.88 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + 0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.998 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + 0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + 0.004 wm_rt*repeat + 0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.002 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.006 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 166/1000 --- L(Train): 0.0633174 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.791 1 + 0.468 value_reward_diff[t] + 1.11 reward_diff + -0.504 value_reward_diff^2 + 0.841 value_reward_diff*reward_diff + 1.094 reward_diff^2 \n",
      "value_persistance[t+1] = -0.089 1 + 0.88 value_persistance[t] + 0.0 repeat + -0.0 value_persistance^2 + 0.0 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + -0.001 repeat + -0.002 switch + -0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.0 wm_rt^2 + 0.003 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.007 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 167/1000 --- L(Train): 0.0628642 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.789 1 + 0.468 value_reward_diff[t] + 1.114 reward_diff + -0.504 value_reward_diff^2 + 0.842 value_reward_diff*reward_diff + 1.098 reward_diff^2 \n",
      "value_persistance[t+1] = -0.09 1 + 0.88 value_persistance[t] + 0.0 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + 0.0 repeat + -0.002 switch + -0.0 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.0 wm_rt^2 + 0.002 wm_rt*repeat + -0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.001 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.0 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.007 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + 0.003 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 168/1000 --- L(Train): 0.0624155 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.788 1 + 0.468 value_reward_diff[t] + 1.118 reward_diff + -0.504 value_reward_diff^2 + 0.843 value_reward_diff*reward_diff + 1.101 reward_diff^2 \n",
      "value_persistance[t+1] = -0.09 1 + 0.88 value_persistance[t] + -0.0 repeat + 0.0 value_persistance^2 + -0.002 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.001 wm_rt[t] + 0.0 repeat + -0.001 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.0 wm_rt^2 + -0.001 wm_rt*repeat + -0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.001 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + -0.007 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.003 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 169/1000 --- L(Train): 0.0619773 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.786 1 + 0.468 value_reward_diff[t] + 1.121 reward_diff + -0.503 value_reward_diff^2 + 0.844 value_reward_diff*reward_diff + 1.105 reward_diff^2 \n",
      "value_persistance[t+1] = -0.09 1 + 0.879 value_persistance[t] + 0.0 repeat + -0.0 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.002 wm_rt[t] + -0.001 repeat + 0.001 switch + 0.0 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.005 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.0 wm_rt^2 + -0.002 wm_rt*repeat + -0.0 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.004 switch*wm_rt[t-3] + -0.006 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.003 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 170/1000 --- L(Train): 0.0615430 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.784 1 + 0.469 value_reward_diff[t] + 1.125 reward_diff + -0.503 value_reward_diff^2 + 0.844 value_reward_diff*reward_diff + 1.108 reward_diff^2 \n",
      "value_persistance[t+1] = -0.089 1 + 0.878 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + 0.0 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.002 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.005 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.0 wm_rt^2 + -0.002 wm_rt*repeat + 0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + -0.004 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 171/1000 --- L(Train): 0.0611124 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.782 1 + 0.469 value_reward_diff[t] + 1.129 reward_diff + -0.503 value_reward_diff^2 + 0.845 value_reward_diff*reward_diff + 1.112 reward_diff^2 \n",
      "value_persistance[t+1] = -0.088 1 + 0.877 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + 0.0 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.0 repeat + 0.0 switch + -0.003 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.005 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.003 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.004 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 172/1000 --- L(Train): 0.0606897 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.78 1 + 0.469 value_reward_diff[t] + 1.133 reward_diff + -0.503 value_reward_diff^2 + 0.845 value_reward_diff*reward_diff + 1.115 reward_diff^2 \n",
      "value_persistance[t+1] = -0.088 1 + 0.876 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.004 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.004 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.0 wm_rt^2 + 0.001 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.0 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.0 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.005 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 173/1000 --- L(Train): 0.0602735 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.778 1 + 0.469 value_reward_diff[t] + 1.136 reward_diff + -0.503 value_reward_diff^2 + 0.846 value_reward_diff*reward_diff + 1.119 reward_diff^2 \n",
      "value_persistance[t+1] = -0.088 1 + 0.876 value_persistance[t] + -0.0 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.002 switch + -0.004 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + 0.001 wm_rt*repeat + 0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.003 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.005 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 174/1000 --- L(Train): 0.0598622 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.776 1 + 0.469 value_reward_diff[t] + 1.14 reward_diff + -0.503 value_reward_diff^2 + 0.846 value_reward_diff*reward_diff + 1.123 reward_diff^2 \n",
      "value_persistance[t+1] = -0.089 1 + 0.876 value_persistance[t] + 0.0 repeat + -0.002 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.003 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.003 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.004 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 175/1000 --- L(Train): 0.0594525 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.774 1 + 0.469 value_reward_diff[t] + 1.144 reward_diff + -0.503 value_reward_diff^2 + 0.847 value_reward_diff*reward_diff + 1.126 reward_diff^2 \n",
      "value_persistance[t+1] = -0.09 1 + 0.877 value_persistance[t] + 0.0 repeat + -0.002 value_persistance^2 + 0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.002 wm_rt[t] + -0.0 repeat + -0.0 switch + -0.002 wm_rt[t-1] + -0.005 wm_rt[t-2] + 0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 176/1000 --- L(Train): 0.0590503 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.772 1 + 0.47 value_reward_diff[t] + 1.147 reward_diff + -0.502 value_reward_diff^2 + 0.847 value_reward_diff*reward_diff + 1.13 reward_diff^2 \n",
      "value_persistance[t+1] = -0.09 1 + 0.877 value_persistance[t] + 0.0 repeat + -0.001 value_persistance^2 + 0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + 0.003 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 177/1000 --- L(Train): 0.0586587 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.77 1 + 0.47 value_reward_diff[t] + 1.151 reward_diff + -0.502 value_reward_diff^2 + 0.847 value_reward_diff*reward_diff + 1.133 reward_diff^2 \n",
      "value_persistance[t+1] = -0.09 1 + 0.876 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + 0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + -0.0 repeat + 0.001 switch + 0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + 0.002 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.0 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.002 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.004 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 178/1000 --- L(Train): 0.0582686 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.768 1 + 0.47 value_reward_diff[t] + 1.155 reward_diff + -0.502 value_reward_diff^2 + 0.847 value_reward_diff*reward_diff + 1.137 reward_diff^2 \n",
      "value_persistance[t+1] = -0.09 1 + 0.875 value_persistance[t] + -0.0 repeat + 0.004 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 0.999 wm_rt[t] + 0.0 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + 0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.002 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.0 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.006 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.004 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 179/1000 --- L(Train): 0.0578841 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.766 1 + 0.47 value_reward_diff[t] + 1.158 reward_diff + -0.502 value_reward_diff^2 + 0.847 value_reward_diff*reward_diff + 1.14 reward_diff^2 \n",
      "value_persistance[t+1] = -0.09 1 + 0.876 value_persistance[t] + -0.001 repeat + 0.004 value_persistance^2 + -0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 0.999 wm_rt[t] + 0.0 repeat + -0.002 switch + -0.001 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + 0.002 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.0 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.007 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.004 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 180/1000 --- L(Train): 0.0575014 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.764 1 + 0.47 value_reward_diff[t] + 1.162 reward_diff + -0.502 value_reward_diff^2 + 0.847 value_reward_diff*reward_diff + 1.143 reward_diff^2 \n",
      "value_persistance[t+1] = -0.092 1 + 0.877 value_persistance[t] + -0.0 repeat + 0.004 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + -0.0 repeat + -0.002 switch + -0.002 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.003 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.002 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.002 switch^2 + 0.0 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.007 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 181/1000 --- L(Train): 0.0571237 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.762 1 + 0.471 value_reward_diff[t] + 1.166 reward_diff + -0.501 value_reward_diff^2 + 0.847 value_reward_diff*reward_diff + 1.147 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.878 value_persistance[t] + 0.001 repeat + 0.003 value_persistance^2 + -0.0 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.002 wm_rt[t-1] + 0.003 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.001 wm_rt^2 + 0.0 wm_rt*repeat + -0.0 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.006 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 182/1000 --- L(Train): 0.0567556 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.76 1 + 0.471 value_reward_diff[t] + 1.169 reward_diff + -0.501 value_reward_diff^2 + 0.847 value_reward_diff*reward_diff + 1.15 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.878 value_persistance[t] + 0.002 repeat + 0.002 value_persistance^2 + 0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + 0.0 repeat + 0.0 switch + -0.001 wm_rt[t-1] + 0.003 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.001 wm_rt^2 + -0.002 wm_rt*repeat + -0.0 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.001 switch*wm_rt[t-1] + 0.003 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.005 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 183/1000 --- L(Train): 0.0563890 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.758 1 + 0.471 value_reward_diff[t] + 1.173 reward_diff + -0.501 value_reward_diff^2 + 0.846 value_reward_diff*reward_diff + 1.154 reward_diff^2 \n",
      "value_persistance[t+1] = -0.093 1 + 0.876 value_persistance[t] + 0.003 repeat + 0.001 value_persistance^2 + 0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.0 switch + -0.0 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.001 wm_rt^2 + -0.003 wm_rt*repeat + 0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.002 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.0 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.003 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 184/1000 --- L(Train): 0.0560282 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.756 1 + 0.471 value_reward_diff[t] + 1.177 reward_diff + -0.501 value_reward_diff^2 + 0.846 value_reward_diff*reward_diff + 1.157 reward_diff^2 \n",
      "value_persistance[t+1] = -0.091 1 + 0.875 value_persistance[t] + 0.004 repeat + -0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + -0.002 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.003 wm_rt*repeat + 0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.003 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 185/1000 --- L(Train): 0.0556723 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.754 1 + 0.472 value_reward_diff[t] + 1.18 reward_diff + -0.501 value_reward_diff^2 + 0.846 value_reward_diff*reward_diff + 1.161 reward_diff^2 \n",
      "value_persistance[t+1] = -0.09 1 + 0.873 value_persistance[t] + 0.003 repeat + -0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.001 switch + 0.0 wm_rt[t-1] + -0.006 wm_rt[t-2] + 0.001 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + -0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.001 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.003 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 186/1000 --- L(Train): 0.0553225 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.752 1 + 0.472 value_reward_diff[t] + 1.184 reward_diff + -0.5 value_reward_diff^2 + 0.845 value_reward_diff*reward_diff + 1.164 reward_diff^2 \n",
      "value_persistance[t+1] = -0.091 1 + 0.873 value_persistance[t] + 0.002 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.002 wm_rt[t-1] + -0.008 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.001 wm_rt^2 + -0.0 wm_rt*repeat + -0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.0 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.004 switch*wm_rt[t-4] + -0.005 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 187/1000 --- L(Train): 0.0549740 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.75 1 + 0.472 value_reward_diff[t] + 1.187 reward_diff + -0.5 value_reward_diff^2 + 0.845 value_reward_diff*reward_diff + 1.167 reward_diff^2 \n",
      "value_persistance[t+1] = -0.092 1 + 0.874 value_persistance[t] + -0.001 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + 0.002 repeat + 0.0 switch + -0.004 wm_rt[t-1] + -0.01 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.006 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.004 switch*wm_rt[t-4] + -0.005 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 188/1000 --- L(Train): 0.0546293 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.748 1 + 0.472 value_reward_diff[t] + 1.191 reward_diff + -0.5 value_reward_diff^2 + 0.844 value_reward_diff*reward_diff + 1.171 reward_diff^2 \n",
      "value_persistance[t+1] = -0.092 1 + 0.874 value_persistance[t] + -0.001 repeat + -0.0 value_persistance^2 + 0.0 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.004 wm_rt[t-1] + -0.01 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + 0.004 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.003 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.006 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.003 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.004 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 189/1000 --- L(Train): 0.0542913 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.746 1 + 0.473 value_reward_diff[t] + 1.195 reward_diff + -0.5 value_reward_diff^2 + 0.844 value_reward_diff*reward_diff + 1.174 reward_diff^2 \n",
      "value_persistance[t+1] = -0.092 1 + 0.873 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + 0.0 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.002 wm_rt[t] + -0.0 repeat + 0.0 switch + -0.004 wm_rt[t-1] + -0.009 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.001 wm_rt^2 + 0.004 wm_rt*repeat + 0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.003 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.005 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 190/1000 --- L(Train): 0.0539564 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.744 1 + 0.473 value_reward_diff[t] + 1.198 reward_diff + -0.499 value_reward_diff^2 + 0.843 value_reward_diff*reward_diff + 1.178 reward_diff^2 \n",
      "value_persistance[t+1] = -0.091 1 + 0.873 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.002 wm_rt[t] + -0.0 repeat + -0.001 switch + -0.004 wm_rt[t-1] + -0.008 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + 0.003 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 191/1000 --- L(Train): 0.0536284 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.742 1 + 0.473 value_reward_diff[t] + 1.202 reward_diff + -0.499 value_reward_diff^2 + 0.842 value_reward_diff*reward_diff + 1.181 reward_diff^2 \n",
      "value_persistance[t+1] = -0.092 1 + 0.873 value_persistance[t] + 0.002 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.0 repeat + -0.0 switch + -0.002 wm_rt[t-1] + -0.006 wm_rt[t-2] + 0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.004 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.004 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 192/1000 --- L(Train): 0.0533023 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.74 1 + 0.473 value_reward_diff[t] + 1.205 reward_diff + -0.499 value_reward_diff^2 + 0.842 value_reward_diff*reward_diff + 1.184 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.874 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + 0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.0 repeat + 0.001 switch + -0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + 0.002 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.001 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.004 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.005 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 193/1000 --- L(Train): 0.0529826 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.738 1 + 0.474 value_reward_diff[t] + 1.209 reward_diff + -0.499 value_reward_diff^2 + 0.841 value_reward_diff*reward_diff + 1.188 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.874 value_persistance[t] + 0.0 repeat + -0.0 value_persistance^2 + -0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + -0.0 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.0 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + -0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.003 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.006 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.003 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 194/1000 --- L(Train): 0.0526632 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.736 1 + 0.474 value_reward_diff[t] + 1.212 reward_diff + -0.498 value_reward_diff^2 + 0.84 value_reward_diff*reward_diff + 1.191 reward_diff^2 \n",
      "value_persistance[t+1] = -0.093 1 + 0.874 value_persistance[t] + -0.0 repeat + 0.002 value_persistance^2 + 0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.001 switch + 0.001 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.001 wm_rt^2 + -0.002 wm_rt*repeat + 0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.0 switch*wm_rt[t-1] + 0.003 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.005 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.003 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 195/1000 --- L(Train): 0.0523531 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.734 1 + 0.474 value_reward_diff[t] + 1.216 reward_diff + -0.498 value_reward_diff^2 + 0.839 value_reward_diff*reward_diff + 1.194 reward_diff^2 \n",
      "value_persistance[t+1] = -0.093 1 + 0.873 value_persistance[t] + -0.0 repeat + 0.003 value_persistance^2 + -0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.0 wm_rt[t-1] + 0.003 wm_rt[t-2] + -0.005 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.0 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.004 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.004 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 196/1000 --- L(Train): 0.0520448 --- L(Val, SINDy): 0.0000000 --- Time: 0.11s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.732 1 + 0.475 value_reward_diff[t] + 1.22 reward_diff + -0.498 value_reward_diff^2 + 0.838 value_reward_diff*reward_diff + 1.198 reward_diff^2 \n",
      "value_persistance[t+1] = -0.093 1 + 0.872 value_persistance[t] + 0.001 repeat + 0.003 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + -0.0 repeat + -0.001 switch + -0.001 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.006 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.001 wm_rt^2 + 0.001 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 197/1000 --- L(Train): 0.0517380 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.73 1 + 0.475 value_reward_diff[t] + 1.223 reward_diff + -0.498 value_reward_diff^2 + 0.837 value_reward_diff*reward_diff + 1.201 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.873 value_persistance[t] + 0.001 repeat + 0.003 value_persistance^2 + 0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.002 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.007 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.001 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.0 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 198/1000 --- L(Train): 0.0514370 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.728 1 + 0.475 value_reward_diff[t] + 1.227 reward_diff + -0.497 value_reward_diff^2 + 0.836 value_reward_diff*reward_diff + 1.204 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.873 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + 0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.001 switch + -0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.007 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.001 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.002 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.004 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 199/1000 --- L(Train): 0.0511391 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.726 1 + 0.475 value_reward_diff[t] + 1.23 reward_diff + -0.497 value_reward_diff^2 + 0.835 value_reward_diff*reward_diff + 1.208 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.873 value_persistance[t] + -0.001 repeat + 0.0 value_persistance^2 + -0.0 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + 0.0 switch + -0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.006 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.0 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 200/1000 --- L(Train): 0.0508443 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.724 1 + 0.476 value_reward_diff[t] + 1.234 reward_diff + -0.497 value_reward_diff^2 + 0.834 value_reward_diff*reward_diff + 1.211 reward_diff^2 \n",
      "value_persistance[t+1] = -0.093 1 + 0.872 value_persistance[t] + 0.0 repeat + -0.001 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.001 switch + 0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 201/1000 --- L(Train): 0.0505545 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.722 1 + 0.476 value_reward_diff[t] + 1.237 reward_diff + -0.497 value_reward_diff^2 + 0.833 value_reward_diff*reward_diff + 1.214 reward_diff^2 \n",
      "value_persistance[t+1] = -0.093 1 + 0.871 value_persistance[t] + 0.0 repeat + -0.002 value_persistance^2 + 0.0 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.001 wm_rt[t] + -0.0 repeat + -0.001 switch + 0.0 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.002 wm_rt[t-4] + -0.001 wm_rt^2 + -0.0 wm_rt*repeat + 0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.004 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 202/1000 --- L(Train): 0.0502665 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.72 1 + 0.476 value_reward_diff[t] + 1.241 reward_diff + -0.496 value_reward_diff^2 + 0.832 value_reward_diff*reward_diff + 1.218 reward_diff^2 \n",
      "value_persistance[t+1] = -0.092 1 + 0.87 value_persistance[t] + -0.0 repeat + -0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.002 wm_rt[t-1] + 0.001 wm_rt[t-2] + 0.0 wm_rt[t-3] + 0.002 wm_rt[t-4] + -0.0 wm_rt^2 + 0.002 wm_rt*repeat + 0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.004 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.004 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 203/1000 --- L(Train): 0.0499864 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.717 1 + 0.477 value_reward_diff[t] + 1.244 reward_diff + -0.496 value_reward_diff^2 + 0.83 value_reward_diff*reward_diff + 1.221 reward_diff^2 \n",
      "value_persistance[t+1] = -0.092 1 + 0.87 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + -0.0 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + 0.002 repeat + 0.001 switch + -0.003 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.002 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.002 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.004 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.003 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 204/1000 --- L(Train): 0.0497062 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.715 1 + 0.477 value_reward_diff[t] + 1.248 reward_diff + -0.496 value_reward_diff^2 + 0.829 value_reward_diff*reward_diff + 1.224 reward_diff^2 \n",
      "value_persistance[t+1] = -0.093 1 + 0.87 value_persistance[t] + -0.0 repeat + 0.003 value_persistance^2 + 0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.998 wm_rt[t] + 0.002 repeat + 0.001 switch + -0.004 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.004 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.004 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 205/1000 --- L(Train): 0.0494268 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.713 1 + 0.477 value_reward_diff[t] + 1.251 reward_diff + -0.495 value_reward_diff^2 + 0.828 value_reward_diff*reward_diff + 1.227 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.871 value_persistance[t] + 0.0 repeat + 0.003 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.998 wm_rt[t] + -0.0 repeat + 0.0 switch + -0.004 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.001 wm_rt^2 + 0.0 wm_rt*repeat + -0.002 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.004 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 206/1000 --- L(Train): 0.0491501 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.711 1 + 0.478 value_reward_diff[t] + 1.255 reward_diff + -0.495 value_reward_diff^2 + 0.827 value_reward_diff*reward_diff + 1.231 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.873 value_persistance[t] + 0.0 repeat + 0.003 value_persistance^2 + 0.0 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.998 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.003 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.006 wm_rt[t-4] + -0.001 wm_rt^2 + -0.002 wm_rt*repeat + -0.0 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.003 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.004 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 207/1000 --- L(Train): 0.0488828 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.709 1 + 0.478 value_reward_diff[t] + 1.258 reward_diff + -0.495 value_reward_diff^2 + 0.825 value_reward_diff*reward_diff + 1.234 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.873 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + -0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.002 switch + -0.002 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.007 wm_rt[t-4] + -0.001 wm_rt^2 + -0.003 wm_rt*repeat + 0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.003 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.004 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.003 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.004 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 208/1000 --- L(Train): 0.0486127 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.707 1 + 0.478 value_reward_diff[t] + 1.262 reward_diff + -0.495 value_reward_diff^2 + 0.824 value_reward_diff*reward_diff + 1.237 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.873 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + 0.0 repeat + -0.002 switch + -0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.006 wm_rt[t-4] + -0.0 wm_rt^2 + -0.003 wm_rt*repeat + 0.003 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.004 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 209/1000 --- L(Train): 0.0483491 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.705 1 + 0.478 value_reward_diff[t] + 1.265 reward_diff + -0.494 value_reward_diff^2 + 0.822 value_reward_diff*reward_diff + 1.24 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.872 value_persistance[t] + 0.0 repeat + -0.001 value_persistance^2 + -0.003 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.002 wm_rt[t] + 0.0 repeat + -0.001 switch + 0.001 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.006 wm_rt[t-4] + 0.001 wm_rt^2 + -0.002 wm_rt*repeat + 0.003 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.004 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 210/1000 --- L(Train): 0.0480868 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.703 1 + 0.479 value_reward_diff[t] + 1.269 reward_diff + -0.494 value_reward_diff^2 + 0.821 value_reward_diff*reward_diff + 1.244 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.87 value_persistance[t] + 0.0 repeat + -0.0 value_persistance^2 + -0.002 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.002 wm_rt[t] + -0.001 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.0 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.002 wm_rt^2 + -0.0 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.003 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.002 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 211/1000 --- L(Train): 0.0478293 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.701 1 + 0.479 value_reward_diff[t] + 1.272 reward_diff + -0.494 value_reward_diff^2 + 0.82 value_reward_diff*reward_diff + 1.247 reward_diff^2 \n",
      "value_persistance[t+1] = -0.093 1 + 0.869 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.0 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.001 wm_rt^2 + 0.003 wm_rt*repeat + -0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.003 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 212/1000 --- L(Train): 0.0475751 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.699 1 + 0.479 value_reward_diff[t] + 1.275 reward_diff + -0.493 value_reward_diff^2 + 0.818 value_reward_diff*reward_diff + 1.25 reward_diff^2 \n",
      "value_persistance[t+1] = -0.093 1 + 0.868 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + 0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + 0.001 repeat + 0.0 switch + -0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + 0.004 wm_rt*repeat + -0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + -0.004 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 213/1000 --- L(Train): 0.0473268 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.697 1 + 0.48 value_reward_diff[t] + 1.279 reward_diff + -0.493 value_reward_diff^2 + 0.816 value_reward_diff*reward_diff + 1.253 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.869 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + 0.003 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.997 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.002 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.002 wm_rt[t-4] + -0.001 wm_rt^2 + 0.004 wm_rt*repeat + -0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.005 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 214/1000 --- L(Train): 0.0470783 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.694 1 + 0.48 value_reward_diff[t] + 1.282 reward_diff + -0.493 value_reward_diff^2 + 0.815 value_reward_diff*reward_diff + 1.257 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.869 value_persistance[t] + 0.002 repeat + 0.0 value_persistance^2 + 0.003 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.997 wm_rt[t] + 0.001 repeat + -0.002 switch + -0.002 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.001 wm_rt[t-3] + 0.002 wm_rt[t-4] + -0.0 wm_rt^2 + 0.003 wm_rt*repeat + -0.0 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.005 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 215/1000 --- L(Train): 0.0468329 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.692 1 + 0.48 value_reward_diff[t] + 1.286 reward_diff + -0.492 value_reward_diff^2 + 0.813 value_reward_diff*reward_diff + 1.26 reward_diff^2 \n",
      "value_persistance[t+1] = -0.095 1 + 0.87 value_persistance[t] + 0.003 repeat + -0.001 value_persistance^2 + 0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.997 wm_rt[t] + -0.001 repeat + -0.002 switch + -0.001 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.002 wm_rt^2 + 0.002 wm_rt*repeat + 0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.004 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.005 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 216/1000 --- L(Train): 0.0465863 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.69 1 + 0.481 value_reward_diff[t] + 1.289 reward_diff + -0.492 value_reward_diff^2 + 0.812 value_reward_diff*reward_diff + 1.263 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.87 value_persistance[t] + 0.003 repeat + -0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.001 switch + 0.0 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.002 wm_rt^2 + -0.001 wm_rt*repeat + 0.003 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.004 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 217/1000 --- L(Train): 0.0463507 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.688 1 + 0.481 value_reward_diff[t] + 1.293 reward_diff + -0.492 value_reward_diff^2 + 0.81 value_reward_diff*reward_diff + 1.266 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.87 value_persistance[t] + 0.003 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.002 wm_rt[t] + -0.001 repeat + 0.0 switch + -0.0 wm_rt[t-1] + 0.001 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.001 wm_rt^2 + -0.002 wm_rt*repeat + 0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.003 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.004 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 218/1000 --- L(Train): 0.0461159 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.686 1 + 0.481 value_reward_diff[t] + 1.296 reward_diff + -0.491 value_reward_diff^2 + 0.808 value_reward_diff*reward_diff + 1.269 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.871 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.003 wm_rt[t] + 0.001 repeat + 0.0 switch + -0.0 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + -0.0 wm_rt^2 + -0.002 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 219/1000 --- L(Train): 0.0458813 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.684 1 + 0.482 value_reward_diff[t] + 1.3 reward_diff + -0.491 value_reward_diff^2 + 0.807 value_reward_diff*reward_diff + 1.273 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.87 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.003 wm_rt[t] + 0.001 repeat + -0.001 switch + 0.0 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.002 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 220/1000 --- L(Train): 0.0456470 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.682 1 + 0.482 value_reward_diff[t] + 1.303 reward_diff + -0.491 value_reward_diff^2 + 0.805 value_reward_diff*reward_diff + 1.276 reward_diff^2 \n",
      "value_persistance[t+1] = -0.095 1 + 0.869 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 1.002 wm_rt[t] + -0.0 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.0 wm_rt^2 + 0.001 wm_rt*repeat + -0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.003 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 221/1000 --- L(Train): 0.0454216 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.68 1 + 0.482 value_reward_diff[t] + 1.306 reward_diff + -0.49 value_reward_diff^2 + 0.803 value_reward_diff*reward_diff + 1.279 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.867 value_persistance[t] + -0.001 repeat + -0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.0 switch + -0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.0 wm_rt^2 + 0.001 wm_rt*repeat + -0.002 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.003 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.003 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 222/1000 --- L(Train): 0.0451959 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.678 1 + 0.483 value_reward_diff[t] + 1.31 reward_diff + -0.49 value_reward_diff^2 + 0.801 value_reward_diff*reward_diff + 1.282 reward_diff^2 \n",
      "value_persistance[t+1] = -0.093 1 + 0.867 value_persistance[t] + -0.0 repeat + 0.0 value_persistance^2 + -0.0 value_persistance*repeat + 0.003 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.999 wm_rt[t] + 0.0 repeat + 0.001 switch + -0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.0 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.001 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.0 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 223/1000 --- L(Train): 0.0449723 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.676 1 + 0.483 value_reward_diff[t] + 1.313 reward_diff + -0.49 value_reward_diff^2 + 0.8 value_reward_diff*reward_diff + 1.285 reward_diff^2 \n",
      "value_persistance[t+1] = -0.094 1 + 0.868 value_persistance[t] + 0.0 repeat + -0.0 value_persistance^2 + 0.0 value_persistance*repeat + 0.003 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.0 switch + -0.001 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + -0.001 wm_rt*repeat + 0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.0 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.004 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 224/1000 --- L(Train): 0.0447485 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.673 1 + 0.483 value_reward_diff[t] + 1.317 reward_diff + -0.489 value_reward_diff^2 + 0.798 value_reward_diff*reward_diff + 1.288 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.869 value_persistance[t] + 0.0 repeat + 0.0 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + -0.0 repeat + -0.0 switch + 0.0 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.004 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 225/1000 --- L(Train): 0.0445301 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.671 1 + 0.484 value_reward_diff[t] + 1.32 reward_diff + -0.489 value_reward_diff^2 + 0.796 value_reward_diff*reward_diff + 1.292 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.87 value_persistance[t] + -0.001 repeat + 0.0 value_persistance^2 + 0.0 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.002 wm_rt[t] + 0.0 repeat + 0.0 switch + -0.001 wm_rt[t-1] + 0.002 wm_rt[t-2] + 0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.0 wm_rt^2 + -0.0 wm_rt*repeat + 0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.004 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.002 switch*wm_rt[t-1] + -0.004 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.004 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 226/1000 --- L(Train): 0.0443153 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.669 1 + 0.484 value_reward_diff[t] + 1.323 reward_diff + -0.489 value_reward_diff^2 + 0.794 value_reward_diff*reward_diff + 1.295 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.87 value_persistance[t] + 0.0 repeat + -0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.003 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.001 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.004 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.004 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 227/1000 --- L(Train): 0.0440986 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.667 1 + 0.484 value_reward_diff[t] + 1.327 reward_diff + -0.488 value_reward_diff^2 + 0.792 value_reward_diff*reward_diff + 1.298 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.868 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.002 wm_rt[t] + -0.0 repeat + -0.001 switch + -0.0 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.001 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.004 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 228/1000 --- L(Train): 0.0438861 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.665 1 + 0.485 value_reward_diff[t] + 1.33 reward_diff + -0.488 value_reward_diff^2 + 0.79 value_reward_diff*reward_diff + 1.301 reward_diff^2 \n",
      "value_persistance[t+1] = -0.095 1 + 0.867 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.001 wm_rt[t] + 0.001 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.0 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.001 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 229/1000 --- L(Train): 0.0436813 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.663 1 + 0.485 value_reward_diff[t] + 1.333 reward_diff + -0.488 value_reward_diff^2 + 0.789 value_reward_diff*reward_diff + 1.304 reward_diff^2 \n",
      "value_persistance[t+1] = -0.095 1 + 0.867 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + 0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.0 switch + 0.0 wm_rt[t-1] + -0.006 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + 0.0 wm_rt*repeat + 0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 230/1000 --- L(Train): 0.0434702 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.661 1 + 0.485 value_reward_diff[t] + 1.337 reward_diff + -0.487 value_reward_diff^2 + 0.787 value_reward_diff*reward_diff + 1.307 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.868 value_persistance[t] + -0.001 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + 0.0 repeat + -0.0 switch + -0.002 wm_rt[t-1] + -0.006 wm_rt[t-2] + -0.004 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.002 wm_rt^2 + -0.002 wm_rt*repeat + 0.002 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 231/1000 --- L(Train): 0.0432657 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.659 1 + 0.486 value_reward_diff[t] + 1.34 reward_diff + -0.487 value_reward_diff^2 + 0.785 value_reward_diff*reward_diff + 1.31 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.867 value_persistance[t] + -0.001 repeat + -0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.998 wm_rt[t] + -0.001 repeat + 0.0 switch + -0.004 wm_rt[t-1] + -0.006 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + -0.003 wm_rt*repeat + 0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.002 switch^2 + 0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 232/1000 --- L(Train): 0.0430662 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.657 1 + 0.486 value_reward_diff[t] + 1.344 reward_diff + -0.487 value_reward_diff^2 + 0.783 value_reward_diff*reward_diff + 1.314 reward_diff^2 \n",
      "value_persistance[t+1] = -0.095 1 + 0.867 value_persistance[t] + 0.0 repeat + -0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + 0.0 switch + -0.005 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + -0.003 wm_rt*repeat + -0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.002 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.003 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.003 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 233/1000 --- L(Train): 0.0428678 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.654 1 + 0.487 value_reward_diff[t] + 1.347 reward_diff + -0.486 value_reward_diff^2 + 0.781 value_reward_diff*reward_diff + 1.317 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.867 value_persistance[t] + 0.0 repeat + 0.0 value_persistance^2 + -0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.005 wm_rt[t-1] + -0.003 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.001 wm_rt^2 + -0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.003 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 234/1000 --- L(Train): 0.0426682 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.652 1 + 0.487 value_reward_diff[t] + 1.35 reward_diff + -0.486 value_reward_diff^2 + 0.779 value_reward_diff*reward_diff + 1.32 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.868 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + -0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + -0.0 repeat + -0.0 switch + -0.005 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.0 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.0 wm_rt^2 + -0.0 wm_rt*repeat + -0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.0 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 235/1000 --- L(Train): 0.0424738 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.65 1 + 0.487 value_reward_diff[t] + 1.354 reward_diff + -0.486 value_reward_diff^2 + 0.777 value_reward_diff*reward_diff + 1.323 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.868 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + 0.001 switch + -0.004 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.0 wm_rt^2 + 0.002 wm_rt*repeat + 0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.003 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.004 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 236/1000 --- L(Train): 0.0422824 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.648 1 + 0.488 value_reward_diff[t] + 1.357 reward_diff + -0.485 value_reward_diff^2 + 0.775 value_reward_diff*reward_diff + 1.326 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.868 value_persistance[t] + 0.002 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.999 wm_rt[t] + 0.001 repeat + 0.0 switch + -0.002 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.0 wm_rt^2 + 0.004 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.003 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.004 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 237/1000 --- L(Train): 0.0420930 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.646 1 + 0.488 value_reward_diff[t] + 1.36 reward_diff + -0.485 value_reward_diff^2 + 0.773 value_reward_diff*reward_diff + 1.329 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.867 value_persistance[t] + 0.003 repeat + -0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.998 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.0 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.0 wm_rt^2 + 0.004 wm_rt*repeat + 0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.002 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.004 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 238/1000 --- L(Train): 0.0419037 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.644 1 + 0.488 value_reward_diff[t] + 1.364 reward_diff + -0.485 value_reward_diff^2 + 0.771 value_reward_diff*reward_diff + 1.332 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.867 value_persistance[t] + 0.003 repeat + -0.001 value_persistance^2 + 0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + 0.0 repeat + -0.001 switch + 0.002 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.001 wm_rt^2 + 0.004 wm_rt*repeat + -0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 239/1000 --- L(Train): 0.0417155 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.642 1 + 0.489 value_reward_diff[t] + 1.367 reward_diff + -0.484 value_reward_diff^2 + 0.769 value_reward_diff*reward_diff + 1.335 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.867 value_persistance[t] + 0.002 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.0 switch + 0.002 wm_rt[t-1] + -0.003 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.0 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 240/1000 --- L(Train): 0.0415310 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.64 1 + 0.489 value_reward_diff[t] + 1.37 reward_diff + -0.484 value_reward_diff^2 + 0.767 value_reward_diff*reward_diff + 1.338 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.867 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + -0.002 repeat + 0.001 switch + 0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.004 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 241/1000 --- L(Train): 0.0413480 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.638 1 + 0.489 value_reward_diff[t] + 1.374 reward_diff + -0.483 value_reward_diff^2 + 0.765 value_reward_diff*reward_diff + 1.341 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.868 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + -0.0 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + -0.001 repeat + 0.0 switch + -0.002 wm_rt[t-1] + -0.004 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.001 wm_rt^2 + -0.002 wm_rt*repeat + 0.0 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.004 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.004 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 242/1000 --- L(Train): 0.0411669 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.636 1 + 0.49 value_reward_diff[t] + 1.377 reward_diff + -0.483 value_reward_diff^2 + 0.763 value_reward_diff*reward_diff + 1.345 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.868 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.005 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + -0.002 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.004 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 243/1000 --- L(Train): 0.0409879 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.633 1 + 0.49 value_reward_diff[t] + 1.38 reward_diff + -0.483 value_reward_diff^2 + 0.761 value_reward_diff*reward_diff + 1.348 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.867 value_persistance[t] + -0.0 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.001 repeat + -0.002 switch + -0.006 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.0 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.003 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.003 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 244/1000 --- L(Train): 0.0408115 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.631 1 + 0.49 value_reward_diff[t] + 1.384 reward_diff + -0.482 value_reward_diff^2 + 0.758 value_reward_diff*reward_diff + 1.351 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.866 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.002 repeat + -0.001 switch + -0.007 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.0 wm_rt^2 + 0.001 wm_rt*repeat + 0.0 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.003 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 245/1000 --- L(Train): 0.0406372 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.629 1 + 0.491 value_reward_diff[t] + 1.387 reward_diff + -0.482 value_reward_diff^2 + 0.756 value_reward_diff*reward_diff + 1.354 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.866 value_persistance[t] + 0.002 repeat + -0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + 0.0 switch + -0.007 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.0 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 246/1000 --- L(Train): 0.0404596 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.627 1 + 0.491 value_reward_diff[t] + 1.39 reward_diff + -0.482 value_reward_diff^2 + 0.754 value_reward_diff*reward_diff + 1.357 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.866 value_persistance[t] + 0.002 repeat + -0.0 value_persistance^2 + 0.0 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + -0.0 repeat + 0.0 switch + -0.006 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.0 wm_rt^2 + 0.001 wm_rt*repeat + -0.0 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 247/1000 --- L(Train): 0.0402869 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.625 1 + 0.491 value_reward_diff[t] + 1.393 reward_diff + -0.481 value_reward_diff^2 + 0.752 value_reward_diff*reward_diff + 1.36 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.867 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + 0.0 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.997 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.005 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.0 wm_rt^2 + -0.0 wm_rt*repeat + 0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 248/1000 --- L(Train): 0.0401172 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.623 1 + 0.492 value_reward_diff[t] + 1.397 reward_diff + -0.481 value_reward_diff^2 + 0.75 value_reward_diff*reward_diff + 1.363 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.867 value_persistance[t] + -0.0 repeat + 0.004 value_persistance^2 + -0.001 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.998 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.003 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.001 wm_rt^2 + -0.0 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.002 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.005 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 249/1000 --- L(Train): 0.0399494 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.621 1 + 0.492 value_reward_diff[t] + 1.4 reward_diff + -0.481 value_reward_diff^2 + 0.748 value_reward_diff*reward_diff + 1.366 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.867 value_persistance[t] + -0.0 repeat + 0.004 value_persistance^2 + -0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.002 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.0 wm_rt^2 + 0.0 wm_rt*repeat + -0.0 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.005 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 250/1000 --- L(Train): 0.0397810 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.619 1 + 0.493 value_reward_diff[t] + 1.403 reward_diff + -0.48 value_reward_diff^2 + 0.746 value_reward_diff*reward_diff + 1.369 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.867 value_persistance[t] + 0.001 repeat + 0.004 value_persistance^2 + -0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.001 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.001 wm_rt^2 + -0.0 wm_rt*repeat + -0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.004 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 0, 0, 0, 0\n",
      "wm_rt: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 251/1000 --- L(Train): 0.0396192 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.617 1 + 0.493 value_reward_diff[t] + 1.407 reward_diff + -0.48 value_reward_diff^2 + 0.744 value_reward_diff*reward_diff + 1.372 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.868 value_persistance[t] + 0.0 repeat + 0.004 value_persistance^2 + 0.001 value_persistance*repeat + 0.003 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.002 wm_rt[t] + 0.001 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.006 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.001 wm_rt^2 + 0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.004 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.004 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 1, 1, 1, 1\n",
      "wm_rt: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 252/1000 --- L(Train): 0.0394558 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.615 1 + 0.493 value_reward_diff[t] + 1.41 reward_diff + -0.48 value_reward_diff^2 + 0.742 value_reward_diff*reward_diff + 1.375 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.869 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + 0.001 value_persistance*repeat + 0.003 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + -0.001 repeat + -0.0 switch + -0.0 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.0 wm_rt^2 + 0.0 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.002 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 2, 2, 2, 2\n",
      "wm_rt: 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 253/1000 --- L(Train): 0.0392912 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.612 1 + 0.494 value_reward_diff[t] + 1.413 reward_diff + -0.479 value_reward_diff^2 + 0.739 value_reward_diff*reward_diff + 1.378 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.868 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + 0.0 value_persistance*repeat + 0.003 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.0 switch + -0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.0 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.003 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.0 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + -0.005 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 3, 3, 3, 3\n",
      "wm_rt: 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 254/1000 --- L(Train): 0.0391320 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.61 1 + 0.494 value_reward_diff[t] + 1.416 reward_diff + -0.479 value_reward_diff^2 + 0.737 value_reward_diff*reward_diff + 1.381 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.867 value_persistance[t] + 0.0 repeat + -0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + -0.0 repeat + 0.0 switch + -0.001 wm_rt[t-1] + -0.003 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + -0.001 wm_rt*repeat + -0.0 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + -0.005 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.003 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 4, 4, 4, 4\n",
      "wm_rt: 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 255/1000 --- L(Train): 0.0389730 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.608 1 + 0.494 value_reward_diff[t] + 1.42 reward_diff + -0.478 value_reward_diff^2 + 0.735 value_reward_diff*reward_diff + 1.384 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.865 value_persistance[t] + -0.0 repeat + -0.001 value_persistance^2 + -0.003 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + -0.0 wm_rt*repeat + 0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.002 switch^2 + 0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.003 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 5, 5, 5, 5\n",
      "wm_rt: 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 256/1000 --- L(Train): 0.0388169 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.606 1 + 0.495 value_reward_diff[t] + 1.423 reward_diff + -0.478 value_reward_diff^2 + 0.733 value_reward_diff*reward_diff + 1.387 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.864 value_persistance[t] + 0.0 repeat + 0.0 value_persistance^2 + -0.003 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.0 switch + 0.0 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.0 wm_rt^2 + 0.002 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.003 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 6, 6, 6, 6\n",
      "wm_rt: 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 257/1000 --- L(Train): 0.0386626 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.604 1 + 0.495 value_reward_diff[t] + 1.426 reward_diff + -0.478 value_reward_diff^2 + 0.731 value_reward_diff*reward_diff + 1.39 reward_diff^2 \n",
      "value_persistance[t+1] = -0.096 1 + 0.864 value_persistance[t] + -0.0 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.0 repeat + 0.001 switch + -0.001 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.0 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.001 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.001 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 7, 7, 7, 7\n",
      "wm_rt: 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 258/1000 --- L(Train): 0.0385108 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.602 1 + 0.495 value_reward_diff[t] + 1.43 reward_diff + -0.477 value_reward_diff^2 + 0.729 value_reward_diff*reward_diff + 1.393 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.865 value_persistance[t] + -0.0 repeat + -0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + -0.0 repeat + 0.001 switch + -0.001 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.003 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.003 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.0 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 8, 8, 8, 8\n",
      "wm_rt: 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 259/1000 --- L(Train): 0.0383560 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.6 1 + 0.496 value_reward_diff[t] + 1.433 reward_diff + -0.477 value_reward_diff^2 + 0.726 value_reward_diff*reward_diff + 1.396 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.866 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.003 wm_rt^2 + 0.0 wm_rt*repeat + 0.0 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 9, 9, 9, 9\n",
      "wm_rt: 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 260/1000 --- L(Train): 0.0382045 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.598 1 + 0.496 value_reward_diff[t] + 1.436 reward_diff + -0.477 value_reward_diff^2 + 0.724 value_reward_diff*reward_diff + 1.399 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.866 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.001 switch + 0.0 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.002 wm_rt*repeat + 0.0 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.003 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.004 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.004 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 10, 10, 10, 10\n",
      "wm_rt: 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 261/1000 --- L(Train): 0.0380572 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.596 1 + 0.496 value_reward_diff[t] + 1.439 reward_diff + -0.476 value_reward_diff^2 + 0.722 value_reward_diff*reward_diff + 1.402 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.866 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + -0.003 wm_rt*repeat + -0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.005 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 11, 11, 11, 11\n",
      "wm_rt: 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 262/1000 --- L(Train): 0.0379090 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.594 1 + 0.497 value_reward_diff[t] + 1.442 reward_diff + -0.476 value_reward_diff^2 + 0.72 value_reward_diff*reward_diff + 1.405 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.866 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 0.999 wm_rt[t] + -0.002 repeat + -0.0 switch + -0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + 0.001 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.003 wm_rt^2 + -0.003 wm_rt*repeat + -0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + 0.002 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.005 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 12, 12, 12, 12\n",
      "wm_rt: 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 263/1000 --- L(Train): 0.0377634 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.592 1 + 0.497 value_reward_diff[t] + 1.446 reward_diff + -0.476 value_reward_diff^2 + 0.718 value_reward_diff*reward_diff + 1.408 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.865 value_persistance[t] + -0.0 repeat + -0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 1.0 wm_rt[t] + -0.002 repeat + 0.002 switch + -0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.004 wm_rt^2 + -0.002 wm_rt*repeat + -0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.004 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.005 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 13, 13, 13, 13\n",
      "wm_rt: 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 264/1000 --- L(Train): 0.0376209 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.589 1 + 0.497 value_reward_diff[t] + 1.449 reward_diff + -0.475 value_reward_diff^2 + 0.715 value_reward_diff*reward_diff + 1.411 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.865 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + 0.0 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.001 wm_rt[t] + -0.001 repeat + 0.003 switch + -0.0 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.004 wm_rt^2 + 0.0 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.004 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 14, 14, 14, 14\n",
      "wm_rt: 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 265/1000 --- L(Train): 0.0374760 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.587 1 + 0.498 value_reward_diff[t] + 1.452 reward_diff + -0.475 value_reward_diff^2 + 0.713 value_reward_diff*reward_diff + 1.414 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.864 value_persistance[t] + -0.0 repeat + 0.003 value_persistance^2 + 0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.001 wm_rt[t] + 0.001 repeat + 0.003 switch + 0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.004 wm_rt^2 + 0.001 wm_rt*repeat + 0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.004 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.004 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 15, 15, 15, 15\n",
      "wm_rt: 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 266/1000 --- L(Train): 0.0373324 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.585 1 + 0.498 value_reward_diff[t] + 1.455 reward_diff + -0.474 value_reward_diff^2 + 0.711 value_reward_diff*reward_diff + 1.417 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.865 value_persistance[t] + 0.0 repeat + 0.003 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + 0.001 switch + 0.001 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + 0.0 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.004 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.003 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 16, 16, 16, 16\n",
      "wm_rt: 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 267/1000 --- L(Train): 0.0371981 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.583 1 + 0.499 value_reward_diff[t] + 1.459 reward_diff + -0.474 value_reward_diff^2 + 0.709 value_reward_diff*reward_diff + 1.42 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.866 value_persistance[t] + -0.0 repeat + 0.002 value_persistance^2 + -0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.002 switch + -0.001 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.0 wm_rt^2 + -0.001 wm_rt*repeat + 0.0 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 17, 17, 17, 17\n",
      "wm_rt: 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 268/1000 --- L(Train): 0.0370574 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.581 1 + 0.499 value_reward_diff[t] + 1.462 reward_diff + -0.474 value_reward_diff^2 + 0.707 value_reward_diff*reward_diff + 1.423 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.867 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + 0.0 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + -0.001 repeat + -0.004 switch + -0.003 wm_rt[t-1] + 0.003 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 18, 18, 18, 18\n",
      "wm_rt: 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 269/1000 --- L(Train): 0.0369198 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.579 1 + 0.499 value_reward_diff[t] + 1.465 reward_diff + -0.473 value_reward_diff^2 + 0.704 value_reward_diff*reward_diff + 1.426 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.866 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + -0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + -0.001 repeat + -0.004 switch + -0.003 wm_rt[t-1] + 0.002 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.0 wm_rt*repeat + -0.003 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 19, 19, 19, 19\n",
      "wm_rt: 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 270/1000 --- L(Train): 0.0367817 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.577 1 + 0.5 value_reward_diff[t] + 1.468 reward_diff + -0.473 value_reward_diff^2 + 0.702 value_reward_diff*reward_diff + 1.429 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.865 value_persistance[t] + 0.001 repeat + -0.0 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.003 switch + -0.004 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.001 wm_rt^2 + 0.001 wm_rt*repeat + -0.003 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 20, 20, 20, 20\n",
      "wm_rt: 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 271/1000 --- L(Train): 0.0366502 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.575 1 + 0.5 value_reward_diff[t] + 1.471 reward_diff + -0.473 value_reward_diff^2 + 0.7 value_reward_diff*reward_diff + 1.432 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.864 value_persistance[t] + 0.0 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + 0.0 repeat + -0.002 switch + -0.003 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.004 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.003 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.003 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.004 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 21, 21, 21, 21\n",
      "wm_rt: 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 272/1000 --- L(Train): 0.0365151 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.573 1 + 0.5 value_reward_diff[t] + 1.475 reward_diff + -0.472 value_reward_diff^2 + 0.698 value_reward_diff*reward_diff + 1.435 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.864 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.0 repeat + -0.0 switch + -0.002 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.004 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.003 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.004 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 22, 22, 22, 22\n",
      "wm_rt: 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 273/1000 --- L(Train): 0.0363846 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.571 1 + 0.501 value_reward_diff[t] + 1.478 reward_diff + -0.472 value_reward_diff^2 + 0.695 value_reward_diff*reward_diff + 1.437 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.864 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + 0.0 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + -0.0 repeat + 0.002 switch + -0.001 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + 0.0 wm_rt*repeat + 0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.004 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 23, 23, 23, 23\n",
      "wm_rt: 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 274/1000 --- L(Train): 0.0362523 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.569 1 + 0.501 value_reward_diff[t] + 1.481 reward_diff + -0.471 value_reward_diff^2 + 0.693 value_reward_diff*reward_diff + 1.44 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.865 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + -0.002 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.003 switch + 0.001 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.0 wm_rt^2 + -0.002 wm_rt*repeat + 0.003 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 24, 24, 24, 24\n",
      "wm_rt: 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 275/1000 --- L(Train): 0.0361202 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.567 1 + 0.501 value_reward_diff[t] + 1.484 reward_diff + -0.471 value_reward_diff^2 + 0.691 value_reward_diff*reward_diff + 1.443 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.865 value_persistance[t] + 0.002 repeat + -0.001 value_persistance^2 + -0.003 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 1.0 wm_rt[t] + -0.0 repeat + 0.002 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.002 wm_rt^2 + -0.003 wm_rt*repeat + 0.003 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 25, 25, 25, 25\n",
      "wm_rt: 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 276/1000 --- L(Train): 0.0359924 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.565 1 + 0.502 value_reward_diff[t] + 1.487 reward_diff + -0.471 value_reward_diff^2 + 0.689 value_reward_diff*reward_diff + 1.446 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.865 value_persistance[t] + 0.003 repeat + -0.0 value_persistance^2 + -0.003 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.004 1 + 0.999 wm_rt[t] + 0.001 repeat + 0.001 switch + -0.0 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.003 wm_rt^2 + -0.003 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.003 repeat^2 + 0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.003 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 26, 26, 26, 26\n",
      "wm_rt: 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 277/1000 --- L(Train): 0.0358632 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.563 1 + 0.502 value_reward_diff[t] + 1.491 reward_diff + -0.47 value_reward_diff^2 + 0.687 value_reward_diff*reward_diff + 1.449 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.866 value_persistance[t] + 0.002 repeat + 0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.004 1 + 0.998 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.001 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.003 wm_rt^2 + -0.002 wm_rt*repeat + 0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.003 repeat^2 + -0.0 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.004 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 27, 27, 27, 27\n",
      "wm_rt: 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 278/1000 --- L(Train): 0.0357364 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.561 1 + 0.502 value_reward_diff[t] + 1.494 reward_diff + -0.47 value_reward_diff^2 + 0.684 value_reward_diff*reward_diff + 1.452 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.867 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + 0.0 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.004 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.002 switch + -0.002 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.002 wm_rt^2 + -0.0 wm_rt*repeat + -0.002 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.005 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 28, 28, 28, 28\n",
      "wm_rt: 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 279/1000 --- L(Train): 0.0356118 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.559 1 + 0.503 value_reward_diff[t] + 1.497 reward_diff + -0.47 value_reward_diff^2 + 0.682 value_reward_diff*reward_diff + 1.455 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.867 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + 0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.002 switch + -0.002 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.0 wm_rt^2 + 0.003 wm_rt*repeat + -0.004 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 29, 29, 29, 29\n",
      "wm_rt: 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 280/1000 --- L(Train): 0.0354866 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.556 1 + 0.503 value_reward_diff[t] + 1.5 reward_diff + -0.469 value_reward_diff^2 + 0.68 value_reward_diff*reward_diff + 1.458 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.865 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + 0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.0 switch + -0.001 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.001 wm_rt^2 + 0.004 wm_rt*repeat + -0.004 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.005 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 30, 30, 30, 30\n",
      "wm_rt: 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 281/1000 --- L(Train): 0.0353654 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.554 1 + 0.504 value_reward_diff[t] + 1.503 reward_diff + -0.469 value_reward_diff^2 + 0.678 value_reward_diff*reward_diff + 1.461 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.863 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.003 1 + 0.999 wm_rt[t] + 0.001 repeat + 0.001 switch + 0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + 0.004 wm_rt*repeat + -0.003 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.005 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 31, 31, 31, 31\n",
      "wm_rt: 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 282/1000 --- L(Train): 0.0352428 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.552 1 + 0.504 value_reward_diff[t] + 1.506 reward_diff + -0.469 value_reward_diff^2 + 0.675 value_reward_diff*reward_diff + 1.464 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.863 value_persistance[t] + 0.0 repeat + -0.001 value_persistance^2 + -0.003 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.003 1 + 0.999 wm_rt[t] + -0.0 repeat + -0.001 switch + 0.0 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + 0.003 wm_rt*repeat + -0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + -0.001 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.005 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 32, 32, 32, 32\n",
      "wm_rt: 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 283/1000 --- L(Train): 0.0351203 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.55 1 + 0.504 value_reward_diff[t] + 1.51 reward_diff + -0.468 value_reward_diff^2 + 0.673 value_reward_diff*reward_diff + 1.466 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.864 value_persistance[t] + -0.0 repeat + -0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + -0.0 repeat + -0.002 switch + -0.002 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + 0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.0 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.005 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 33, 33, 33, 33\n",
      "wm_rt: 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 284/1000 --- L(Train): 0.0350020 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.548 1 + 0.505 value_reward_diff[t] + 1.513 reward_diff + -0.468 value_reward_diff^2 + 0.671 value_reward_diff*reward_diff + 1.469 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.865 value_persistance[t] + -0.0 repeat + -0.0 value_persistance^2 + -0.0 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.001 repeat + -0.003 switch + -0.004 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.0 wm_rt^2 + -0.001 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.002 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.003 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 34, 34, 34, 34\n",
      "wm_rt: 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 285/1000 --- L(Train): 0.0348852 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.546 1 + 0.505 value_reward_diff[t] + 1.516 reward_diff + -0.467 value_reward_diff^2 + 0.669 value_reward_diff*reward_diff + 1.472 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.865 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + 0.002 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.002 switch + -0.004 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.0 wm_rt^2 + -0.002 wm_rt*repeat + 0.002 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.002 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 35, 35, 35, 35\n",
      "wm_rt: 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 286/1000 --- L(Train): 0.0347656 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.544 1 + 0.505 value_reward_diff[t] + 1.519 reward_diff + -0.467 value_reward_diff^2 + 0.667 value_reward_diff*reward_diff + 1.475 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.865 value_persistance[t] + 0.003 repeat + 0.004 value_persistance^2 + 0.002 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.999 wm_rt[t] + -0.0 repeat + 0.0 switch + -0.005 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.002 wm_rt*repeat + 0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.003 repeat^2 + 0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 36, 36, 36, 36\n",
      "wm_rt: 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 287/1000 --- L(Train): 0.0346516 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.542 1 + 0.506 value_reward_diff[t] + 1.522 reward_diff + -0.467 value_reward_diff^2 + 0.664 value_reward_diff*reward_diff + 1.478 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.865 value_persistance[t] + 0.003 repeat + 0.005 value_persistance^2 + 0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.004 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.003 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.002 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.003 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 37, 37, 37, 37\n",
      "wm_rt: 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 288/1000 --- L(Train): 0.0345372 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.54 1 + 0.506 value_reward_diff[t] + 1.525 reward_diff + -0.466 value_reward_diff^2 + 0.662 value_reward_diff*reward_diff + 1.481 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.866 value_persistance[t] + 0.002 repeat + 0.004 value_persistance^2 + 0.0 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.003 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.0 wm_rt^2 + 0.001 wm_rt*repeat + -0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.002 switch^2 + 0.0 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.003 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 38, 38, 38, 38\n",
      "wm_rt: 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 289/1000 --- L(Train): 0.0344186 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.538 1 + 0.506 value_reward_diff[t] + 1.528 reward_diff + -0.466 value_reward_diff^2 + 0.66 value_reward_diff*reward_diff + 1.484 reward_diff^2 \n",
      "value_persistance[t+1] = -0.101 1 + 0.867 value_persistance[t] + 0.001 repeat + 0.004 value_persistance^2 + -0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.002 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.002 wm_rt^2 + 0.001 wm_rt*repeat + -0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.003 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.004 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 39, 39, 39, 39\n",
      "wm_rt: 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 290/1000 --- L(Train): 0.0343053 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.536 1 + 0.507 value_reward_diff[t] + 1.531 reward_diff + -0.466 value_reward_diff^2 + 0.658 value_reward_diff*reward_diff + 1.486 reward_diff^2 \n",
      "value_persistance[t+1] = -0.102 1 + 0.867 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + -0.003 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.001 switch + 0.0 wm_rt[t-1] + 0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.002 wm_rt^2 + 0.001 wm_rt*repeat + -0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.003 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.003 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 40, 40, 40, 40\n",
      "wm_rt: 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 291/1000 --- L(Train): 0.0341970 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.534 1 + 0.507 value_reward_diff[t] + 1.535 reward_diff + -0.465 value_reward_diff^2 + 0.655 value_reward_diff*reward_diff + 1.489 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.866 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + -0.003 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.001 switch + 0.0 wm_rt[t-1] + -0.0 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.002 wm_rt^2 + -0.0 wm_rt*repeat + 0.002 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + -0.002 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.004 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 41, 41, 41, 41\n",
      "wm_rt: 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 292/1000 --- L(Train): 0.0340849 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.532 1 + 0.507 value_reward_diff[t] + 1.538 reward_diff + -0.465 value_reward_diff^2 + 0.653 value_reward_diff*reward_diff + 1.492 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.864 value_persistance[t] + -0.0 repeat + -0.001 value_persistance^2 + -0.003 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.002 1 + 1.001 wm_rt[t] + 0.0 repeat + 0.0 switch + -0.002 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.004 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + 0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.003 switch^2 + -0.003 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 42, 42, 42, 42\n",
      "wm_rt: 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 293/1000 --- L(Train): 0.0339745 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.53 1 + 0.508 value_reward_diff[t] + 1.541 reward_diff + -0.465 value_reward_diff^2 + 0.651 value_reward_diff*reward_diff + 1.495 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.862 value_persistance[t] + 0.002 repeat + -0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.004 1 + 1.001 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.003 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.003 wm_rt[t-4] + -0.001 wm_rt^2 + -0.0 wm_rt*repeat + 0.002 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.003 switch^2 + -0.002 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 43, 43, 43, 43\n",
      "wm_rt: 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 294/1000 --- L(Train): 0.0338632 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.528 1 + 0.508 value_reward_diff[t] + 1.544 reward_diff + -0.464 value_reward_diff^2 + 0.649 value_reward_diff*reward_diff + 1.498 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.862 value_persistance[t] + 0.002 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.004 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.0 switch + -0.003 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + 0.002 wm_rt*repeat + 0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.003 switch^2 + -0.0 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 44, 44, 44, 44\n",
      "wm_rt: 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 295/1000 --- L(Train): 0.0337561 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.526 1 + 0.508 value_reward_diff[t] + 1.547 reward_diff + -0.464 value_reward_diff^2 + 0.647 value_reward_diff*reward_diff + 1.501 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.864 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + 0.002 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.003 1 + 0.999 wm_rt[t] + -0.002 repeat + -0.001 switch + -0.003 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.002 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 45, 45, 45, 45\n",
      "wm_rt: 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 296/1000 --- L(Train): 0.0336477 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.524 1 + 0.509 value_reward_diff[t] + 1.55 reward_diff + -0.463 value_reward_diff^2 + 0.644 value_reward_diff*reward_diff + 1.503 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.865 value_persistance[t] + -0.0 repeat + -0.001 value_persistance^2 + 0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.002 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + 0.003 repeat^2 + -0.001 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.003 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 46, 46, 46, 46\n",
      "wm_rt: 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 297/1000 --- L(Train): 0.0335455 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.522 1 + 0.509 value_reward_diff[t] + 1.553 reward_diff + -0.463 value_reward_diff^2 + 0.642 value_reward_diff*reward_diff + 1.506 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.865 value_persistance[t] + 0.0 repeat + 0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + 0.0 wm_rt*repeat + -0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.0 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 47, 47, 47, 47\n",
      "wm_rt: 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 298/1000 --- L(Train): 0.0334387 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.52 1 + 0.509 value_reward_diff[t] + 1.556 reward_diff + -0.463 value_reward_diff^2 + 0.64 value_reward_diff*reward_diff + 1.509 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.865 value_persistance[t] + 0.0 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.005 1 + 1.001 wm_rt[t] + 0.001 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.002 wm_rt^2 + -0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 48, 48, 48, 48\n",
      "wm_rt: 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 299/1000 --- L(Train): 0.0333329 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.518 1 + 0.51 value_reward_diff[t] + 1.559 reward_diff + -0.462 value_reward_diff^2 + 0.638 value_reward_diff*reward_diff + 1.512 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.864 value_persistance[t] + -0.001 repeat + 0.0 value_persistance^2 + -0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.005 1 + 1.001 wm_rt[t] + 0.002 repeat + 0.001 switch + 0.001 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.003 wm_rt[t-4] + 0.001 wm_rt^2 + -0.003 wm_rt*repeat + 0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 49, 49, 49, 49\n",
      "wm_rt: 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 300/1000 --- L(Train): 0.0332299 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.516 1 + 0.51 value_reward_diff[t] + 1.562 reward_diff + -0.462 value_reward_diff^2 + 0.636 value_reward_diff*reward_diff + 1.515 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.864 value_persistance[t] + -0.0 repeat + -0.001 value_persistance^2 + -0.002 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.004 1 + 1.0 wm_rt[t] + 0.002 repeat + 0.001 switch + -0.001 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.0 wm_rt^2 + -0.003 wm_rt*repeat + 0.002 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.004 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.002 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 50, 50, 50, 50\n",
      "wm_rt: 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 301/1000 --- L(Train): 0.0331247 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.514 1 + 0.51 value_reward_diff[t] + 1.566 reward_diff + -0.462 value_reward_diff^2 + 0.633 value_reward_diff*reward_diff + 1.517 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.863 value_persistance[t] + 0.001 repeat + -0.0 value_persistance^2 + -0.002 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + 0.002 repeat + 0.001 switch + -0.002 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.002 wm_rt*repeat + 0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.005 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 51, 51, 51, 51\n",
      "wm_rt: 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 302/1000 --- L(Train): 0.0330216 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.512 1 + 0.511 value_reward_diff[t] + 1.569 reward_diff + -0.461 value_reward_diff^2 + 0.631 value_reward_diff*reward_diff + 1.52 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.863 value_persistance[t] + 0.002 repeat + 0.002 value_persistance^2 + -0.0 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.997 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.003 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.003 wm_rt^2 + -0.0 wm_rt*repeat + -0.0 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.002 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.004 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.005 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 52, 52, 52, 52\n",
      "wm_rt: 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 303/1000 --- L(Train): 0.0329222 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.51 1 + 0.511 value_reward_diff[t] + 1.572 reward_diff + -0.461 value_reward_diff^2 + 0.629 value_reward_diff*reward_diff + 1.523 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.864 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + 0.002 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.998 wm_rt[t] + -0.001 repeat + -0.002 switch + -0.003 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.003 wm_rt^2 + 0.003 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.004 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 53, 53, 53, 53\n",
      "wm_rt: 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 304/1000 --- L(Train): 0.0328240 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.508 1 + 0.511 value_reward_diff[t] + 1.575 reward_diff + -0.461 value_reward_diff^2 + 0.627 value_reward_diff*reward_diff + 1.526 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.865 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + 0.003 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.999 wm_rt[t] + -0.003 repeat + -0.002 switch + -0.002 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.001 wm_rt^2 + 0.004 wm_rt*repeat + 0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.004 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.003 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 54, 54, 54, 54\n",
      "wm_rt: 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 305/1000 --- L(Train): 0.0327259 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.506 1 + 0.512 value_reward_diff[t] + 1.578 reward_diff + -0.46 value_reward_diff^2 + 0.625 value_reward_diff*reward_diff + 1.528 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.865 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + 0.003 value_persistance*repeat + 0.003 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.001 wm_rt[t] + -0.003 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + 0.004 wm_rt*repeat + -0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.004 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.003 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 55, 55, 55, 55\n",
      "wm_rt: 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 306/1000 --- L(Train): 0.0326266 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.504 1 + 0.512 value_reward_diff[t] + 1.581 reward_diff + -0.46 value_reward_diff^2 + 0.622 value_reward_diff*reward_diff + 1.531 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.865 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + 0.002 value_persistance*repeat + 0.004 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.001 wm_rt[t] + -0.001 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.002 wm_rt^2 + 0.004 wm_rt*repeat + 0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.002 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 56, 56, 56, 56\n",
      "wm_rt: 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 307/1000 --- L(Train): 0.0325287 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.502 1 + 0.512 value_reward_diff[t] + 1.584 reward_diff + -0.459 value_reward_diff^2 + 0.62 value_reward_diff*reward_diff + 1.534 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.864 value_persistance[t] + 0.0 repeat + -0.001 value_persistance^2 + -0.001 value_persistance*repeat + 0.004 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.001 wm_rt[t] + 0.001 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.0 wm_rt*switch + -0.0 wm_rt*wm_rt[t-1] + -0.003 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.003 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.003 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 57, 57, 57, 57\n",
      "wm_rt: 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 308/1000 --- L(Train): 0.0324323 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.5 1 + 0.513 value_reward_diff[t] + 1.587 reward_diff + -0.459 value_reward_diff^2 + 0.618 value_reward_diff*reward_diff + 1.537 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.864 value_persistance[t] + 0.0 repeat + -0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + 0.002 repeat + -0.001 switch + -0.001 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.0 wm_rt^2 + -0.001 wm_rt*repeat + 0.0 wm_rt*switch + -0.0 wm_rt*wm_rt[t-1] + -0.004 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.003 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.003 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + -0.005 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 58, 58, 58, 58\n",
      "wm_rt: 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 309/1000 --- L(Train): 0.0323405 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.498 1 + 0.513 value_reward_diff[t] + 1.59 reward_diff + -0.459 value_reward_diff^2 + 0.616 value_reward_diff*reward_diff + 1.539 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.864 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.999 wm_rt[t] + 0.002 repeat + -0.001 switch + -0.002 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.002 wm_rt*repeat + -0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.005 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.004 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 59, 59, 59, 59\n",
      "wm_rt: 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 310/1000 --- L(Train): 0.0322470 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.496 1 + 0.513 value_reward_diff[t] + 1.593 reward_diff + -0.458 value_reward_diff^2 + 0.614 value_reward_diff*reward_diff + 1.542 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.864 value_persistance[t] + -0.0 repeat + 0.001 value_persistance^2 + -0.001 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 1.0 wm_rt[t] + 0.0 repeat + -0.0 switch + -0.003 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.0 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.003 wm_rt^2 + -0.002 wm_rt*repeat + 0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.004 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.002 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.005 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 60, 60, 60, 60\n",
      "wm_rt: 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 311/1000 --- L(Train): 0.0321471 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.494 1 + 0.514 value_reward_diff[t] + 1.596 reward_diff + -0.458 value_reward_diff^2 + 0.612 value_reward_diff*reward_diff + 1.545 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.864 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + -0.004 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 1.001 wm_rt[t] + -0.002 repeat + 0.001 switch + -0.003 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.003 wm_rt^2 + -0.001 wm_rt*repeat + -0.0 wm_rt*switch + -0.0 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.003 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.005 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 61, 61, 61, 61\n",
      "wm_rt: 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 312/1000 --- L(Train): 0.0320530 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.492 1 + 0.514 value_reward_diff[t] + 1.599 reward_diff + -0.458 value_reward_diff^2 + 0.609 value_reward_diff*reward_diff + 1.548 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.863 value_persistance[t] + 0.002 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + -0.003 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.001 wm_rt[t] + -0.002 repeat + 0.002 switch + -0.002 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.001 wm_rt^2 + 0.001 wm_rt*repeat + 0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.003 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.004 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.004 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 62, 62, 62, 62\n",
      "wm_rt: 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 313/1000 --- L(Train): 0.0319591 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.491 1 + 0.514 value_reward_diff[t] + 1.602 reward_diff + -0.457 value_reward_diff^2 + 0.607 value_reward_diff*reward_diff + 1.55 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.862 value_persistance[t] + 0.003 repeat + -0.0 value_persistance^2 + -0.0 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + 0.001 wm_rt*repeat + -0.0 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.004 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 63, 63, 63, 63\n",
      "wm_rt: 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 314/1000 --- L(Train): 0.0318708 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.489 1 + 0.515 value_reward_diff[t] + 1.605 reward_diff + -0.457 value_reward_diff^2 + 0.605 value_reward_diff*reward_diff + 1.553 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.863 value_persistance[t] + 0.003 repeat + -0.0 value_persistance^2 + -0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.0 switch + 0.001 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + 0.001 wm_rt*repeat + 0.0 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.001 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.003 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 64, 64, 64, 64\n",
      "wm_rt: 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 315/1000 --- L(Train): 0.0317783 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.487 1 + 0.515 value_reward_diff[t] + 1.608 reward_diff + -0.457 value_reward_diff^2 + 0.603 value_reward_diff*reward_diff + 1.556 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.864 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.998 wm_rt[t] + 0.002 repeat + -0.001 switch + 0.0 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + -0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.002 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 65, 65, 65, 65\n",
      "wm_rt: 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 316/1000 --- L(Train): 0.0316887 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.485 1 + 0.515 value_reward_diff[t] + 1.611 reward_diff + -0.456 value_reward_diff^2 + 0.601 value_reward_diff*reward_diff + 1.558 reward_diff^2 \n",
      "value_persistance[t+1] = -0.101 1 + 0.865 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.004 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.002 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.0 wm_rt^2 + -0.001 wm_rt*repeat + -0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.004 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + 0.003 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 66, 66, 66, 66\n",
      "wm_rt: 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 317/1000 --- L(Train): 0.0316006 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.483 1 + 0.516 value_reward_diff[t] + 1.614 reward_diff + -0.456 value_reward_diff^2 + 0.599 value_reward_diff*reward_diff + 1.561 reward_diff^2 \n",
      "value_persistance[t+1] = -0.101 1 + 0.866 value_persistance[t] + -0.002 repeat + 0.001 value_persistance^2 + 0.0 value_persistance*repeat + 0.004 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.0 switch + -0.003 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.0 wm_rt*repeat + 0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + -0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.004 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.003 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 67, 67, 67, 67\n",
      "wm_rt: 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 318/1000 --- L(Train): 0.0315102 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.481 1 + 0.516 value_reward_diff[t] + 1.617 reward_diff + -0.456 value_reward_diff^2 + 0.596 value_reward_diff*reward_diff + 1.564 reward_diff^2 \n",
      "value_persistance[t+1] = -0.101 1 + 0.866 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + -0.002 value_persistance*repeat + 0.004 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.002 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.003 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.003 wm_rt^2 + 0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.002 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.003 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 68, 68, 68, 68\n",
      "wm_rt: 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 319/1000 --- L(Train): 0.0314227 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.479 1 + 0.516 value_reward_diff[t] + 1.62 reward_diff + -0.455 value_reward_diff^2 + 0.594 value_reward_diff*reward_diff + 1.566 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.865 value_persistance[t] + 0.0 repeat + -0.0 value_persistance^2 + -0.003 value_persistance*repeat + 0.003 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.002 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.003 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.002 wm_rt^2 + 0.002 wm_rt*repeat + 0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.0 repeat*switch + 0.0 repeat*wm_rt[t-1] + 0.002 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.005 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 69, 69, 69, 69\n",
      "wm_rt: 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 320/1000 --- L(Train): 0.0313371 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.477 1 + 0.517 value_reward_diff[t] + 1.623 reward_diff + -0.455 value_reward_diff^2 + 0.592 value_reward_diff*reward_diff + 1.569 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.864 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + -0.003 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.002 wm_rt[t] + 0.001 repeat + 0.0 switch + -0.003 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.002 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.001 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + 0.002 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.004 wm_rt[t-1]*wm_rt[t-4] + 0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 70, 70, 70, 70\n",
      "wm_rt: 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 321/1000 --- L(Train): 0.0312557 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.475 1 + 0.517 value_reward_diff[t] + 1.626 reward_diff + -0.454 value_reward_diff^2 + 0.59 value_reward_diff*reward_diff + 1.572 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.864 value_persistance[t] + -0.0 repeat + 0.0 value_persistance^2 + -0.002 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.002 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.002 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.001 wm_rt^2 + -0.0 wm_rt*repeat + -0.002 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.002 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.003 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 71, 71, 71, 71\n",
      "wm_rt: 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 322/1000 --- L(Train): 0.0311663 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.473 1 + 0.517 value_reward_diff[t] + 1.629 reward_diff + -0.454 value_reward_diff^2 + 0.588 value_reward_diff*reward_diff + 1.574 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.863 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + 0.0 value_persistance*repeat + -0.003 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.999 wm_rt[t] + -0.0 repeat + -0.002 switch + -0.0 wm_rt[t-1] + 0.001 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + -0.0 wm_rt*repeat + -0.002 wm_rt*switch + -0.0 wm_rt*wm_rt[t-1] + 0.003 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.002 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.002 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.003 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 72, 72, 72, 72\n",
      "wm_rt: 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 323/1000 --- L(Train): 0.0310809 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.471 1 + 0.518 value_reward_diff[t] + 1.632 reward_diff + -0.454 value_reward_diff^2 + 0.586 value_reward_diff*reward_diff + 1.577 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.862 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + 0.0 value_persistance*repeat + -0.003 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.998 wm_rt[t] + 0.0 repeat + -0.002 switch + 0.002 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.002 wm_rt^2 + 0.0 wm_rt*repeat + -0.0 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.002 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 73, 73, 73, 73\n",
      "wm_rt: 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 324/1000 --- L(Train): 0.0309966 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.469 1 + 0.518 value_reward_diff[t] + 1.635 reward_diff + -0.453 value_reward_diff^2 + 0.584 value_reward_diff*reward_diff + 1.58 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.861 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.002 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 0.998 wm_rt[t] + -0.0 repeat + -0.001 switch + 0.002 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.001 wm_rt^2 + -0.0 wm_rt*repeat + 0.002 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.003 repeat^2 + -0.002 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + 0.004 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 74, 74, 74, 74\n",
      "wm_rt: 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 325/1000 --- L(Train): 0.0309133 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.467 1 + 0.518 value_reward_diff[t] + 1.638 reward_diff + -0.453 value_reward_diff^2 + 0.581 value_reward_diff*reward_diff + 1.582 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.862 value_persistance[t] + -0.0 repeat + -0.001 value_persistance^2 + -0.0 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.998 wm_rt[t] + 0.001 repeat + -0.0 switch + 0.0 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.006 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.001 wm_rt^2 + 0.0 wm_rt*repeat + 0.003 wm_rt*switch + -0.0 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.003 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + 0.003 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 75, 75, 75, 75\n",
      "wm_rt: 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 326/1000 --- L(Train): 0.0308281 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.465 1 + 0.519 value_reward_diff[t] + 1.641 reward_diff + -0.453 value_reward_diff^2 + 0.579 value_reward_diff*reward_diff + 1.585 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + -0.0 repeat + -0.0 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + 0.002 switch + -0.003 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.006 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.0 wm_rt*repeat + 0.003 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.003 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.003 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 76, 76, 76, 76\n",
      "wm_rt: 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 327/1000 --- L(Train): 0.0307473 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.464 1 + 0.519 value_reward_diff[t] + 1.644 reward_diff + -0.452 value_reward_diff^2 + 0.577 value_reward_diff*reward_diff + 1.588 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.864 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 + 0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + -0.0 repeat + 0.002 switch + -0.006 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.006 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.002 wm_rt^2 + 0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.002 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 77, 77, 77, 77\n",
      "wm_rt: 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 328/1000 --- L(Train): 0.0306681 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.462 1 + 0.519 value_reward_diff[t] + 1.647 reward_diff + -0.452 value_reward_diff^2 + 0.575 value_reward_diff*reward_diff + 1.59 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.865 value_persistance[t] + 0.001 repeat + 0.002 value_persistance^2 + -0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + -0.0 repeat + 0.001 switch + -0.007 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.005 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.001 wm_rt^2 + 0.0 wm_rt*repeat + -0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.002 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.004 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 78, 78, 78, 78\n",
      "wm_rt: 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 329/1000 --- L(Train): 0.0305865 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.46 1 + 0.52 value_reward_diff[t] + 1.65 reward_diff + -0.452 value_reward_diff^2 + 0.573 value_reward_diff*reward_diff + 1.593 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.865 value_persistance[t] + 0.001 repeat + 0.003 value_persistance^2 + -0.0 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.999 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.008 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.001 repeat*switch + -0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 79, 79, 79, 79\n",
      "wm_rt: 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 330/1000 --- L(Train): 0.0305038 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.458 1 + 0.52 value_reward_diff[t] + 1.653 reward_diff + -0.451 value_reward_diff^2 + 0.571 value_reward_diff*reward_diff + 1.596 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.865 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + 0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 0.998 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.008 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.002 wm_rt^2 + -0.002 wm_rt*repeat + -0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.001 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.002 switch^2 + -0.002 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.002 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.003 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + 0.003 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 80, 80, 80, 80\n",
      "wm_rt: 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 331/1000 --- L(Train): 0.0304242 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.456 1 + 0.52 value_reward_diff[t] + 1.655 reward_diff + -0.451 value_reward_diff^2 + 0.569 value_reward_diff*reward_diff + 1.598 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.864 value_persistance[t] + -0.001 repeat + 0.002 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.007 wm_rt[t-1] + 0.002 wm_rt[t-2] + 0.002 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.002 wm_rt^2 + -0.001 wm_rt*repeat + -0.0 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.0 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.003 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 81, 81, 81, 81\n",
      "wm_rt: 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 332/1000 --- L(Train): 0.0303464 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.454 1 + 0.521 value_reward_diff[t] + 1.658 reward_diff + -0.451 value_reward_diff^2 + 0.567 value_reward_diff*reward_diff + 1.601 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + -0.0 repeat + 0.0 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.002 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.0 switch + -0.006 wm_rt[t-1] + 0.001 wm_rt[t-2] + 0.002 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.001 wm_rt^2 + 0.001 wm_rt*repeat + 0.002 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.001 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.0 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 82, 82, 82, 82\n",
      "wm_rt: 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 333/1000 --- L(Train): 0.0302693 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.452 1 + 0.521 value_reward_diff[t] + 1.661 reward_diff + -0.45 value_reward_diff^2 + 0.565 value_reward_diff*reward_diff + 1.603 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.863 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + 0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + 0.001 switch + -0.005 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + 0.001 wm_rt*repeat + 0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.003 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.0 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 83, 83, 83, 83\n",
      "wm_rt: 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 334/1000 --- L(Train): 0.0301901 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.45 1 + 0.521 value_reward_diff[t] + 1.664 reward_diff + -0.45 value_reward_diff^2 + 0.563 value_reward_diff*reward_diff + 1.606 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.862 value_persistance[t] + 0.001 repeat + -0.002 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + 0.0 switch + -0.003 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.002 wm_rt^2 + 0.001 wm_rt*repeat + 0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.003 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.0 switch*wm_rt[t-2] + -0.002 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 84, 84, 84, 84\n",
      "wm_rt: 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 335/1000 --- L(Train): 0.0301094 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.448 1 + 0.522 value_reward_diff[t] + 1.667 reward_diff + -0.45 value_reward_diff^2 + 0.56 value_reward_diff*reward_diff + 1.609 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.862 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + -0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.003 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.002 wm_rt^2 + -0.001 wm_rt*repeat + 0.001 wm_rt*switch + -0.0 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + 0.002 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 85, 85, 85, 85\n",
      "wm_rt: 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 336/1000 --- L(Train): 0.0300310 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.447 1 + 0.522 value_reward_diff[t] + 1.67 reward_diff + -0.449 value_reward_diff^2 + 0.558 value_reward_diff*reward_diff + 1.611 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + -0.0 repeat + 0.002 value_persistance^2 + 0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.001 switch + 0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.004 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.002 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.0 switch^2 + -0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.002 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 86, 86, 86, 86\n",
      "wm_rt: 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 337/1000 --- L(Train): 0.0299574 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.445 1 + 0.522 value_reward_diff[t] + 1.673 reward_diff + -0.449 value_reward_diff^2 + 0.556 value_reward_diff*reward_diff + 1.614 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.864 value_persistance[t] + -0.0 repeat + 0.003 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.999 wm_rt[t] + 0.0 repeat + 0.0 switch + 0.002 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.005 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + -0.0 wm_rt*repeat + -0.002 wm_rt*switch + 0.003 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.003 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 87, 87, 87, 87\n",
      "wm_rt: 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 338/1000 --- L(Train): 0.0298792 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.443 1 + 0.522 value_reward_diff[t] + 1.676 reward_diff + -0.449 value_reward_diff^2 + 0.554 value_reward_diff*reward_diff + 1.616 reward_diff^2 \n",
      "value_persistance[t+1] = -0.101 1 + 0.865 value_persistance[t] + 0.0 repeat + 0.003 value_persistance^2 + -0.001 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.0 switch + 0.001 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.004 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.003 wm_rt^2 + 0.001 wm_rt*repeat + -0.002 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.004 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 88, 88, 88, 88\n",
      "wm_rt: 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 339/1000 --- L(Train): 0.0298086 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.441 1 + 0.523 value_reward_diff[t] + 1.679 reward_diff + -0.448 value_reward_diff^2 + 0.552 value_reward_diff*reward_diff + 1.619 reward_diff^2 \n",
      "value_persistance[t+1] = -0.102 1 + 0.866 value_persistance[t] + 0.0 repeat + 0.003 value_persistance^2 + -0.0 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.0 repeat + -0.001 switch + -0.002 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.004 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.002 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.0 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 89, 89, 89, 89\n",
      "wm_rt: 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 340/1000 --- L(Train): 0.0297365 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.439 1 + 0.523 value_reward_diff[t] + 1.682 reward_diff + -0.448 value_reward_diff^2 + 0.55 value_reward_diff*reward_diff + 1.621 reward_diff^2 \n",
      "value_persistance[t+1] = -0.101 1 + 0.866 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + 0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + 0.0 repeat + -0.002 switch + -0.004 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.001 wm_rt^2 + 0.001 wm_rt*repeat + 0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + -0.004 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + 0.002 repeat*switch + 0.003 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.001 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + 0.0 switch^2 + 0.002 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.0 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + 0.003 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 90, 90, 90, 90\n",
      "wm_rt: 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 341/1000 --- L(Train): 0.0296655 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.437 1 + 0.523 value_reward_diff[t] + 1.684 reward_diff + -0.448 value_reward_diff^2 + 0.548 value_reward_diff*reward_diff + 1.624 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.864 value_persistance[t] + 0.0 repeat + 0.002 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + -0.0 repeat + -0.002 switch + -0.005 wm_rt[t-1] + 0.002 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.0 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.002 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.003 switch^2 + 0.002 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-1]*wm_rt[t-4] + 0.0 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 91, 91, 91, 91\n",
      "wm_rt: 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 342/1000 --- L(Train): 0.0295903 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.435 1 + 0.524 value_reward_diff[t] + 1.687 reward_diff + -0.447 value_reward_diff^2 + 0.546 value_reward_diff*reward_diff + 1.627 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.862 value_persistance[t] + -0.0 repeat + 0.0 value_persistance^2 + 0.0 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.0 repeat + -0.001 switch + -0.006 wm_rt[t-1] + 0.002 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.002 wm_rt^2 + -0.001 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + 0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.002 repeat*switch + -0.0 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.004 switch^2 + 0.001 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + 0.001 switch*wm_rt[t-4] + -0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + 0.0 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 92, 92, 92, 92\n",
      "wm_rt: 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 343/1000 --- L(Train): 0.0295149 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.434 1 + 0.524 value_reward_diff[t] + 1.69 reward_diff + -0.447 value_reward_diff^2 + 0.544 value_reward_diff*reward_diff + 1.629 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.861 value_persistance[t] + 0.0 repeat + -0.002 value_persistance^2 + -0.002 value_persistance*repeat + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.001 repeat + 0.001 switch + -0.006 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.0 wm_rt*repeat + -0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.003 repeat*switch + -0.001 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.001 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.004 switch^2 + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.003 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 93, 93, 93, 93\n",
      "wm_rt: 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 344/1000 --- L(Train): 0.0294455 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.432 1 + 0.524 value_reward_diff[t] + 1.693 reward_diff + -0.447 value_reward_diff^2 + 0.542 value_reward_diff*reward_diff + 1.632 reward_diff^2 \n",
      "value_persistance[t+1] = -0.097 1 + 0.861 value_persistance[t] + -0.001 repeat + -0.002 value_persistance^2 + -0.002 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + -0.001 repeat + 0.002 switch + -0.005 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.001 wm_rt[t-4] + -0.0 wm_rt^2 + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.003 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.003 switch^2 + -0.002 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 94, 94, 94, 94\n",
      "wm_rt: 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 345/1000 --- L(Train): 0.0293756 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.43 1 + 0.525 value_reward_diff[t] + 1.696 reward_diff + -0.446 value_reward_diff^2 + 0.54 value_reward_diff*reward_diff + 1.634 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + -0.001 repeat + -0.002 value_persistance^2 + -0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.002 switch + -0.004 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.002 wm_rt^2 + 0.002 wm_rt*repeat + -0.0 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.002 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.003 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.005 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 95, 95, 95, 95\n",
      "wm_rt: 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 346/1000 --- L(Train): 0.0293039 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.428 1 + 0.525 value_reward_diff[t] + 1.699 reward_diff + -0.446 value_reward_diff^2 + 0.538 value_reward_diff*reward_diff + 1.637 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.864 value_persistance[t] + -0.0 repeat + -0.0 value_persistance^2 + -0.001 value_persistance*repeat + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.0 repeat + 0.001 switch + -0.002 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.003 wm_rt^2 + 0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.002 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + 0.001 switch^2 + -0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.001 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + -0.0 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.005 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 96, 96, 96, 96\n",
      "wm_rt: 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 347/1000 --- L(Train): 0.0292321 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.426 1 + 0.525 value_reward_diff[t] + 1.702 reward_diff + -0.445 value_reward_diff^2 + 0.536 value_reward_diff*reward_diff + 1.639 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.864 value_persistance[t] + 0.002 repeat + 0.003 value_persistance^2 + 0.001 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.001 switch + -0.0 wm_rt[t-1] + -0.004 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.003 wm_rt^2 + -0.0 wm_rt*repeat + 0.002 wm_rt*switch + 0.0 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.001 repeat*switch + 0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.002 switch^2 + -0.0 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.0 switch*wm_rt[t-3] + -0.001 switch*wm_rt[t-4] + 0.001 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.004 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 97, 97, 97, 97\n",
      "wm_rt: 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 348/1000 --- L(Train): 0.0291608 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.424 1 + 0.526 value_reward_diff[t] + 1.704 reward_diff + -0.445 value_reward_diff^2 + 0.534 value_reward_diff*reward_diff + 1.642 reward_diff^2 \n",
      "value_persistance[t+1] = -0.101 1 + 0.865 value_persistance[t] + 0.003 repeat + 0.005 value_persistance^2 + 0.002 value_persistance*repeat + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.003 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.003 switch + 0.002 wm_rt[t-1] + -0.003 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.002 wm_rt[t-4] + 0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + -0.002 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.0 repeat^2 + 0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + -0.0 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.002 switch^2 + 0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + 0.0 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 98, 98, 98, 98\n",
      "wm_rt: 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 349/1000 --- L(Train): 0.0290928 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 48):\n",
      "value_reward_diff[t+1] = 0.422 1 + 0.526 value_reward_diff[t] + 1.707 reward_diff + -0.445 value_reward_diff^2 + 0.532 value_reward_diff*reward_diff + 1.644 reward_diff^2 \n",
      "value_persistance[t+1] = -0.101 1 + 0.865 value_persistance[t] + 0.003 repeat + 0.007 value_persistance^2 + 0.001 value_persistance*repeat + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.998 wm_rt[t] + 0.001 repeat + -0.003 switch + 0.003 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.002 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.0 wm_rt*repeat + -0.0 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.0 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + 0.001 switch^2 + 0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + -0.0 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + 0.002 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 99, 99, 99, 99\n",
      "wm_rt: 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 350/1000 --- L(Train): 0.0290245 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 47):\n",
      "value_reward_diff[t+1] = 0.421 1 + 0.526 value_reward_diff[t] + 1.71 reward_diff + -0.444 value_reward_diff^2 + 0.53 value_reward_diff*reward_diff + 1.647 reward_diff^2 \n",
      "value_persistance[t+1] = -0.102 1 + 0.866 value_persistance[t] + 0.003 repeat + 0.007 value_persistance^2 + -0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + 0.001 repeat + -0.002 switch + 0.001 wm_rt[t-1] + -0.001 wm_rt[t-2] + -0.003 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + -0.003 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + -0.002 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-1] + 0.0 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + 0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-3] + 0.0 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-1]*wm_rt[t-4] + -0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 100, 100, -, 100\n",
      "wm_rt: 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 351/1000 --- L(Train): 0.0289552 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 46):\n",
      "value_reward_diff[t+1] = 0.419 1 + 0.526 value_reward_diff[t] + 1.713 reward_diff + -0.444 value_reward_diff^2 + 0.528 value_reward_diff*reward_diff + 1.649 reward_diff^2 \n",
      "value_persistance[t+1] = -0.102 1 + 0.866 value_persistance[t] + 0.002 repeat + 0.007 value_persistance^2 + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.0 repeat + -0.0 switch + -0.002 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.0 wm_rt[t-4] + -0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.003 wm_rt*wm_rt[t-3] + -0.001 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + 0.0 repeat*switch + -0.0 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.003 repeat*wm_rt[t-3] + 0.0 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.001 switch*wm_rt[t-1] + -0.003 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + -0.002 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-1]*wm_rt[t-4] + -0.002 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-3]^2 + 0.0 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 101, 101, -, 101\n",
      "wm_rt: 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, -, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 352/1000 --- L(Train): 0.0288855 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 45):\n",
      "value_reward_diff[t+1] = 0.417 1 + 0.527 value_reward_diff[t] + 1.716 reward_diff + -0.444 value_reward_diff^2 + 0.526 value_reward_diff*reward_diff + 1.652 reward_diff^2 \n",
      "value_persistance[t+1] = -0.102 1 + 0.866 value_persistance[t] + 0.0 repeat + 0.006 value_persistance^2 + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.002 wm_rt[t] + -0.001 repeat + 0.002 switch + -0.004 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.003 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.0 wm_rt^2 + 0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + -0.0 wm_rt*wm_rt[t-2] + 0.003 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + 0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.001 repeat*wm_rt[t-2] + 0.002 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.001 switch^2 + -0.002 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.001 switch*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.002 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.003 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 102, 102, -, 102\n",
      "wm_rt: 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, -, 102, 102, 102, 102, -, 102, 102, 102, 102, 102, 102\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 353/1000 --- L(Train): 0.0288186 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 44):\n",
      "value_reward_diff[t+1] = 0.415 1 + 0.527 value_reward_diff[t] + 1.719 reward_diff + -0.443 value_reward_diff^2 + 0.524 value_reward_diff*reward_diff + 1.654 reward_diff^2 \n",
      "value_persistance[t+1] = -0.101 1 + 0.865 value_persistance[t] + -0.001 repeat + 0.005 value_persistance^2 + -0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.003 wm_rt[t] + -0.002 repeat + 0.003 switch + -0.005 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.002 wm_rt[t-3] + 0.0 wm_rt[t-4] + 0.0 wm_rt^2 + -0.0 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + 0.002 wm_rt*wm_rt[t-3] + 0.001 wm_rt*wm_rt[t-4] + -0.002 repeat^2 + -0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + 0.0 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + 0.0 switch^2 + -0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.002 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 103, 103, -, 103\n",
      "wm_rt: 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, -, -, 103, 103, 103, -, 103, 103, 103, 103, 103, 103\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 354/1000 --- L(Train): 0.0287498 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 43):\n",
      "value_reward_diff[t+1] = 0.413 1 + 0.527 value_reward_diff[t] + 1.721 reward_diff + -0.443 value_reward_diff^2 + 0.522 value_reward_diff*reward_diff + 1.657 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.864 value_persistance[t] + -0.001 repeat + 0.004 value_persistance^2 + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.002 wm_rt[t] + -0.001 repeat + 0.002 switch + -0.005 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.001 wm_rt^2 + -0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.001 wm_rt*wm_rt[t-1] + 0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + -0.001 repeat^2 + -0.001 repeat*switch + 0.003 repeat*wm_rt[t-1] + 0.001 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.002 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.001 wm_rt[t-1]^2 + -0.003 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-2]^2 + -0.003 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.002 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 104, 104, -, 104\n",
      "wm_rt: 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, -, 104, 104, -, -, 104, 104, 104, -, 104, 104, 104, 104, 104, 104\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 355/1000 --- L(Train): 0.0286816 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 42):\n",
      "value_reward_diff[t+1] = 0.412 1 + 0.528 value_reward_diff[t] + 1.724 reward_diff + -0.443 value_reward_diff^2 + 0.52 value_reward_diff*reward_diff + 1.659 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.862 value_persistance[t] + -0.0 repeat + 0.002 value_persistance^2 + 0.001 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + 0.001 repeat + 0.0 switch + -0.005 wm_rt[t-1] + -0.004 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + -0.0 wm_rt^2 + -0.0 wm_rt*repeat + -0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + 0.0 wm_rt*wm_rt[t-4] + 0.0 repeat^2 + -0.0 repeat*switch + 0.002 repeat*wm_rt[t-1] + -0.0 repeat*wm_rt[t-2] + -0.004 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + -0.001 switch*wm_rt[t-1] + 0.002 switch*wm_rt[t-2] + 0.002 wm_rt[t-1]^2 + -0.002 wm_rt[t-1]*wm_rt[t-2] + 0.002 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 105, 105, -, 105\n",
      "wm_rt: 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, -, 105, 105, 105, 105, 105, 105, 105, 105, 105, -, 105, 105, -, -, 105, 105, 105, -, 105, 105, 105, 105, 105, 105\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 356/1000 --- L(Train): 0.0286194 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 41):\n",
      "value_reward_diff[t+1] = 0.41 1 + 0.528 value_reward_diff[t] + 1.727 reward_diff + -0.442 value_reward_diff^2 + 0.518 value_reward_diff*reward_diff + 1.662 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.862 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.999 wm_rt[t] + 0.002 repeat + -0.002 switch + -0.005 wm_rt[t-1] + -0.005 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.001 wm_rt[t-4] + 0.001 wm_rt^2 + 0.001 wm_rt*repeat + -0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-2] + -0.001 wm_rt*wm_rt[t-3] + -0.0 wm_rt*wm_rt[t-4] + 0.001 repeat^2 + 0.002 repeat*switch + -0.0 repeat*wm_rt[t-2] + -0.004 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.001 switch*wm_rt[t-1] + 0.001 switch*wm_rt[t-2] + 0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.002 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 106, 106, -, 106\n",
      "wm_rt: 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, -, 106, 106, 106, 106, 106, -, 106, 106, 106, -, 106, 106, -, -, 106, 106, 106, -, 106, 106, 106, 106, 106, 106\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 357/1000 --- L(Train): 0.0285549 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 40):\n",
      "value_reward_diff[t+1] = 0.408 1 + 0.528 value_reward_diff[t] + 1.73 reward_diff + -0.442 value_reward_diff^2 + 0.516 value_reward_diff*reward_diff + 1.664 reward_diff^2 \n",
      "value_persistance[t+1] = -0.098 1 + 0.862 value_persistance[t] + 0.002 repeat + -0.002 value_persistance^2 + -0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.998 wm_rt[t] + 0.002 repeat + -0.004 switch + -0.004 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.0 wm_rt[t-4] + 0.001 wm_rt^2 + 0.002 wm_rt*repeat + -0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + 0.0 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-2] + -0.003 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + 0.002 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + 0.001 wm_rt[t-3]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 107, 107, -, 107\n",
      "wm_rt: 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, -, 107, 107, -, 107, 107, -, 107, 107, 107, -, 107, 107, -, -, 107, 107, 107, -, 107, 107, 107, 107, 107, 107\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 358/1000 --- L(Train): 0.0284878 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 39):\n",
      "value_reward_diff[t+1] = 0.406 1 + 0.528 value_reward_diff[t] + 1.733 reward_diff + -0.442 value_reward_diff^2 + 0.514 value_reward_diff*reward_diff + 1.667 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + 0.002 repeat + -0.001 value_persistance^2 + 0.0 repeat^2 \n",
      "wm_rt[t+1] = -0.001 1 + 0.998 wm_rt[t] + 0.0 repeat + -0.004 switch + -0.002 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.001 wm_rt[t-4] + 0.001 wm_rt*repeat + 0.0 wm_rt*switch + 0.0 wm_rt*wm_rt[t-2] + 0.0 wm_rt*wm_rt[t-3] + -0.001 repeat^2 + 0.002 repeat*switch + 0.001 repeat*wm_rt[t-2] + -0.002 repeat*wm_rt[t-3] + 0.004 repeat*wm_rt[t-4] + 0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-2]^2 + 0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.0 wm_rt[t-3]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 108, 108, -, 108\n",
      "wm_rt: 108, 108, 108, 108, 108, 108, 108, 108, -, 108, 108, -, 108, 108, -, 108, 108, -, 108, 108, 108, -, 108, 108, -, -, 108, 108, 108, -, 108, 108, 108, 108, 108, 108\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 359/1000 --- L(Train): 0.0284236 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 38):\n",
      "value_reward_diff[t+1] = 0.404 1 + 0.529 value_reward_diff[t] + 1.735 reward_diff + -0.441 value_reward_diff^2 + 0.512 value_reward_diff*reward_diff + 1.669 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.864 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 + 0.0 repeat^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.003 switch + -0.0 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.001 wm_rt[t-3] + 0.0 wm_rt[t-4] + -0.0 wm_rt*repeat + 0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.002 repeat^2 + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-3] + 0.004 repeat*wm_rt[t-4] + -0.001 switch*wm_rt[t-1] + -0.002 switch*wm_rt[t-2] + -0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-3]^2 + -0.001 wm_rt[t-3]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 109, 109, -, 109\n",
      "wm_rt: 109, 109, 109, 109, 109, 109, 109, 109, -, 109, 109, -, 109, 109, -, 109, 109, -, -, 109, 109, -, 109, 109, -, -, 109, 109, 109, -, 109, 109, 109, 109, 109, 109\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 360/1000 --- L(Train): 0.0283584 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 37):\n",
      "value_reward_diff[t+1] = 0.403 1 + 0.529 value_reward_diff[t] + 1.738 reward_diff + -0.441 value_reward_diff^2 + 0.51 value_reward_diff*reward_diff + 1.671 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.864 value_persistance[t] + -0.001 repeat + 0.001 value_persistance^2 + -0.001 repeat^2 \n",
      "wm_rt[t+1] = 0.002 1 + 1.0 wm_rt[t] + -0.002 repeat + -0.001 switch + 0.002 wm_rt[t-1] + -0.003 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.002 wm_rt[t-4] + -0.001 wm_rt*repeat + -0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + -0.001 repeat^2 + -0.002 repeat*switch + 0.002 repeat*wm_rt[t-3] + 0.003 repeat*wm_rt[t-4] + -0.003 switch*wm_rt[t-1] + -0.001 switch*wm_rt[t-2] + -0.002 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + -0.003 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-3]^2 + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 110, 110, -, 110\n",
      "wm_rt: 110, 110, 110, 110, 110, 110, 110, 110, -, 110, 110, -, 110, 110, -, 110, 110, -, -, 110, 110, -, 110, 110, -, -, 110, 110, 110, -, 110, 110, 110, 110, -, 110\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 361/1000 --- L(Train): 0.0282971 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 36):\n",
      "value_reward_diff[t+1] = 0.401 1 + 0.529 value_reward_diff[t] + 1.741 reward_diff + -0.441 value_reward_diff^2 + 0.508 value_reward_diff*reward_diff + 1.674 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.864 value_persistance[t] + -0.0 repeat + 0.002 value_persistance^2 \n",
      "wm_rt[t+1] = 0.003 1 + 1.001 wm_rt[t] + -0.002 repeat + 0.001 switch + 0.002 wm_rt[t-1] + -0.001 wm_rt[t-2] + 0.0 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.0 wm_rt*repeat + -0.0 wm_rt*switch + 0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 repeat^2 + -0.003 repeat*switch + 0.002 repeat*wm_rt[t-3] + 0.001 repeat*wm_rt[t-4] + -0.003 switch*wm_rt[t-1] + 0.0 switch*wm_rt[t-2] + -0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 111, 111, -, -\n",
      "wm_rt: 111, 111, 111, 111, 111, 111, 111, 111, -, 111, 111, -, 111, 111, -, 111, 111, -, -, 111, 111, -, 111, 111, -, -, 111, 111, 111, -, 111, 111, 111, 111, -, 111\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 362/1000 --- L(Train): 0.0282338 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 35):\n",
      "value_reward_diff[t+1] = 0.399 1 + 0.53 value_reward_diff[t] + 1.744 reward_diff + -0.44 value_reward_diff^2 + 0.506 value_reward_diff*reward_diff + 1.676 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + 0.001 repeat + 0.001 value_persistance^2 \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + -0.001 repeat + 0.001 switch + 0.001 wm_rt[t-1] + 0.001 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.0 wm_rt*wm_rt[t-2] + 0.001 wm_rt*wm_rt[t-3] + 0.001 repeat^2 + -0.003 repeat*switch + 0.001 repeat*wm_rt[t-3] + -0.001 repeat*wm_rt[t-4] + -0.002 switch*wm_rt[t-1] + 0.001 wm_rt[t-1]^2 + 0.001 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-2]^2 + -0.002 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-3]^2 + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 112, 112, -, -\n",
      "wm_rt: 112, 112, 112, 112, 112, 112, 112, 112, -, 112, 112, -, 112, 112, -, 112, 112, -, -, 112, 112, -, 112, -, -, -, 112, 112, 112, -, 112, 112, 112, 112, -, 112\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 363/1000 --- L(Train): 0.0281666 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 34):\n",
      "value_reward_diff[t+1] = 0.397 1 + 0.53 value_reward_diff[t] + 1.746 reward_diff + -0.44 value_reward_diff^2 + 0.504 value_reward_diff*reward_diff + 1.679 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + 0.001 repeat + 0.0 value_persistance^2 \n",
      "wm_rt[t+1] = -0.002 1 + 0.999 wm_rt[t] + 0.0 repeat + -0.0 switch + -0.002 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.001 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.002 wm_rt*repeat + 0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-2] + -0.0 wm_rt*wm_rt[t-3] + 0.0 repeat^2 + -0.002 repeat*switch + -0.001 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + -0.001 switch*wm_rt[t-1] + 0.002 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-2] + 0.001 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 113, 113, -, -\n",
      "wm_rt: 113, 113, 113, 113, 113, 113, 113, 113, -, 113, 113, -, 113, 113, -, 113, 113, -, -, 113, 113, -, 113, -, -, -, 113, 113, 113, -, 113, 113, 113, -, -, 113\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 364/1000 --- L(Train): 0.0281056 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 33):\n",
      "value_reward_diff[t+1] = 0.396 1 + 0.53 value_reward_diff[t] + 1.749 reward_diff + -0.44 value_reward_diff^2 + 0.502 value_reward_diff*reward_diff + 1.681 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + 0.001 repeat + -0.001 value_persistance^2 \n",
      "wm_rt[t+1] = -0.004 1 + 0.999 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.005 wm_rt[t-1] + 0.0 wm_rt[t-2] + -0.0 wm_rt[t-3] + -0.005 wm_rt[t-4] + 0.001 wm_rt*repeat + 0.0 wm_rt*switch + -0.001 wm_rt*wm_rt[t-2] + -0.001 repeat^2 + 0.0 repeat*switch + -0.001 repeat*wm_rt[t-3] + -0.003 repeat*wm_rt[t-4] + 0.001 switch*wm_rt[t-1] + 0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.0 wm_rt[t-1]*wm_rt[t-3] + 0.001 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, 114, 114, -, -\n",
      "wm_rt: 114, 114, 114, 114, 114, 114, 114, 114, -, 114, 114, -, 114, -, -, 114, 114, -, -, 114, 114, -, 114, -, -, -, 114, 114, 114, -, 114, 114, 114, -, -, 114\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 365/1000 --- L(Train): 0.0280482 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 32):\n",
      "value_reward_diff[t+1] = 0.394 1 + 0.531 value_reward_diff[t] + 1.752 reward_diff + -0.439 value_reward_diff^2 + 0.5 value_reward_diff*reward_diff + 1.684 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + -0.001 value_persistance^2 \n",
      "wm_rt[t+1] = -0.004 1 + 0.999 wm_rt[t] + 0.001 repeat + 0.0 switch + -0.006 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.001 wm_rt[t-3] + -0.004 wm_rt[t-4] + -0.001 wm_rt*repeat + -0.002 wm_rt*switch + 0.0 wm_rt*wm_rt[t-2] + -0.001 repeat^2 + 0.001 repeat*switch + -0.001 repeat*wm_rt[t-3] + -0.002 repeat*wm_rt[t-4] + 0.001 switch*wm_rt[t-1] + -0.001 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-2] + -0.001 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, 115, -, -\n",
      "wm_rt: 115, 115, 115, 115, 115, 115, 115, 115, -, 115, 115, -, 115, -, -, 115, 115, -, -, 115, 115, -, 115, -, -, -, 115, 115, 115, -, 115, 115, 115, -, -, 115\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 366/1000 --- L(Train): 0.0279875 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 31):\n",
      "value_reward_diff[t+1] = 0.392 1 + 0.531 value_reward_diff[t] + 1.755 reward_diff + -0.439 value_reward_diff^2 + 0.498 value_reward_diff*reward_diff + 1.686 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + 0.0 value_persistance^2 \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + 0.001 repeat + 0.001 switch + -0.007 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.003 wm_rt[t-4] + -0.001 wm_rt*repeat + -0.003 wm_rt*switch + 0.0 wm_rt*wm_rt[t-2] + -0.0 repeat^2 + 0.001 repeat*switch + 0.001 repeat*wm_rt[t-3] + -0.0 repeat*wm_rt[t-4] + 0.0 switch*wm_rt[t-1] + -0.002 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-2] + 0.0 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, 116, -, -\n",
      "wm_rt: 116, 116, 116, 116, 116, 116, -, 116, -, 116, 116, -, 116, -, -, 116, 116, -, -, 116, 116, -, 116, -, -, -, 116, 116, 116, -, 116, 116, 116, -, -, 116\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 367/1000 --- L(Train): 0.0279254 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 30):\n",
      "value_reward_diff[t+1] = 0.39 1 + 0.531 value_reward_diff[t] + 1.757 reward_diff + -0.439 value_reward_diff^2 + 0.496 value_reward_diff*reward_diff + 1.688 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + 0.001 value_persistance^2 \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.007 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.001 wm_rt[t-4] + -0.001 wm_rt*repeat + -0.002 wm_rt*switch + -0.001 wm_rt*wm_rt[t-2] + 0.002 repeat^2 + -0.0 repeat*switch + 0.001 repeat*wm_rt[t-3] + 0.002 repeat*wm_rt[t-4] + -0.002 switch*wm_rt[t-1] + -0.003 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, 117, -, -\n",
      "wm_rt: 117, 117, 117, 117, 117, 117, -, 117, -, 117, 117, -, 117, -, -, 117, 117, -, -, 117, 117, -, 117, -, -, -, 117, -, 117, -, 117, 117, 117, -, -, 117\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 368/1000 --- L(Train): 0.0278661 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 29):\n",
      "value_reward_diff[t+1] = 0.388 1 + 0.531 value_reward_diff[t] + 1.76 reward_diff + -0.438 value_reward_diff^2 + 0.494 value_reward_diff*reward_diff + 1.691 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + 0.001 value_persistance^2 \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + -0.001 repeat + -0.0 switch + -0.006 wm_rt[t-1] + -0.005 wm_rt[t-2] + 0.001 wm_rt[t-4] + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.001 wm_rt*wm_rt[t-2] + 0.003 repeat^2 + -0.001 repeat*switch + 0.003 repeat*wm_rt[t-4] + -0.003 switch*wm_rt[t-1] + -0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-1]*wm_rt[t-3] + -0.001 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, 118, -, -\n",
      "wm_rt: 118, 118, 118, 118, 118, 118, -, 118, -, 118, 118, -, 118, -, -, 118, 118, -, -, -, 118, -, 118, -, -, -, 118, -, 118, -, 118, 118, 118, -, -, 118\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 369/1000 --- L(Train): 0.0278075 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 28):\n",
      "value_reward_diff[t+1] = 0.387 1 + 0.532 value_reward_diff[t] + 1.763 reward_diff + -0.438 value_reward_diff^2 + 0.492 value_reward_diff*reward_diff + 1.693 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + -0.0 value_persistance^2 \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + -0.003 repeat + -0.0 switch + -0.005 wm_rt[t-1] + -0.005 wm_rt[t-2] + 0.002 wm_rt[t-4] + 0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.002 repeat^2 + 0.0 repeat*switch + 0.003 repeat*wm_rt[t-4] + -0.003 switch*wm_rt[t-1] + -0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-1]*wm_rt[t-3] + 0.0 wm_rt[t-2]^2 + 0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.003 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, 119, -, -\n",
      "wm_rt: 119, 119, 119, 119, 119, 119, -, 119, -, 119, 119, -, -, -, -, 119, 119, -, -, -, 119, -, 119, -, -, -, 119, -, 119, -, 119, 119, 119, -, -, 119\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 370/1000 --- L(Train): 0.0277457 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 27):\n",
      "value_reward_diff[t+1] = 0.385 1 + 0.532 value_reward_diff[t] + 1.766 reward_diff + -0.438 value_reward_diff^2 + 0.49 value_reward_diff*reward_diff + 1.695 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + 0.001 value_persistance^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + -0.003 repeat + 0.001 switch + -0.004 wm_rt[t-1] + -0.004 wm_rt[t-2] + 0.001 wm_rt*repeat + 0.002 wm_rt*switch + 0.001 repeat^2 + 0.0 repeat*switch + 0.002 repeat*wm_rt[t-4] + -0.003 switch*wm_rt[t-1] + 0.001 wm_rt[t-1]^2 + -0.0 wm_rt[t-1]*wm_rt[t-3] + -0.0 wm_rt[t-2]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.002 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, 120, -, -\n",
      "wm_rt: 120, 120, 120, 120, 120, 120, -, -, -, 120, 120, -, -, -, -, 120, 120, -, -, -, 120, -, 120, -, -, -, 120, -, 120, -, 120, 120, 120, -, -, 120\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 371/1000 --- L(Train): 0.0276867 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 26):\n",
      "value_reward_diff[t+1] = 0.383 1 + 0.532 value_reward_diff[t] + 1.768 reward_diff + -0.437 value_reward_diff^2 + 0.488 value_reward_diff*reward_diff + 1.698 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] + 0.001 value_persistance^2 \n",
      "wm_rt[t+1] = -0.001 1 + 1.001 wm_rt[t] + -0.002 repeat + 0.0 switch + -0.002 wm_rt[t-1] + -0.002 wm_rt[t-2] + -0.001 wm_rt*repeat + 0.002 wm_rt*switch + -0.001 repeat^2 + -0.001 repeat*switch + 0.0 repeat*wm_rt[t-4] + -0.002 switch*wm_rt[t-1] + 0.001 wm_rt[t-1]^2 + 0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-2]*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, 121, -, -\n",
      "wm_rt: 121, 121, 121, 121, 121, 121, -, -, -, 121, 121, -, -, -, -, 121, 121, -, -, -, 121, -, 121, -, -, -, 121, -, -, -, 121, 121, 121, -, -, 121\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 372/1000 --- L(Train): 0.0276307 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 25):\n",
      "value_reward_diff[t+1] = 0.381 1 + 0.532 value_reward_diff[t] + 1.771 reward_diff + -0.437 value_reward_diff^2 + 0.486 value_reward_diff*reward_diff + 1.7 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = -0.0 1 + 1.0 wm_rt[t] + 0.0 repeat + -0.001 switch + 0.0 wm_rt[t-1] + -0.0 wm_rt[t-2] + -0.001 wm_rt*repeat + 0.0 wm_rt*switch + -0.002 repeat^2 + -0.001 repeat*switch + -0.002 repeat*wm_rt[t-4] + 0.0 switch*wm_rt[t-1] + 0.0 wm_rt[t-1]^2 + -0.0 wm_rt[t-2]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 122, 122, 122, 122, 122, 122, -, -, -, 122, 122, -, -, -, -, 122, 122, -, -, -, 122, -, 122, -, -, -, 122, -, -, -, 122, 122, 122, -, -, 122\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 373/1000 --- L(Train): 0.0275695 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 24):\n",
      "value_reward_diff[t+1] = 0.38 1 + 0.533 value_reward_diff[t] + 1.774 reward_diff + -0.437 value_reward_diff^2 + 0.484 value_reward_diff*reward_diff + 1.703 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 0.001 1 + 0.999 wm_rt[t] + 0.002 repeat + -0.001 switch + 0.001 wm_rt[t-1] + 0.002 wm_rt[t-2] + -0.001 wm_rt*repeat + -0.002 wm_rt*switch + -0.001 repeat^2 + -0.0 repeat*switch + -0.004 repeat*wm_rt[t-4] + 0.001 switch*wm_rt[t-1] + -0.002 wm_rt[t-1]^2 + -0.001 wm_rt[t-2]*wm_rt[t-3] + 0.001 wm_rt[t-2]*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 123, 123, 123, 123, 123, 123, -, -, -, 123, 123, -, -, -, -, 123, 123, -, -, -, 123, -, 123, -, -, -, 123, -, -, -, -, 123, 123, -, -, 123\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 374/1000 --- L(Train): 0.0275118 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 23):\n",
      "value_reward_diff[t+1] = 0.378 1 + 0.533 value_reward_diff[t] + 1.776 reward_diff + -0.437 value_reward_diff^2 + 0.482 value_reward_diff*reward_diff + 1.705 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 0.001 1 + 1.0 wm_rt[t] + 0.002 repeat + -0.001 switch + -0.001 wm_rt[t-1] + 0.002 wm_rt[t-2] + 0.001 wm_rt*repeat + -0.003 wm_rt*switch + 0.002 repeat*switch + -0.004 repeat*wm_rt[t-4] + -0.0 switch*wm_rt[t-1] + -0.003 wm_rt[t-1]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 124, 124, 124, 124, 124, 124, -, -, -, 124, 124, -, -, -, -, -, 124, -, -, -, 124, -, 124, -, -, -, 124, -, -, -, -, 124, 124, -, -, 124\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 375/1000 --- L(Train): 0.0274600 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 22):\n",
      "value_reward_diff[t+1] = 0.376 1 + 0.533 value_reward_diff[t] + 1.779 reward_diff + -0.436 value_reward_diff^2 + 0.48 value_reward_diff*reward_diff + 1.707 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.001 switch + -0.002 wm_rt[t-1] + 0.001 wm_rt[t-2] + 0.001 wm_rt*repeat + -0.003 wm_rt*switch + 0.002 repeat*switch + -0.003 repeat*wm_rt[t-4] + -0.001 switch*wm_rt[t-1] + -0.003 wm_rt[t-1]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 125, -, 125, 125, 125, 125, -, -, -, 125, 125, -, -, -, -, -, 125, -, -, -, 125, -, 125, -, -, -, 125, -, -, -, -, 125, 125, -, -, 125\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 376/1000 --- L(Train): 0.0274014 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 21):\n",
      "value_reward_diff[t+1] = 0.375 1 + 0.534 value_reward_diff[t] + 1.782 reward_diff + -0.436 value_reward_diff^2 + 0.478 value_reward_diff*reward_diff + 1.71 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + -0.001 repeat + 0.0 switch + -0.002 wm_rt[t-1] + -0.002 wm_rt[t-2] + 0.0 wm_rt*repeat + -0.001 wm_rt*switch + 0.002 repeat*switch + -0.002 repeat*wm_rt[t-4] + -0.003 wm_rt[t-1]^2 + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-2]*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 126, -, 126, 126, 126, 126, -, -, -, 126, 126, -, -, -, -, -, 126, -, -, -, 126, -, -, -, -, -, 126, -, -, -, -, 126, 126, -, -, 126\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 377/1000 --- L(Train): 0.0273430 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 20):\n",
      "value_reward_diff[t+1] = 0.373 1 + 0.534 value_reward_diff[t] + 1.785 reward_diff + -0.436 value_reward_diff^2 + 0.476 value_reward_diff*reward_diff + 1.712 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + -0.002 repeat + 0.0 switch + -0.002 wm_rt[t-1] + -0.004 wm_rt[t-2] + -0.001 wm_rt*repeat + 0.001 wm_rt*switch + 0.0 repeat*switch + 0.001 repeat*wm_rt[t-4] + -0.002 wm_rt[t-1]^2 + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 127, -, 127, 127, 127, 127, -, -, -, 127, 127, -, -, -, -, -, 127, -, -, -, 127, -, -, -, -, -, 127, -, -, -, -, 127, -, -, -, 127\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 378/1000 --- L(Train): 0.0272856 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 19):\n",
      "value_reward_diff[t+1] = 0.371 1 + 0.534 value_reward_diff[t] + 1.787 reward_diff + -0.435 value_reward_diff^2 + 0.475 value_reward_diff*reward_diff + 1.714 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.0 switch + -0.001 wm_rt[t-1] + -0.005 wm_rt[t-2] + -0.002 wm_rt*repeat + 0.001 wm_rt*switch + -0.002 repeat*switch + 0.002 repeat*wm_rt[t-4] + -0.0 wm_rt[t-2]*wm_rt[t-3] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 128, -, 128, 128, 128, 128, -, -, -, 128, 128, -, -, -, -, -, 128, -, -, -, 128, -, -, -, -, -, -, -, -, -, -, 128, -, -, -, 128\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 379/1000 --- L(Train): 0.0272316 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 18):\n",
      "value_reward_diff[t+1] = 0.369 1 + 0.534 value_reward_diff[t] + 1.79 reward_diff + -0.435 value_reward_diff^2 + 0.473 value_reward_diff*reward_diff + 1.717 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.0 switch + -0.006 wm_rt[t-2] + -0.001 wm_rt*repeat + 0.001 wm_rt*switch + -0.003 repeat*switch + 0.002 repeat*wm_rt[t-4] + 0.0 wm_rt[t-2]*wm_rt[t-3] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 129, -, 129, 129, -, 129, -, -, -, 129, 129, -, -, -, -, -, 129, -, -, -, 129, -, -, -, -, -, -, -, -, -, -, 129, -, -, -, 129\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 380/1000 --- L(Train): 0.0271758 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 17):\n",
      "value_reward_diff[t+1] = 0.368 1 + 0.535 value_reward_diff[t] + 1.793 reward_diff + -0.435 value_reward_diff^2 + 0.471 value_reward_diff*reward_diff + 1.719 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 0.0 1 + 1.0 wm_rt[t] + 0.002 repeat + 0.0 switch + -0.006 wm_rt[t-2] + 0.0 wm_rt*repeat + -0.0 wm_rt*switch + -0.003 repeat*switch + 0.001 repeat*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 130, -, 130, 130, -, 130, -, -, -, 130, 130, -, -, -, -, -, 130, -, -, -, 130, -, -, -, -, -, -, -, -, -, -, -, -, -, -, 130\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 381/1000 --- L(Train): 0.0271231 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 16):\n",
      "value_reward_diff[t+1] = 0.366 1 + 0.535 value_reward_diff[t] + 1.795 reward_diff + -0.434 value_reward_diff^2 + 0.469 value_reward_diff*reward_diff + 1.721 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + 0.001 repeat + -0.005 wm_rt[t-2] + 0.001 wm_rt*repeat + -0.001 wm_rt*switch + -0.002 repeat*switch + -0.001 repeat*wm_rt[t-4] + 0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 131, -, 131, -, -, 131, -, -, -, 131, 131, -, -, -, -, -, 131, -, -, -, 131, -, -, -, -, -, -, -, -, -, -, -, -, -, -, 131\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 382/1000 --- L(Train): 0.0270691 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 15):\n",
      "value_reward_diff[t+1] = 0.364 1 + 0.535 value_reward_diff[t] + 1.798 reward_diff + -0.434 value_reward_diff^2 + 0.467 value_reward_diff*reward_diff + 1.724 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = -0.003 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.004 wm_rt[t-2] + 0.0 wm_rt*switch + 0.0 repeat*switch + -0.002 repeat*wm_rt[t-4] + -0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 132, -, 132, -, -, 132, -, -, -, -, 132, -, -, -, -, -, 132, -, -, -, 132, -, -, -, -, -, -, -, -, -, -, -, -, -, -, 132\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 383/1000 --- L(Train): 0.0270163 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 14):\n",
      "value_reward_diff[t+1] = 0.363 1 + 0.536 value_reward_diff[t] + 1.801 reward_diff + -0.434 value_reward_diff^2 + 0.465 value_reward_diff*reward_diff + 1.726 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = -0.002 1 + 1.0 wm_rt[t] + -0.001 repeat + -0.002 wm_rt[t-2] + 0.001 repeat*switch + -0.002 repeat*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 133, -, 133, -, -, 133, -, -, -, -, -, -, -, -, -, -, 133, -, -, -, 133, -, -, -, -, -, -, -, -, -, -, -, -, -, -, 133\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 384/1000 --- L(Train): 0.0269617 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 13):\n",
      "value_reward_diff[t+1] = 0.361 1 + 0.536 value_reward_diff[t] + 1.803 reward_diff + -0.433 value_reward_diff^2 + 0.463 value_reward_diff*reward_diff + 1.728 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = -0.001 1 + 1.0 wm_rt[t] + -0.0 wm_rt[t-2] + 0.001 repeat*switch + -0.001 repeat*wm_rt[t-4] + -0.001 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: 134, -, -, -, -, 134, -, -, -, -, -, -, -, -, -, -, 134, -, -, -, 134, -, -, -, -, -, -, -, -, -, -, -, -, -, -, 134\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 385/1000 --- L(Train): 0.0269076 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 12):\n",
      "value_reward_diff[t+1] = 0.359 1 + 0.536 value_reward_diff[t] + 1.806 reward_diff + -0.433 value_reward_diff^2 + 0.461 value_reward_diff*reward_diff + 1.73 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.002 wm_rt[t-2] + -0.0 repeat*switch + 0.001 repeat*wm_rt[t-4] + 0.0 wm_rt[t-4]^2 \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, 135, -, -, -, -, -, -, -, -, -, -, 135, -, -, -, 135, -, -, -, -, -, -, -, -, -, -, -, -, -, -, 135\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 386/1000 --- L(Train): 0.0268570 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 11):\n",
      "value_reward_diff[t+1] = 0.358 1 + 0.536 value_reward_diff[t] + 1.808 reward_diff + -0.433 value_reward_diff^2 + 0.459 value_reward_diff*reward_diff + 1.733 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.003 wm_rt[t-2] + -0.001 repeat*switch + 0.002 repeat*wm_rt[t-4] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, 136, -, -, -, -, -, -, -, -, -, -, 136, -, -, -, 136, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 387/1000 --- L(Train): 0.0268110 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 10):\n",
      "value_reward_diff[t+1] = 0.356 1 + 0.537 value_reward_diff[t] + 1.811 reward_diff + -0.432 value_reward_diff^2 + 0.458 value_reward_diff*reward_diff + 1.735 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + 0.001 wm_rt[t-2] + 0.002 repeat*wm_rt[t-4] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, 137, -, -, -, -, -, -, -, -, -, -, -, -, -, -, 137, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 388/1000 --- L(Train): 0.0267658 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 9):\n",
      "value_reward_diff[t+1] = 0.354 1 + 0.537 value_reward_diff[t] + 1.814 reward_diff + -0.432 value_reward_diff^2 + 0.456 value_reward_diff*reward_diff + 1.737 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + -0.002 wm_rt[t-2] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, 138, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 389/1000 --- L(Train): 0.0267375 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 9):\n",
      "value_reward_diff[t+1] = 0.352 1 + 0.537 value_reward_diff[t] + 1.816 reward_diff + -0.432 value_reward_diff^2 + 0.454 value_reward_diff*reward_diff + 1.739 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 390/1000 --- L(Train): 0.0266851 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.351 1 + 0.537 value_reward_diff[t] + 1.819 reward_diff + -0.431 value_reward_diff^2 + 0.452 value_reward_diff*reward_diff + 1.742 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 391/1000 --- L(Train): 0.0266267 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.349 1 + 0.538 value_reward_diff[t] + 1.822 reward_diff + -0.431 value_reward_diff^2 + 0.45 value_reward_diff*reward_diff + 1.744 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 392/1000 --- L(Train): 0.0265774 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.347 1 + 0.538 value_reward_diff[t] + 1.824 reward_diff + -0.431 value_reward_diff^2 + 0.448 value_reward_diff*reward_diff + 1.746 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 393/1000 --- L(Train): 0.0265277 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.346 1 + 0.538 value_reward_diff[t] + 1.827 reward_diff + -0.43 value_reward_diff^2 + 0.446 value_reward_diff*reward_diff + 1.748 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 394/1000 --- L(Train): 0.0264791 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.344 1 + 0.538 value_reward_diff[t] + 1.829 reward_diff + -0.43 value_reward_diff^2 + 0.445 value_reward_diff*reward_diff + 1.751 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 395/1000 --- L(Train): 0.0264332 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.343 1 + 0.539 value_reward_diff[t] + 1.832 reward_diff + -0.43 value_reward_diff^2 + 0.443 value_reward_diff*reward_diff + 1.753 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 396/1000 --- L(Train): 0.0263855 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.341 1 + 0.539 value_reward_diff[t] + 1.834 reward_diff + -0.43 value_reward_diff^2 + 0.441 value_reward_diff*reward_diff + 1.755 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 397/1000 --- L(Train): 0.0263322 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.339 1 + 0.539 value_reward_diff[t] + 1.837 reward_diff + -0.429 value_reward_diff^2 + 0.439 value_reward_diff*reward_diff + 1.757 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 398/1000 --- L(Train): 0.0262841 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.338 1 + 0.539 value_reward_diff[t] + 1.84 reward_diff + -0.429 value_reward_diff^2 + 0.437 value_reward_diff*reward_diff + 1.759 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 399/1000 --- L(Train): 0.0262336 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.336 1 + 0.54 value_reward_diff[t] + 1.842 reward_diff + -0.429 value_reward_diff^2 + 0.436 value_reward_diff*reward_diff + 1.762 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.863 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 400/1000 --- L(Train): 0.0261874 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.334 1 + 0.54 value_reward_diff[t] + 1.845 reward_diff + -0.428 value_reward_diff^2 + 0.434 value_reward_diff*reward_diff + 1.764 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 401/1000 --- L(Train): 0.0261421 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.333 1 + 0.54 value_reward_diff[t] + 1.847 reward_diff + -0.428 value_reward_diff^2 + 0.432 value_reward_diff*reward_diff + 1.766 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 402/1000 --- L(Train): 0.0260958 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.331 1 + 0.54 value_reward_diff[t] + 1.85 reward_diff + -0.428 value_reward_diff^2 + 0.43 value_reward_diff*reward_diff + 1.768 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 403/1000 --- L(Train): 0.0260489 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.329 1 + 0.541 value_reward_diff[t] + 1.852 reward_diff + -0.427 value_reward_diff^2 + 0.428 value_reward_diff*reward_diff + 1.77 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 404/1000 --- L(Train): 0.0260014 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.328 1 + 0.541 value_reward_diff[t] + 1.855 reward_diff + -0.427 value_reward_diff^2 + 0.427 value_reward_diff*reward_diff + 1.772 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 405/1000 --- L(Train): 0.0259543 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.326 1 + 0.541 value_reward_diff[t] + 1.857 reward_diff + -0.427 value_reward_diff^2 + 0.425 value_reward_diff*reward_diff + 1.775 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 406/1000 --- L(Train): 0.0259100 --- L(Val, SINDy): 0.0000000 --- Time: 0.11s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.325 1 + 0.541 value_reward_diff[t] + 1.86 reward_diff + -0.426 value_reward_diff^2 + 0.423 value_reward_diff*reward_diff + 1.777 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 407/1000 --- L(Train): 0.0258622 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.323 1 + 0.542 value_reward_diff[t] + 1.862 reward_diff + -0.426 value_reward_diff^2 + 0.421 value_reward_diff*reward_diff + 1.779 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 408/1000 --- L(Train): 0.0258174 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.321 1 + 0.542 value_reward_diff[t] + 1.865 reward_diff + -0.426 value_reward_diff^2 + 0.419 value_reward_diff*reward_diff + 1.781 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 409/1000 --- L(Train): 0.0257759 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.32 1 + 0.542 value_reward_diff[t] + 1.867 reward_diff + -0.426 value_reward_diff^2 + 0.418 value_reward_diff*reward_diff + 1.783 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 410/1000 --- L(Train): 0.0257330 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.318 1 + 0.542 value_reward_diff[t] + 1.87 reward_diff + -0.425 value_reward_diff^2 + 0.416 value_reward_diff*reward_diff + 1.785 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 411/1000 --- L(Train): 0.0256825 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.317 1 + 0.543 value_reward_diff[t] + 1.872 reward_diff + -0.425 value_reward_diff^2 + 0.414 value_reward_diff*reward_diff + 1.787 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 412/1000 --- L(Train): 0.0256380 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.315 1 + 0.543 value_reward_diff[t] + 1.875 reward_diff + -0.425 value_reward_diff^2 + 0.412 value_reward_diff*reward_diff + 1.789 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 413/1000 --- L(Train): 0.0255979 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.314 1 + 0.543 value_reward_diff[t] + 1.877 reward_diff + -0.424 value_reward_diff^2 + 0.411 value_reward_diff*reward_diff + 1.792 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 414/1000 --- L(Train): 0.0255679 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.312 1 + 0.543 value_reward_diff[t] + 1.88 reward_diff + -0.424 value_reward_diff^2 + 0.409 value_reward_diff*reward_diff + 1.794 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 415/1000 --- L(Train): 0.0255221 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.31 1 + 0.544 value_reward_diff[t] + 1.882 reward_diff + -0.424 value_reward_diff^2 + 0.407 value_reward_diff*reward_diff + 1.796 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 416/1000 --- L(Train): 0.0254838 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.309 1 + 0.544 value_reward_diff[t] + 1.885 reward_diff + -0.423 value_reward_diff^2 + 0.405 value_reward_diff*reward_diff + 1.798 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 417/1000 --- L(Train): 0.0254424 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.307 1 + 0.544 value_reward_diff[t] + 1.887 reward_diff + -0.423 value_reward_diff^2 + 0.404 value_reward_diff*reward_diff + 1.8 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 418/1000 --- L(Train): 0.0253986 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.306 1 + 0.544 value_reward_diff[t] + 1.89 reward_diff + -0.423 value_reward_diff^2 + 0.402 value_reward_diff*reward_diff + 1.802 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 419/1000 --- L(Train): 0.0253590 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.304 1 + 0.545 value_reward_diff[t] + 1.892 reward_diff + -0.422 value_reward_diff^2 + 0.4 value_reward_diff*reward_diff + 1.804 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 420/1000 --- L(Train): 0.0253219 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.303 1 + 0.545 value_reward_diff[t] + 1.895 reward_diff + -0.422 value_reward_diff^2 + 0.399 value_reward_diff*reward_diff + 1.806 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 421/1000 --- L(Train): 0.0252776 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.301 1 + 0.545 value_reward_diff[t] + 1.897 reward_diff + -0.422 value_reward_diff^2 + 0.397 value_reward_diff*reward_diff + 1.808 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 422/1000 --- L(Train): 0.0252328 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.299 1 + 0.545 value_reward_diff[t] + 1.899 reward_diff + -0.422 value_reward_diff^2 + 0.395 value_reward_diff*reward_diff + 1.81 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 423/1000 --- L(Train): 0.0251949 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.298 1 + 0.546 value_reward_diff[t] + 1.902 reward_diff + -0.421 value_reward_diff^2 + 0.393 value_reward_diff*reward_diff + 1.812 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 424/1000 --- L(Train): 0.0251599 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.296 1 + 0.546 value_reward_diff[t] + 1.904 reward_diff + -0.421 value_reward_diff^2 + 0.392 value_reward_diff*reward_diff + 1.814 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 425/1000 --- L(Train): 0.0251202 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.295 1 + 0.546 value_reward_diff[t] + 1.907 reward_diff + -0.421 value_reward_diff^2 + 0.39 value_reward_diff*reward_diff + 1.816 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 426/1000 --- L(Train): 0.0250793 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.293 1 + 0.546 value_reward_diff[t] + 1.909 reward_diff + -0.42 value_reward_diff^2 + 0.388 value_reward_diff*reward_diff + 1.818 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 427/1000 --- L(Train): 0.0250370 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.292 1 + 0.547 value_reward_diff[t] + 1.912 reward_diff + -0.42 value_reward_diff^2 + 0.387 value_reward_diff*reward_diff + 1.82 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 428/1000 --- L(Train): 0.0250029 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.29 1 + 0.547 value_reward_diff[t] + 1.914 reward_diff + -0.42 value_reward_diff^2 + 0.385 value_reward_diff*reward_diff + 1.822 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 429/1000 --- L(Train): 0.0249649 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.289 1 + 0.547 value_reward_diff[t] + 1.916 reward_diff + -0.42 value_reward_diff^2 + 0.383 value_reward_diff*reward_diff + 1.824 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 430/1000 --- L(Train): 0.0249246 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.287 1 + 0.547 value_reward_diff[t] + 1.919 reward_diff + -0.419 value_reward_diff^2 + 0.382 value_reward_diff*reward_diff + 1.826 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 431/1000 --- L(Train): 0.0248880 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.286 1 + 0.547 value_reward_diff[t] + 1.921 reward_diff + -0.419 value_reward_diff^2 + 0.38 value_reward_diff*reward_diff + 1.828 reward_diff^2 \n",
      "value_persistance[t+1] = -0.099 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 432/1000 --- L(Train): 0.0248489 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.284 1 + 0.548 value_reward_diff[t] + 1.923 reward_diff + -0.419 value_reward_diff^2 + 0.378 value_reward_diff*reward_diff + 1.83 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 433/1000 --- L(Train): 0.0248087 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.283 1 + 0.548 value_reward_diff[t] + 1.926 reward_diff + -0.418 value_reward_diff^2 + 0.377 value_reward_diff*reward_diff + 1.832 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 434/1000 --- L(Train): 0.0247705 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.281 1 + 0.548 value_reward_diff[t] + 1.928 reward_diff + -0.418 value_reward_diff^2 + 0.375 value_reward_diff*reward_diff + 1.834 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 435/1000 --- L(Train): 0.0247347 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.28 1 + 0.548 value_reward_diff[t] + 1.931 reward_diff + -0.418 value_reward_diff^2 + 0.373 value_reward_diff*reward_diff + 1.836 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 436/1000 --- L(Train): 0.0247002 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.278 1 + 0.549 value_reward_diff[t] + 1.933 reward_diff + -0.418 value_reward_diff^2 + 0.372 value_reward_diff*reward_diff + 1.838 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 437/1000 --- L(Train): 0.0246646 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.277 1 + 0.549 value_reward_diff[t] + 1.935 reward_diff + -0.417 value_reward_diff^2 + 0.37 value_reward_diff*reward_diff + 1.84 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 438/1000 --- L(Train): 0.0246272 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.275 1 + 0.549 value_reward_diff[t] + 1.938 reward_diff + -0.417 value_reward_diff^2 + 0.368 value_reward_diff*reward_diff + 1.842 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 439/1000 --- L(Train): 0.0245892 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.274 1 + 0.549 value_reward_diff[t] + 1.94 reward_diff + -0.417 value_reward_diff^2 + 0.367 value_reward_diff*reward_diff + 1.844 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 440/1000 --- L(Train): 0.0245621 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.272 1 + 0.549 value_reward_diff[t] + 1.942 reward_diff + -0.416 value_reward_diff^2 + 0.365 value_reward_diff*reward_diff + 1.846 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 441/1000 --- L(Train): 0.0245234 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.271 1 + 0.55 value_reward_diff[t] + 1.945 reward_diff + -0.416 value_reward_diff^2 + 0.364 value_reward_diff*reward_diff + 1.848 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 442/1000 --- L(Train): 0.0244833 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.269 1 + 0.55 value_reward_diff[t] + 1.947 reward_diff + -0.416 value_reward_diff^2 + 0.362 value_reward_diff*reward_diff + 1.85 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 443/1000 --- L(Train): 0.0244490 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.268 1 + 0.55 value_reward_diff[t] + 1.949 reward_diff + -0.416 value_reward_diff^2 + 0.36 value_reward_diff*reward_diff + 1.851 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 444/1000 --- L(Train): 0.0244196 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.267 1 + 0.55 value_reward_diff[t] + 1.951 reward_diff + -0.415 value_reward_diff^2 + 0.359 value_reward_diff*reward_diff + 1.853 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 445/1000 --- L(Train): 0.0243863 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.265 1 + 0.55 value_reward_diff[t] + 1.954 reward_diff + -0.415 value_reward_diff^2 + 0.357 value_reward_diff*reward_diff + 1.855 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 446/1000 --- L(Train): 0.0243535 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.264 1 + 0.551 value_reward_diff[t] + 1.956 reward_diff + -0.415 value_reward_diff^2 + 0.356 value_reward_diff*reward_diff + 1.857 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 447/1000 --- L(Train): 0.0243162 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.262 1 + 0.551 value_reward_diff[t] + 1.958 reward_diff + -0.414 value_reward_diff^2 + 0.354 value_reward_diff*reward_diff + 1.859 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 448/1000 --- L(Train): 0.0242831 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.261 1 + 0.551 value_reward_diff[t] + 1.961 reward_diff + -0.414 value_reward_diff^2 + 0.352 value_reward_diff*reward_diff + 1.861 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 449/1000 --- L(Train): 0.0242494 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.259 1 + 0.551 value_reward_diff[t] + 1.963 reward_diff + -0.414 value_reward_diff^2 + 0.351 value_reward_diff*reward_diff + 1.863 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 450/1000 --- L(Train): 0.0242157 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.258 1 + 0.552 value_reward_diff[t] + 1.965 reward_diff + -0.414 value_reward_diff^2 + 0.349 value_reward_diff*reward_diff + 1.864 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 451/1000 --- L(Train): 0.0241773 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.257 1 + 0.552 value_reward_diff[t] + 1.967 reward_diff + -0.413 value_reward_diff^2 + 0.348 value_reward_diff*reward_diff + 1.866 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\u001b[H\u001b[2J\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 452/1000 --- L(Train): 0.0241424 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.255 1 + 0.552 value_reward_diff[t] + 1.97 reward_diff + -0.413 value_reward_diff^2 + 0.346 value_reward_diff*reward_diff + 1.868 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 453/1000 --- L(Train): 0.0241114 --- L(Val, SINDy): 0.0000000 --- Time: 0.12s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.254 1 + 0.552 value_reward_diff[t] + 1.972 reward_diff + -0.413 value_reward_diff^2 + 0.344 value_reward_diff*reward_diff + 1.87 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 454/1000 --- L(Train): 0.0240793 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.252 1 + 0.552 value_reward_diff[t] + 1.974 reward_diff + -0.412 value_reward_diff^2 + 0.343 value_reward_diff*reward_diff + 1.872 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 455/1000 --- L(Train): 0.0240462 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.251 1 + 0.553 value_reward_diff[t] + 1.976 reward_diff + -0.412 value_reward_diff^2 + 0.341 value_reward_diff*reward_diff + 1.874 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 456/1000 --- L(Train): 0.0240162 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.249 1 + 0.553 value_reward_diff[t] + 1.979 reward_diff + -0.412 value_reward_diff^2 + 0.34 value_reward_diff*reward_diff + 1.875 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 457/1000 --- L(Train): 0.0239840 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.248 1 + 0.553 value_reward_diff[t] + 1.981 reward_diff + -0.412 value_reward_diff^2 + 0.338 value_reward_diff*reward_diff + 1.877 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 458/1000 --- L(Train): 0.0239555 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.247 1 + 0.553 value_reward_diff[t] + 1.983 reward_diff + -0.411 value_reward_diff^2 + 0.337 value_reward_diff*reward_diff + 1.879 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 459/1000 --- L(Train): 0.0239251 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.245 1 + 0.553 value_reward_diff[t] + 1.985 reward_diff + -0.411 value_reward_diff^2 + 0.335 value_reward_diff*reward_diff + 1.881 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 460/1000 --- L(Train): 0.0238927 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.244 1 + 0.554 value_reward_diff[t] + 1.987 reward_diff + -0.411 value_reward_diff^2 + 0.334 value_reward_diff*reward_diff + 1.883 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 461/1000 --- L(Train): 0.0238620 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.243 1 + 0.554 value_reward_diff[t] + 1.99 reward_diff + -0.411 value_reward_diff^2 + 0.332 value_reward_diff*reward_diff + 1.884 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 462/1000 --- L(Train): 0.0238303 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.241 1 + 0.554 value_reward_diff[t] + 1.992 reward_diff + -0.41 value_reward_diff^2 + 0.331 value_reward_diff*reward_diff + 1.886 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 463/1000 --- L(Train): 0.0237970 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.24 1 + 0.554 value_reward_diff[t] + 1.994 reward_diff + -0.41 value_reward_diff^2 + 0.329 value_reward_diff*reward_diff + 1.888 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 464/1000 --- L(Train): 0.0237659 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.238 1 + 0.554 value_reward_diff[t] + 1.996 reward_diff + -0.41 value_reward_diff^2 + 0.328 value_reward_diff*reward_diff + 1.89 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 465/1000 --- L(Train): 0.0237377 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.237 1 + 0.555 value_reward_diff[t] + 1.998 reward_diff + -0.409 value_reward_diff^2 + 0.326 value_reward_diff*reward_diff + 1.891 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 466/1000 --- L(Train): 0.0237069 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.236 1 + 0.555 value_reward_diff[t] + 2.001 reward_diff + -0.409 value_reward_diff^2 + 0.325 value_reward_diff*reward_diff + 1.893 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 467/1000 --- L(Train): 0.0236734 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.234 1 + 0.555 value_reward_diff[t] + 2.003 reward_diff + -0.409 value_reward_diff^2 + 0.323 value_reward_diff*reward_diff + 1.895 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 468/1000 --- L(Train): 0.0236460 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.233 1 + 0.555 value_reward_diff[t] + 2.005 reward_diff + -0.409 value_reward_diff^2 + 0.322 value_reward_diff*reward_diff + 1.897 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 469/1000 --- L(Train): 0.0236164 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.232 1 + 0.555 value_reward_diff[t] + 2.007 reward_diff + -0.408 value_reward_diff^2 + 0.32 value_reward_diff*reward_diff + 1.898 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 470/1000 --- L(Train): 0.0235854 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.23 1 + 0.555 value_reward_diff[t] + 2.009 reward_diff + -0.408 value_reward_diff^2 + 0.319 value_reward_diff*reward_diff + 1.9 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 471/1000 --- L(Train): 0.0235524 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.229 1 + 0.556 value_reward_diff[t] + 2.011 reward_diff + -0.408 value_reward_diff^2 + 0.317 value_reward_diff*reward_diff + 1.902 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 472/1000 --- L(Train): 0.0235264 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.228 1 + 0.556 value_reward_diff[t] + 2.014 reward_diff + -0.408 value_reward_diff^2 + 0.316 value_reward_diff*reward_diff + 1.904 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 473/1000 --- L(Train): 0.0235011 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.226 1 + 0.556 value_reward_diff[t] + 2.016 reward_diff + -0.407 value_reward_diff^2 + 0.314 value_reward_diff*reward_diff + 1.905 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 474/1000 --- L(Train): 0.0234735 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.225 1 + 0.556 value_reward_diff[t] + 2.018 reward_diff + -0.407 value_reward_diff^2 + 0.313 value_reward_diff*reward_diff + 1.907 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 475/1000 --- L(Train): 0.0234402 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.224 1 + 0.556 value_reward_diff[t] + 2.02 reward_diff + -0.407 value_reward_diff^2 + 0.311 value_reward_diff*reward_diff + 1.909 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 476/1000 --- L(Train): 0.0234140 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.222 1 + 0.557 value_reward_diff[t] + 2.022 reward_diff + -0.407 value_reward_diff^2 + 0.31 value_reward_diff*reward_diff + 1.91 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 477/1000 --- L(Train): 0.0233871 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.221 1 + 0.557 value_reward_diff[t] + 2.024 reward_diff + -0.406 value_reward_diff^2 + 0.308 value_reward_diff*reward_diff + 1.912 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 478/1000 --- L(Train): 0.0233593 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.22 1 + 0.557 value_reward_diff[t] + 2.026 reward_diff + -0.406 value_reward_diff^2 + 0.307 value_reward_diff*reward_diff + 1.914 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 479/1000 --- L(Train): 0.0233356 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.218 1 + 0.557 value_reward_diff[t] + 2.028 reward_diff + -0.406 value_reward_diff^2 + 0.305 value_reward_diff*reward_diff + 1.915 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 480/1000 --- L(Train): 0.0233087 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.217 1 + 0.557 value_reward_diff[t] + 2.03 reward_diff + -0.405 value_reward_diff^2 + 0.304 value_reward_diff*reward_diff + 1.917 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 481/1000 --- L(Train): 0.0232778 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.216 1 + 0.558 value_reward_diff[t] + 2.033 reward_diff + -0.405 value_reward_diff^2 + 0.303 value_reward_diff*reward_diff + 1.919 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 482/1000 --- L(Train): 0.0232497 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.214 1 + 0.558 value_reward_diff[t] + 2.035 reward_diff + -0.405 value_reward_diff^2 + 0.301 value_reward_diff*reward_diff + 1.92 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 483/1000 --- L(Train): 0.0232246 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.213 1 + 0.558 value_reward_diff[t] + 2.037 reward_diff + -0.405 value_reward_diff^2 + 0.3 value_reward_diff*reward_diff + 1.922 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 484/1000 --- L(Train): 0.0231962 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.212 1 + 0.558 value_reward_diff[t] + 2.039 reward_diff + -0.404 value_reward_diff^2 + 0.298 value_reward_diff*reward_diff + 1.923 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 485/1000 --- L(Train): 0.0231692 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.21 1 + 0.558 value_reward_diff[t] + 2.041 reward_diff + -0.404 value_reward_diff^2 + 0.297 value_reward_diff*reward_diff + 1.925 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 486/1000 --- L(Train): 0.0231444 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.209 1 + 0.558 value_reward_diff[t] + 2.043 reward_diff + -0.404 value_reward_diff^2 + 0.295 value_reward_diff*reward_diff + 1.927 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 487/1000 --- L(Train): 0.0231168 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.208 1 + 0.559 value_reward_diff[t] + 2.045 reward_diff + -0.404 value_reward_diff^2 + 0.294 value_reward_diff*reward_diff + 1.928 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 488/1000 --- L(Train): 0.0230919 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.207 1 + 0.559 value_reward_diff[t] + 2.047 reward_diff + -0.403 value_reward_diff^2 + 0.293 value_reward_diff*reward_diff + 1.93 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 489/1000 --- L(Train): 0.0230653 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.205 1 + 0.559 value_reward_diff[t] + 2.049 reward_diff + -0.403 value_reward_diff^2 + 0.291 value_reward_diff*reward_diff + 1.932 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 490/1000 --- L(Train): 0.0230377 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.204 1 + 0.559 value_reward_diff[t] + 2.051 reward_diff + -0.403 value_reward_diff^2 + 0.29 value_reward_diff*reward_diff + 1.933 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 491/1000 --- L(Train): 0.0230117 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.203 1 + 0.559 value_reward_diff[t] + 2.053 reward_diff + -0.403 value_reward_diff^2 + 0.288 value_reward_diff*reward_diff + 1.935 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 492/1000 --- L(Train): 0.0229880 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.202 1 + 0.559 value_reward_diff[t] + 2.055 reward_diff + -0.402 value_reward_diff^2 + 0.287 value_reward_diff*reward_diff + 1.936 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 493/1000 --- L(Train): 0.0229634 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.2 1 + 0.56 value_reward_diff[t] + 2.057 reward_diff + -0.402 value_reward_diff^2 + 0.286 value_reward_diff*reward_diff + 1.938 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 494/1000 --- L(Train): 0.0229496 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.199 1 + 0.56 value_reward_diff[t] + 2.059 reward_diff + -0.402 value_reward_diff^2 + 0.284 value_reward_diff*reward_diff + 1.939 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 495/1000 --- L(Train): 0.0229203 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.198 1 + 0.56 value_reward_diff[t] + 2.061 reward_diff + -0.402 value_reward_diff^2 + 0.283 value_reward_diff*reward_diff + 1.941 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 496/1000 --- L(Train): 0.0228975 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.197 1 + 0.56 value_reward_diff[t] + 2.063 reward_diff + -0.401 value_reward_diff^2 + 0.282 value_reward_diff*reward_diff + 1.943 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 497/1000 --- L(Train): 0.0228747 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.195 1 + 0.56 value_reward_diff[t] + 2.065 reward_diff + -0.401 value_reward_diff^2 + 0.28 value_reward_diff*reward_diff + 1.944 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 498/1000 --- L(Train): 0.0228501 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.194 1 + 0.56 value_reward_diff[t] + 2.067 reward_diff + -0.401 value_reward_diff^2 + 0.279 value_reward_diff*reward_diff + 1.946 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 499/1000 --- L(Train): 0.0228203 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.193 1 + 0.561 value_reward_diff[t] + 2.069 reward_diff + -0.401 value_reward_diff^2 + 0.277 value_reward_diff*reward_diff + 1.947 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 500/1000 --- L(Train): 0.0228001 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.192 1 + 0.561 value_reward_diff[t] + 2.071 reward_diff + -0.4 value_reward_diff^2 + 0.276 value_reward_diff*reward_diff + 1.949 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 501/1000 --- L(Train): 0.0227759 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.19 1 + 0.561 value_reward_diff[t] + 2.073 reward_diff + -0.4 value_reward_diff^2 + 0.275 value_reward_diff*reward_diff + 1.95 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 502/1000 --- L(Train): 0.0227491 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.189 1 + 0.561 value_reward_diff[t] + 2.075 reward_diff + -0.4 value_reward_diff^2 + 0.273 value_reward_diff*reward_diff + 1.952 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 503/1000 --- L(Train): 0.0227238 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.188 1 + 0.561 value_reward_diff[t] + 2.077 reward_diff + -0.4 value_reward_diff^2 + 0.272 value_reward_diff*reward_diff + 1.953 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 504/1000 --- L(Train): 0.0227051 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.187 1 + 0.561 value_reward_diff[t] + 2.079 reward_diff + -0.399 value_reward_diff^2 + 0.271 value_reward_diff*reward_diff + 1.955 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 505/1000 --- L(Train): 0.0226816 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.186 1 + 0.562 value_reward_diff[t] + 2.081 reward_diff + -0.399 value_reward_diff^2 + 0.269 value_reward_diff*reward_diff + 1.956 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 506/1000 --- L(Train): 0.0226594 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.184 1 + 0.562 value_reward_diff[t] + 2.083 reward_diff + -0.399 value_reward_diff^2 + 0.268 value_reward_diff*reward_diff + 1.958 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 507/1000 --- L(Train): 0.0226331 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.183 1 + 0.562 value_reward_diff[t] + 2.085 reward_diff + -0.399 value_reward_diff^2 + 0.267 value_reward_diff*reward_diff + 1.959 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 508/1000 --- L(Train): 0.0226098 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.182 1 + 0.562 value_reward_diff[t] + 2.087 reward_diff + -0.398 value_reward_diff^2 + 0.265 value_reward_diff*reward_diff + 1.961 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 509/1000 --- L(Train): 0.0225890 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.181 1 + 0.562 value_reward_diff[t] + 2.089 reward_diff + -0.398 value_reward_diff^2 + 0.264 value_reward_diff*reward_diff + 1.962 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 510/1000 --- L(Train): 0.0225690 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.18 1 + 0.562 value_reward_diff[t] + 2.091 reward_diff + -0.398 value_reward_diff^2 + 0.263 value_reward_diff*reward_diff + 1.964 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 511/1000 --- L(Train): 0.0225424 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.178 1 + 0.562 value_reward_diff[t] + 2.093 reward_diff + -0.398 value_reward_diff^2 + 0.261 value_reward_diff*reward_diff + 1.965 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 512/1000 --- L(Train): 0.0225207 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.177 1 + 0.563 value_reward_diff[t] + 2.094 reward_diff + -0.397 value_reward_diff^2 + 0.26 value_reward_diff*reward_diff + 1.966 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 513/1000 --- L(Train): 0.0225000 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.176 1 + 0.563 value_reward_diff[t] + 2.096 reward_diff + -0.397 value_reward_diff^2 + 0.259 value_reward_diff*reward_diff + 1.968 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 514/1000 --- L(Train): 0.0224799 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.175 1 + 0.563 value_reward_diff[t] + 2.098 reward_diff + -0.397 value_reward_diff^2 + 0.258 value_reward_diff*reward_diff + 1.969 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 515/1000 --- L(Train): 0.0224599 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.174 1 + 0.563 value_reward_diff[t] + 2.1 reward_diff + -0.397 value_reward_diff^2 + 0.256 value_reward_diff*reward_diff + 1.971 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 516/1000 --- L(Train): 0.0224368 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.172 1 + 0.563 value_reward_diff[t] + 2.102 reward_diff + -0.396 value_reward_diff^2 + 0.255 value_reward_diff*reward_diff + 1.972 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 517/1000 --- L(Train): 0.0224116 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.171 1 + 0.563 value_reward_diff[t] + 2.104 reward_diff + -0.396 value_reward_diff^2 + 0.254 value_reward_diff*reward_diff + 1.974 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 518/1000 --- L(Train): 0.0223907 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.17 1 + 0.563 value_reward_diff[t] + 2.106 reward_diff + -0.396 value_reward_diff^2 + 0.252 value_reward_diff*reward_diff + 1.975 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 519/1000 --- L(Train): 0.0223701 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.169 1 + 0.564 value_reward_diff[t] + 2.108 reward_diff + -0.396 value_reward_diff^2 + 0.251 value_reward_diff*reward_diff + 1.976 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 520/1000 --- L(Train): 0.0223496 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.168 1 + 0.564 value_reward_diff[t] + 2.11 reward_diff + -0.396 value_reward_diff^2 + 0.25 value_reward_diff*reward_diff + 1.978 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 521/1000 --- L(Train): 0.0223289 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.167 1 + 0.564 value_reward_diff[t] + 2.111 reward_diff + -0.395 value_reward_diff^2 + 0.249 value_reward_diff*reward_diff + 1.979 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 522/1000 --- L(Train): 0.0223048 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.166 1 + 0.564 value_reward_diff[t] + 2.113 reward_diff + -0.395 value_reward_diff^2 + 0.247 value_reward_diff*reward_diff + 1.981 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 523/1000 --- L(Train): 0.0222994 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.164 1 + 0.564 value_reward_diff[t] + 2.115 reward_diff + -0.395 value_reward_diff^2 + 0.246 value_reward_diff*reward_diff + 1.982 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 524/1000 --- L(Train): 0.0222740 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.163 1 + 0.564 value_reward_diff[t] + 2.117 reward_diff + -0.395 value_reward_diff^2 + 0.245 value_reward_diff*reward_diff + 1.983 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 525/1000 --- L(Train): 0.0222475 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.162 1 + 0.564 value_reward_diff[t] + 2.119 reward_diff + -0.394 value_reward_diff^2 + 0.244 value_reward_diff*reward_diff + 1.985 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 526/1000 --- L(Train): 0.0222222 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.161 1 + 0.565 value_reward_diff[t] + 2.121 reward_diff + -0.394 value_reward_diff^2 + 0.242 value_reward_diff*reward_diff + 1.986 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 527/1000 --- L(Train): 0.0222004 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.16 1 + 0.565 value_reward_diff[t] + 2.123 reward_diff + -0.394 value_reward_diff^2 + 0.241 value_reward_diff*reward_diff + 1.987 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 528/1000 --- L(Train): 0.0221885 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.159 1 + 0.565 value_reward_diff[t] + 2.124 reward_diff + -0.394 value_reward_diff^2 + 0.24 value_reward_diff*reward_diff + 1.989 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 529/1000 --- L(Train): 0.0221713 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.158 1 + 0.565 value_reward_diff[t] + 2.126 reward_diff + -0.393 value_reward_diff^2 + 0.239 value_reward_diff*reward_diff + 1.99 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 530/1000 --- L(Train): 0.0221477 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.157 1 + 0.565 value_reward_diff[t] + 2.128 reward_diff + -0.393 value_reward_diff^2 + 0.238 value_reward_diff*reward_diff + 1.991 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 531/1000 --- L(Train): 0.0221238 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.155 1 + 0.565 value_reward_diff[t] + 2.13 reward_diff + -0.393 value_reward_diff^2 + 0.236 value_reward_diff*reward_diff + 1.993 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 532/1000 --- L(Train): 0.0221020 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.154 1 + 0.565 value_reward_diff[t] + 2.132 reward_diff + -0.393 value_reward_diff^2 + 0.235 value_reward_diff*reward_diff + 1.994 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 533/1000 --- L(Train): 0.0220810 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.153 1 + 0.566 value_reward_diff[t] + 2.133 reward_diff + -0.392 value_reward_diff^2 + 0.234 value_reward_diff*reward_diff + 1.995 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 534/1000 --- L(Train): 0.0220608 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.152 1 + 0.566 value_reward_diff[t] + 2.135 reward_diff + -0.392 value_reward_diff^2 + 0.233 value_reward_diff*reward_diff + 1.997 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 535/1000 --- L(Train): 0.0220408 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.151 1 + 0.566 value_reward_diff[t] + 2.137 reward_diff + -0.392 value_reward_diff^2 + 0.232 value_reward_diff*reward_diff + 1.998 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 536/1000 --- L(Train): 0.0220238 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.15 1 + 0.566 value_reward_diff[t] + 2.139 reward_diff + -0.392 value_reward_diff^2 + 0.23 value_reward_diff*reward_diff + 1.999 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 537/1000 --- L(Train): 0.0220034 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.149 1 + 0.566 value_reward_diff[t] + 2.141 reward_diff + -0.392 value_reward_diff^2 + 0.229 value_reward_diff*reward_diff + 2.0 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 538/1000 --- L(Train): 0.0219825 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.148 1 + 0.566 value_reward_diff[t] + 2.142 reward_diff + -0.391 value_reward_diff^2 + 0.228 value_reward_diff*reward_diff + 2.002 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 539/1000 --- L(Train): 0.0219657 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.147 1 + 0.566 value_reward_diff[t] + 2.144 reward_diff + -0.391 value_reward_diff^2 + 0.227 value_reward_diff*reward_diff + 2.003 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 540/1000 --- L(Train): 0.0219492 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.146 1 + 0.566 value_reward_diff[t] + 2.146 reward_diff + -0.391 value_reward_diff^2 + 0.226 value_reward_diff*reward_diff + 2.004 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 541/1000 --- L(Train): 0.0219261 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.145 1 + 0.567 value_reward_diff[t] + 2.148 reward_diff + -0.391 value_reward_diff^2 + 0.224 value_reward_diff*reward_diff + 2.006 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 542/1000 --- L(Train): 0.0219068 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.143 1 + 0.567 value_reward_diff[t] + 2.149 reward_diff + -0.39 value_reward_diff^2 + 0.223 value_reward_diff*reward_diff + 2.007 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 543/1000 --- L(Train): 0.0218888 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.142 1 + 0.567 value_reward_diff[t] + 2.151 reward_diff + -0.39 value_reward_diff^2 + 0.222 value_reward_diff*reward_diff + 2.008 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 544/1000 --- L(Train): 0.0218708 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.141 1 + 0.567 value_reward_diff[t] + 2.153 reward_diff + -0.39 value_reward_diff^2 + 0.221 value_reward_diff*reward_diff + 2.009 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 545/1000 --- L(Train): 0.0218530 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.14 1 + 0.567 value_reward_diff[t] + 2.155 reward_diff + -0.39 value_reward_diff^2 + 0.22 value_reward_diff*reward_diff + 2.011 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 546/1000 --- L(Train): 0.0218359 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.139 1 + 0.567 value_reward_diff[t] + 2.156 reward_diff + -0.39 value_reward_diff^2 + 0.219 value_reward_diff*reward_diff + 2.012 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 547/1000 --- L(Train): 0.0218128 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.138 1 + 0.567 value_reward_diff[t] + 2.158 reward_diff + -0.389 value_reward_diff^2 + 0.217 value_reward_diff*reward_diff + 2.013 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 548/1000 --- L(Train): 0.0217947 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.137 1 + 0.567 value_reward_diff[t] + 2.16 reward_diff + -0.389 value_reward_diff^2 + 0.216 value_reward_diff*reward_diff + 2.014 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 549/1000 --- L(Train): 0.0217791 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.136 1 + 0.568 value_reward_diff[t] + 2.161 reward_diff + -0.389 value_reward_diff^2 + 0.215 value_reward_diff*reward_diff + 2.015 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 550/1000 --- L(Train): 0.0217599 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.135 1 + 0.568 value_reward_diff[t] + 2.163 reward_diff + -0.389 value_reward_diff^2 + 0.214 value_reward_diff*reward_diff + 2.017 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 551/1000 --- L(Train): 0.0217377 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.134 1 + 0.568 value_reward_diff[t] + 2.165 reward_diff + -0.388 value_reward_diff^2 + 0.213 value_reward_diff*reward_diff + 2.018 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 552/1000 --- L(Train): 0.0217218 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.133 1 + 0.568 value_reward_diff[t] + 2.167 reward_diff + -0.388 value_reward_diff^2 + 0.212 value_reward_diff*reward_diff + 2.019 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 553/1000 --- L(Train): 0.0217053 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.132 1 + 0.568 value_reward_diff[t] + 2.168 reward_diff + -0.388 value_reward_diff^2 + 0.211 value_reward_diff*reward_diff + 2.02 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 554/1000 --- L(Train): 0.0216894 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.131 1 + 0.568 value_reward_diff[t] + 2.17 reward_diff + -0.388 value_reward_diff^2 + 0.209 value_reward_diff*reward_diff + 2.021 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 555/1000 --- L(Train): 0.0216700 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.13 1 + 0.568 value_reward_diff[t] + 2.172 reward_diff + -0.388 value_reward_diff^2 + 0.208 value_reward_diff*reward_diff + 2.023 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 556/1000 --- L(Train): 0.0216536 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.129 1 + 0.568 value_reward_diff[t] + 2.173 reward_diff + -0.387 value_reward_diff^2 + 0.207 value_reward_diff*reward_diff + 2.024 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 557/1000 --- L(Train): 0.0216376 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.128 1 + 0.568 value_reward_diff[t] + 2.175 reward_diff + -0.387 value_reward_diff^2 + 0.206 value_reward_diff*reward_diff + 2.025 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 558/1000 --- L(Train): 0.0216192 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.127 1 + 0.569 value_reward_diff[t] + 2.177 reward_diff + -0.387 value_reward_diff^2 + 0.205 value_reward_diff*reward_diff + 2.026 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 559/1000 --- L(Train): 0.0215991 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.126 1 + 0.569 value_reward_diff[t] + 2.178 reward_diff + -0.387 value_reward_diff^2 + 0.204 value_reward_diff*reward_diff + 2.027 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 560/1000 --- L(Train): 0.0215800 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.125 1 + 0.569 value_reward_diff[t] + 2.18 reward_diff + -0.386 value_reward_diff^2 + 0.203 value_reward_diff*reward_diff + 2.028 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 561/1000 --- L(Train): 0.0215637 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.124 1 + 0.569 value_reward_diff[t] + 2.182 reward_diff + -0.386 value_reward_diff^2 + 0.202 value_reward_diff*reward_diff + 2.03 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 562/1000 --- L(Train): 0.0215488 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.123 1 + 0.569 value_reward_diff[t] + 2.183 reward_diff + -0.386 value_reward_diff^2 + 0.201 value_reward_diff*reward_diff + 2.031 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 563/1000 --- L(Train): 0.0215338 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.122 1 + 0.569 value_reward_diff[t] + 2.185 reward_diff + -0.386 value_reward_diff^2 + 0.2 value_reward_diff*reward_diff + 2.032 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 564/1000 --- L(Train): 0.0215176 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.121 1 + 0.569 value_reward_diff[t] + 2.187 reward_diff + -0.386 value_reward_diff^2 + 0.199 value_reward_diff*reward_diff + 2.033 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 565/1000 --- L(Train): 0.0214994 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.12 1 + 0.569 value_reward_diff[t] + 2.188 reward_diff + -0.385 value_reward_diff^2 + 0.197 value_reward_diff*reward_diff + 2.034 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 566/1000 --- L(Train): 0.0214835 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.119 1 + 0.569 value_reward_diff[t] + 2.19 reward_diff + -0.385 value_reward_diff^2 + 0.196 value_reward_diff*reward_diff + 2.035 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 567/1000 --- L(Train): 0.0214647 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.118 1 + 0.57 value_reward_diff[t] + 2.192 reward_diff + -0.385 value_reward_diff^2 + 0.195 value_reward_diff*reward_diff + 2.036 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 568/1000 --- L(Train): 0.0214515 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.117 1 + 0.57 value_reward_diff[t] + 2.193 reward_diff + -0.385 value_reward_diff^2 + 0.194 value_reward_diff*reward_diff + 2.037 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 569/1000 --- L(Train): 0.0214368 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.116 1 + 0.57 value_reward_diff[t] + 2.195 reward_diff + -0.385 value_reward_diff^2 + 0.193 value_reward_diff*reward_diff + 2.039 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 570/1000 --- L(Train): 0.0214210 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.115 1 + 0.57 value_reward_diff[t] + 2.196 reward_diff + -0.384 value_reward_diff^2 + 0.192 value_reward_diff*reward_diff + 2.04 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 571/1000 --- L(Train): 0.0214028 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.114 1 + 0.57 value_reward_diff[t] + 2.198 reward_diff + -0.384 value_reward_diff^2 + 0.191 value_reward_diff*reward_diff + 2.041 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 572/1000 --- L(Train): 0.0213831 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.113 1 + 0.57 value_reward_diff[t] + 2.2 reward_diff + -0.384 value_reward_diff^2 + 0.19 value_reward_diff*reward_diff + 2.042 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 573/1000 --- L(Train): 0.0213661 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.112 1 + 0.57 value_reward_diff[t] + 2.201 reward_diff + -0.384 value_reward_diff^2 + 0.189 value_reward_diff*reward_diff + 2.043 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 574/1000 --- L(Train): 0.0213504 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.111 1 + 0.57 value_reward_diff[t] + 2.203 reward_diff + -0.384 value_reward_diff^2 + 0.188 value_reward_diff*reward_diff + 2.044 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 575/1000 --- L(Train): 0.0213362 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.11 1 + 0.57 value_reward_diff[t] + 2.204 reward_diff + -0.383 value_reward_diff^2 + 0.187 value_reward_diff*reward_diff + 2.045 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 576/1000 --- L(Train): 0.0213208 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.109 1 + 0.57 value_reward_diff[t] + 2.206 reward_diff + -0.383 value_reward_diff^2 + 0.186 value_reward_diff*reward_diff + 2.046 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 577/1000 --- L(Train): 0.0213066 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.108 1 + 0.571 value_reward_diff[t] + 2.207 reward_diff + -0.383 value_reward_diff^2 + 0.185 value_reward_diff*reward_diff + 2.047 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 578/1000 --- L(Train): 0.0212914 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.107 1 + 0.571 value_reward_diff[t] + 2.209 reward_diff + -0.383 value_reward_diff^2 + 0.184 value_reward_diff*reward_diff + 2.048 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 579/1000 --- L(Train): 0.0212731 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.106 1 + 0.571 value_reward_diff[t] + 2.211 reward_diff + -0.382 value_reward_diff^2 + 0.183 value_reward_diff*reward_diff + 2.049 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 580/1000 --- L(Train): 0.0212611 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.105 1 + 0.571 value_reward_diff[t] + 2.212 reward_diff + -0.382 value_reward_diff^2 + 0.182 value_reward_diff*reward_diff + 2.05 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 581/1000 --- L(Train): 0.0212457 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.105 1 + 0.571 value_reward_diff[t] + 2.214 reward_diff + -0.382 value_reward_diff^2 + 0.181 value_reward_diff*reward_diff + 2.051 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 582/1000 --- L(Train): 0.0212308 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.104 1 + 0.571 value_reward_diff[t] + 2.215 reward_diff + -0.382 value_reward_diff^2 + 0.18 value_reward_diff*reward_diff + 2.052 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 583/1000 --- L(Train): 0.0212134 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.103 1 + 0.571 value_reward_diff[t] + 2.217 reward_diff + -0.382 value_reward_diff^2 + 0.179 value_reward_diff*reward_diff + 2.053 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 584/1000 --- L(Train): 0.0212020 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.102 1 + 0.571 value_reward_diff[t] + 2.218 reward_diff + -0.381 value_reward_diff^2 + 0.178 value_reward_diff*reward_diff + 2.054 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 585/1000 --- L(Train): 0.0211881 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.101 1 + 0.571 value_reward_diff[t] + 2.22 reward_diff + -0.381 value_reward_diff^2 + 0.177 value_reward_diff*reward_diff + 2.055 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 586/1000 --- L(Train): 0.0211729 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.1 1 + 0.571 value_reward_diff[t] + 2.221 reward_diff + -0.381 value_reward_diff^2 + 0.176 value_reward_diff*reward_diff + 2.056 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 587/1000 --- L(Train): 0.0211563 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.099 1 + 0.571 value_reward_diff[t] + 2.223 reward_diff + -0.381 value_reward_diff^2 + 0.175 value_reward_diff*reward_diff + 2.057 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 588/1000 --- L(Train): 0.0211406 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.098 1 + 0.572 value_reward_diff[t] + 2.224 reward_diff + -0.381 value_reward_diff^2 + 0.174 value_reward_diff*reward_diff + 2.058 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 589/1000 --- L(Train): 0.0211252 --- L(Val, SINDy): 0.0000000 --- Time: 0.11s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.097 1 + 0.572 value_reward_diff[t] + 2.226 reward_diff + -0.38 value_reward_diff^2 + 0.173 value_reward_diff*reward_diff + 2.059 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 590/1000 --- L(Train): 0.0211113 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.096 1 + 0.572 value_reward_diff[t] + 2.227 reward_diff + -0.38 value_reward_diff^2 + 0.172 value_reward_diff*reward_diff + 2.06 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 591/1000 --- L(Train): 0.0210954 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.095 1 + 0.572 value_reward_diff[t] + 2.229 reward_diff + -0.38 value_reward_diff^2 + 0.171 value_reward_diff*reward_diff + 2.061 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 592/1000 --- L(Train): 0.0210802 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.095 1 + 0.572 value_reward_diff[t] + 2.23 reward_diff + -0.38 value_reward_diff^2 + 0.17 value_reward_diff*reward_diff + 2.062 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 593/1000 --- L(Train): 0.0210649 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.094 1 + 0.572 value_reward_diff[t] + 2.232 reward_diff + -0.38 value_reward_diff^2 + 0.169 value_reward_diff*reward_diff + 2.063 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 594/1000 --- L(Train): 0.0210497 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.093 1 + 0.572 value_reward_diff[t] + 2.233 reward_diff + -0.379 value_reward_diff^2 + 0.168 value_reward_diff*reward_diff + 2.064 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 595/1000 --- L(Train): 0.0210322 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.092 1 + 0.572 value_reward_diff[t] + 2.235 reward_diff + -0.379 value_reward_diff^2 + 0.167 value_reward_diff*reward_diff + 2.065 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 596/1000 --- L(Train): 0.0210198 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.091 1 + 0.572 value_reward_diff[t] + 2.236 reward_diff + -0.379 value_reward_diff^2 + 0.166 value_reward_diff*reward_diff + 2.066 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 597/1000 --- L(Train): 0.0210051 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.09 1 + 0.572 value_reward_diff[t] + 2.238 reward_diff + -0.379 value_reward_diff^2 + 0.165 value_reward_diff*reward_diff + 2.067 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 598/1000 --- L(Train): 0.0209996 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.089 1 + 0.572 value_reward_diff[t] + 2.239 reward_diff + -0.379 value_reward_diff^2 + 0.164 value_reward_diff*reward_diff + 2.068 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 599/1000 --- L(Train): 0.0209910 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.088 1 + 0.572 value_reward_diff[t] + 2.241 reward_diff + -0.378 value_reward_diff^2 + 0.163 value_reward_diff*reward_diff + 2.069 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 600/1000 --- L(Train): 0.0209796 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.087 1 + 0.573 value_reward_diff[t] + 2.242 reward_diff + -0.378 value_reward_diff^2 + 0.162 value_reward_diff*reward_diff + 2.07 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 601/1000 --- L(Train): 0.0209614 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.087 1 + 0.573 value_reward_diff[t] + 2.244 reward_diff + -0.378 value_reward_diff^2 + 0.161 value_reward_diff*reward_diff + 2.071 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 602/1000 --- L(Train): 0.0209456 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.086 1 + 0.573 value_reward_diff[t] + 2.245 reward_diff + -0.378 value_reward_diff^2 + 0.16 value_reward_diff*reward_diff + 2.071 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 603/1000 --- L(Train): 0.0209339 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.085 1 + 0.573 value_reward_diff[t] + 2.247 reward_diff + -0.378 value_reward_diff^2 + 0.159 value_reward_diff*reward_diff + 2.072 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 604/1000 --- L(Train): 0.0209212 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.084 1 + 0.573 value_reward_diff[t] + 2.248 reward_diff + -0.378 value_reward_diff^2 + 0.159 value_reward_diff*reward_diff + 2.073 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 605/1000 --- L(Train): 0.0209098 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.083 1 + 0.573 value_reward_diff[t] + 2.25 reward_diff + -0.377 value_reward_diff^2 + 0.158 value_reward_diff*reward_diff + 2.074 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 606/1000 --- L(Train): 0.0208966 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.082 1 + 0.573 value_reward_diff[t] + 2.251 reward_diff + -0.377 value_reward_diff^2 + 0.157 value_reward_diff*reward_diff + 2.075 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 607/1000 --- L(Train): 0.0208782 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.082 1 + 0.573 value_reward_diff[t] + 2.252 reward_diff + -0.377 value_reward_diff^2 + 0.156 value_reward_diff*reward_diff + 2.076 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 608/1000 --- L(Train): 0.0208689 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.081 1 + 0.573 value_reward_diff[t] + 2.254 reward_diff + -0.377 value_reward_diff^2 + 0.155 value_reward_diff*reward_diff + 2.077 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 609/1000 --- L(Train): 0.0208576 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.08 1 + 0.573 value_reward_diff[t] + 2.255 reward_diff + -0.377 value_reward_diff^2 + 0.154 value_reward_diff*reward_diff + 2.078 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 610/1000 --- L(Train): 0.0208414 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.079 1 + 0.573 value_reward_diff[t] + 2.257 reward_diff + -0.376 value_reward_diff^2 + 0.153 value_reward_diff*reward_diff + 2.078 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 611/1000 --- L(Train): 0.0208262 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.078 1 + 0.573 value_reward_diff[t] + 2.258 reward_diff + -0.376 value_reward_diff^2 + 0.152 value_reward_diff*reward_diff + 2.079 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 612/1000 --- L(Train): 0.0208183 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.077 1 + 0.573 value_reward_diff[t] + 2.26 reward_diff + -0.376 value_reward_diff^2 + 0.151 value_reward_diff*reward_diff + 2.08 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 613/1000 --- L(Train): 0.0208035 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.077 1 + 0.573 value_reward_diff[t] + 2.261 reward_diff + -0.376 value_reward_diff^2 + 0.15 value_reward_diff*reward_diff + 2.081 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 614/1000 --- L(Train): 0.0207876 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.076 1 + 0.574 value_reward_diff[t] + 2.262 reward_diff + -0.376 value_reward_diff^2 + 0.149 value_reward_diff*reward_diff + 2.082 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 615/1000 --- L(Train): 0.0207746 --- L(Val, SINDy): 0.0000000 --- Time: 0.11s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.075 1 + 0.574 value_reward_diff[t] + 2.264 reward_diff + -0.375 value_reward_diff^2 + 0.149 value_reward_diff*reward_diff + 2.083 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 616/1000 --- L(Train): 0.0207635 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.074 1 + 0.574 value_reward_diff[t] + 2.265 reward_diff + -0.375 value_reward_diff^2 + 0.148 value_reward_diff*reward_diff + 2.084 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 617/1000 --- L(Train): 0.0207515 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.073 1 + 0.574 value_reward_diff[t] + 2.267 reward_diff + -0.375 value_reward_diff^2 + 0.147 value_reward_diff*reward_diff + 2.084 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 618/1000 --- L(Train): 0.0207394 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.072 1 + 0.574 value_reward_diff[t] + 2.268 reward_diff + -0.375 value_reward_diff^2 + 0.146 value_reward_diff*reward_diff + 2.085 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 619/1000 --- L(Train): 0.0207237 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.072 1 + 0.574 value_reward_diff[t] + 2.269 reward_diff + -0.375 value_reward_diff^2 + 0.145 value_reward_diff*reward_diff + 2.086 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 620/1000 --- L(Train): 0.0207109 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.071 1 + 0.574 value_reward_diff[t] + 2.271 reward_diff + -0.374 value_reward_diff^2 + 0.144 value_reward_diff*reward_diff + 2.087 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 621/1000 --- L(Train): 0.0206983 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.07 1 + 0.574 value_reward_diff[t] + 2.272 reward_diff + -0.374 value_reward_diff^2 + 0.143 value_reward_diff*reward_diff + 2.088 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 622/1000 --- L(Train): 0.0206834 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.069 1 + 0.574 value_reward_diff[t] + 2.273 reward_diff + -0.374 value_reward_diff^2 + 0.142 value_reward_diff*reward_diff + 2.088 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 623/1000 --- L(Train): 0.0206736 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.068 1 + 0.574 value_reward_diff[t] + 2.275 reward_diff + -0.374 value_reward_diff^2 + 0.142 value_reward_diff*reward_diff + 2.089 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 624/1000 --- L(Train): 0.0206638 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.068 1 + 0.574 value_reward_diff[t] + 2.276 reward_diff + -0.374 value_reward_diff^2 + 0.141 value_reward_diff*reward_diff + 2.09 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 625/1000 --- L(Train): 0.0206502 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.067 1 + 0.574 value_reward_diff[t] + 2.277 reward_diff + -0.374 value_reward_diff^2 + 0.14 value_reward_diff*reward_diff + 2.091 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 626/1000 --- L(Train): 0.0206423 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.066 1 + 0.574 value_reward_diff[t] + 2.279 reward_diff + -0.373 value_reward_diff^2 + 0.139 value_reward_diff*reward_diff + 2.092 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 627/1000 --- L(Train): 0.0206308 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.065 1 + 0.574 value_reward_diff[t] + 2.28 reward_diff + -0.373 value_reward_diff^2 + 0.138 value_reward_diff*reward_diff + 2.092 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 628/1000 --- L(Train): 0.0206159 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.065 1 + 0.574 value_reward_diff[t] + 2.281 reward_diff + -0.373 value_reward_diff^2 + 0.137 value_reward_diff*reward_diff + 2.093 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 629/1000 --- L(Train): 0.0206035 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.064 1 + 0.574 value_reward_diff[t] + 2.283 reward_diff + -0.373 value_reward_diff^2 + 0.137 value_reward_diff*reward_diff + 2.094 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 630/1000 --- L(Train): 0.0205917 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.063 1 + 0.575 value_reward_diff[t] + 2.284 reward_diff + -0.373 value_reward_diff^2 + 0.136 value_reward_diff*reward_diff + 2.095 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 631/1000 --- L(Train): 0.0205760 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.062 1 + 0.575 value_reward_diff[t] + 2.285 reward_diff + -0.372 value_reward_diff^2 + 0.135 value_reward_diff*reward_diff + 2.095 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 632/1000 --- L(Train): 0.0205649 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.061 1 + 0.575 value_reward_diff[t] + 2.287 reward_diff + -0.372 value_reward_diff^2 + 0.134 value_reward_diff*reward_diff + 2.096 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 633/1000 --- L(Train): 0.0205567 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.061 1 + 0.575 value_reward_diff[t] + 2.288 reward_diff + -0.372 value_reward_diff^2 + 0.133 value_reward_diff*reward_diff + 2.097 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 634/1000 --- L(Train): 0.0205450 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.06 1 + 0.575 value_reward_diff[t] + 2.289 reward_diff + -0.372 value_reward_diff^2 + 0.132 value_reward_diff*reward_diff + 2.098 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 635/1000 --- L(Train): 0.0205300 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.059 1 + 0.575 value_reward_diff[t] + 2.291 reward_diff + -0.372 value_reward_diff^2 + 0.132 value_reward_diff*reward_diff + 2.098 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 636/1000 --- L(Train): 0.0205198 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.058 1 + 0.575 value_reward_diff[t] + 2.292 reward_diff + -0.372 value_reward_diff^2 + 0.131 value_reward_diff*reward_diff + 2.099 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 637/1000 --- L(Train): 0.0205086 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.058 1 + 0.575 value_reward_diff[t] + 2.293 reward_diff + -0.371 value_reward_diff^2 + 0.13 value_reward_diff*reward_diff + 2.1 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 638/1000 --- L(Train): 0.0204991 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.057 1 + 0.575 value_reward_diff[t] + 2.295 reward_diff + -0.371 value_reward_diff^2 + 0.129 value_reward_diff*reward_diff + 2.101 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 639/1000 --- L(Train): 0.0204847 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.056 1 + 0.575 value_reward_diff[t] + 2.296 reward_diff + -0.371 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 2.101 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 640/1000 --- L(Train): 0.0204764 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.055 1 + 0.575 value_reward_diff[t] + 2.297 reward_diff + -0.371 value_reward_diff^2 + 0.128 value_reward_diff*reward_diff + 2.102 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 641/1000 --- L(Train): 0.0204677 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.055 1 + 0.575 value_reward_diff[t] + 2.298 reward_diff + -0.371 value_reward_diff^2 + 0.127 value_reward_diff*reward_diff + 2.103 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 642/1000 --- L(Train): 0.0204559 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.054 1 + 0.575 value_reward_diff[t] + 2.3 reward_diff + -0.371 value_reward_diff^2 + 0.126 value_reward_diff*reward_diff + 2.103 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 643/1000 --- L(Train): 0.0204430 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.053 1 + 0.575 value_reward_diff[t] + 2.301 reward_diff + -0.37 value_reward_diff^2 + 0.125 value_reward_diff*reward_diff + 2.104 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 644/1000 --- L(Train): 0.0204297 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.052 1 + 0.575 value_reward_diff[t] + 2.302 reward_diff + -0.37 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 2.105 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 645/1000 --- L(Train): 0.0204194 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.052 1 + 0.575 value_reward_diff[t] + 2.304 reward_diff + -0.37 value_reward_diff^2 + 0.124 value_reward_diff*reward_diff + 2.105 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 646/1000 --- L(Train): 0.0204104 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.051 1 + 0.575 value_reward_diff[t] + 2.305 reward_diff + -0.37 value_reward_diff^2 + 0.123 value_reward_diff*reward_diff + 2.106 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 647/1000 --- L(Train): 0.0204052 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 8):\n",
      "value_reward_diff[t+1] = 0.05 1 + 0.575 value_reward_diff[t] + 2.306 reward_diff + -0.37 value_reward_diff^2 + 0.122 value_reward_diff*reward_diff + 2.107 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 0, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 648/1000 --- L(Train): 0.0204015 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.05 1 + 0.575 value_reward_diff[t] + 2.307 reward_diff + -0.37 value_reward_diff^2 + 0.121 value_reward_diff*reward_diff + 2.107 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 1, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 649/1000 --- L(Train): 0.0203879 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.049 1 + 0.575 value_reward_diff[t] + 2.309 reward_diff + -0.369 value_reward_diff^2 + 0.121 value_reward_diff*reward_diff + 2.108 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 2, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 650/1000 --- L(Train): 0.0203752 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.048 1 + 0.575 value_reward_diff[t] + 2.31 reward_diff + -0.369 value_reward_diff^2 + 0.12 value_reward_diff*reward_diff + 2.109 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 3, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 651/1000 --- L(Train): 0.0203585 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.047 1 + 0.575 value_reward_diff[t] + 2.311 reward_diff + -0.369 value_reward_diff^2 + 0.119 value_reward_diff*reward_diff + 2.109 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 4, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 652/1000 --- L(Train): 0.0203500 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.047 1 + 0.576 value_reward_diff[t] + 2.312 reward_diff + -0.369 value_reward_diff^2 + 0.118 value_reward_diff*reward_diff + 2.11 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 5, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 653/1000 --- L(Train): 0.0203372 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.046 1 + 0.576 value_reward_diff[t] + 2.313 reward_diff + -0.369 value_reward_diff^2 + 0.118 value_reward_diff*reward_diff + 2.111 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 6, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 654/1000 --- L(Train): 0.0203286 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.045 1 + 0.576 value_reward_diff[t] + 2.315 reward_diff + -0.368 value_reward_diff^2 + 0.117 value_reward_diff*reward_diff + 2.111 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 7, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 655/1000 --- L(Train): 0.0203194 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.045 1 + 0.576 value_reward_diff[t] + 2.316 reward_diff + -0.368 value_reward_diff^2 + 0.116 value_reward_diff*reward_diff + 2.112 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 8, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 656/1000 --- L(Train): 0.0203119 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.044 1 + 0.576 value_reward_diff[t] + 2.317 reward_diff + -0.368 value_reward_diff^2 + 0.115 value_reward_diff*reward_diff + 2.113 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 9, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 657/1000 --- L(Train): 0.0202976 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.043 1 + 0.576 value_reward_diff[t] + 2.318 reward_diff + -0.368 value_reward_diff^2 + 0.115 value_reward_diff*reward_diff + 2.113 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 10, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 658/1000 --- L(Train): 0.0202853 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.043 1 + 0.576 value_reward_diff[t] + 2.32 reward_diff + -0.368 value_reward_diff^2 + 0.114 value_reward_diff*reward_diff + 2.114 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 11, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 659/1000 --- L(Train): 0.0202768 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.042 1 + 0.576 value_reward_diff[t] + 2.321 reward_diff + -0.368 value_reward_diff^2 + 0.113 value_reward_diff*reward_diff + 2.114 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 12, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 660/1000 --- L(Train): 0.0202679 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.041 1 + 0.576 value_reward_diff[t] + 2.322 reward_diff + -0.367 value_reward_diff^2 + 0.112 value_reward_diff*reward_diff + 2.115 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 13, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 661/1000 --- L(Train): 0.0202571 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.041 1 + 0.576 value_reward_diff[t] + 2.323 reward_diff + -0.367 value_reward_diff^2 + 0.112 value_reward_diff*reward_diff + 2.116 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 14, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 662/1000 --- L(Train): 0.0202432 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.04 1 + 0.576 value_reward_diff[t] + 2.324 reward_diff + -0.367 value_reward_diff^2 + 0.111 value_reward_diff*reward_diff + 2.116 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 15, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 663/1000 --- L(Train): 0.0202323 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.039 1 + 0.576 value_reward_diff[t] + 2.326 reward_diff + -0.367 value_reward_diff^2 + 0.11 value_reward_diff*reward_diff + 2.117 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 16, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 664/1000 --- L(Train): 0.0202251 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.038 1 + 0.576 value_reward_diff[t] + 2.327 reward_diff + -0.367 value_reward_diff^2 + 0.109 value_reward_diff*reward_diff + 2.117 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 17, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 665/1000 --- L(Train): 0.0202186 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.038 1 + 0.576 value_reward_diff[t] + 2.328 reward_diff + -0.367 value_reward_diff^2 + 0.109 value_reward_diff*reward_diff + 2.118 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 18, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 666/1000 --- L(Train): 0.0202074 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.037 1 + 0.576 value_reward_diff[t] + 2.329 reward_diff + -0.366 value_reward_diff^2 + 0.108 value_reward_diff*reward_diff + 2.119 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 19, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 667/1000 --- L(Train): 0.0201913 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.036 1 + 0.576 value_reward_diff[t] + 2.33 reward_diff + -0.366 value_reward_diff^2 + 0.107 value_reward_diff*reward_diff + 2.119 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 20, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 668/1000 --- L(Train): 0.0201858 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.036 1 + 0.576 value_reward_diff[t] + 2.331 reward_diff + -0.366 value_reward_diff^2 + 0.107 value_reward_diff*reward_diff + 2.12 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 21, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 669/1000 --- L(Train): 0.0201770 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.035 1 + 0.576 value_reward_diff[t] + 2.333 reward_diff + -0.366 value_reward_diff^2 + 0.106 value_reward_diff*reward_diff + 2.12 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 22, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 670/1000 --- L(Train): 0.0201647 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.035 1 + 0.576 value_reward_diff[t] + 2.334 reward_diff + -0.366 value_reward_diff^2 + 0.105 value_reward_diff*reward_diff + 2.121 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 23, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 671/1000 --- L(Train): 0.0201530 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.034 1 + 0.576 value_reward_diff[t] + 2.335 reward_diff + -0.366 value_reward_diff^2 + 0.105 value_reward_diff*reward_diff + 2.121 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 24, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 672/1000 --- L(Train): 0.0201459 --- L(Val, SINDy): 0.0000000 --- Time: 0.11s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.033 1 + 0.576 value_reward_diff[t] + 2.336 reward_diff + -0.365 value_reward_diff^2 + 0.104 value_reward_diff*reward_diff + 2.122 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 25, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 673/1000 --- L(Train): 0.0201362 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.033 1 + 0.576 value_reward_diff[t] + 2.337 reward_diff + -0.365 value_reward_diff^2 + 0.103 value_reward_diff*reward_diff + 2.123 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 26, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 674/1000 --- L(Train): 0.0201258 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.032 1 + 0.576 value_reward_diff[t] + 2.338 reward_diff + -0.365 value_reward_diff^2 + 0.102 value_reward_diff*reward_diff + 2.123 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 27, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 675/1000 --- L(Train): 0.0201152 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.031 1 + 0.576 value_reward_diff[t] + 2.34 reward_diff + -0.365 value_reward_diff^2 + 0.102 value_reward_diff*reward_diff + 2.124 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 28, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 676/1000 --- L(Train): 0.0201058 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.031 1 + 0.576 value_reward_diff[t] + 2.341 reward_diff + -0.365 value_reward_diff^2 + 0.101 value_reward_diff*reward_diff + 2.124 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 29, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 677/1000 --- L(Train): 0.0200939 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.03 1 + 0.576 value_reward_diff[t] + 2.342 reward_diff + -0.365 value_reward_diff^2 + 0.1 value_reward_diff*reward_diff + 2.125 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 30, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 678/1000 --- L(Train): 0.0200847 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.029 1 + 0.576 value_reward_diff[t] + 2.343 reward_diff + -0.365 value_reward_diff^2 + 0.1 value_reward_diff*reward_diff + 2.125 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 31, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 679/1000 --- L(Train): 0.0200746 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.029 1 + 0.576 value_reward_diff[t] + 2.344 reward_diff + -0.364 value_reward_diff^2 + 0.099 value_reward_diff*reward_diff + 2.126 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 32, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 680/1000 --- L(Train): 0.0200666 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.028 1 + 0.576 value_reward_diff[t] + 2.345 reward_diff + -0.364 value_reward_diff^2 + 0.098 value_reward_diff*reward_diff + 2.126 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 33, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 681/1000 --- L(Train): 0.0200555 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.027 1 + 0.576 value_reward_diff[t] + 2.346 reward_diff + -0.364 value_reward_diff^2 + 0.098 value_reward_diff*reward_diff + 2.127 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 34, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 682/1000 --- L(Train): 0.0200444 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.027 1 + 0.576 value_reward_diff[t] + 2.347 reward_diff + -0.364 value_reward_diff^2 + 0.097 value_reward_diff*reward_diff + 2.127 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 35, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 683/1000 --- L(Train): 0.0200376 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.026 1 + 0.576 value_reward_diff[t] + 2.349 reward_diff + -0.364 value_reward_diff^2 + 0.096 value_reward_diff*reward_diff + 2.128 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 36, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 684/1000 --- L(Train): 0.0200289 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.026 1 + 0.576 value_reward_diff[t] + 2.35 reward_diff + -0.364 value_reward_diff^2 + 0.096 value_reward_diff*reward_diff + 2.128 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 37, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 685/1000 --- L(Train): 0.0200161 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.025 1 + 0.576 value_reward_diff[t] + 2.351 reward_diff + -0.363 value_reward_diff^2 + 0.095 value_reward_diff*reward_diff + 2.129 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 38, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 686/1000 --- L(Train): 0.0200069 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.024 1 + 0.576 value_reward_diff[t] + 2.352 reward_diff + -0.363 value_reward_diff^2 + 0.094 value_reward_diff*reward_diff + 2.129 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 39, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 687/1000 --- L(Train): 0.0199962 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.024 1 + 0.576 value_reward_diff[t] + 2.353 reward_diff + -0.363 value_reward_diff^2 + 0.094 value_reward_diff*reward_diff + 2.13 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 40, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 688/1000 --- L(Train): 0.0199900 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.023 1 + 0.577 value_reward_diff[t] + 2.354 reward_diff + -0.363 value_reward_diff^2 + 0.093 value_reward_diff*reward_diff + 2.13 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 41, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\u001b[H\u001b[2J\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 689/1000 --- L(Train): 0.0199854 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.023 1 + 0.577 value_reward_diff[t] + 2.355 reward_diff + -0.363 value_reward_diff^2 + 0.092 value_reward_diff*reward_diff + 2.131 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 42, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 690/1000 --- L(Train): 0.0199768 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.022 1 + 0.577 value_reward_diff[t] + 2.356 reward_diff + -0.363 value_reward_diff^2 + 0.092 value_reward_diff*reward_diff + 2.131 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 43, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 691/1000 --- L(Train): 0.0199609 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.021 1 + 0.577 value_reward_diff[t] + 2.357 reward_diff + -0.362 value_reward_diff^2 + 0.091 value_reward_diff*reward_diff + 2.132 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 44, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 692/1000 --- L(Train): 0.0199506 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.021 1 + 0.577 value_reward_diff[t] + 2.358 reward_diff + -0.362 value_reward_diff^2 + 0.091 value_reward_diff*reward_diff + 2.132 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 45, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 693/1000 --- L(Train): 0.0199434 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.02 1 + 0.577 value_reward_diff[t] + 2.36 reward_diff + -0.362 value_reward_diff^2 + 0.09 value_reward_diff*reward_diff + 2.133 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 46, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 694/1000 --- L(Train): 0.0199373 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.02 1 + 0.577 value_reward_diff[t] + 2.361 reward_diff + -0.362 value_reward_diff^2 + 0.089 value_reward_diff*reward_diff + 2.133 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 47, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 695/1000 --- L(Train): 0.0199300 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.019 1 + 0.577 value_reward_diff[t] + 2.362 reward_diff + -0.362 value_reward_diff^2 + 0.089 value_reward_diff*reward_diff + 2.133 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 48, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 696/1000 --- L(Train): 0.0199225 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.018 1 + 0.577 value_reward_diff[t] + 2.363 reward_diff + -0.362 value_reward_diff^2 + 0.088 value_reward_diff*reward_diff + 2.134 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 49, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 697/1000 --- L(Train): 0.0199114 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.018 1 + 0.577 value_reward_diff[t] + 2.364 reward_diff + -0.362 value_reward_diff^2 + 0.087 value_reward_diff*reward_diff + 2.134 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 50, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 698/1000 --- L(Train): 0.0199039 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.017 1 + 0.577 value_reward_diff[t] + 2.365 reward_diff + -0.361 value_reward_diff^2 + 0.087 value_reward_diff*reward_diff + 2.135 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 51, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 699/1000 --- L(Train): 0.0198967 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.017 1 + 0.577 value_reward_diff[t] + 2.366 reward_diff + -0.361 value_reward_diff^2 + 0.086 value_reward_diff*reward_diff + 2.135 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 52, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 700/1000 --- L(Train): 0.0198886 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.016 1 + 0.577 value_reward_diff[t] + 2.367 reward_diff + -0.361 value_reward_diff^2 + 0.086 value_reward_diff*reward_diff + 2.136 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 53, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 701/1000 --- L(Train): 0.0198783 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.016 1 + 0.577 value_reward_diff[t] + 2.368 reward_diff + -0.361 value_reward_diff^2 + 0.085 value_reward_diff*reward_diff + 2.136 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 54, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 702/1000 --- L(Train): 0.0198672 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.015 1 + 0.577 value_reward_diff[t] + 2.369 reward_diff + -0.361 value_reward_diff^2 + 0.084 value_reward_diff*reward_diff + 2.136 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 55, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 703/1000 --- L(Train): 0.0198594 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.014 1 + 0.577 value_reward_diff[t] + 2.37 reward_diff + -0.361 value_reward_diff^2 + 0.084 value_reward_diff*reward_diff + 2.137 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 56, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 704/1000 --- L(Train): 0.0198501 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.014 1 + 0.577 value_reward_diff[t] + 2.371 reward_diff + -0.36 value_reward_diff^2 + 0.083 value_reward_diff*reward_diff + 2.137 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 57, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 705/1000 --- L(Train): 0.0198388 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.013 1 + 0.577 value_reward_diff[t] + 2.372 reward_diff + -0.36 value_reward_diff^2 + 0.083 value_reward_diff*reward_diff + 2.138 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 58, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 706/1000 --- L(Train): 0.0198305 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.013 1 + 0.577 value_reward_diff[t] + 2.373 reward_diff + -0.36 value_reward_diff^2 + 0.082 value_reward_diff*reward_diff + 2.138 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 59, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 707/1000 --- L(Train): 0.0198217 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.012 1 + 0.577 value_reward_diff[t] + 2.374 reward_diff + -0.36 value_reward_diff^2 + 0.081 value_reward_diff*reward_diff + 2.138 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 60, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 708/1000 --- L(Train): 0.0198159 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.012 1 + 0.577 value_reward_diff[t] + 2.375 reward_diff + -0.36 value_reward_diff^2 + 0.081 value_reward_diff*reward_diff + 2.139 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 61, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 709/1000 --- L(Train): 0.0198068 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.011 1 + 0.577 value_reward_diff[t] + 2.376 reward_diff + -0.36 value_reward_diff^2 + 0.08 value_reward_diff*reward_diff + 2.139 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 62, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 710/1000 --- L(Train): 0.0197993 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.01 1 + 0.577 value_reward_diff[t] + 2.377 reward_diff + -0.36 value_reward_diff^2 + 0.08 value_reward_diff*reward_diff + 2.14 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 63, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 711/1000 --- L(Train): 0.0197912 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.01 1 + 0.577 value_reward_diff[t] + 2.378 reward_diff + -0.359 value_reward_diff^2 + 0.079 value_reward_diff*reward_diff + 2.14 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 64, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 712/1000 --- L(Train): 0.0197820 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.009 1 + 0.577 value_reward_diff[t] + 2.379 reward_diff + -0.359 value_reward_diff^2 + 0.078 value_reward_diff*reward_diff + 2.14 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 65, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 713/1000 --- L(Train): 0.0197863 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.009 1 + 0.577 value_reward_diff[t] + 2.38 reward_diff + -0.359 value_reward_diff^2 + 0.078 value_reward_diff*reward_diff + 2.141 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 66, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 714/1000 --- L(Train): 0.0197736 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.008 1 + 0.577 value_reward_diff[t] + 2.381 reward_diff + -0.359 value_reward_diff^2 + 0.077 value_reward_diff*reward_diff + 2.141 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 67, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 715/1000 --- L(Train): 0.0197616 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.008 1 + 0.577 value_reward_diff[t] + 2.382 reward_diff + -0.359 value_reward_diff^2 + 0.077 value_reward_diff*reward_diff + 2.142 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 68, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 716/1000 --- L(Train): 0.0197564 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.007 1 + 0.577 value_reward_diff[t] + 2.383 reward_diff + -0.359 value_reward_diff^2 + 0.076 value_reward_diff*reward_diff + 2.142 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 69, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 717/1000 --- L(Train): 0.0197486 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.007 1 + 0.577 value_reward_diff[t] + 2.384 reward_diff + -0.359 value_reward_diff^2 + 0.076 value_reward_diff*reward_diff + 2.142 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 70, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 718/1000 --- L(Train): 0.0197367 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.006 1 + 0.577 value_reward_diff[t] + 2.385 reward_diff + -0.358 value_reward_diff^2 + 0.075 value_reward_diff*reward_diff + 2.143 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 71, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 719/1000 --- L(Train): 0.0197271 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.006 1 + 0.577 value_reward_diff[t] + 2.386 reward_diff + -0.358 value_reward_diff^2 + 0.075 value_reward_diff*reward_diff + 2.143 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 72, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 720/1000 --- L(Train): 0.0197237 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.005 1 + 0.577 value_reward_diff[t] + 2.387 reward_diff + -0.358 value_reward_diff^2 + 0.074 value_reward_diff*reward_diff + 2.143 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 73, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 721/1000 --- L(Train): 0.0197155 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.005 1 + 0.577 value_reward_diff[t] + 2.388 reward_diff + -0.358 value_reward_diff^2 + 0.073 value_reward_diff*reward_diff + 2.144 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 74, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 722/1000 --- L(Train): 0.0197030 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.004 1 + 0.577 value_reward_diff[t] + 2.389 reward_diff + -0.358 value_reward_diff^2 + 0.073 value_reward_diff*reward_diff + 2.144 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 75, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 723/1000 --- L(Train): 0.0196937 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.003 1 + 0.577 value_reward_diff[t] + 2.39 reward_diff + -0.358 value_reward_diff^2 + 0.072 value_reward_diff*reward_diff + 2.144 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 76, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 724/1000 --- L(Train): 0.0196904 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.003 1 + 0.577 value_reward_diff[t] + 2.391 reward_diff + -0.358 value_reward_diff^2 + 0.072 value_reward_diff*reward_diff + 2.145 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 77, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 725/1000 --- L(Train): 0.0196849 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.002 1 + 0.577 value_reward_diff[t] + 2.392 reward_diff + -0.357 value_reward_diff^2 + 0.071 value_reward_diff*reward_diff + 2.145 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 78, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 726/1000 --- L(Train): 0.0196776 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.002 1 + 0.577 value_reward_diff[t] + 2.393 reward_diff + -0.357 value_reward_diff^2 + 0.071 value_reward_diff*reward_diff + 2.145 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 79, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 727/1000 --- L(Train): 0.0196635 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.001 1 + 0.577 value_reward_diff[t] + 2.394 reward_diff + -0.357 value_reward_diff^2 + 0.07 value_reward_diff*reward_diff + 2.146 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 80, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 728/1000 --- L(Train): 0.0196576 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.001 1 + 0.577 value_reward_diff[t] + 2.395 reward_diff + -0.357 value_reward_diff^2 + 0.07 value_reward_diff*reward_diff + 2.146 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 81, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 729/1000 --- L(Train): 0.0196523 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.0 1 + 0.577 value_reward_diff[t] + 2.396 reward_diff + -0.357 value_reward_diff^2 + 0.069 value_reward_diff*reward_diff + 2.146 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 82, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 730/1000 --- L(Train): 0.0196437 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.0 1 + 0.577 value_reward_diff[t] + 2.397 reward_diff + -0.357 value_reward_diff^2 + 0.069 value_reward_diff*reward_diff + 2.147 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 83, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 731/1000 --- L(Train): 0.0196344 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.001 1 + 0.577 value_reward_diff[t] + 2.398 reward_diff + -0.357 value_reward_diff^2 + 0.068 value_reward_diff*reward_diff + 2.147 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 84, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 732/1000 --- L(Train): 0.0196244 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.001 1 + 0.577 value_reward_diff[t] + 2.399 reward_diff + -0.356 value_reward_diff^2 + 0.068 value_reward_diff*reward_diff + 2.147 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 85, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 733/1000 --- L(Train): 0.0196151 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.001 1 + 0.577 value_reward_diff[t] + 2.4 reward_diff + -0.356 value_reward_diff^2 + 0.067 value_reward_diff*reward_diff + 2.147 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 86, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 734/1000 --- L(Train): 0.0196103 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.002 1 + 0.577 value_reward_diff[t] + 2.401 reward_diff + -0.356 value_reward_diff^2 + 0.066 value_reward_diff*reward_diff + 2.148 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 87, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 735/1000 --- L(Train): 0.0196020 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.002 1 + 0.577 value_reward_diff[t] + 2.402 reward_diff + -0.356 value_reward_diff^2 + 0.066 value_reward_diff*reward_diff + 2.148 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 88, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 736/1000 --- L(Train): 0.0195939 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.002 1 + 0.577 value_reward_diff[t] + 2.403 reward_diff + -0.356 value_reward_diff^2 + 0.065 value_reward_diff*reward_diff + 2.148 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 89, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 737/1000 --- L(Train): 0.0195847 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.003 1 + 0.577 value_reward_diff[t] + 2.404 reward_diff + -0.356 value_reward_diff^2 + 0.065 value_reward_diff*reward_diff + 2.149 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 90, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 738/1000 --- L(Train): 0.0195765 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.003 1 + 0.577 value_reward_diff[t] + 2.405 reward_diff + -0.356 value_reward_diff^2 + 0.064 value_reward_diff*reward_diff + 2.149 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 91, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 739/1000 --- L(Train): 0.0195758 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.003 1 + 0.576 value_reward_diff[t] + 2.405 reward_diff + -0.356 value_reward_diff^2 + 0.064 value_reward_diff*reward_diff + 2.149 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 92, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 740/1000 --- L(Train): 0.0195638 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.003 1 + 0.576 value_reward_diff[t] + 2.406 reward_diff + -0.355 value_reward_diff^2 + 0.063 value_reward_diff*reward_diff + 2.149 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 93, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 741/1000 --- L(Train): 0.0195551 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.004 1 + 0.576 value_reward_diff[t] + 2.407 reward_diff + -0.355 value_reward_diff^2 + 0.063 value_reward_diff*reward_diff + 2.15 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 94, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 742/1000 --- L(Train): 0.0195470 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.004 1 + 0.576 value_reward_diff[t] + 2.408 reward_diff + -0.355 value_reward_diff^2 + 0.062 value_reward_diff*reward_diff + 2.15 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 95, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 743/1000 --- L(Train): 0.0195401 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.004 1 + 0.576 value_reward_diff[t] + 2.409 reward_diff + -0.355 value_reward_diff^2 + 0.062 value_reward_diff*reward_diff + 2.15 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 96, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 744/1000 --- L(Train): 0.0195344 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.004 1 + 0.576 value_reward_diff[t] + 2.41 reward_diff + -0.355 value_reward_diff^2 + 0.061 value_reward_diff*reward_diff + 2.15 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 97, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 745/1000 --- L(Train): 0.0195283 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.005 1 + 0.576 value_reward_diff[t] + 2.411 reward_diff + -0.355 value_reward_diff^2 + 0.061 value_reward_diff*reward_diff + 2.15 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 98, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 746/1000 --- L(Train): 0.0195204 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = -0.005 1 + 0.576 value_reward_diff[t] + 2.412 reward_diff + -0.355 value_reward_diff^2 + 0.06 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: 99, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 747/1000 --- L(Train): 0.0195085 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.413 reward_diff + -0.355 value_reward_diff^2 + 0.06 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 748/1000 --- L(Train): 0.0195037 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.413 reward_diff + -0.355 value_reward_diff^2 + 0.059 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 749/1000 --- L(Train): 0.0194998 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.414 reward_diff + -0.355 value_reward_diff^2 + 0.059 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 750/1000 --- L(Train): 0.0194914 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.415 reward_diff + -0.355 value_reward_diff^2 + 0.058 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 751/1000 --- L(Train): 0.0194778 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.416 reward_diff + -0.355 value_reward_diff^2 + 0.058 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 752/1000 --- L(Train): 0.0194746 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.417 reward_diff + -0.355 value_reward_diff^2 + 0.057 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 753/1000 --- L(Train): 0.0194705 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.417 reward_diff + -0.355 value_reward_diff^2 + 0.056 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 754/1000 --- L(Train): 0.0194599 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.418 reward_diff + -0.355 value_reward_diff^2 + 0.056 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 755/1000 --- L(Train): 0.0194520 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.419 reward_diff + -0.355 value_reward_diff^2 + 0.055 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 756/1000 --- L(Train): 0.0194450 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.42 reward_diff + -0.355 value_reward_diff^2 + 0.055 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 757/1000 --- L(Train): 0.0194354 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.42 reward_diff + -0.355 value_reward_diff^2 + 0.054 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 758/1000 --- L(Train): 0.0194286 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.421 reward_diff + -0.355 value_reward_diff^2 + 0.054 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 759/1000 --- L(Train): 0.0194218 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.422 reward_diff + -0.355 value_reward_diff^2 + 0.053 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 760/1000 --- L(Train): 0.0194175 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.422 reward_diff + -0.355 value_reward_diff^2 + 0.053 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 761/1000 --- L(Train): 0.0194085 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.423 reward_diff + -0.355 value_reward_diff^2 + 0.053 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 762/1000 --- L(Train): 0.0194031 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.424 reward_diff + -0.355 value_reward_diff^2 + 0.052 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 763/1000 --- L(Train): 0.0193947 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.424 reward_diff + -0.355 value_reward_diff^2 + 0.052 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 764/1000 --- L(Train): 0.0193892 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.425 reward_diff + -0.355 value_reward_diff^2 + 0.052 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 765/1000 --- L(Train): 0.0193807 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.426 reward_diff + -0.355 value_reward_diff^2 + 0.052 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 766/1000 --- L(Train): 0.0193752 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.426 reward_diff + -0.355 value_reward_diff^2 + 0.051 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 767/1000 --- L(Train): 0.0193714 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.427 reward_diff + -0.355 value_reward_diff^2 + 0.051 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 768/1000 --- L(Train): 0.0193625 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.428 reward_diff + -0.355 value_reward_diff^2 + 0.051 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 769/1000 --- L(Train): 0.0193530 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.428 reward_diff + -0.355 value_reward_diff^2 + 0.051 value_reward_diff*reward_diff + 2.152 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 770/1000 --- L(Train): 0.0193458 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.429 reward_diff + -0.355 value_reward_diff^2 + 0.051 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 771/1000 --- L(Train): 0.0193368 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.429 reward_diff + -0.355 value_reward_diff^2 + 0.051 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 772/1000 --- L(Train): 0.0193297 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.43 reward_diff + -0.355 value_reward_diff^2 + 0.05 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 773/1000 --- L(Train): 0.0193244 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.431 reward_diff + -0.355 value_reward_diff^2 + 0.05 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 774/1000 --- L(Train): 0.0193168 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.431 reward_diff + -0.355 value_reward_diff^2 + 0.05 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 775/1000 --- L(Train): 0.0193060 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.432 reward_diff + -0.355 value_reward_diff^2 + 0.05 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 776/1000 --- L(Train): 0.0193009 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.432 reward_diff + -0.355 value_reward_diff^2 + 0.05 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 777/1000 --- L(Train): 0.0192952 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.433 reward_diff + -0.355 value_reward_diff^2 + 0.05 value_reward_diff*reward_diff + 2.151 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 0, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 778/1000 --- L(Train): 0.0192907 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.433 reward_diff + -0.355 value_reward_diff^2 + 0.05 value_reward_diff*reward_diff + 2.15 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 1, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 779/1000 --- L(Train): 0.0192852 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.434 reward_diff + -0.355 value_reward_diff^2 + 0.05 value_reward_diff*reward_diff + 2.15 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 2, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 780/1000 --- L(Train): 0.0192809 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.434 reward_diff + -0.355 value_reward_diff^2 + 0.05 value_reward_diff*reward_diff + 2.15 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 3, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 781/1000 --- L(Train): 0.0192713 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.435 reward_diff + -0.355 value_reward_diff^2 + 0.05 value_reward_diff*reward_diff + 2.15 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 4, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 782/1000 --- L(Train): 0.0192626 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.435 reward_diff + -0.355 value_reward_diff^2 + 0.05 value_reward_diff*reward_diff + 2.15 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 5, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 783/1000 --- L(Train): 0.0192570 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.436 reward_diff + -0.355 value_reward_diff^2 + 0.049 value_reward_diff*reward_diff + 2.149 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 6, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 784/1000 --- L(Train): 0.0192546 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.436 reward_diff + -0.355 value_reward_diff^2 + 0.049 value_reward_diff*reward_diff + 2.149 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 7, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 785/1000 --- L(Train): 0.0192466 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.437 reward_diff + -0.355 value_reward_diff^2 + 0.049 value_reward_diff*reward_diff + 2.149 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 8, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 786/1000 --- L(Train): 0.0192411 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.437 reward_diff + -0.355 value_reward_diff^2 + 0.049 value_reward_diff*reward_diff + 2.149 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 9, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 787/1000 --- L(Train): 0.0192339 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.438 reward_diff + -0.355 value_reward_diff^2 + 0.049 value_reward_diff*reward_diff + 2.149 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 10, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 788/1000 --- L(Train): 0.0192289 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.438 reward_diff + -0.355 value_reward_diff^2 + 0.049 value_reward_diff*reward_diff + 2.148 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 11, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 789/1000 --- L(Train): 0.0192200 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.439 reward_diff + -0.355 value_reward_diff^2 + 0.049 value_reward_diff*reward_diff + 2.148 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 12, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 790/1000 --- L(Train): 0.0192095 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.439 reward_diff + -0.355 value_reward_diff^2 + 0.049 value_reward_diff*reward_diff + 2.148 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 13, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 791/1000 --- L(Train): 0.0192030 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.44 reward_diff + -0.355 value_reward_diff^2 + 0.048 value_reward_diff*reward_diff + 2.148 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 14, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 792/1000 --- L(Train): 0.0191980 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.44 reward_diff + -0.355 value_reward_diff^2 + 0.048 value_reward_diff*reward_diff + 2.147 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 15, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 793/1000 --- L(Train): 0.0191900 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.441 reward_diff + -0.355 value_reward_diff^2 + 0.048 value_reward_diff*reward_diff + 2.147 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 16, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 794/1000 --- L(Train): 0.0191835 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.441 reward_diff + -0.355 value_reward_diff^2 + 0.048 value_reward_diff*reward_diff + 2.147 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 17, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 795/1000 --- L(Train): 0.0191780 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.441 reward_diff + -0.355 value_reward_diff^2 + 0.048 value_reward_diff*reward_diff + 2.147 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 18, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 796/1000 --- L(Train): 0.0191722 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.442 reward_diff + -0.355 value_reward_diff^2 + 0.048 value_reward_diff*reward_diff + 2.146 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 19, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 797/1000 --- L(Train): 0.0191737 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.442 reward_diff + -0.355 value_reward_diff^2 + 0.048 value_reward_diff*reward_diff + 2.146 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 20, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 798/1000 --- L(Train): 0.0191628 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.443 reward_diff + -0.355 value_reward_diff^2 + 0.048 value_reward_diff*reward_diff + 2.146 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 21, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 799/1000 --- L(Train): 0.0191588 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.443 reward_diff + -0.355 value_reward_diff^2 + 0.048 value_reward_diff*reward_diff + 2.145 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 22, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 800/1000 --- L(Train): 0.0191556 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.444 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.145 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 23, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 801/1000 --- L(Train): 0.0191480 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.444 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.145 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 24, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 802/1000 --- L(Train): 0.0191387 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.444 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.144 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 25, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 803/1000 --- L(Train): 0.0191313 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.445 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.144 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 26, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 804/1000 --- L(Train): 0.0191275 --- L(Val, SINDy): 0.0000000 --- Time: 0.10s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.445 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.144 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 27, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 805/1000 --- L(Train): 0.0191210 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.446 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.144 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 28, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 806/1000 --- L(Train): 0.0191155 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.446 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.143 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 29, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 807/1000 --- L(Train): 0.0191054 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.446 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.143 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 30, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 808/1000 --- L(Train): 0.0191017 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.447 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.143 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 31, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 809/1000 --- L(Train): 0.0191000 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.447 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.142 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 32, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 810/1000 --- L(Train): 0.0191034 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.448 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.142 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 33, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 811/1000 --- L(Train): 0.0190849 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.448 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.142 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 34, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 812/1000 --- L(Train): 0.0190777 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.448 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.141 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 35, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 813/1000 --- L(Train): 0.0190760 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.449 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.141 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 36, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 814/1000 --- L(Train): 0.0190696 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.449 reward_diff + -0.355 value_reward_diff^2 + 0.047 value_reward_diff*reward_diff + 2.14 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 37, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 815/1000 --- L(Train): 0.0190604 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.449 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.14 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 38, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 816/1000 --- L(Train): 0.0190557 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.45 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.14 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 39, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 817/1000 --- L(Train): 0.0190502 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.45 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.139 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 40, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 818/1000 --- L(Train): 0.0190441 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.451 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.139 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 41, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 819/1000 --- L(Train): 0.0190350 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.451 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.139 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 42, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 820/1000 --- L(Train): 0.0190304 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.451 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.138 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 43, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 821/1000 --- L(Train): 0.0190239 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.452 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.138 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 44, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 822/1000 --- L(Train): 0.0190185 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.452 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.138 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 45, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 823/1000 --- L(Train): 0.0190228 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.452 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.137 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 46, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 824/1000 --- L(Train): 0.0190097 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.453 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.137 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 47, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 825/1000 --- L(Train): 0.0190047 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.453 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.136 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 48, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 826/1000 --- L(Train): 0.0190003 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.453 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.136 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 49, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 827/1000 --- L(Train): 0.0189899 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.454 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.136 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 50, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 828/1000 --- L(Train): 0.0189820 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.454 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.135 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 51, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 829/1000 --- L(Train): 0.0189767 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.454 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.135 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 52, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 830/1000 --- L(Train): 0.0189689 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.455 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.134 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 53, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 831/1000 --- L(Train): 0.0189588 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.455 reward_diff + -0.355 value_reward_diff^2 + 0.046 value_reward_diff*reward_diff + 2.134 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 54, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 832/1000 --- L(Train): 0.0189687 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.455 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.134 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 55, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 833/1000 --- L(Train): 0.0189559 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.456 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.133 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 56, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 834/1000 --- L(Train): 0.0189520 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.574 value_reward_diff[t] + 2.456 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.133 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 57, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 835/1000 --- L(Train): 0.0189451 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.456 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.132 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 58, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 836/1000 --- L(Train): 0.0189524 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.457 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.132 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 59, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 837/1000 --- L(Train): 0.0189390 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.457 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.132 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 60, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 838/1000 --- L(Train): 0.0189326 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.457 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.131 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 61, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 839/1000 --- L(Train): 0.0189327 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.458 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.131 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 62, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 840/1000 --- L(Train): 0.0189261 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.458 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.13 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 63, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 841/1000 --- L(Train): 0.0189134 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.458 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.13 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 64, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 842/1000 --- L(Train): 0.0189052 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.459 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.129 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 65, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 843/1000 --- L(Train): 0.0189254 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.459 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.129 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 66, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 844/1000 --- L(Train): 0.0189088 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.459 reward_diff + -0.355 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.129 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 67, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 845/1000 --- L(Train): 0.0188984 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.46 reward_diff + -0.356 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.128 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 68, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 846/1000 --- L(Train): 0.0189264 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.46 reward_diff + -0.356 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.128 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 69, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 847/1000 --- L(Train): 0.0189059 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.46 reward_diff + -0.356 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.127 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 70, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 848/1000 --- L(Train): 0.0189602 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.461 reward_diff + -0.356 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.127 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 71, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 849/1000 --- L(Train): 0.0189425 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.461 reward_diff + -0.356 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.127 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 72, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 850/1000 --- L(Train): 0.0189336 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.461 reward_diff + -0.356 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.126 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 73, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 851/1000 --- L(Train): 0.0189239 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.462 reward_diff + -0.356 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.126 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 74, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 852/1000 --- L(Train): 0.0189058 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.462 reward_diff + -0.356 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.125 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 75, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 853/1000 --- L(Train): 0.0188855 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.462 reward_diff + -0.356 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.125 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 76, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 854/1000 --- L(Train): 0.0188760 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.463 reward_diff + -0.356 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.124 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 77, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 855/1000 --- L(Train): 0.0188737 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.463 reward_diff + -0.356 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.124 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 78, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 856/1000 --- L(Train): 0.0188705 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.463 reward_diff + -0.356 value_reward_diff^2 + 0.045 value_reward_diff*reward_diff + 2.123 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 79, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 857/1000 --- L(Train): 0.0188614 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.464 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.123 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 80, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 858/1000 --- L(Train): 0.0188449 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.464 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.123 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 81, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 859/1000 --- L(Train): 0.0188284 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.464 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.122 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 82, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 860/1000 --- L(Train): 0.0188217 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.465 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.122 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 83, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 861/1000 --- L(Train): 0.0188145 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.465 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.121 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 84, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 862/1000 --- L(Train): 0.0188032 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.465 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.121 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 85, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 863/1000 --- L(Train): 0.0188257 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.466 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.12 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 86, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 864/1000 --- L(Train): 0.0188153 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.466 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.12 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 87, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 865/1000 --- L(Train): 0.0188015 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.466 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.119 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 88, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 866/1000 --- L(Train): 0.0187895 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.466 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.119 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 89, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 867/1000 --- L(Train): 0.0187743 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.467 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.119 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 90, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 868/1000 --- L(Train): 0.0187629 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.467 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.118 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 91, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 869/1000 --- L(Train): 0.0187568 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.575 value_reward_diff[t] + 2.467 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.118 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 92, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 870/1000 --- L(Train): 0.0187554 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.468 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.117 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 93, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 871/1000 --- L(Train): 0.0187497 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.468 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.117 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 94, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 872/1000 --- L(Train): 0.0187440 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.468 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.116 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 95, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 873/1000 --- L(Train): 0.0187366 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.469 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.116 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 96, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 874/1000 --- L(Train): 0.0187293 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.469 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.115 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 97, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 875/1000 --- L(Train): 0.0187222 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.469 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.115 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 98, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 876/1000 --- L(Train): 0.0187151 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.47 reward_diff + -0.356 value_reward_diff^2 + 0.044 value_reward_diff*reward_diff + 2.114 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, 99, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 877/1000 --- L(Train): 0.0187052 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.47 reward_diff + -0.356 value_reward_diff^2 + 2.114 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 878/1000 --- L(Train): 0.0187036 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.47 reward_diff + -0.356 value_reward_diff^2 + 2.114 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 879/1000 --- L(Train): 0.0186994 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.471 reward_diff + -0.356 value_reward_diff^2 + 2.113 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 880/1000 --- L(Train): 0.0186921 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.471 reward_diff + -0.356 value_reward_diff^2 + 2.113 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 881/1000 --- L(Train): 0.0186862 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.576 value_reward_diff[t] + 2.471 reward_diff + -0.355 value_reward_diff^2 + 2.112 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 882/1000 --- L(Train): 0.0186782 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.577 value_reward_diff[t] + 2.472 reward_diff + -0.355 value_reward_diff^2 + 2.112 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 883/1000 --- L(Train): 0.0186711 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.577 value_reward_diff[t] + 2.472 reward_diff + -0.355 value_reward_diff^2 + 2.112 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 884/1000 --- L(Train): 0.0186653 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.577 value_reward_diff[t] + 2.472 reward_diff + -0.355 value_reward_diff^2 + 2.111 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 885/1000 --- L(Train): 0.0186623 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.578 value_reward_diff[t] + 2.473 reward_diff + -0.355 value_reward_diff^2 + 2.111 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 886/1000 --- L(Train): 0.0186559 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.578 value_reward_diff[t] + 2.473 reward_diff + -0.354 value_reward_diff^2 + 2.11 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 887/1000 --- L(Train): 0.0186457 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.578 value_reward_diff[t] + 2.474 reward_diff + -0.354 value_reward_diff^2 + 2.11 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 888/1000 --- L(Train): 0.0186435 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.578 value_reward_diff[t] + 2.474 reward_diff + -0.354 value_reward_diff^2 + 2.11 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 889/1000 --- L(Train): 0.0186382 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.474 reward_diff + -0.354 value_reward_diff^2 + 2.109 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 890/1000 --- L(Train): 0.0186409 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.475 reward_diff + -0.354 value_reward_diff^2 + 2.109 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 891/1000 --- L(Train): 0.0186265 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.475 reward_diff + -0.354 value_reward_diff^2 + 2.109 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 892/1000 --- L(Train): 0.0186270 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.476 reward_diff + -0.354 value_reward_diff^2 + 2.108 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 893/1000 --- L(Train): 0.0186255 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.476 reward_diff + -0.354 value_reward_diff^2 + 2.108 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 894/1000 --- L(Train): 0.0186169 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.476 reward_diff + -0.354 value_reward_diff^2 + 2.107 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 895/1000 --- L(Train): 0.0186057 --- L(Val, SINDy): 0.0000000 --- Time: 0.05s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.477 reward_diff + -0.354 value_reward_diff^2 + 2.107 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 896/1000 --- L(Train): 0.0186017 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.477 reward_diff + -0.354 value_reward_diff^2 + 2.107 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 897/1000 --- L(Train): 0.0186117 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.478 reward_diff + -0.354 value_reward_diff^2 + 2.106 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 898/1000 --- L(Train): 0.0186180 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.478 reward_diff + -0.354 value_reward_diff^2 + 2.106 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 899/1000 --- L(Train): 0.0186162 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.478 reward_diff + -0.354 value_reward_diff^2 + 2.105 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 900/1000 --- L(Train): 0.0186088 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.479 reward_diff + -0.354 value_reward_diff^2 + 2.105 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 901/1000 --- L(Train): 0.0186005 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.479 reward_diff + -0.354 value_reward_diff^2 + 2.105 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 902/1000 --- L(Train): 0.0185890 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.48 reward_diff + -0.355 value_reward_diff^2 + 2.104 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 903/1000 --- L(Train): 0.0185799 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.48 reward_diff + -0.355 value_reward_diff^2 + 2.104 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 904/1000 --- L(Train): 0.0185788 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.48 reward_diff + -0.355 value_reward_diff^2 + 2.103 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 905/1000 --- L(Train): 0.0185741 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.481 reward_diff + -0.355 value_reward_diff^2 + 2.103 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 906/1000 --- L(Train): 0.0185669 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.481 reward_diff + -0.355 value_reward_diff^2 + 2.103 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 907/1000 --- L(Train): 0.0185560 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.481 reward_diff + -0.355 value_reward_diff^2 + 2.102 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 908/1000 --- L(Train): 0.0185504 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.482 reward_diff + -0.355 value_reward_diff^2 + 2.102 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 909/1000 --- L(Train): 0.0185433 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.482 reward_diff + -0.355 value_reward_diff^2 + 2.101 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 910/1000 --- L(Train): 0.0185355 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.483 reward_diff + -0.355 value_reward_diff^2 + 2.101 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 911/1000 --- L(Train): 0.0185292 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.483 reward_diff + -0.356 value_reward_diff^2 + 2.101 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 912/1000 --- L(Train): 0.0185208 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.483 reward_diff + -0.356 value_reward_diff^2 + 2.1 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 913/1000 --- L(Train): 0.0185163 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.484 reward_diff + -0.356 value_reward_diff^2 + 2.1 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 914/1000 --- L(Train): 0.0185247 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.579 value_reward_diff[t] + 2.484 reward_diff + -0.356 value_reward_diff^2 + 2.099 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 915/1000 --- L(Train): 0.0185103 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.58 value_reward_diff[t] + 2.485 reward_diff + -0.356 value_reward_diff^2 + 2.099 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 916/1000 --- L(Train): 0.0185056 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.58 value_reward_diff[t] + 2.485 reward_diff + -0.356 value_reward_diff^2 + 2.098 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 917/1000 --- L(Train): 0.0185015 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.58 value_reward_diff[t] + 2.485 reward_diff + -0.356 value_reward_diff^2 + 2.098 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.862 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 918/1000 --- L(Train): 0.0184933 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.58 value_reward_diff[t] + 2.486 reward_diff + -0.356 value_reward_diff^2 + 2.098 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 919/1000 --- L(Train): 0.0184825 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.58 value_reward_diff[t] + 2.486 reward_diff + -0.356 value_reward_diff^2 + 2.097 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 920/1000 --- L(Train): 0.0184800 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.58 value_reward_diff[t] + 2.486 reward_diff + -0.356 value_reward_diff^2 + 2.097 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 921/1000 --- L(Train): 0.0184748 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.58 value_reward_diff[t] + 2.487 reward_diff + -0.356 value_reward_diff^2 + 2.096 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 922/1000 --- L(Train): 0.0184672 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.58 value_reward_diff[t] + 2.487 reward_diff + -0.356 value_reward_diff^2 + 2.096 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 923/1000 --- L(Train): 0.0184603 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.58 value_reward_diff[t] + 2.488 reward_diff + -0.356 value_reward_diff^2 + 2.095 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 924/1000 --- L(Train): 0.0184541 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.488 reward_diff + -0.356 value_reward_diff^2 + 2.095 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 925/1000 --- L(Train): 0.0184467 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.488 reward_diff + -0.356 value_reward_diff^2 + 2.095 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 926/1000 --- L(Train): 0.0184412 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.489 reward_diff + -0.356 value_reward_diff^2 + 2.094 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 927/1000 --- L(Train): 0.0184348 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.489 reward_diff + -0.356 value_reward_diff^2 + 2.094 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 928/1000 --- L(Train): 0.0184300 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.489 reward_diff + -0.356 value_reward_diff^2 + 2.093 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 929/1000 --- L(Train): 0.0184264 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.49 reward_diff + -0.356 value_reward_diff^2 + 2.093 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 930/1000 --- L(Train): 0.0184204 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.49 reward_diff + -0.356 value_reward_diff^2 + 2.092 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 931/1000 --- L(Train): 0.0184104 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.491 reward_diff + -0.356 value_reward_diff^2 + 2.092 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 932/1000 --- L(Train): 0.0184068 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.491 reward_diff + -0.356 value_reward_diff^2 + 2.092 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 933/1000 --- L(Train): 0.0184028 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.491 reward_diff + -0.356 value_reward_diff^2 + 2.091 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 934/1000 --- L(Train): 0.0183962 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.492 reward_diff + -0.356 value_reward_diff^2 + 2.091 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 935/1000 --- L(Train): 0.0183916 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.492 reward_diff + -0.356 value_reward_diff^2 + 2.09 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 936/1000 --- L(Train): 0.0183872 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.581 value_reward_diff[t] + 2.492 reward_diff + -0.356 value_reward_diff^2 + 2.09 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 937/1000 --- L(Train): 0.0183808 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.493 reward_diff + -0.356 value_reward_diff^2 + 2.089 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 938/1000 --- L(Train): 0.0183768 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.493 reward_diff + -0.356 value_reward_diff^2 + 2.089 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 939/1000 --- L(Train): 0.0183701 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.493 reward_diff + -0.356 value_reward_diff^2 + 2.088 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 940/1000 --- L(Train): 0.0183669 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.494 reward_diff + -0.357 value_reward_diff^2 + 2.088 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 941/1000 --- L(Train): 0.0183632 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.494 reward_diff + -0.357 value_reward_diff^2 + 2.087 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 942/1000 --- L(Train): 0.0183583 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.494 reward_diff + -0.357 value_reward_diff^2 + 2.087 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 943/1000 --- L(Train): 0.0183496 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.495 reward_diff + -0.357 value_reward_diff^2 + 2.086 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 944/1000 --- L(Train): 0.0183462 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.495 reward_diff + -0.357 value_reward_diff^2 + 2.086 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 945/1000 --- L(Train): 0.0183422 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.495 reward_diff + -0.357 value_reward_diff^2 + 2.085 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 946/1000 --- L(Train): 0.0183370 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.496 reward_diff + -0.357 value_reward_diff^2 + 2.085 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 947/1000 --- L(Train): 0.0183295 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.496 reward_diff + -0.357 value_reward_diff^2 + 2.085 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 948/1000 --- L(Train): 0.0183261 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.496 reward_diff + -0.357 value_reward_diff^2 + 2.084 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 949/1000 --- L(Train): 0.0183224 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.582 value_reward_diff[t] + 2.497 reward_diff + -0.357 value_reward_diff^2 + 2.084 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 950/1000 --- L(Train): 0.0183158 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.583 value_reward_diff[t] + 2.497 reward_diff + -0.357 value_reward_diff^2 + 2.083 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 951/1000 --- L(Train): 0.0183077 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.583 value_reward_diff[t] + 2.498 reward_diff + -0.357 value_reward_diff^2 + 2.083 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 952/1000 --- L(Train): 0.0183023 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.583 value_reward_diff[t] + 2.498 reward_diff + -0.357 value_reward_diff^2 + 2.082 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 953/1000 --- L(Train): 0.0182996 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.583 value_reward_diff[t] + 2.498 reward_diff + -0.357 value_reward_diff^2 + 2.082 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 954/1000 --- L(Train): 0.0183093 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.583 value_reward_diff[t] + 2.499 reward_diff + -0.357 value_reward_diff^2 + 2.081 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 955/1000 --- L(Train): 0.0182961 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.583 value_reward_diff[t] + 2.499 reward_diff + -0.357 value_reward_diff^2 + 2.081 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 956/1000 --- L(Train): 0.0182907 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.583 value_reward_diff[t] + 2.499 reward_diff + -0.357 value_reward_diff^2 + 2.08 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 957/1000 --- L(Train): 0.0182954 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.583 value_reward_diff[t] + 2.5 reward_diff + -0.357 value_reward_diff^2 + 2.08 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 958/1000 --- L(Train): 0.0182845 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.583 value_reward_diff[t] + 2.5 reward_diff + -0.357 value_reward_diff^2 + 2.079 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 959/1000 --- L(Train): 0.0182816 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.583 value_reward_diff[t] + 2.5 reward_diff + -0.357 value_reward_diff^2 + 2.079 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 960/1000 --- L(Train): 0.0182831 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.583 value_reward_diff[t] + 2.501 reward_diff + -0.357 value_reward_diff^2 + 2.078 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 961/1000 --- L(Train): 0.0182722 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.583 value_reward_diff[t] + 2.501 reward_diff + -0.357 value_reward_diff^2 + 2.078 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 962/1000 --- L(Train): 0.0182638 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.584 value_reward_diff[t] + 2.501 reward_diff + -0.357 value_reward_diff^2 + 2.077 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 963/1000 --- L(Train): 0.0182619 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.584 value_reward_diff[t] + 2.502 reward_diff + -0.358 value_reward_diff^2 + 2.077 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 964/1000 --- L(Train): 0.0182571 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.584 value_reward_diff[t] + 2.502 reward_diff + -0.358 value_reward_diff^2 + 2.076 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 965/1000 --- L(Train): 0.0182478 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.584 value_reward_diff[t] + 2.502 reward_diff + -0.358 value_reward_diff^2 + 2.076 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 966/1000 --- L(Train): 0.0182414 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.584 value_reward_diff[t] + 2.503 reward_diff + -0.358 value_reward_diff^2 + 2.075 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 967/1000 --- L(Train): 0.0182352 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.584 value_reward_diff[t] + 2.503 reward_diff + -0.358 value_reward_diff^2 + 2.075 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 968/1000 --- L(Train): 0.0182312 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.584 value_reward_diff[t] + 2.503 reward_diff + -0.358 value_reward_diff^2 + 2.074 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "================================================================================\n",
      "Epoch 969/1000 --- L(Train): 0.0182679 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.584 value_reward_diff[t] + 2.504 reward_diff + -0.358 value_reward_diff^2 + 2.074 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J\u001b[H\u001b[2J================================================================================\n",
      "Epoch 970/1000 --- L(Train): 0.0182483 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.584 value_reward_diff[t] + 2.504 reward_diff + -0.358 value_reward_diff^2 + 2.073 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 971/1000 --- L(Train): 0.0182332 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.584 value_reward_diff[t] + 2.504 reward_diff + -0.358 value_reward_diff^2 + 2.073 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 972/1000 --- L(Train): 0.0182325 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.584 value_reward_diff[t] + 2.505 reward_diff + -0.358 value_reward_diff^2 + 2.072 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 973/1000 --- L(Train): 0.0182309 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.584 value_reward_diff[t] + 2.505 reward_diff + -0.358 value_reward_diff^2 + 2.072 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 974/1000 --- L(Train): 0.0182218 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.585 value_reward_diff[t] + 2.505 reward_diff + -0.358 value_reward_diff^2 + 2.071 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 975/1000 --- L(Train): 0.0182084 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.585 value_reward_diff[t] + 2.506 reward_diff + -0.358 value_reward_diff^2 + 2.071 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 976/1000 --- L(Train): 0.0182056 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.585 value_reward_diff[t] + 2.506 reward_diff + -0.358 value_reward_diff^2 + 2.07 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 977/1000 --- L(Train): 0.0181997 --- L(Val, SINDy): 0.0000000 --- Time: 0.08s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.585 value_reward_diff[t] + 2.506 reward_diff + -0.358 value_reward_diff^2 + 2.07 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 978/1000 --- L(Train): 0.0181934 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.585 value_reward_diff[t] + 2.507 reward_diff + -0.358 value_reward_diff^2 + 2.069 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 979/1000 --- L(Train): 0.0181848 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.585 value_reward_diff[t] + 2.507 reward_diff + -0.358 value_reward_diff^2 + 2.069 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 980/1000 --- L(Train): 0.0181799 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.585 value_reward_diff[t] + 2.507 reward_diff + -0.358 value_reward_diff^2 + 2.068 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 981/1000 --- L(Train): 0.0181707 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.585 value_reward_diff[t] + 2.508 reward_diff + -0.358 value_reward_diff^2 + 2.068 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 982/1000 --- L(Train): 0.0181624 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.585 value_reward_diff[t] + 2.508 reward_diff + -0.358 value_reward_diff^2 + 2.067 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 983/1000 --- L(Train): 0.0181602 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.585 value_reward_diff[t] + 2.508 reward_diff + -0.358 value_reward_diff^2 + 2.067 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 984/1000 --- L(Train): 0.0181548 --- L(Val, SINDy): 0.0000000 --- Time: 0.09s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.585 value_reward_diff[t] + 2.509 reward_diff + -0.359 value_reward_diff^2 + 2.066 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 985/1000 --- L(Train): 0.0181490 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.585 value_reward_diff[t] + 2.509 reward_diff + -0.359 value_reward_diff^2 + 2.066 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 986/1000 --- L(Train): 0.0181455 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.586 value_reward_diff[t] + 2.509 reward_diff + -0.359 value_reward_diff^2 + 2.065 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 987/1000 --- L(Train): 0.0181524 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.586 value_reward_diff[t] + 2.51 reward_diff + -0.359 value_reward_diff^2 + 2.065 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 988/1000 --- L(Train): 0.0181385 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.586 value_reward_diff[t] + 2.51 reward_diff + -0.359 value_reward_diff^2 + 2.064 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 989/1000 --- L(Train): 0.0181355 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.586 value_reward_diff[t] + 2.51 reward_diff + -0.359 value_reward_diff^2 + 2.064 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 990/1000 --- L(Train): 0.0181522 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.586 value_reward_diff[t] + 2.511 reward_diff + -0.359 value_reward_diff^2 + 2.063 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 991/1000 --- L(Train): 0.0181347 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.586 value_reward_diff[t] + 2.511 reward_diff + -0.359 value_reward_diff^2 + 2.063 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 992/1000 --- L(Train): 0.0181267 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.586 value_reward_diff[t] + 2.511 reward_diff + -0.359 value_reward_diff^2 + 2.062 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 993/1000 --- L(Train): 0.0181240 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.586 value_reward_diff[t] + 2.512 reward_diff + -0.359 value_reward_diff^2 + 2.062 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 994/1000 --- L(Train): 0.0181179 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.586 value_reward_diff[t] + 2.512 reward_diff + -0.359 value_reward_diff^2 + 2.061 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 995/1000 --- L(Train): 0.0181103 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.586 value_reward_diff[t] + 2.512 reward_diff + -0.359 value_reward_diff^2 + 2.061 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 996/1000 --- L(Train): 0.0181055 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.586 value_reward_diff[t] + 2.513 reward_diff + -0.359 value_reward_diff^2 + 2.06 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 997/1000 --- L(Train): 0.0181000 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.586 value_reward_diff[t] + 2.513 reward_diff + -0.359 value_reward_diff^2 + 2.06 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 998/1000 --- L(Train): 0.0180945 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.587 value_reward_diff[t] + 2.513 reward_diff + -0.359 value_reward_diff^2 + 2.059 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 999/1000 --- L(Train): 0.0180855 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.587 value_reward_diff[t] + 2.514 reward_diff + -0.359 value_reward_diff^2 + 2.059 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 1000/1000 --- L(Train): 0.0180802 --- L(Val, SINDy): 0.0000000 --- Time: 0.06s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.587 value_reward_diff[t] + 2.514 reward_diff + -0.359 value_reward_diff^2 + 2.058 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\u001b[H\u001b[2J================================================================================\n",
      "Epoch 1001/1000 --- L(Train): 0.0180753 --- L(Val, SINDy): 0.0000000 --- Time: 0.07s;\n",
      "--------------------------------------------------------------------------------\n",
      "SPICE Model (Coefficients: 7):\n",
      "value_reward_diff[t+1] = 0.587 value_reward_diff[t] + 2.514 reward_diff + -0.359 value_reward_diff^2 + 2.058 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n",
      "Cutoff patience:\n",
      "value_reward_diff: -, 0, 0, 0, -, 0\n",
      "value_persistance: 0, 0, -, -, -, -\n",
      "wm_rt: -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -\n",
      "================================================================================\n",
      "\n",
      "Training result:\n",
      "L(Train): 0.3282743 --- L(Val, RNN): 0.2947629 --- L(Val, SINDy): 0.3757429 --- LR: 1.0000000e-02\n",
      "\n",
      "RNN training finished.\n",
      "Training took 1624.09 seconds.\n",
      "Saving SPICE model to ../params/braun2018/spice_braun2018.pkl...\n",
      "================================================================================\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Example SPICE model (participant 0):\n",
      "--------------------------------------------------------------------------------\n",
      "value_reward_diff[t+1] = 0.587 value_reward_diff[t] + 2.514 reward_diff + -0.359 value_reward_diff^2 + 2.058 reward_diff^2 \n",
      "value_persistance[t+1] = -0.1 1 + 0.861 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nStarting training on {estimator.device}...\")\n",
    "print(\"=\" * 80)\n",
    "estimator.fit(*dataset_tuple)\n",
    "# estimator.load_spice(args.model)\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "# Print example SPICE model for first participant\n",
    "print(\"\\nExample SPICE model (participant 0):\")\n",
    "print(\"-\" * 80)\n",
    "estimator.print_spice_model(participant_id=0)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.load_spice(path_spice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Model\n",
    "\n",
    "(add here EVC model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "from weinhardt2025.benchmarking.benchmarking_gru import GRU, training, setup_agent_gru\n",
    "\n",
    "gru = GRU(n_actions, additional_inputs=n_additional_inputs).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "epochs = 1000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=0.01)\n",
    "\n",
    "path_gru = '../params/braun2018/gru_braun2018.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [210540] at index 0 does not match the shape of the indexed tensor [183570, 2] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m gru = \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgru\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m torch.save(gru.state_dict(), path_gru)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/SPICE/weinhardt2025/aux/../../weinhardt2025/benchmarking/benchmarking_gru.py:92\u001b[39m, in \u001b[36mtraining\u001b[39m\u001b[34m(gru, optimizer, dataset_train, dataset_test, epochs, batch_size, criterion, device)\u001b[39m\n\u001b[32m     90\u001b[39m nan_mask = ~xs[..., :n_actions].sum(dim=-\u001b[32m1\u001b[39m).reshape(-\u001b[32m1\u001b[39m).isnan().to(device)\n\u001b[32m     91\u001b[39m logits_flat = logits.reshape(-\u001b[32m1\u001b[39m, n_actions)[nan_mask]\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m labels_flat = torch.argmax(\u001b[43mys\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnan_mask\u001b[49m\u001b[43m]\u001b[49m, dim=-\u001b[32m1\u001b[39m).reshape(-\u001b[32m1\u001b[39m).long()\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m     95\u001b[39m loss = criterion(logits_flat, labels_flat)\n",
      "\u001b[31mIndexError\u001b[39m: The shape of the mask [210540] at index 0 does not match the shape of the indexed tensor [183570, 2] at index 0"
     ]
    }
   ],
   "source": [
    "gru = training(\n",
    "    gru=gru,\n",
    "    optimizer=optimizer,\n",
    "    dataset_train=dataset_train,\n",
    "    dataset_test=dataset_test,\n",
    "    epochs=epochs,\n",
    "    )\n",
    "\n",
    "torch.save(gru.state_dict(), path_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_agent = setup_agent_gru(path_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot SPICE against benchmark models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_reward_diff[t+1] = -0.426 1 + 0.126 value_reward_diff[t] + 2.445 reward_diff + -0.094 value_reward_diff^2 + 1.768 reward_diff^2 \n",
      "value_persistance[t+1] = 1.0 value_persistance[t] \n",
      "wm_rt[t+1] = 1.0 wm_rt[t] + -0.57 wm_rt[t-1] + -0.144 wm_rt[t-2] \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting\n",
    "participant_id = 2\n",
    "\n",
    "estimator.print_spice_model(participant_id)\n",
    "\n",
    "agents = {\n",
    "    'rnn': estimator.rnn_agent,\n",
    "    'spice': estimator.spice_agent,\n",
    "    # 'evc': evc_agent,\n",
    "    'gru': gru_agent,\n",
    "}\n",
    "\n",
    "mask_participant_id = dataset.xs[:, 0, -1] == participant_id\n",
    "\n",
    "fig, axs = plot_session(\n",
    "    agents, \n",
    "    dataset.xs[mask_participant_id][5], \n",
    "    signals_to_plot=['value_reward_diff', 'value_persistance', 'value_wm_rt'], \n",
    "    display_choice=1,  # display_choice=1 because switch is action index 1 (see convert_dataset -> ValueWarning in output)\n",
    "    )  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.xs[:, 0, -1]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
